[{"blogurl": "http://allendowney.blogspot.com\n", "blogroll": [], "title": "Probably Overthinking It"}, {"content": ["Abstract \n \n Based on 2000-2010 data from the General Social Survey (GSS), I present results of a logistic regression that measures the relationship between Internet use and religious affiliation, controlling for religious upbringing, income and socioeconomic index, year born (age), and education. \n \n \n \n I find that moderate Internet use reduces the chance of religious affiliation by 2 percentage points (odds ratio 0.8); heavier Internet use reduces affiliation by an additional 5 percentage points (odds ratio 0.7).  Four years of college reduces affiliation by an additional 2 percentage points (odds ratio 0.8). \n \n A ll reported effects are statistically significant with N=8960 respondents. \n \n \n \n Results of logistic regression can be difficult to interpret; it might help to imagine the following progression: \n \n \n Start with a hypothetical baseline person raised in any religion, with moderate or high household income ($25,000 per year or more), born in 1960, with high school education but no college, and low Internet use (less than 2 hours per week): in the GSS survey, 91% of people in this category have a religious affiliation. Now we change one variable at a time. \n If this person were born 10 years later (in 1970) the fraction would drop to 89%. \n If this person went to college, the fraction would drop to 87% \n If this person used the Internet 2 or more hours per week, the fraction would drop to 85%. \n If this person used the Internet 8 or more hours per week, the fraction would drop to 80%. \n \n \nTaken together, college education and Internet use are associated with a decrease in religious affiliation of 9 percentage points. \n \n Introduction \n From 1990 to 2010 the fraction of Protestants in the U.S. population dropped from 62% to 51%; at the same time the fraction of people with no religious preference increased from 8% to 18%. The following graph shows these trends: \n \n \n \n In a previous article I presented evidence that something happened in the 1990s, continuing through the 2000s, that is causing disaffiliation from religion across all generations, with the largest effect on the youngest generations in the survey, people born in the 1960s and 1970s. \n \n \n \n There are many possible explanations, but for me, the Internet pops to the top of this list. First, the timing is at least approximately right. Here is data from the World Bank , showing number of Internet users per hundred people in the U.S. \n \n \n \n \n \nInternet use increased rapidly from 1995 to 2010, which is the interval of steepest change in religious affiliation. \n \nRegressions \n \nTo identify factors that contribute to disaffiliation, I ran logistic regressions with the following dependent variable: \n \n \n \n has_relig : 1 if the respondent reported any religious affiliation when interviewed as an adult, or 0 if the respondent reported \"None\" (based on the GSS variable RELIG) \n \n \n \nAnd these explanatory variables: \n \n \n \n had_relig : 1 if the respondent reported being raised in a religion, 0 otherwise (based on RELIG16) \n \n \n \n born_from_1960 : year the respondent was born minus 1960 (based on AGE and survey year). Subtracting 1960 makes it easier to interpret the results of the regression. \n \n \n \n educ_from_12 : number of years of school completed, minus 12 (based on EDUC). \n \n \n \n somewww : 1 if the respondent reported using the Internet 2 of more hours per week, 0 otherwise (based on WWWHR, with the threshold chosen near the median) \n \n \n \n heavywww : 1 if the respondent uses the Internet more than 8 hours per week, 0 otherwise (threshold chosen near the 75th percentile) \n \n \n \n SEI : Socioeconomic index (a measure of occupational prestige developed by the GSS). \n \n \n \n high_income : 1 if the respondent reports annual household income of $25,000 or more, which includes 62% of respondents who answered the question. \n \n \n \nI used data from GSS survey years 2000, 2002, 2004, 2006, and 2010 (the relevant questions were not asked in 2008). I excluded respondents who were not asked or did not answer one or more of the questions I used in my analysis. \n \n \n \nIt turns out that SEI does not make a contribution that is either statistically or practically significant, so I omit it from the model. \n \n \n \nHere are the results of the model as reported by R: \n \n \n \n \n Coefficients: \n \n     Estimate Std. Error z value Pr(>|z|)  \n \n (Intercept) -0.164434 0.094978 -1.731 0.0834 . \n \n had_relig  2.318141 0.087372 26.532 < 2e-16 *** \n \n high_income  0.166673 0.072345 2.304 0.0212 * \n \n born_from_1960 -0.020161 0.002128 -9.474 < 2e-16 *** \n \n educ_from_12 -0.051850 0.012228 -4.240 2.23e-05 *** \n \n somewww  -0.178409 0.078490 -2.273 0.0230 * \n \n heavywww  -0.336658 0.080546 -4.180 2.92e-05 *** \n \n --- \n \n \n \n (Dispersion parameter for binomial family taken to be 1) \n \n \n \n  Null deviance: 7860.3 on 8959 degrees of freedom \n \n Residual deviance: 6872.5 on 8953 degrees of freedom \n \n AIC: 6886.5 \n \n \n \n Number of Fisher Scoring iterations: 5 \n \n \n \n \nAll explanatory variables are statistically significant: high_income and somewww are borderline, both at p=0.02. \n \n \n \nThe odds ratios and cumulative probabilities are: \n \n \n \n \n     odds cumulative \n \n     ratio probability \n \n (Intercept) 0.85 46 \n \n had_relig 10.16 90 \n \n high_income 1.18 91 \n \n born_from_1960 0.82 89 \n \n educ_from_12 0.81 87 \n \n  somewww 0.84 85 \n \n heavywww 0.71 80 \n \n \n \n \nThese results are summarized and interpreted in the Abstract, above. \n \n \n \nDiscussion \n \nAs always, statistical association does not prove causation, but in this case I think there are reasons to believe that Internet use causes disaffiliation from religion: \n \n \n It is easy to imagine how Internet use could allow a person in a homogeneous community to find information about people of other religions (and none), and to interact with them personally. And there is anecdotal evidence that those interactions contribute to religious disaffiliation (for example, numerous personal reports on reddit.com/r/atheism ). \n Conversely it is harder to imagine plausible reasons why disaffiliation might cause increased Internet use (except possibly on Sunday mornings). \n Although it is possible that a third factor causes both disaffiliation and Internet use, that factor would also have to be new, coincidentally rising in prevalence, like the Internet, during the 1990s and 2000s. \n Whatever causes disaffiliation has the strongest effect on the youngest generations, which is consistent with the hypothesis that Internet use during adolescence and young adulthood has the strongest effect on religious affiliation. \n \n \n \n So with appropriate caution, I think there is a strong case here for causation, and not just statistical association. \n \n Furthermore, the magnitude of the effect is large enough to explain a substantial part of the observed changes in religious affiliation. In my next article I will incorporate this regression model into the generational model I presented in Part Six , in order to estimate the effect of Internet use on these trends. \n \n Summary of previous reports \n In  Part One  I described trends in market share of major religions in the U.S.: since 1988, the fraction of Protestants dropped from 60% to 51%, and the fraction of people with no religious affiliation increased from 8% to 18%. \n \nIn Part Two I used data from the 1988 General Social Survey (GSS) to model transmission of religion from parent to child, and found that the model failed to predict the decrease in Protestants and the increase in Nones that occurred between 1988 and 2010. \n \nIn Part Three I looked at changes, between 1988 and 2008, in the spouse tables (which describe the tendencies of people to marry within their religions), the environment table (which describes parents' decisions about their children's religious upbringing), and the transmission table (which describes the likely outcomes for children raised within each religion). I found that the transmission table has changed substantially since 1988, and accounts for a large part of the observed increase in Nones, but not the decrease in Protestants. \n \nIn Part Four I looked at changes in religiosity over the lifetime of respondents. I tentatively concluded that the differences between generations were larger than changes in affiliation, within generations, over time. \n \nBut in Part Five I looked more closely and saw that all generations were becoming more religious, or staying the same, prior to 1990, and that all generations began to disaffiliate during the 1990s, continuing into the 2000s. \n \n \n \nIn Part Six I presented a generational model that retroactively \"predicts\" the changes we have seen since 1988, and used it to predict how those changes are likely to continue in the next 30 years. I expect the fraction of Protestants to continue to decrease, and the fraction of Nones to increase and overtake Catholic as the second-largest affiliation by 2030."], "link": "http://allendowney.blogspot.com/feeds/6028977819806744241/comments/default", "bloglinks": {}, "links": {"http://data.worldbank.org/": 1, "http://feedads.doubleclick.net/": 2, "http://www3.norc.org/": 1, "http://4.blogspot.com/": 1, "http://allendowney.blogspot.com/": 8, "http://2.blogspot.com/": 1, "http://reddit.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["Summary so far \n In  Part One  I described trends in market share of major religions in the U.S.: since 1988, the fraction of Protestants dropped from 60% to 51%, and the fraction of people with no religious affiliation increased from 8% to 18%. \n \nIn Part Two I used data from the 1988 General Social Survey (GSS) to model transmission of religion from parent to child, and found that the model failed to predict the decrease in Protestants and the increase in Nones that occurred between 1988 and 2010. \n \nIn Part Three I looked at changes, between 1988 and 2008, in the spouse tables (which describe the tendencies of people to marry within their religions), the environment table (which describes parents' decisions about their children's religious upbringing), and the transmission table (which describes the likely outcomes for children raised within each religion). I found that the transmission table has changed substantially since 1988, and accounts for a large part of the observed increase in Nones, but not the decrease in Protestants. \n \nIn Part Four I looked at changes in religiosity over the lifetime of respondents. I tentatively concluded that the differences between generations were larger than changes in affiliation, within generations, over time. \n \nBut in Part Five I looked more closely and saw that all generations were becoming more religious, or staying the same, prior to 1990, and that all generations began to disaffiliate during the 1990s, continuing into the 2000s. \n \n \nGenerational Model \nNow I am ready to get back to the generational model I have been working up to. The goal of the generational model is to separate these three effects: \n \n \n Changes in religious preference from one generation to the next. \n Changes in religious affiliation over the lifetime of respondents. \n Changes in the composition of the GSS cohort over time. \n \n \nThe model works by simulation. Assuming that we are starting in 1988, here are the steps: \n \n \n Read the survey data from 1988 and resample it. Compute and store the distribution of ages. \n For each respondent, generate a hypothetical child. Use the BirthModel to determine year of birth, the UpbringingModel to determine what religion the child is raised in, and the TransmissionModel to determine what affiliation the child will have as an adult. Details of these models follow. \n Form a combined cohort of parents and simulated children. Since the cohort of parents is a representative sample of the US population, the cohort of simulated children is a representative sample of the population one generation later (based, for now, on the simplifying assumptions that all groups have the same number of children on average, and there is no immigration). \n In order to generate a cohort from a future survey year, draw a sample from the combined cohort, weighted so that the distribution of ages in the future year is the same as the original distribution of ages in 1988. As the simulation goes forward in time, this generated cohort contains fewer of the parents and more of the simulated children. After 20 years, about 25% of the \"real\" respondents have been replaced with \"fake\" respondents. \n \n \nNow, where do all these auxiliary models come from? \n \n \n \n \n BirthModel : This is just the distribution of parent's age when each child is born. It is based on data from the 1994 GSS, which includes questions about children. I had to do some work to correct for an obvious bias due to the ages of the respondents; I will skip the details here. \n \n \n \n UpbringingModel : This is a combination of the SpouseTable and the EnvironmentTable, described in Part Three . It is a map from the parent's religion to the distribution of possible religions the child might be raised in. \n \n \n \n TransmissionModel : This is the TransmissionTable described in Part Three. It is a map from the religious environment of the child to the distribution of religious affiliation reported by the child as an adult. \n \n \n \nThe Upbringing and Transmission models come in two flavors: \n \n \n \n Time invariant : We use all respondents to estimate the parameters of the model, and apply the same model to generate all simulated children. \n \n \n \n Time variant : We estimate different parameters for each generation (partitioned by decade born) and use different models to generate simulated children, depending on what year they are born. \n \n \n \nFor the time variant model, we have to extrapolate from observed data into the future. To keep this simple we copy the latest reliable data (based on sample size) and apply it to people born in later decades. \n \n \n \nOk, that's enough methodology for now. Let's take a look at some... \n \n \n \nResults \n \nThe first step is to validate the model by showing that it can predict the observed changes using past data. Here I mean \"predict\" in a peculiar sense, which is that I will use the entire dataset (including data after 1988) to build the auxiliary models, then use the simulator to generate trends from 1988 to 2010. \n \n \n \nHere is what the results look like : \n \n \n \n \n \n \n \n \n \nThe thick lines are the observed data; the thin lines are simulations. Here are my observations: \n \n \n For Jews and Catholics, the observed data falls within the bounds of the simulations, so the model validates. \n For Other, the observed data sometimes exceeds the bounds of the simulations, which may be due to immigration (not included in this model). \n For None, the observed data is at the high end of the range, and for Prot it is at the low end of the range. This is most likely due to the disaffiliation we saw in Part Five , which is only partly captured in this model. \n \n \n \nI conclude that the model is capturing a large part of the observed changes since 1988, but of course I am cheating by using data from after 1988. So these results validate my modeling decisions (what to include and what to leave out) but they don't test the predictive power of the model. \n \n \n \nPredictive power \n \nTo make an honest test, we have to restrict ourselves to data from before 1988. That way we can tell what part of the observed changes would have been predictable in 1988. \n \n \n \nHere's what the result looks like: \n \n \n \n \n \nSo if we had used this model in 1988, we would have predicted a small decrease in the fraction of Protestants and a small increase in None, but we would have underestimated both trends. \n \n \n \nThis supports my conclusion in Part Five that something happened in the 1990s that changed trends in religious affiliation, and suggests that these changes were unpredictable based on data observable before 1988. \n \n \n \nPredictions \n \nFinally, we can use all data to build the models, use 2010 as the starting place for the simulations, and make some predictions for the next 30 years: \n \n \n \n \n \n \n \nSo what should we expect? \n \n \n The decline in fraction of Protestants will continue. The fraction of Catholics will also decrease, but more slowly. \n The fraction of Nones will increase, overtaking Catholics as the second-largest religious affiliation around 2030. \n The fraction of Others will increase slowly, about 1 percentage point in 30 years. If immigration from Asia continues at current rates, that would add another percentage point, bringing the total to 6%. \n The fraction of Jews will decrease, possibly by half by 2040. \n \n \nThese predictions are likely to be conservative; that is, the rate of secularization will almost certainly be faster. Why? \n \n \n \n Over the last several generations, the UpbringingModel and the TransmissionModel have changed substantially. Parents are less likely to raise their children with religion, and those children are less likely to adopt the religion they are raised with. The model captures these trends, but assumes that they will level off in 2010. It would probably be more accurate to assume that they will continue. \n Rates of disaffiliation among adults are also increasing. Again, the model includes trends that have already occurred, but it assumes that they will level off rather than continue. \n \n \nSo there are reasons to expect the fraction of Nones to accelerate. \n \n \n \nConversely, it is hard to imagine that the trends will be any slower than these predictions. To a large extent, these results are not predictions about things that will happen in the future; rather, they are the future consequences of things that have already happened. For example, in 2020, the GSS survey will include a cohort of people in their 40s. What will they be like? They will be a lot like the people in the 2010 survey who are in their 30s. But they will be older. Changes in the general population are slow because is takes a long time to replace each generation with the next; but as a result, they are also predictable. \n \n \n \n \nNext time: Was Rick Santorum right?  Is college the #1 enemy of religious belief? (Hint: no.) I will look more closely at the TransmissionModel to see what factors make vertical transmission of religion more (or less) likely."], "link": "http://allendowney.blogspot.com/feeds/4919761051919806222/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 2, "http://4.blogspot.com/": 2, "http://www.youtube.com/": 1, "http://allendowney.blogspot.com/": 8}, "blogtitle": "Probably Overthinking It"}, {"content": ["Summary so far \n In  Part One  I described trends in market share of major religions in the U.S.: since 1988, the fraction of Protestants dropped from 60% to 51%, and the fraction of people with no religious affiliation increased from 8% to 18%. \n \nIn Part Two I used data from the 1988 General Social Survey (GSS) to model transmission of religion from parent to child, and found that the model failed to predict the decrease in Protestants and the increase in Nones that occurred between 1988 and 2010. \n \nIn Part Three I looked at changes, between 1988 and 2008, in the spouse tables (which describe the tendencies of people to marry within their religions), the environment table (which describes parents' decisions about their children's religious upbringing), and the transmission table (which describes the likely outcomes for children raised within each religion). I found that the transmission table has changed substantially since 1988, and accounts for a large part of the observed increase in Nones, but not the decrease in Protestants. \n \n \nPart Four revisited \nIn Part Four I looked at changes in religiosity over the lifetime of respondents. The GSS is not a longitudinal survey, so we can't follow individuals, but we can follow generations (which I partition by decade of birth) over time. \n \nLast time I presented this figure, which shows religiosity (the fraction of respondents with any religious preference) as a function of respondent's age, partitioned by decade of birth, for people who were raised Protestant: \n \n \n \n \nEach line represents a different generation. For example, the red line shows that people born in the 1920s were about 96% likely to report a religious preference when they were interviewed in their 40s, 50s, and 60s, and possibly less likely to be religious when they were in their 80s. \n \nThe conclusion I drew from this figure is that the differences between generations are larger than the changes, over time, within each generation. For purposes of modeling I concluded that religious disaffiliation accounts for only a small part of the observed changes in religious identity. \n \nBut I was bothered by one feature of these curves: many of them are concave down, and the maximum point in the curves is apparently shifting toward younger ages. I came to suspect that this picture of the data is \"out of focus\". \n \nWe can refocus the image by plotting the date of the survey (rather than the respondent's age) on the x-axis. Here's what that looks like: \n \n \n \n \nIn this figure, two trends are more apparent: before 1990, most generations were becoming more religious; after 1990, they all became less religious. So it seems clear that the explanation is something that affected all generations at a particular interval in time, not something that affects all people as they age. \n \nWe can see these changes more clearly by normalizing each curve with its 1990 value: \n \n \n \nAgain, most generation were becoming more religious before 1990; after 1990, all of them became less religious. Among people born in the 1960s, more than 10% lost their religion between 1990 and 2010 (when they were in their 30s and 40s). \n \nHere's the same graph for people raised Catholic: \n \n \n \n \nThe general shape is the same: religious affiliation was flat or increasing prior to 1990, and decreasing for almost all generations after 1990. \n \nSince the trends are similar for Catholics and Protestants, we can get a less noisy picture by combining them. Here is the same graph for respondents raised with any religion. \n \n \n \n \nThis figures makes it easier to compare across generations. It appears that more recent generations (born in the 1960s and 1970s) are disaffiliating at higher rates than earlier generations. \n [As an aside, this result contradicts one of the primary (and widely-reported) claims of this article: Schwadel, Period and Cohort Effects on Religious Nonaffiliation and Religious Disaffiliation . Schwadel reports that people born in the 1960s and 1970s were disaffiliating at a slower rate than the previous generations. Some reasons my results might be different: Schwadel only had GSS data up to 2006, and he discards people under 30 years of age. So very little data about the youngest generations is included. Also, his results are based on statistical models that (if I understand correctly) don't include time as an explanatory variable, so they cannot account for an event that affects all generations during a particular interval. ] \n \nAll right, it's audience participation time. What happened in the 1990s that caused widespread religious disaffiliation? Remember, idle speculations only. No evidence, please!"], "link": "http://allendowney.blogspot.com/feeds/2358524410634109286/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://onlinelibrary.wiley.com/": 1, "http://4.blogspot.com/": 2, "http://allendowney.blogspot.com/": 4, "http://2.blogspot.com/": 2}, "blogtitle": "Probably Overthinking It"}, {"content": ["Summary so far \n In  Part One  I described trends in market share of major religions in the U.S.: since 1988, the fraction of Protestants dropped from 60% to 51%, and the fraction of people with no religious affiliation increased from 8% to 18%. \n \nIn Part Two I used data from the 1988 General Social Survey (GSS) to model transmission of religion from parent to child, and found that the model failed to predict the decrease in Protestants and the increase in Nones that occurred between 1988 and 2010. \n \nIn Part Three I looked at changes, between 1988 and 2008, in the spouse tables (which describe the tendencies of people to marry within their religions), the environment table (which describes parents' decisions about their children's religious upbringing), and the transmission table (which describes the likely outcomes for children raised within each religion). I found that the transmission table has changed substantially since 1988, and accounts for a large part of the observed increase in Nones, but not the decrease in Protestants. \n \n \nReligiosity curves \nRespondents in the GSS are surveyed at different ages, so we can get a sense of when people lose their religion (or acquire one). I collected all GSS respondents and partitioned them by the religion they were raised in and the decade they were born. For each of these subgroups, I plotted religiosity (the fraction with some religious preference) as a function of age when surveyed. \n \nHere are the curves for people raised Protestant: \n \n \n \n \nIn the top right, we see that people born between 1900 and 1910 and raised Protestant were likely to be religious when they were interviewed in their 70s and 80s. In the lower left, we see that people born in the 1980s were less likely to be religious when they were interviewed in their 20s. \n \nFor the middle generations, we have a better sense of changes in religiosity over a respondent's lifetime. Several of the curves have an apparent peak in middle age; if this apparent effect is real, the location of the peak may be shifting left. \n \nOverall, these curves are relatively flat, which suggests that respondents are not changing substantially after adulthood (everyone in the GSS is 18 or older). \n \nThe curves for Catholics are similar: \n \n \n \n \nAgain, there is a substantial differences between generations, but within each generation, little change over the respondents' lifetimes. People born in the 50s, 60s and 70s might be leaving the church as they age, but it is hard to tell in this plot whether these trends are statistically significant. \n \nFinally, here are the curves for people raised with no religion: \n \n \n \n \nThere are only enough respondents in this category to plot curves for a few generations, and even then, the curves are noisy. Not surprisingly, people raised without religion are less likely to be religious, and recent generation are less religious than their elders. Again, the curves are generally flat, suggesting that people generally do not change religious affiliation as adults. \n \nA possible exception is that people born in the 1970s and raised without religion might be finding religion in their 30s. But this data point is based on a small number of respondents, so it is probably too early to tell. \n \n \nWhy people switch \nIn 1988 the GSS asked respondents questions about changes in religious affiliation and the reasons for the change. Unfortunately, it looks like this data won't do me much good, because: \n \n \n In many cases where a respondent switched from a religious preference to None, they were not asked why. \n There are so many inconsistencies in the data, I wonder if it might have been mangled. \n Because these questions were only asked once, we can't track trends. \n \n \nSo that's disappointing. \n \n \n \nModeling a mixed-age cohort \nOne of the challenges of working with GSS data is that the respondents each year are a mixture of people of all ages. From year to year, the oldest generation drops out of the cohort and the youngest generation joins the mix. \n \nSo when there is a trend from each generation to the next, as with religious behavior, there is a lag before the trend appears in a GSS time series, and the slope of the trend is much slower. \n \nHowever, for purposes of prediction, this lag is actually useful. For example, 18 years after a baby boom, there is likely to be a spike in college enrollment; that's not really a prediction about the future; it's just a consequence of something that has already happened. \n \n Similarly, we already know what most of the GSS cohort will look like next year. It will look like the cohort this year, one year older. The difference is that a few of the oldest respondents are replaced by the next group of 18 year olds. \n \nIn the 2010 cohort, the age range is roughly 20-80. To predict the 2020 cohort, we can: \n \n \n Remove respondents older than 80. \n Age the rest of the respondents by 10 years. \n Add a new batch of respondents in their 20s. \n \n \nStep 2 might be hard if people were changing religious affiliation as they age, but as we saw above, they generally do not. Step 3 is harder, but there are two reasonable options: \n \n \n Conservatively, we can assume that the next generation will be like their immediate predecessors. \n Alternatively, we can extrapolate from current trends. This option is probably better for prediction, but in some ways unsatisfying because it does not explain the cause of the trends, or why we should expect them to continue. \n \n \nIf we use this method to predict 20 years into the future, we replace about 25% of the cohort with simulated respondents. But since 75% of the prediction is based on simple population aging, it is likely to reliable."], "link": "http://allendowney.blogspot.com/feeds/1087905700450516133/comments/default", "bloglinks": {}, "links": {"http://allendowney.blogspot.com/": 3, "http://4.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["Let's take a break from statistics and do some physics! \n \n \n \nMy friend Ted Bunn recently wrote about the falling slinky problem in his blog . He points to this video , which shows a falling slinky in slow motion. After the top of the slinky is released, the bottom seems to hover until the top reaches it. The effect is particularly strange because if you look carefully, the top of the slinky does not accelerate as we expect for an object in free fall. Rather, it falls at a constant rate. \n \n \n \nTed explains: \n \n ...the information that the top end has been dropped can\u2019t propagate down the slinky any faster than the speed of sound in the slinky (i.e., the speed at which waves propagate down it), so there\u2019s a delay before the bottom end \u201cknows\u201d it\u2019s been dropped. But it\u2019s surprising (at least to me) to see how long the delay is. \n \n This explains why there is a delay, but to me it doesn't explain why the delay is the same as the time it takes for the top of the slinky to reach the bottom. There are lots of models out there that explain parts of this behavior, but the ones I found are either complicated or wrong . \n \n  \n \n Here's my take on it. First, let's assume that what we see in the video is correct: the slinky collapses from top to bottom, so that each coil doesn't move until the one above it comes down and (nearly) hits it. \n \n \n \n Let's call the initial length L and the mass m. After some time, a fraction of the slinky, x, has collapsed. At that point, the collapsed part of the slinky has mass xm at height (1-x)L. The rest of the slinky is spread uniformly [EDIT: this assumption is not right...see Ted's comment below] between height 0 and (1-x)L. So the center of mass is \n \n \n \n x(1-x)L + (1-x)(1-x)L/2 \n \n \n \n Since the slinky is in free fall, we know the center of mass as a function of time: \n \n \n \n L/2 - g/2 t^2 \n \n \n \n If we set those equal and type them into WolframAlpha , we get \n \n \n \n x = sqrt(g/L) t \n \n \n \n Which means that the top of the slinky is moving at constant speed. Remember that x is the fraction of the slinky that collapsed; to get the distance traveled, we multiply by L: \n \n \n \n d = xL = sqrt(gL) t \n \n \n \n So the speed of the top of the slinky is sqrt(gL). \n \n \n \n We can get to the same result a different way by using the formula for wave speed in a vibrating string : sqrt(T/ \u03bc ), where T is tension and \u03bc is mass per linear measure. In this case T=mg and \u03bc =m/L. Plug that in and get wave speed sqrt(gL). \n \n \n \n I think this analysis is useful, but to be rigorous, I haven't really explained why the slinky behaves the way it does. I have only shown that if the slinky collapses from top to bottom (as it appears to), then the top moves at a constant speed (as it appears to). \n \n \n [UPDATE: Provoked by my amateurish attempts at Physics, Ted Bunn wrote up a version of this model that deals correctly with the change in the density of the spring from top to bottom. The result is that the speed of the top of the slinky is almost constant -- it slows down a bit at the end. ]"], "link": "http://allendowney.blogspot.com/feeds/8052625774468037067/comments/default", "bloglinks": {}, "links": {"http://tpt.aapt.org/": 1, "http://blog.richmond.edu/": 2, "http://physics.umd.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://www.wolframalpha.com/": 1, "http://www.youtube.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["In  Part One  I described trends in market share of major religions in the U.S.: since 1988, the fraction of Protestants dropped from 60% to 51%, and the fraction of people with no religious affiliation increased from 8% to 18%. \n \nIn Part Two I used data from the 1988 General Social Survey (GSS) to model transmission of religion from parent to child, and found that the model failed to predict the decrease in Protestants and the increase in Nones that occurred between 1988 and 2010. \n \nI proposed several reasons the model might have failed: \n \n \n The spouse tables are based on the parents of 1988 respondents. People from later generations might be increasingly likely to marry outside their religion. \n The environment table is also based on the previous generation; again, later parents might be making different decisions about the religious environment of their children. \n The transmission table is based on 1988 respondents; it's possible that after 1988, children were less likely to adopt the religion they were raised in. Anecdotally, the culprits most often blamed for this effect are college and the Internet. \n Finally, I have not considered adult conversions from one religious identity to another. The GSS has data on these switches, so I could add them to the model. \n \n \nI will investigate each possibility in turn, starting with the prevalence of mixed-religion marriages. In Secularization , Steve Bruce presents results from a study of intermarriage in the UK that found that the rate of vertical transmission: \n \n \"is halved if the parents are of different faiths (even when the differences are just Methodist-Anglican). Even if the parents agree on which faith they wish to pass on, the fact of disagreement makes the child aware that there are good people in other churches and introduces the relativism that weakens conviction. [page 71]\" \n \nSo if the rate of mixed marriages is increasing, that could contribute to the increasing number of Nones. \n \nTo measure this effect, I used these GSS variables: \n \n \n RELIG: What is your religous preference? \n SPREL: What is your husband's/wife's religious preference? \n \n \n In cases where one partner converts to the other's religion before marriage, that would count (for this model) as a same-religion marriage, since we are interested in the decision the couple makes about the religious environment they raise children in. \n \n \n \n The Spouse Tables \n \n \n \nThe following graph shows the fraction of same-religion marriages over the history of the survey (data for SPREL were not collected every year): \n \n \n \n \n \n \n \nBefore 1988, the fraction of same-religion marriages was around 84%; after 1988 it fell to 78%. The abruptness of the change makes me worry that it may be an artifact; for example, a chance in the wording of the question. Also, t hese results only include respondents who are married, so they are biased toward older people and socio-economic groups that are more likely to be married. \n \n \n \nBut as it turns out, even if we take the data at face value, it has a small effect on the model's predictions. \n \nI used the respondents from 2004-2010 to build spouse tables for men and women (see Part Two ), then ran the 1988 model again with the anachronistic data. The results are almost identical to what we saw last time: \n \n \n \n \n \nThe only noticeable effect is that the prediction for Other got worse. I conclude: \n \n \n It's possible that people are more likely now to marry outside their religion than in 1988, but the difference is small, and \n Even if we cheat by using the 2004-2010 data in 1988, this change does not explain the subsequent changes in the fractions of Protestants and Nones. \n \n \n\nThe Environment Table \n \n \nIt seems unlikely that parents now are making different decisions about what religious environment to raise their children in, but just to rule it out, I compared the environment tables for 1988 and 2008. \n \n \n \n \n  prot  cath  jew  other  none change N  excess \n \n prot-prot 97 1 0 0 2 +1 634 8.7 \n prot-cath 43 46 0 1 10 +4 49 1.9 \n prot- jew 0 0 0 0 0 +0 1 0.0 \n prot-othe 44 21 0 36 0 +0 5 0.0 \n prot-none 85 4 0 0 11 +5 90 4.2 \n cath-prot 30 56 0 0 14 +14 37 5.1 \n cath-cath 1 98 0 0 1 +0 294 0.8 \n cath- jew 0 0 0 0 0 +0 0 0.0 \n cath-othe 0 0 0 0 0 +0 2 0.0 \n cath-none 2 82 0 0 16 +7 20 1.5 \n jew-prot 0 0 0 0 0 +0 1 0.0 \n jew-cath 0 100 0 0 0 +0 0 0.0 \n jew- jew 0 0 96 0 4 +0 27 0.0 \n jew-othe 0 0 0 0 0 +0 0 0.0 \n jew-none 0 0 67 0 33 +33 0 0.0 \n othe-prot 27 0 0 54 18 +18 4 0.7 \n othe-cath 0 43 0 57 0 +0 3 0.0 \n othe- jew 0 0 0 0 0 +0 0 0.0 \n othe-othe 7 0 0 84 9 +3 28 0.7 \n othe-none 25 25 0 25 25 +25 4 1.0 \n none-prot 83 0 0 0 17 +17 6 1.0 \n none-cath 14 68 0 0 18 -82 2 -1.6 \n none- jew 0 0 100 0 0 +0 0 0.0 \n none-othe 0 0 0 0 0 -100 1 -1.0 \n none-none 23 4 0 0 74 +18 34 6.1 \n \n \n \n \nThe left column is the mother's-father's religion. The next five columns show the religious environments those parents chose, as reported by their children in 2008. For example, the second row shows that if the mother is Protestant and the father Catholic, 43% of the children were raised Protestant, 46% Catholic, and 10% None. \n \n \n \nThe next column shows the change in the None column, in percentage points, since the 1988 survey. N is the number of families in 1988 that fell into each category. Finally, Nones is the product of change and N, an estimate of the number of additional Nones in the 1988 survey that could be explained by changes in the environment table. The total of this column is 29, which is not nearly enough to explain the actual excess of 177. \n \n \n \nOf course, most of the numbers in the change column are based on small samples, so we should not take them too seriously. By running simulations with resampled survey data, we can take account of these sample sizes. \n \n \n \nUsing the tables from 1988 to predict the fractions of Nones in 2008, we expect only 8.0% (compared to the actual 16.8%). If we used the environment table from 2008, the prediction goes to 8.5%. If we also use the spouse table, it goes up to 8.7%. So clearly the changes in these tables were not enough to explain the observed changes. \n \n \n \nThe transmission table \nThe transmission table is a cross-tabulation of the religion the respondent was brought up in and the religion reported when surveyed. It shows the outcome, after some years, of parents' decisions about their children's religious upbringing and the effect of the environment. \n \nThe following is the transmission table for 2008, with changes since 1988: \n \n \n prot cath jew other none change N  excess \n prot 82 3 1 2 13 +7 951 67.7 \n cath 17 70 0 2 12 +6 414 24.6 \n jew 9 2 73 1 14 +9 31 2.7 \n other 12 2 0 75 10 +1 31 0.4 \n none 31 5 1 2 62 +5 53 2.6 \n \n \n \nEach row corresponds to a religious upbringing; each column shows a possible outcome. For example, the first row shows that of children raised Protestant, 82% report that their religious preference is Protestant, and 13% report None. The fraction of Nones has increased 7 percentage points since 1988. Since there are 951 people in this row, this increase accounts of 68 excess Nones in the 2008 survey. \n \n \n \nOverall, the changes in the transmission table account for 98 excess Nones, which is a little more than half of the observed increase. \n \n \n \nIf we run the simulations again, applying the transmission table from 2008 in 1988, we get the following predictions: \n \n \n \nThe prediction for Nones is better, but it's clear that this model still misses the mark: it predicts that the fraction of Catholics should be going down, and fails to predict the decrease in the fraction of Protestants. \n \n \n \nThe problem is that I am treating everyone interviewed in 1988 as a cohort, but they represent people of all ages, who were raised in different environments. Also, I am using data from 2008 to predict what will happen in 2008, so I have got away from the original goal, to see whether the changes that occurred between 1988 and 2008 could have been predicted in 1988. \n \n \n \nHowever, this model has given me some leads. It looks like a large part of the increase in Nones is due to changes in the transmission table, possibly a small part due to the environment table, and little or none due to the spouse tables. \n \n \n \nNext time I will present a different model that reorganizes respondents into cohorts by age of birth, which will make it possible to compare people raised over the same time span. It will also allow me to look for trends that began prior to 1988."], "link": "http://allendowney.blogspot.com/feeds/4899731578432322082/comments/default", "bloglinks": {}, "links": {"http://allendowney.blogspot.com/": 3, "http://amzn.to/": 1, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1, "http://4.blogspot.com/": 2}, "blogtitle": "Probably Overthinking It"}, {"content": ["In Part One I described some trends in market share of the major religions in the U.S.; in particular, since 1988, the fraction of Protestants dropped from 60% to 51%, and the fraction of people with no religious affiliation increased from 8% to 18%. \n \nI would like to know if something happened after 1988 to cause these changes, or if they could have been predicted based on patterns occurring before 1988. As a first step, I will use data from 1988 to model vertical transmission (from parent to child) and see if it predicts the observed changes \n \nMy model of vertical transmission works like this: \n \n \n Each respondent chooses a spouse, \n Each pair decides what religion to bring their children up in, \n Each child chooses a religion. \n \n \nI model each step of this process using data from the General Social Survey (GSS); specifically, I used these variables. \n \n \n RELIG: What is your religous preference? \n RELIG16: In what religion were you raised? \n MARELIG: What was your mother's religious preference when you were growing up? \n PARELIG: What was your fathers's religious preference when you were growing up? \n \n \nThe first two questions were asked every year, but questions about parents' religion were only asked in 1988 and 2008. I will use the data from 1988 to build and validate models, then use the data from 2008 to make predictions. \n \n \nI used MARELIG and PARELIG to build two \"Spouse tables\", one for men and one for women. Here is the table for men: \n \n \n \n\nSpouse Table (men) \n \n \n \n \n prot \n cath \n jew \n other \n none \n \n \n prot \n 93 \n 6 \n 0 \n 0 \n 1 \n \n \n cath \n 14 \n 85 \n 0 \n 0 \n 1 \n \n \n jew \n 4 \n 0 \n 96 \n 0 \n 0 \n \n \n other \n 6 \n 4 \n 0 \n 90 \n 0 \n \n \n none \n 59 \n 13 \n 0 \n 3 \n 24 \n \n \n \n \n \n\n Each row indicates the religion of a male respondent; each column is the religion of a possible spouse; the numbers are percents. For example, the first row indicates that 93% of male Protestants married other Protestants, and another 6% married Catholics. \n \n Here is the spouse table for women: \n \n \n\nSpouse Table (women) \n \n \n \n \n prot \n cath \n jew \n other \n none \n \n \n prot \n 82 \n 7 \n 0 \n 0 \n 12 \n \n \n cath \n 10 \n 85 \n 0 \n 0 \n 5 \n \n \n jew \n 4 \n 0 \n 96 \n 0 \n 0 \n \n \n other \n 8 \n 3 \n 0 \n 74 \n 15 \n \n \n none \n 13 \n 7 \n 0 \n 0 \n 80 \n \n \n \n \n \n\n In general, women are more likely to marry out of their religion than men, but still the great majority marry a co-religionist. One asymmetry is apparent: men with no religion seldom marry another None (24%), but women with no religion usually do (80%). This effect is partly due to the gender gap: 11% of male respondents are Nones, but only 5% of the women are (there is a similar, possibly smaller, gender gap in the CIRP data ). \n \nOnce the respondents have paired up, they decide what religion to raise the children in. The following table shows results from the 1988 data. The rows enumerate all pairs of mother's and father's religion; the columns indicate the religious environment they chose. For example, the second row indicates that if a Protestant woman marries a Catholic man, they raise the children Protestant 58% of the time, Catholic 36% of the time, and None 6%. \n \n \n\nEnvironment table \n \n \n \n parents \n prot \n cath \n jew \n other \n none \n \n \n prot-prot \n 99 \n 1 \n 0 \n 0 \n 1 \n \n \n prot-cath \n 58 \n 36 \n 0 \n 0 \n 6 \n \n \n prot-jew \n 100 \n 0 \n 0 \n 0 \n 0 \n \n \n prot-other \n 100 \n 0 \n 0 \n 0 \n 0 \n \n \n prot-none \n 89 \n 4 \n 0 \n 1 \n 7 \n \n \n cath-prot \n 39 \n 61 \n 0 \n 0 \n 0 \n \n \n cath-cath \n 1 \n 99 \n 0 \n 0 \n 0 \n \n \n cath-jew \n 0 \n 0 \n 0 \n 0 \n 0 \n \n \n cath-other \n 100 \n 0 \n 0 \n 0 \n 0 \n \n \n cath-none \n 17 \n 69 \n 0 \n 6 \n 8 \n \n \n jew-prot \n 0 \n 0 \n 100 \n 0 \n 0 \n \n \n jew-cath \n 0 \n 0 \n 0 \n 0 \n 0 \n \n \n jew-jew \n 0 \n 0 \n 96 \n 0 \n 4 \n \n \n jew-other \n 0 \n 0 \n 0 \n 0 \n 0 \n \n \n jew-none \n 0 \n 0 \n 0 \n 0 \n 0 \n \n \n other-prot \n 60 \n 0 \n 0 \n 40 \n 0 \n \n \n other-cath \n 0 \n 0 \n 0 \n 100 \n 0 \n \n \n other-jew \n 0 \n 0 \n 0 \n 0 \n 0 \n \n \n other-other \n 7 \n 2 \n 0 \n 89 \n 2 \n \n \n other-none \n 33 \n 0 \n 0 \n 67 \n 0 \n \n \n none-prot \n 100 \n 0 \n 0 \n 0 \n 0 \n \n \n none-cath \n 0 \n 0 \n 0 \n 0 \n 100 \n \n \n none-jew \n 0 \n 0 \n 0 \n 0 \n 0 \n \n \n none-other \n 0 \n 0 \n 0 \n 0 \n 0 \n \n \n none-none \n 40 \n 4 \n 0 \n 0 \n 56 \n \n \n \n \n  \n One surprise in this table is the last row: when two people with no religion marry, 40% of the time they apparently choose to raise their children Protestant. This seems unlikely, but there are several possible explanations: (1) the parents might have chosen to raise their children in the prevalent religion of their community, (2) a respondent might not have been raised by his parents, (3) a respondent might not be reporting his parents' religion accurately . F or purposes of modeling I take these responses at face value. \n \nChildren raised with a religion usually adopt that religion, but not always. The following \"transition table\" shows possible outcomes for each religious environment. For example, 89% of respondents who say they were raised Protestant also report that their religious preference is Protestant, but 3% are Catholic and 6% have no religious preference. More people convert from Catholic to Protestant than the other way around. \n \n \n\nTransition table \n \n \n \n \n prot \n cath \n jew \n other \n none \n \n \n prot \n 89 \n 3 \n 0 \n 1 \n 6 \n \n \n cath \n 11 \n 83 \n 0 \n 0 \n 6 \n \n \n jew \n 0 \n 0 \n 95 \n 0 \n 5 \n \n \n other \n 5 \n 3 \n 0 \n 83 \n 9 \n \n \n none \n 32 \n 11 \n 0 \n 0 \n 57 \n \n \n \n \n \nAs expected, the majority of people raised with no religion report no religious preference, but 32% of them identify as Protestant and 11% identify as Catholic. I found that surprising. I will look more closely later, but for now, again, I will take it at face value. \n \nFinally, we can combine these results into a single \"Generation table\" that shows the transitions from one generation to the next. I ran simulations with following steps. \n \n \n For each respondent, choose a spouse's religion from the Spouse Table. \n For each parent pair, choose a religious environment from the Environment Table. \n For each hypothetical child, choose a religious identity from the Transition Table. \n For each parent-child pair, make an entry in the Generation Table, below. \n \n \nSince this computation is based on random simulations, it varies from run to run, but here is a typical outcome: \n \n \n\n Generation table \n \n \n \n \n prot \n cath \n jew \n other \n none \n \n \n prot \n 86 \n 6 \n 0 \n 1 \n 7 \n \n \n cath \n 19 \n 72 \n 1 \n 1 \n 7 \n \n \n jew \n 0 \n 0 \n 95 \n 0 \n 5 \n \n \n other \n 29 \n 10 \n 0 \n 55 \n 6 \n \n \n none \n 67 \n 9 \n 0 \n 1 \n 23 \n \n \n \n \n \nAssuming that a generation time is about 22 years, we can use this model to predict the distribution of religions in 2010 (using only data from 1988). This figure shows the actual time series and the model predictions for each group: \n \n \n \n \n \nOn the right side of the plot, the vertical lines show the 90% confidence interval; the boxes show the mean of 20 simulation runs. [One technical note: each simulation is based on tables from resampled survey data, so the confidence intervals reflect both the sampling error of the survey and random variation of the simulations.] \n \n \n \nThe actual values for Catholics, Jews and Other fall within the prediction intervals, but the model fails to predict the decrease in Protestants or the increase in None. \n \n \n \nSo, what's missing from this model that could account for the observed changes? \n \n \n The spouse tables are based on the parents of 1988 respondents. People from later generations are increasingly likely to marry outside their religion. \n The environment table is also based on the previous generation; again, later parents might be making different decisions about the religious environment of their children. \n The transition table is based on 1988 respondents; it's possible that after 1988, children were less likely to adopt the religion they were raised in. Anecdotally, the culprits most often blamed for this effect are college and the Internet. \n Finally, I have not considered adult conversions from one religious identity to another. The GSS has data on these switches, so I could add them to the model. \n \n \nOver the next few installments, I will investigate each of these factors to see which, if any, account for the observed changes."], "link": "http://allendowney.blogspot.com/feeds/171064178008558050/comments/default", "bloglinks": {}, "links": {"http://allendowney.blogspot.com/": 1, "http://4.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.secularhumanism.org/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["In the last year or so I have written several articles about trends in religion among college students: \n \n \n Are religious colleges getting more religious? \n Freshman hordes more godless than ever! \n Freshman hordes even more godless! \n \n \nAll of these are based on data from the Cooperative Institutional Research Program (CIRP) which runs the Freshman Survey , an annual survey of more than 200,000 incoming students at 270 colleges and universities in the U.S. \n \n \n \n More recently, I read Secularization: In Defence of an Unfashionable Theory , by Steve Bruce. Bruce presents the \"unfashionable theory\" that as societies modernize, they secularize. In his formulation, modernization includes trends toward individualism, industrial capitalism, science and technology; and secularization means \"decline in the social significance of religion.\" \n \n \n \nThe poster child for secularization is Western Europe, where the social influence of religion has been in decline for centuries, and where in every country the fraction of people with no religious affiliation has been increasing for decades. \n \n \n \nBut skeptics have suggested that countries where people are still religious, like the United States and many countries in the Middle East, are exceptions that disprove the theory. Bruce replies that religious countries in the Middle East are not exceptions because they are not modern, and the United States is not an exception because it is, in fact, secularizing. \n \n \n \nThe data from the Freshman Survey are consistent with secularization. The number of incoming college students with no religious affiliation has been climbing consistently since 1978, and the number of students reporting participation in religious service has fallen at about the same rate. \n \n \n \nOf course, college students are not a random sample of the population; for that, we can use data from the General Social Survey (GSS), which is (according to the GSS) \"widely regarded as the single best source of data on societal trends.\" It has run since 1972; each year (or every other year since 1994) it surveys a sample of about 2000 adults randomly sampled from the U.S. population. Respondents answer hundreds of questions about their background, life history, and beliefs. Many questions are repeated from year to year for trend analysis. \n \n \n \nI will use this dataset to answer several questions: \n \n \n Is there evidence of secularization in the U.S. (Hint: yes). \n Can we explain the causes? \n Can we predict how these trends will continue over the next few decades. \n \n \nTo get started, I tracked responses to the question, \"What is your current religious preference?\" The original set of options was Protestant, Catholic, Jewish, some other religion, or no religion. After 1994, the set of options was expanded, but for my purposes the original options are enough to describe large-scale trends. The following graph shows the fraction of the population in each group over time. \n \n \n \n \n \n \n A few trends are apparent: the percentage of Protestants is declining; the percentages of Other and None are increasing. These trends are clearer in the following figures, broken into two intervals: \n \n \n \n \n \nFrom 1972 to 1988, the fraction of Protestants and Catholics was unchanged, but the fraction of Nones may have increased. \n \n \n \nFrom 1988 to 2010 (the most recent survey year), the fraction of Protestants and Jews declined, and the fraction of Nones increased by almost 250%. The number of Others increased during both intervals, with more variability. \n \n \n \nThis dataset shows signs of secularization in the U.S., at least since 1972. But religious affiliation is just one aspect of religious identity; there is a lot more data in the GSS to look at. \n \n \n \nMy particular interest is in explaining the trends we have seen so far, and predicting what's coming next. It is tempting to think that something happened in 1988 to cause the inflections in these curves, but I think it is more likely that the origin of these changes goes back farther. \n \n \n \nTo test that idea, let's pretend that it's 1988. We have see some changes in the market share of different religions since 1972, but nothing bigger than a few percentage points, and no indication of acceleration. Could we have predicted the much larger changes coming between 1988 and 2010? \n \n \n \nIn the next few articles, I develop several models intended to answer that question. Then I turn to prediction: using the data up to 2010 (and 2012 when it is available) what can we expect in the next 20 years?"], "link": "http://allendowney.blogspot.com/feeds/8754370668378154976/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1, "http://www3.norc.org/": 1, "http://4.blogspot.com/": 1, "http://allendowney.blogspot.com/": 2, "http://amzn.to/": 1, "http://2.blogspot.com/": 1, "http://www.ucla.edu/": 2, "http://are%20religious%20colleges%20getting%20more%20religious/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["In response to my article about the increasing numbers of students entering college with no religious affiliation, a reader wrote: \n \n A potentially interesting trend to look at is the religious participation of students attending Catholic or other religious institutions. I wonder if the trend is toward religious students (those who report both religious affiliation and participation in students) being more likely to go to religiously-affiliated colleges. This would mean a reduction of religious students at state/non-sectarian schools and an increase in the religiosity of students at affiliated schools (this might even be skewed because the survey doesn't include some of the most religious schools such as Liberty University). This would be a reflection of the increasing polarization of our society.  \n \n  On another subject, I tend to distrust steadily increasing social trends. From a complexity theory perspective, I would expect more of a cycle in religious (dis-)belief, so I wouldn't be surprised to see a crash in the number of non-believers sometime in the near future, although predicting exactly when that will happen is virtually impossible with the relatively small amount of data available (40 years is not enough).  \n There are several interesting questions here. The first is whether religious colleges are getting more religious. This one is relatively easy to investigate: the HERI survey provides data broken down by several types of colleges, including Nonsectarian, Catholic, and Other Religious Affiliation. \n  \n The good news is that their reports are available in PDF now . The bad news is that most of the older ones are scanned and not OCRed, so they are not easy to search. I went through them by hand, but I only extracted the data at 5-year intervals. Here is what the trends look like for students responding \"None\" for religious preference at Private nonsectarian 4-year colleges, Catholic colleges, and Other religious colleges: \n \n \n The percentage of Nones is increasing in all categories, more slowly at religious colleges than at other private colleges. Nevertheless, the fraction of students at religious colleges with no religious preference has nearly tripled in the last 35 years. \n  \n As an aside, I also plotted data for historically black colleges and universities (HBCU). Here's what that looks like: \n \n \n \n \n Clearly the trend is slower; in fact, it is not obvious that it is statistically significant. And since about 1990, the percentage of Nones is higher at religious colleges than at HBCUs, by more than a factor of two (13%, compared with 7%). \n \n Getting back to the reader's question, it doesn't look like the religious schools are getting more religious. In fact, they are getting less religious at almost the same rate as other schools. But maybe the fraction of students going to religious colleges is increasing? \n \n Here is a table from the National Center For Educational Statistics, which publishes the Digest of Education Statistics . It shows \"Fall enrollment and number of degree-granting institutions, by control and affiliation of institution: Selected years, 1980 through 2009.\" \n \n Here's what that data looks like: \n \n \n \n Enrollments have been increasing for all college types, with religious colleges growing faster than private nonsectarian colleges until 1995. Here's what these data look like expressed as a percent of the total: \n \n \n \n \n Before 1995, religious colleges were gaining market share, at the expense of nonsectarian colleges; other than that, there is not much going on. \n \n So, to address the reader's questions: \n \n \n Are religious students more likely to attend religious schools? Maybe. It's hard to tell with the data I have. \n Are more students going to religious schools? Not lately. Since 1995, the fraction of students at religious colleges has been flat. \n Are students at religious schools increasingly religious? No. The percentage of Nones has increased in all college types, including religious colleges. \n Do these trends introduce a bias in the results I presented? Not that I can see. \n \n \n As for distrusting steadily increasing social trends, I agree that some caution is needed. The percentage of Nones can't keep accelerating forever. But it can keep growing forever. \n \n \n \n Steven Pinker presents several one-way trends in his new book, The Better Angels of Our Nature . And Peter Singer, who reviewed Pinker's book in the New York Times , discussed related ideas in The Expanding Circle . \n \n \n \n But who knows? I guess we'll see what next year's data point looks like. \n \n \n \n Many thanks to the reader who posted the comments that prompted this update. \n \n \n \n Here is my original article from March 2011 . Here is the update from January 2012 ."], "link": "http://allendowney.blogspot.com/feeds/1291851513354252112/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 2, "http://nces.ed.gov/": 1, "http://4.blogspot.com/": 1, "http://allendowney.blogspot.com/": 3, "http://amzn.to/": 2, "http://2.blogspot.com/": 1, "http://www.nytimes.com/": 1, "http://www.ucla.edu/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["Background : I am trying to evaluate the effect on traffic safety of a fog warning system deployed in California in November 1996. The system was installed by CalTrans on a section of I-5 and SR-120 near Stockton where the accident rate is generally high, particularly during the morning commute when ground fog is common.  The warning system consists of (1) weather monitoring stations that detect fog and (2) changeable message signs that warn drivers to reduce speed. \n \n I will post my findings as I go in order to solicit comments from professionals and demonstrate methods for students. If I can get permission, I will also post my data and code so you can follow along at home. \n \n Previously : In the first installment I reviewed the first batch of data I am working with, and ran some tests to confirm that Poisson regression is appropriate for modeling the number of accidents in a given day. In part two I ran Poisson regressions to identify factors that influence the number of accidents per day. \n \n \n\n\n\n\nCritical events \nI have been waiting to get more details about several events that affected traffic safety during the observation period. I was able to get in touch with a Transportation Engineer in the Traffic Safety Branch of Caltrans District 10, which includes the study area. According to Caltrans records, the speed limit on the relevant section of I-5 was increased from 55 to 70 mph on March 25, 1996. The speed limit on SR-120 was increased from 55 to 65 mph about a month later, on April 22, 1996. Many thanks to my correspondent for this information! \n \nThe automated warning system was activated in November 1996. My collaborator has collected data on weather measurements made by the system and the warning it displayed. I hope to get this data processed soon. \n \n\n\n\n\nAccidents per million vehicles \nIn the previous article, I ran models with raw accident counts as the dependent variable, and found that traffic volume is a significant explanatory variable. Not surprisingly, more cars yield more accidents. \n \nRather than use volume as an explanatory variable, an alternative is to express the dependent variable in terms of accidents per million vehicles. As a reminder, here's what the traffic volume (in thousands of cars per day) looks like during the observation period: \n \n \n \n \nAnd here are the raw accident counts: \n \n \n \n \nI divided counts by volume and converted to accidents per million cars. At the same time I smoothed the curves by aggregating quarterly. Here's what that looks like: \n \n \n \nThe vertical red lines show major events expected to affect traffic safety: increased speed limits in March and April 1996, and the activation of the warning system in November 1996. \n \nThis graph suggests several observations: \n \n \n In the control directions, the accident rate was flat from 1992 through 1994, increased quickly in 1995 ( before the speed limits were increased) and has been flat every since. \n In the treatment directions, the accident rate was trending down until late 1996, including three quarters after the speed limit was increased. The accident rate increased sharply in 1997 and possibly again in 2000. \n The accident rate in both directions was unusually low during the third quarter of 1996, when the warning system was activated. Other than that, there is no obvious relationship between accident rates and the events of 1996. \n \n \nSince we don't expect the warning system to have much effect on the control directions (that's why they're called \"control\"), the speed limit changes are by far the most likely explanation for the accident rate changes. But it is puzzling that a large part of the change occurred before the new speed limits went into effect. One possibility is that as new speed limits were rolled out throughout California, drivers became accustomed to higher speeds and drove faster even on roads where the new limits were not in effect. But if that's true, it doesn't explain the continuing decline in the treatment directions. \n \n \n \nMy collaborator has some data on actual driving speeds before and after 1996. Once I process that data, I will be able to get back to this puzzle. \n \n \n\n\nInjuries and fatal accidents \nIn response to a previous post, a reader suggested that if the warning system causes drivers to slow down, it might affect the severity of accidents more than the raw number. To investigate that possibility, I also plotted the rates for injury accidents (including fatalities) and fatal accidents. \n \nHere is the graph for injury accidents: \n \n \n \nThe patterns we saw in the previous graph appear here, too. In addition, this graph suggests, more strongly, the possibility of a second changepoint in late 1999 or 2000. \n \nAnd here is the graph for fatal accidents: \n \n \n \n \nThe number of fatal accidents is, fortunately, small. During more than 10 years of observation, there were only 26 in the study area. The trends in the other graphs are not apparent here, other than the general increase in the rate of fatal accidents in the second half of the observation period. \n \n \n\n\nSummary \n \n \n \n Accident rates in the control and treatment directions increased sharply around 1996, but neither effect is related in an obvious way to increased speed limits or deployment of the warning system. \n Accident rates were unusually low in the quarter the warning system was activated; other than that, no effect of the warning system is apparent. \n It looks like there was a second increase in accident rates in late 1999 or 2000. I will ask my correspondent at Caltrans if he has an explanation. \n \n \n\n\nNext steps \n \nThere's not much more I want to do with this data. Now I need more numbers! In particular, I will be able to get data from the warning system itself, including: \n \n \n Conditions measured at roadside weather stations, which should be better than the data I have from the airport 8 miles away, and \n Messages displayed when the warning system was active. \n \nIf the warning system has an effect, it should be apparent on the days it is active. By comparing the treatment and control directions, it should be possible to quantify the effect. \n \nAlso, I have permission now to share the data; I will try to get it posted, along with my code, before the next update. \n \n \n[UPDATE April 26, 2012] \nA reader asked \n \n I can think of two ways that overall traffic volume affects accident rates: (1) more cars = more accidents overall, which you control for by measuring accident rates, and now you're seeing rising accident rates per car. So this raises the next thought, (2) more cars = more traffic density, which raises accident rates per car for each car on the road. \n \n What happens if you regress on traffic volume squared, or include traffic volume as an independent variable in the accident rate regression? The density effect is likely nonlinear but it's a thought. \nThis is a great question. If there is a non-linear relationship between traffic volume and the raw number of accidents, then even after we switch to accident rates, there might still be a positive relationship between traffic volume and accident rates. \n \nI ran these regressions, and in fact there is a relationship, but with the limitations of the data I have, I don't think it means much. Specifically, I only have annual estimates for traffic volume, so there's no fluctuation over time; traffic volume increases at a nearly constant rate for the entire observation period (see the figure above). \n \nSo traffic volume will have a positive relationship with anything else that's increasing, and a negative relationship with anything decreasing. And that's what I see in the regressions: \n \n\n \n \nAll of the relationships are statistically significant, but notice that in the treatment directions, before 1996 when the accident rate was declining, the relationship with traffic volume is negative! \n \nI don't think this variable has any explanatory content; any other ramp function would behave the same way. If I can get finer-grain data on traffic volume, I might be able to look for a more meaningful effect."], "link": "http://allendowney.blogspot.com/feeds/1804350540193916200/comments/default", "bloglinks": {}, "links": {"http://allendowney.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://2.blogspot.com/": 3, "http://3.blogspot.com/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["Background : I am trying to evaluate the effect on traffic safety of a fog warning system deployed in California in November 1996. The system was installed by CalTrans on a section of I-5 and SR-120 near Stockton where the accident rate is generally high, particularly during the morning commute when ground fog is common.  The warning system consists of (1) weather monitoring stations that detect fog and (2) changeable message signs that warn drivers to reduce speed. \n \n \n I will post my findings as I go in order to solicit comments from professionals and demonstrate methods for students. If I can get permission, I will also post my data and code so you can follow along at home. \n \n Previously : In the previous installment I reviewed the first batch of data I'll work with, and ran some tests to confirm that Poisson regression is appropriate for modeling the number of accidents in a given day. \n \n \nPoisson regressions \n Traffic volume \n \n \nTo measure the effect of traffic volume on the number of accidents, I ran a Poisson regression with one row of data per day from January 1, 1992 through March 31, 2002, which is 3742 days. \n \n \n Dependent variable: number of accidents in a day. \"Accidents\" includes all accidents, \"injury\" includes only accidents involving an injury or fatality, and \"fatal\" includes only fatal accidents. \n Explanatory variable: AADT, which is annualized average daily traffic, in units of 1000s of cars. \n \n \nHere are the results: \n \n \n \nThe columns are: \n \n sig: statistical significance. * indicates borderline significance (p-values near 0.5); ** is significant (p-values less than 0.01); *** is highly significant (very small p-values). \n coeff: the estimated coefficient of the regression. For example, the coefficient 0.03 means that an increase of one unit of AADT (1000 cars) yields and increase of 0.03 in the expected log(count), where count is the number of accidents. \n % incr is the coefficient converted to percentage increase per unit of AADT. In this example, the coefficient 0.03 indicates that for an increase of 1000 cars per day, we expect an increase in the number of accidents of roughly 3%. \n \n \nNot surprisingly, increase traffic volumes are associated with more accidents (of all types). In the control and treatment directions, the increase is about the same size, 3-4% for each additional 1000 cars. \n \n \n \nFor fatal accidents the association is less statistically significant, most likely because the number of fatal accidents is much smaller. There were 1900 accidents, total, during the observation period; 705 involved injuries but no fatalities; only 26 were fatal. \n \n \n \nI conclude that traffic volume has a substantial effect on the number of accidents, so \n \n \n It should be included as an explanatory variable in subsequent models, and \n It might be important to get additional traffic data, broken down by direction of travel and a finer scale than annual! \n \n \n \n Weather \n \n \n \nThe next set of explanatory variables I considered is: \n \n \n Fog: binary variable, whether fog was detected at the airport weather station during the day. \n Heavy fog: same as above, but apparently based on a different visibility threshold. \n Precip: total precipitation for the day in 0.1 mm units. \n \n \nAnd here's the first surprise. Controlling for traffic volume, there is no significant relationship between fog and the number of accidents (of any kind). \n \n \n \n \nFor heavy fog, there is generally no significant relationship, but: \n \n \n In the control directions only, heavy fog has a significant effect, but the coefficient is negative. If this effect is real, heavy fog decreases the number of accidents by about 30%. \n If we break the data set into before and after November 1996, the effect disappears in the \"before\" dataset. \n There is no apparent effect on fatal accidents. \n \n \n \nThere are several possible conclusions: \n \n \n This effect is real, and for some reason heavy fog actually decreases the number of accidents, but only in the control direction. \n This effect is a statistical fluke, and the fog variables have no explanatory power. In that case, it is possible that fog in the study area does cause accidents, but measurements at the airport do not reflect conditions in the study area (8 miles away). \n \n \n \nOn the other hand, the effect of precipitation is consistent, significant, and (as expected) dangerous. Here are the results for precipitation (controlling for traffic volume): \n \n \n \n \nEach millimeter of precipitation increases the number of accidents by about half a percent. [I am not sure how seriously to take that interpretation, since this relationship is probably non-linear. It might be better to make binary variables like \"rain\" and \"heavy rain\".] \n \n Summary \n \nHere's what we have so far: \n \n \n As expected, more traffic yields more accidents. \n Surprisingly, there is not statistical relationship between our fog measurements and accident rates. \n There is a consistent relationship between precipitation and accidents, but I might have to come back and quantify it more carefully. \n \n \nBefore going farther, I want to get more specific information about when the speed limits were changed on these road segments and when the warning system was deployed. So that's all for now."], "link": "http://allendowney.blogspot.com/feeds/1235354192746759895/comments/default", "bloglinks": {}, "links": {"http://allendowney.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Probably Overthinking It"}, {"content": ["I've started a new project, working with a collaborator at another university, to evaluate the impact of a fog warning system deployed on a highway in California in November 1996. The system was installed by CalTrans on a section of I-5 and SR-120 near Stockton where the accident rate is generally high, particularly during the morning commute when ground fog is common. \n \nThe warning system consists of (1) weather monitoring stations that detect fog and (2) changeable message signs that warn drivers to reduce speed. Among people who study traffic safety, there are two theories about these kinds of systems: \n \n1) The mainstream theory is that when drivers are warned to slow down, they slow down, and lower speeds reduce the accident rate. \n \n2) The heterodox theory, which my collaborator holds, is that warning signs introduce perturbations into the flow of traffic so they can cause more accidents than they prevent. \n \nMy job is to evaluate which theory the data support. Here's what I have to work with: \n \n1) The warning system was activated in November 1996. \n \n2) My collaborator and his students collected data from the CalTrans Traffic Accident Surveillance and Analysis System (TASAS). It includes all fatal and injury accidents and a large portion of \"property damage only\" accidents. Their data runs from Jan 1, 1992 to March 31, 2002, about five years before and six years after the warning system was activated. \n \n3) NOAA's National Climatic Data Center (NCDC) operates a weather station at Stockton Airport (KSCK), about 8 miles from the study area. I downloaded daily weather data from 1992 through 2002. \n \n4) California DOT publishes average daily traffic volume (ADT) through several locations in the study area. I downloaded their reports from 1992 through 2002. \n \nBut there are some challenges: \n \n1) The biggest problem is that the speed limit in the study area changed in January 1996, 11 months before the warning system was deployed. It will be difficult to separate the effect of the warning system from the effect of the speed limit. I have exchanged email messages with someone at CalTrans who might be able to tell me exactly when the speed limit signs were changed on each stretch of road. \n \n2) The traffic volume data is annualized, so it doesn't account for variation on smaller time scales, and it does not distinguish between traffic in different directions. \n \n3) The weather station at the airport is about 8 miles from the study area, and at a higher altitude, so the fog data may not capture the actual conditions in the study area. \n \nHowever, we have one ace in the hole: the system only shows warnings to traffic moving in one direction (toward the merger of the two highways), so traffic in the other direction acts as a natural control. I'll refer to the segments with warning signs, 5S and 120W, as the \"treatment directions,\" and the others, 5N and 120E, as the \"control directions.\" \n \n Accident data \n \nTo get a quick feel for the data, I plotted the number of accidents per month in the treatment and control directions. \n  \n In both directions, the number of accidents increases in 1996. The change is larger in the treatment direction; before 1996, the treatment direction was safer; after 1996, it became more dangerous. It looks like the treatment directions might have become more dangerous again in 2000, but just by looking at this figure, it's hard to say if that effect is significant. \n Based on raw number of accidents, there is no evidence that the warning system is effective. But there are several other factors to consider, including traffic volume, speed, and weather. \n Traffic volume \n This plot shows annualized estimates of average daily traffic volume (ADT) through the study area. \n  The traffic volume on SR-120 increases consistently during the observation period; volume on I-5 is mostly flat; volume after the merge point increases substantially. Since many accidents occur near the merge point, I will use the estimates from after the merge point for analysis. \n These estimates include both directions of travel, so we can't distinguish volumes in the treatment and control directions. \n Weather \n This plot shows the number of days per month with fog, heavy fog, or more than 3mm of precipitation, as observed at Stockton Airport: \n  \n Not surprisingly, all three variables show seasonal variability. Other than that, there are no obvious trends. \n Regression analysis \n Having cleaned and processed the data, we can look for factors that contribute to accidents. In total, there were 932 accidents in the control directions and 968 in the treatment directions. Over 3744 days, the average number of accidents per day is 0.51. Many days have no accidents. On the worst day in the observation period, there were nine! \n I'll use Poisson regression to model the number of accidents in each day as a function of the explanatory factors. A requirement of Poisson regression is that the distribution of the dependent variable should be (wait for it) Poisson. To check this requirement, I computed the number of accidents each day for the control and treatment directions, before and after November 15, 1996 (roughly when the warning system was activated). \n To check whether these distributions are Poisson, I plotted the complementary CDF on a log-y scale; under this transform, a straight line is characteristic of a Poisson distribution.  \n In all four cases the transformed CDF is roughly a straight line, so Poisson regression with this data should be just fine. \n The other characteristic of the Poisson distribution is that the mean and variance are the same; we can check that, too. \n       mean   variance control  before  0.11   0.14    after  0.37   0.46 treatment before  0.06   0.07    after  0.44   0.62  \n In each case, the variance is a little higher than the mean, which suggests that there are more multi-accident days than we expect in a Poisson process (it's easy to imagine an explanatory mechanism). But the difference is small, so again I think we are safe using Poisson regression. \n This table also demonstrates the effect I mentioned earlier. Before the changes in 1996 (increased speed limit and activation of the warning system) there were fewer accidents in the treatment directions (about half). After 1996 it's the other way around: there are more accidents in the treatment directions. \n That's enough with the preliminaries. Next time we'll get into the analysis and see what factors contribute to the accident rate."], "link": "http://allendowney.blogspot.com/feeds/4709484046707027872/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://www.ca.gov/": 1, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1, "http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://www.noaa.gov/": 1, "http://traffic-counts.ca.gov/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["Excessive use of the passive voice in science writing is a self-perpetuated, mutually-perpetrated hoax. To prove it, I am offering a $100 bounty for the first person who can find a journal that explicitly requires authors to write in the passive voice. \n \n UPDATE April 4, 2012: To my shock and dismay, the bounty has been claimed! See the new HALL OF SHAME below. \n \nMost style guides recommend the active voice, and most readers prefer it. But in some academic fields, especially the sciences, authors use a stilted and awkward style that replaces clear concise sentences like, \"We performed the experiment,\" with circumlocutions like \"The experiment was performed.\" \n \nAsked why they write like that, many scientists admit that they don't like it, but they are under the impression that journals require it. They are wrong. Of the journals that have style guides, the vast majority explicitly ask authors to write in the active voice. \n \nHere's what you can do to help stop the carnage: \n If you are writing scientific articles in the passive voice, check the style guide for your journals. Unless you are explicitly required to write in the passive voice, don't! \n \n \n If you are reviewing articles, check the style guide for your journals. Unless the passive voice is explicitly required, don't \"correct\" sentences in the active voice. \n \n \n If you are the editor of a scientific journal, make sure that your style guide explicitly recommends the active voice, and make sure authors and reviewers are aware of your recommendation. \n \n \n If you are teaching students to write scientific papers in the passive voice, STOP! There is no reason for students to practice bad writing. If, at some point in the future, they actually have to write like that, they can write a first draft in the active voice and then translate. \n \n \n If you know of any other style guides that make a recommendation on this topic, let me know and I will add them to this page. So far I haven't found any that actually call for the passive voice. \n \n Here are the style guides from some of the top journals in science: \n Nature \n \" Nature journals like authors to write in the active voice (\"we performed the experiment...\" ) as experience has shown that readers find concepts and results to be conveyed more clearly if written directly.\" The Nature Editorial Staff comment on their style recommendations here , and here is a collection of letters to Nature on this topic. \n \n \n Science \n \"Use active voice when suitable, particularly when necessary for correct syntax (e.g., 'To address this possibility, we constructed a lZap library . . .,' not 'To address this possibility, a lZap library was constructed . . .').\" \n Proceedings of the National Academy of Sciences USA (PNAS) \n \nFrom personal correspondence with PNAS Editorial: \n \"... we do not have a style guide for authors beyond what can be found in the Information for Authors page ( http://www.pnas.org/misc/iforc.shtml#prep ). There are no rules recommending passive vs. active voice in research articles. I would recommend looking at some PNAS articles in your specific area of interest to get a flavor of the style used.\" However, their Production Department adds: \n \"[We] feel that accepted best practice in writing and editing favors active voice over passive.\" Note: many thanks to my correspondent at PNAS for permission to include these quotes. \n \n IEEE \n \nThe IEEE Editorial Style Manual doesn't make an explicit recommendation \non this issue, but for \"guidance on grammar and usage,\" \nit refers to the Chicago Manual of Style , which says: \n \"As a matter of style, passive voice {the matter will be given careful consideration} is typically, though not always, inferior to active voice {we will consider the matter carefully}.\" Update: A reader sent the following: You might be pleased to note that the May 2007 template and instructions for IEEE Transactions articles ( http://www.ieee.org/documents/TRANS-JOUR.pdf ) is at least permissive: \n \"If you wish, you may write in the first person singular or plural and use the active voice (\"I observed that ...\" or \"We observed that ...\" instead of \"It was observed that ...\").\" \n ACM \n \nIf the ACM has a style guide I can't find it, but one of their publications, Crossroads , does, and it couldn't be clearer: \n \"Active voice replaces passive voice whenever possible.\" \n A reader sent me the following note: \n The American Chemical Society Style Guide , 3rd Edition writes as follows: \"Use the active voice when it is less wordy and more direct than the passive.\" And \"Use first person when it helps to keep your meaning clear and to express a purpose or a decision.\" \n The following are journals whose style guides do not address this issue, which I take as implicit permission to use the active voice, as recommended by virtually all non-scientific style guides: \n Physical Review Letters \n \nTheir style guide is silent on this issue. \n \n \n Applied Physics Letters \n \nTheir instructions call for \"good scientific American English,\" but they don't address the issue of voice explicitly. \n \nThey also suggest, \"For general format and style, consult recent issues of the Journal.\" I chose an \narticle at random and found that it was generally in the active voice: \n \n \"We realized the described structure by first creating a 2D hexagonal pattern of etch pits...\" with only a few unfortunate uses of the passive voice: \"...to reduce therewith the number of stitching \ninterfaces, the magnification of the FIB images was reduced.\" \n \n \nSo I take that as implicit permission to write in the active voice...and to use the word \" therewith \". \n \n \n Structure \n \nNothing explicit, but certainly no call for the passive voice: \n \"Research papers should be as concise as possible and written in a style that is accessible to the broad Structure readership.\" \n Unfortunately, many journals provide no style guides at all: \n New England Journal of Medicine : No explicit style guidelines. \n \n \n Here is an interesting report from an author whose paper was tranformed from active to passive by misguided editors. \n \nHere is a note from another reader: \n \"I also found this gem which you may have already read: http://www.amwa.org/default/publications/journal/vol25.3/v25n3.098.feature.pdf  Use of the  Passive  Voice in Medical Journal Articles , Robert J. Amdur, MDa; Jessica Kirwan, MAb; and Christopher G. Morris, MSc, AMWA JOURNAL \u2022 VOL. 25, NO. 3, 2010\" \nAmdur et al measure the use of passive voice in medical articles and find that 20-30% of sentences are passive, compared with 3-5% in their reference corpus, the Wall Street Journal. They write: \n \"We could not find a survey study or consensus statement addressing the question of why authors of medical journal articles use the passive voice so frequently. No publication guideline mentions goals or limits for the use of the passive voice, and some of the most prestigious references are worded in a way that may encourage authors to use the passive voice whenever it is acceptable to do so. For example, the AMA Manual of Style says that, 'Authors should use the active voice, except in instances in which the actor is unknown or the interest focuses on what is acted on.'\" \nOne point of clarification: I am not an absolutist on this issue. The passive voice has its uses. What I am objecting to is the obsolete tradition of writing scientific papers primarily in the passive voice. Finally, please do not send me email triumphantly pointing out the (occasional and appropriate) use of the passive voice in my essays. \n \n The fine print \n \n \nI am offering a $100 bounty for the first person who can find a journal that explicitly requires authors to write in the passive voice. It has to be a mainstream English-language journal with an online style guide that recommends or requires the passive voice for published articles. I'll only pay one bounty, to the person whose email gets to my inbox first. Send email to downey@allendowney.com. I will be the sole arbiter of whether a submission meets these criteria, but I promise to be reasonable. I will post the winning submission here. \n \nBackground: I wrote this essay several years ago. Lately I have heard from several readers pointing to additional resources, so I thought I would update the article, clean up some broken links, and move it from my web page to my blog. Comments and additional resources are welcome. \n \n THE HALL OF SHAME \n \nIn response to my bounty, I heard from several readers who found journals that explicitly ask authors to use the passive voice. \n \nThe first to reply, and the winner of the bounty, is Jonathan Livengood, who fin gered the ICES Journal of Marine Science which recommends the passive voice in its style guide : \n Note too that the Journal prefers text to be written in the passive voice (e.g. \u201cAn experiment on XXX was undertaken \u2026\u201d) rather than in the active voice (e.g. \u201cI undertook an experiment on XXX \u2026\u201d), though modest use of the active voice is acceptable. David Weisman reported the style guide f or Clinical Oncology and Cancer Research, which recommends: \n Materials and Methods: Use the \"passive voice\" when describing experimental detail.  Note too that they compound the offence with spurious use of \"quotation marks.\"  \n  Donna Tucker found a borderline case. She wrote, \"T he American Meteorological Society no longer recommends the use of passive voice. It has not been that many years since they did.. .  They do, however, have specific requirements for the abstract...  First person construction should not be used in the abstract, and references should be omitted because they are not available per se to abstracting services.  Donna continues, \"So if I cannot say 'We collected the data.' I am left with 'The data were collected'.  Although this requirement does not explicitly mandate the use of the passive voice, it does make it unavoidable in certain circumstances.\" The journal gets extra demerits for gratuitous use of \"per se.\"  \n  Finally, Michael Allen indicts the Journal of Animal Ecology for this:  The passive voice is preferred in describing methods and results. The active voice may be used occasionally to emphasize a personal opinion (typically in Introduction and Discussion sections).  Thank you to everyone who submitted a claim. I welcome additional nominations to the Hall of Shame."], "link": "http://allendowney.blogspot.com/feeds/942562235493212833/comments/default", "bloglinks": {}, "links": {"http://jaffeerevises.com/": 1, "http://www.chicagomanualofstyle.org/": 1, "http://www.nature.com/": 1, "http://www.nejm.org/": 1, "http://www.sciencemag.org/": 1, "http://pubs.acs.org/": 1, "http://www.thefreedictionary.com/": 1, "http://www.ieee.org/": 3, "http://blogs.nature.com/": 1, "http://www.springer.com/": 1, "http://www.structure.org/": 1, "http://www.amwa.org/": 1, "http://www.spinellis.gr/blog": 1, "http://www.geosafari.org/": 1, "http://www.pnas.org/": 2, "http://apl.aip.org/": 1, "http://feedads.doubleclick.net/": 2, "http://icesjms.oxfordjournals.org/": 1, "http://prl.aps.org/": 1, "http://www.journalofanimalecology.org/": 1, "http://www.acm.org/": 2, "http://forms.aps.org/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["I am always looking for good examples of Bayesian analysis, so I was interested in this paragraph from The Economist ( September 2000 ): \n \"The canonical example is to imagine that a precocious newborn observes his first sunset, and wonders whether the sun will rise again or not. He assigns equal prior probabilities to both possible outcomes, and represents this by placing one white and one black marble into a bag. The following day, when the sun rises, the child places another white marble in the bag. The probability that a marble plucked randomly from the bag will be white (ie, the child\u2019s degree of belief in future sunrises) has thus gone from a half to two-thirds. After sunrise the next day, the child adds another white marble, and the probability (and thus the degree of belief) goes from two-thirds to three-quarters. And so on. Gradually, the initial belief that the sun is just as likely as not to rise each morning is modified to become a near-certainty that the sun will always rise.\" This example made me wonder about two things: \n \n1) Although they call it a \"canonical example\", I had not heard it before, so I wondered where it came from, and \n \n2) Although the example demonstrates the general idea of updating beliefs in light of new evidence, it is not obvious that the hypothetical newborn is actually doing a Bayesian update. \n \nAt the risk of spoiling the fun, here is what I found: \n \n1) The example is from Richard Price's commentary on the original essay , discovered after Bayes's death, that presented what came to be called Bayes's Theorem. So I am embarrassed that I didn't know it. \n \n2) The analysis is correct for a particular prior distribution and for a particular interpretation of the posterior, but presented in a way that obscures these details. \n \n A false start \n \nIn fact, it is not easy to formulate this example in a Bayesian framework. One option (which fails) is to compute posterior probabilities for two hypotheses: either A (the sun will rise tomorrow) or B (the sun will not rise tomorrow). \n \nWe are given the priors: P(A) = P(B) = 1/2. \n \nBut when the sun rises in the morning, how do we compute the posteriors? We need the likelihoods; that is, the probability of the evidence (sunrise) under the two hypotheses. It is hard to make sense of these likelihoods, so we conclude that this formulation of the problem isn't working. \n \n The beta distribution \n \nA more fruitful (and more complicated) alternative is to imagine that the sun will rise with some probability, p , and try to estimate p . \n \nIf we know nothing about p , we could choose a prior distribution that is uniform between 0 and 1. This is a reasonable choice, but it is not the only choice. For example, we could instead make the odds (rather than the probabilities) uniform over a range of values, which has the intuitive appeal of giving more prior weight to values near 0 and 1. \n \nBut starting with uniform probabilities is equivalent to starting with a Beta distribution with parameters \u03b1 = \u03b2 = 1, which has a mean value of \u03b1 / ( \u03b1 + \u03b2 ), or 1/2. And the Beta distribution has the nice property of being a conjugate prior, which means that after an update the posterior is also a Beta distribution. \n \nWe can show (but I won't) that observing a success has the effect of increasing \u03b1 by 1, and observing a failure increases \u03b2 by 1. And that's where the white and black marbles come in. The author of the example is using the marbles as a sneaky way to talk about a Beta distribution. \n \nAfter one sunrise, the newborn's posterior belief about p is a Beta distribution with \u03b1 =2, \u03b2 =1, which looks like this: \n  This represents the newborn's posterior belief about p , the probability of sunrise. At the far left, the hypothesis that p =0 has been refuted. At the far right, the most likely value (after one success) is p =1. A 90% credible interval is [0.225, 0.975], which indicates that the newborn is still very unsure about p . But if we insist on a single-value estimate, he or she might reasonably compute the mean of the posterior, which is 2/3. \n \nSimilarly, after two successful sunrises, the posterior looks like this: \n  And the mean value of p is 3/4. So the marble method is correct after all! Well, sort of. It is correct if we assume a uniform prior for p , and if we use the mean of the posterior to generate a single-point estimate. For both decisions there are reasonable alternatives; for example, after any number of successes (without a failure) the maximum likelihood estimate of p is 1. \n \nIn summary, this example demonstrates the general idea of a Bayesian update, but think the way the calculation is presented is misleading. \n \n Price's version \n \nAs I said, the example is from Price's comments on Bayes's article . Here is Price's version, from the page numbered 409 [in the original typeface, the letter \"s\" looks like \"f\", but I will fpare you]: \n \"One example here it will not be amiss to give.  \"Let us image to ourselves the case of a person just brought forth into this world and left to collect from his observation of the order and course of events what powers and causes take place in it. The Sun would, probably, be the first object that would engage his attention; but after losing it the first night, he would be entirely ignorant whether he should ever see it again. He would therefore be in the condition of a person making a first experiment about an event entirely unknown to him. But let him see a second appearance or one return of the Sun, and an expectation would be raised in him of a second return, and he might know that there was an odds of 3 to 1 for some probability of this. This odds would increase, as before represented, with the number of returns to which he was witness. But no finite number of returns would be sufficient to produce absolute or physical certainty. For let it be supposed that he has seen it return at regular and stated intervals a million of times. The conclusions this would warrant would be such as follow --- There would be the odds of the millioneth power of 2, to one, that it was likely that it would return again at the end of the usual interval.\" To interpret \"an odds of 3 to 1 for some probability of this\" we have to look back to page 405, which computes the odds \"for somewhat more than an even chance that it would happen on a second trial;\" that is, the odds that p > 0.5. In the example, the odds are 3:1, which corresponds to a probability of 0.75. \n \nIf we use the Beta distribution to compute the same probability, the result is also 0.75. So Price's calculations are consistent with mine; he just uses a different way of summarizing them."], "link": "http://allendowney.blogspot.com/feeds/4433429831348089290/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.ac.uk/": 2, "http://1.blogspot.com/": 1, "http://en.wikipedia.org/": 1, "http://allendowney.blogspot.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["At PyCon last week I taught a tutorial on Bayesian statistics. It is based on Chapters 5 and 8 of Think Stats . Here is the web page I created for the tutorial . \n \nAnd here, courtesy of PyCon and pyvideo.org, is the video . It's three hours long, so get comfortable! \n \nHere's a screencap of me magically deriving Bayes's Theorem: \n \n \n \n \nAnd here's the description and outline, from the PyCon page : \n \n \n \nDescription \n \nThis tutorial is an introduction to Bayesian statistics using Python. My goal is to help participants understand the concepts and solve real problems. We will use material from my book, Think Stats: Probability and Statistics for Programmers (O\u2019Reilly Media). \n \n \nAbstract \n \n \nBayesian statistical methods are becoming more common and more important, but there are not many resources to help beginners get started. People who know Python can use their programming skills to get a head start. \n \nI will present simple programs that demonstrate the concepts of Bayesian statistics, and apply them to a range of example problems. Participants will work hands-on with example code and practice on example problems. \n \nStudents should have at least basic level Python and basic statistics. If you learned about Bayes\u2019s Theorem and probability distributions at some time, that\u2019s enough, even if you don\u2019t remember it! Students should be comfortable with logarithms and plotting data on a log scale. \n \nStudents should bring a laptop with Python 2.x and matplotlib. You can work in any environment; you just need to be able to download a Python program and run it. \n \nOutline: \n \n Bayes\u2019s theorem. \n Representing probability distributions. \n Bayesian estimation. \n Biased coins and student test scores. \n Censored data. \n The locomotive / German tank problem. \n Hierarchical models and the unseen species problem."], "link": "http://allendowney.blogspot.com/feeds/3803282890945664112/comments/default", "bloglinks": {}, "links": {"http://amzn.to/": 1, "http://feedads.doubleclick.net/": 2, "https://us.pycon.org/": 1, "http://pyvideo.org/": 2, "https://sites.google.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["When I started writing Think Stats , I wanted to avoid dire warnings about all the things people do wrong with statistics. I enjoy feeling smug and pointing out other people's mistakes as much as the next statistics Nazi, but I don't see any evidence that the warnings have much effect on the quality of statistical analysis in the press. \n \nMaybe another approach is in order. Instead of making statistics seems like an arcane art that can only be practiced correctly by trained professionals, I would like to emphasize that the majority of statistical analysis is very simple. Deep mathematics is seldom necessary; usually it is enough to ask good questions and apply simple techniques. \n \nAs an example, I'm going to do what I said I wouldn't: point out other people's mistakes. Here is an excerpt from a recent ASEE newsletter, Connections : \n \n \n \n \n I. Databytes \n \n  \n Doctoral Degrees \nby Race and Ethnicity: \nA Decade of Little Change  The percentages of recipients of doctoral degrees from all engineering disciplines by race and ethnicity show a great deal of stability over the last ten years. African Americans, as a percentage of total of all recipients of doctoral degrees grew about half a percent from 2001 to 2010; Hispanics increased by about two percent during the same time period; Asian Americans stayed virtually unchanged; and Caucasians increased by percent.  \n Doctoral Degrees by Race and Ethnicity* \n 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 \n African American 3.9% 3.5% 3.4% 3.8% 3.7% 3.7% 3.6% 3.2% 3.8% 4.4% \n Hispanic 3.3% 3.9% 3.6% 3.5% 3.7% 3.0% 3.5% 3.6% 3.8% 5.2% \n Other 14.2% 11.4% 11.9% 14.0% 14.2% 15.1% 18.7% 19.5% 17.6% 10.7% \n Asian American 13.9% 14.6% 14.4% 14.0% 14.4% 16.6% 12.0% 12.4% 13.2% 14.0% \n Caucasian 64.7% 66.6% 66.7% 64.7% 64.0% 61.6% 62.2% 61.3% 61.6% 65.7% \n *Data on ethnicity does not include schools from Puerto Rico or foreign nationals. The percentage of Hispanic graduates is 5.5% in 2010 if graduates from the University of Puerto Rico, Mayaguez are included. New race and ethnicity categories, first reported in 2010, American Indians (0.4%), Hawaiian/Pacific Islanders (0.1%) and Two or More (0.5%) are combined under \u201cother\u201d. Six institutions reported virtually all degrees in the Unknown field. These institutions were removed from the calculations for race, ethnicity and residency. \n \n \n \nThe paragraph tries to summarize the data in the table, and fails. Let's take it point by point: \n \nClaim 1) The percentages of recipients of doctoral degrees from all engineering disciplines by race and ethnicity show a great deal of stability over the last ten years. \n \nValidity: BASICALLY TRUE. If they had just stopped here, everything would be fine. A graph would make this conclusion easier to see. I copied their data into Google Docs and generated this graph: \n  Yup. Pretty flat. \n \nThe other thing that jumps out of this graph is that something funny happened in 2010. The caption in the article explains, \"New race and ethnicity categories, first reported in 2010 ... are combined under \u201cother\u201d. This change in the survey seems to have caused a decrease in the number of respondents reporting \"other\", and an increase in \"Causasian.\" I can't explain why it had that effect, but it is not surprising that it had an effect. \n \nClaim 2) African Americans, as a percentage of total of all recipients of doctoral degrees grew about half a percent from 2001 to 2010; \n \nValidity: FALSE. Because the survey changed in 2010, it is not a good idea to summarize the results by comparing the first and last data points. If we drop 2010, there is no evidence of any meaningful change in the percentage of African Americans. \n \nClaim 3) Hispanics increased by about two percent during the same time period; \n \nValidity: FALSE. Again, if we ignore 2010, there is no evidence of change. \n \nClaim 4) Asian Americans stayed virtually unchanged; \n \nValidity: MAYBE. If anything, there is a small decrease. Again ignoring 2010, the last three data points are all below the previous six. But if you fit a trend line, the slope is not statistically significant. \n \nClaim 5) Caucasians increased by percent. \n \nValidity: FALSE. If we ignore 2010, there is a clear downward trend. If you fit a trend line, the slope is about -0.6 percentage points per year, and the p-value is 0.003. \n \nClaim 6) No comment on \"Other\" \n \nValidity: ERROR OF OMISSION. There is a clear upward trend, with or without the last data point. The fitted slope is almost 0.9 percentage points per year, and the p-value is 0.005. \n \nSo let's summarize: \n \n Race     Article claims  Actually \n ----     --------------  -------- \n African American  +0.5 %age point  No change \n Hispanic    +2 %age point  No change \n Asian American  No change   Maybe down \n Caucasian   Up     -4 %age point \n Other    No comment   +6 %age point \n          \nWhat's the point of this? Granted, a newsletter from ASEE is not the Proceedings of the National Academy of Sciences , so maybe I should't pick on it. But it makes a nice example of simple statistics gone wrong. I guess that makes me a statistics Nazi after all. \n \nHere's one more lesson: if you run a survey every year, avoid changing the questions, or even the selection of responses. It is almost impossible to do time series analysis across different versions of a question. \n \nIf you read this far, here's a small reward. The electronic edition of Think Stats is on sale now at 50% off, which makes it $8.49. Click here to get the deal."], "link": "http://allendowney.blogspot.com/feeds/5723074369822676449/comments/default", "bloglinks": {}, "links": {"http://www.asee.org/": 1, "http://3.blogspot.com/": 1, "http://www.blogger.com/": 1, "http://feedads.doubleclick.net/": 2, "http://ads.asee.org/": 1, "http://shop.oreilly.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["My new book, Think Complexity , will be published by O'Reilly Media in March. For people who can't stand to wait that long, I am publishing excerpts here. If you really can't wait, you can read the free version at thinkcomplex.com . \n \nIn Part One I outline the topics in Think Complexity and contrasted a classical physical model of planetary orbits with an example from complexity science: Schelling's model of racial segregation. \n \nIn Part Two I outline some of the ways complexity differs from classical science. In Part Three , I describe differences in the ways complex models are used, and their effects in engineering and (of all things) epistemology. \n \n Part Four pulls together discussions from two chapters: the Watts-Strogatz model of small world graphs, and the Barabasi-Albert model of scale free networks. And now, Part Five: Self-organized criticality. \n \n \n Sand piles \n \nIn 1987 Bak, Tang and Wiesenfeld published a paper in Physical Review Letters , ``Self-organized criticality: an explanation of 1/ f noise.'' You can download it from http://prl.aps.org/abstract/PRL/v59/i4/p381_1 . \n \nThe title takes some explaining. A system is ``critical'' if it is in transition between two phases; for example, water at its freezing point is a critical system. A variety of critical systems demonstrate common behaviors: \n \n Long-tailed distributions of some physical quantities: for example, in freezing water the distribution of crystal sizes is characterized by a power law. \n Fractal geometries: freezing water tends to form fractal patterns---the canonical example is a snowflake. Fractals are characterized by self-similarity; that is, parts of the pattern resemble scaled copies of the whole. \n Variations in time that exhibit pink noise: what we call ``noise'' is a time series with many frequency components. In ``white'' noise, all of the components have equal power. In ``pink'' noise, low-frequency components have more power than high-frequency components. Specifically, the power at frequency f is proportional to 1/ f . Visible light with this power spectrum looks pink, hence the name. \n \nCritical systems are usually unstable. For example, to keep water in a partially frozen state requires active control of the temperature. If the system is near the critical temperature, a small deviation tends to move the system into one phase or the other. \n \nMany natural systems exhibit characteristic behaviors of criticality, but if critical points are unstable, they should not be common in nature. This is the puzzle Bak, Tang and Wiesenfeld address. Their solution is called self-organized criticality (SOC), where ``self-organized'' means that from any initial condition, the system tends to move toward a critical state, and stay there, without external control. \n \nAs an example, they propose a model of a sand pile. The model is not realistic, but it has become the standard example of self-organized criticality. \n \nThe model is a 2-D cellular automaton where the state of each cell represents the slope of a part of a sand pile. During each time step, each cell is checked to see whether it exceeds some critical value. If so, an ``avalanche'' occurs that transfers sand to neighboring cells; specifically, the cell's slope is decreased by 4, and each of the 4 neighbors is increased by 1. At the perimeter of the grid, all cells are kept at zero slope, so (in some sense) the excess spills over the edge. \n \nBak et al. let the system run until it is stable, then observe the effect of small perturbations; they choose a cell at random, increment its value by 1, and evolve the system, again, until it stabilizes. \n \nFor each perturbation, they measure the total number of cells that are affected by the resulting avalanche. Most of the time it is small, usually 1. But occasionally a large avalanche affects a substantial fraction \nof the grid. The distribution of turns out to be long-tailed, which supports the claim that the system is in a critical state. \n \n[ Think Complexity presents the details of this model and tests for long-tailed distributions, fractal geometry, and 1/f noise. For this excerpt, I'll skip to the discussion at the end of the chapter.] \n \n \n Reductionism and Holism \n \nThe original paper by Bak, Tang and Wiesenfeld is one of the most frequently-cited papers in the last few decades. Many new systems have been shown to be self-organized critical, and the sand-pile model, in particular, has been studied in detail. \n \nAs it turns out, the sand-pile model is not a very good model of a sand pile. Sand is dense and not very sticky, so momentum has a non-negligible effect on the behavior of avalanches. As a result, there are fewer very large and very small avalanches than the model predicts, and the distribution is not long tailed. \n \nBak has suggested that this observation misses the point. The sand pile model is not meant to be a realistic model of a sand pile; it is meant to be a simple example of a broad category of models. \n \nTo understand this point, it is useful to think about two kinds of models, reductionist and holistic. A reductionist model describes a system by describing its parts and their interactions. When a reductionist model is used as an explanation, it depends on an analogy between the components of the model and the components of the system. \n \nFor example, to explain why the ideal gas law holds, we can model the molecules that make up a gas with point masses, and model their interactions as elastic collisions. If you simulate or analyze this model, you find that it obeys the ideal gas law. This model is satisfactory to the degree that molecules in a gas behave like molecules in the model. The analogy is between the parts of the system and the parts of the model. \n \nHolistic models are more focused on similarities between systems and less interested in analogous parts. A holistic approach to modeling often consists of two steps, not necessarily in this order: \n \n1. Identify a kind of behavior that appears in a variety of systems. \n \n2. Find the simplest model that demonstrates that behavior. \n \nFor example, in The Selfish Gene , Richard Dawkins suggests that genetic evolution is just one example of an evolutionary system. He identifies the essential elements of the category---discrete replicators, variability and differential reproduction---and proposes that any system that has these elements displays similar behavior, including complexity without design. As another example of an evolutionary system, he proposes memes, which are thoughts or behaviors that are ``replicated'' by transmission from person to person. As memes compete for the resource of human attention, they evolve in ways that are similar to genetic evolution. \n \nCritics of memetics have pointed out that memes are a poor analogy for genes. Memes differ from genes in many obvious ways. But Dawkins has argued that these differences are beside the point because memes are not supposed to be analogous to genes. Rather, memetics and genetics are examples of the same category---evolutionary systems. The differences between them emphasize the real point, which is that evolution is a general model that applies to many seemingly disparate systems. The logical structure of this argument is shown in this diagram: \n \n  \nBak has made a similar argument that self-organized criticality is a general model for a broad category of systems. According to Wikipedia, ``SOC is typically observed in slowly-driven non-equilibrium systems with extended degrees of freedom and a high level of nonlinearity.'' \n \nMany natural systems demonstrate behaviors characteristic of critical systems. Bak's explanation for this prevalence is that these systems are examples of the broad category of self-organized criticality. There are two ways to support this argument. One is to build a realistic model of a particular system and show that the model exhibits SOC. The second is to show that SOC is a feature of many diverse models, and to identify the essential characteristics those models have in common. \n \nThe first approach, which I characterize as reductionist, can explain the behavior of a particular system. The second, holistic, approach, explains the prevalence of criticality in natural systems. They are different models with different purposes. \n \nFor reductionist models, realism is the primary virtue, and simplicity is secondary. For holistic models, it is the other way around. \n \nI am using \"reductionism\" and \"holism\" here is a descriptive sense, not as technical labels for these models. For more general discussion of these terms, see http://en.wikipedia.org/wiki/Reductionism and http://en.wikipedia.org/wiki/Holism . \n \n \n \n \n \n \n SOC, causation and prediction \n \nIf a stock market index drops by a fraction of a percent in a day, there is no need for an explanation. But if it drops 10, people want to know why. Pundits on television are willing to offer explanations, but the real answer may be that there is no explanation. \n \nDay-to-day variability in the stock market shows evidence of criticality: the distribution of value changes is long-tailed and the time series exhibits noise. If the stock market is a self-organized critical system, we should expect occasional large changes as part of the ordinary behavior of the market. \n \nThe distribution of earthquake sizes is also long-tailed, and there are simple models of the dynamics of geological faults that might explain this behavior. If these models are right, they imply that large earthquakes are unexceptional; that is, they do not require explanation any more than small earthquakes do. \n \nSimilarly, Charles Perrow has suggested that failures in large engineered systems, like nuclear power plants, are like avalanches in the sand pile model. Most failures are small, isolated and harmless, but occasionally a coincidence of bad fortune yields a catastrophe. When big accidents occur, investigators go looking for the cause, but if Perrow's ``normal accident theory'' is correct, there may be no cause. \n \nThese conclusions are not comforting. Among other things, they imply that large earthquakes and some kinds of accidents are fundamentally unpredictable. It is impossible to look at the state of a critical system and say whether a large avalanche is ``due.'' If the system is in a critical state, then a large avalanche is always possible. It just depends on the next grain of sand. \n \nIn a sand-pile model, what is the cause of a large avalanche? Philosophers sometimes distinguish the proximate cause, which is most immediately responsible, from the ultimate cause, which is, for whatever reason, considered the true cause. \n \nIn the sand-pile model, the proximate cause of an avalanche is a grain of sand, but the grain that causes a large avalanche is identical to any other grain, so it offers no special explanation. The ultimate cause of a large avalanche is the structure and dynamics of the systems as a whole: large avalanches occur because they are a property of the system. \n \nMany social phenomena, including wars, revolutions, epidemics, inventions and terrorist attacks, are characterized by long-tailed distributions. If the reason for these distributions is that social systems are critical, that suggests that major historical events may be fundamentally unpredictable and unexplainable. \n \n \n \n Questions \n \n[Think Complexity can be used as a textbook, so it includes exercises and topics for class discussion. Here are some ideas for discussion and further reading.] \n \n \n1. In a 1996 paper in Nature , Frette et al report the results of experiments with rice piles ( http://www.nature.com/nature/journal/v379/n6560/abs/379049a0.html ). They find that some kinds of rice yield evidence of critical behavior, but others do not. \n \nSimilarly, Pruessner and Jensen studied large-scale versions of the forest fire model (using an algorithm similar to Newman and Ziff's). In their 2004 paper, ``Efficient algorithm for the forest fire model,'' they present evidence that the system is not critical after all ( http://pre.aps.org/abstract/PRE/v70/i6/e066707 ). How do these results bear on Bak's claim that SOC explains the prevalence of critical phenomena in nature? \n \n2. In The Fractal Geometry of Nature , Benoit Mandelbrot proposes what he calls a ``heretical'' explanation for the prevalence of long-tailed distributions in natural systems (page 344). It may not be, as Bak suggests, that many systems can generate this behavior in isolation. Instead there may be only a few, but there may be interactions between systems that cause the behavior to propagate. \n \nTo support this argument, Mandelbrot points out: \n \n The distribution of observed data is often ``the joint effect of a fixed underlying 'true distribution' and a highly variable 'filter.''' \n Long-tailed distributions are robust to filtering; that is, ``a wide variety of filters leave their asymptotic behavior unchanged.'' \n \nWhat do you think of this argument? Would you characterize it as reductionist or holist? \n \n \n3. Read about the ``Great Man'' theory of history at http://en.wikipedia.org/wiki/Great_man_theory . What implication does self-organized criticality have for this theory?"], "link": "http://allendowney.blogspot.com/feeds/1028680593186399708/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://greenteapress.com/": 1, "http://thinkcomplex.com/": 2, "http://pre.aps.org/": 1, "http://prl.aps.org/": 1, "http://allendowney.blogspot.com/": 4, "http://www.nature.com/": 1, "http://en.wikipedia.org/": 3, "http://shop.oreilly.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["My new book, Think Complexity , will be published by O'Reilly Media in March. For people who can't stand to wait that long, I am publishing excerpts here. If you really can't wait, you can read the free version at thinkcomplex.com . \n \n And we need a blurb . Think Complexity goes to press soon and we have a space on the back cover for a couple of endorsements. If you like the book and have something quotable to say about it, let me know. Thanks! \n \nIn Part One I outline the topics in Think Complexity and contrasted a classical physical model of planetary orbits with an example from complexity science: Schelling's model of racial segregation. \n \nIn Part Two I outline some of the ways complexity differs from classical science. In Part Three , I describe differences in the ways complex models are used, and their effects in engineering and (of all things) epistemology. \n \nIn this installment, I pull together discussions from two chapters: the Watts-Strogatz model of small world graphs, and the Barabasi-Albert model of scale free networks. But it all starts with Stanley Milgram. \n \n \n Stanley Milgram \n \nStanley Milgram was an American social psychologist who conducted two of the most famous experiments in social science, the Milgram experiment, which studied people's obedience to authority ( http://en.wikipedia.org/wiki/Milgram_experiment ) and the Small World Experiment ( http://en.wikipedia.org/wiki/Small_world_phenomenon ), which studied the structure of social networks. \n \nIn the Small World Experiment, Milgram sent a package to several randomly-chosen people in Wichita, Kansas, with instructions asking them to forward an enclosed letter to a target person, identified by name and occupation, in Sharon, Massachusetts (which is the town near Boston where I grew up). The subjects were told that they could mail the letter directly to the target person only if they knew him personally; otherwise they were instructed to send it, and the same instructions, to a relative or friend they thought would be more likely to know the target person. \n \nMany of the letters were never delivered, but of the ones that were it turned out that the average path length---the number of times the letters were forwarded---was about six. This result was taken to confirm previous observations (and speculations) that the typical distance between any two people in a social network is about ``six degrees of separation.'' \n \nThis conclusion is surprising because most people expect social networks to be localized---people tend to live near their friends---and in a graph with local connections, path lengths tend to increase in proportion to geographical distance. For example, most of my friends live nearby, so I would guess that the average distance between nodes in a social network is about 50 miles. Wichita is about 1600 miles from Boston, so if Milgram's letters traversed typical links in the social network, they should have taken 32 hops, not six. \n \n Watts and Strogatz \n \nIn 1998 Duncan Watts and Steven Strogatz published a paper in Nature , ``Collective dynamics of 'small-world' networks,'' that proposed an explanation for the small world phenomenon. You can download it from http://www.nature.com/nature/journal/v393/n6684/abs/393440a0.html . \n \nWatts and Strogatz started with two kinds of graph that were well understood: random graphs and regular graphs. They looked at two properties of these graphs, clustering and path length. \n \nClustering is a measure of the ``cliquishness'' of the graph. In a graph, a clique is a subset of nodes that are all connected to each other; in a social network, a clique is a set of friends who all know each other. Watts and Strogatz defined a clustering coefficient that quantifies the likelihood that two nodes that are connected to the same node are also connected to each other. \n \nPath length is a measure of the average distance between two nodes, which corresponds to the degrees of separation in a social network. \n \nTheir initial result is what you might expect: regular graphs have high clustering and high path lengths; random graphs with the same size tend to have low clustering and low path lengths. So neither of these is a good model of social networks, which seem to combine high clustering with short path lengths. \n \nTheir goal was to create a generative model of a social network. A generative model tries to explain a phenomenon by modeling the process that builds or leads to the phenomenon. In this case Watts and Strogatz proposed a process for building small-world graphs: \n \n Start with a regular graph with n nodes and degree k. Watts and Strogatz start with a ring lattice, which is a kind of regular graph. You could replicate their experiment or try instead a graph that is regular but not a ring lattice. \n Choose a subset of the edges in the graph and ``rewire'' them by replacing them with random edges. Again, you could replicate the procedure described in the paper or experiment with alternatives. The proportion of edges that are rewired is a parameter, p, that controls how random the graph is. With p=0, the graph is regular; with p=1 it is random. \n Watts and Strogatz found that small values of p yield graphs with high clustering, like a regular graph, and low path lengths, like a random graph. \n \n \n Barabasi and Albert \n \nIn 1999 Barabasi and Albert published a paper in Science, ``Emergence of Scaling in Random Networks,'' that characterizes the structure (also called ``topology'') of several real-world networks, including graphs that represent the interconnectivity of movie actors, world-wide web (WWW) pages, and elements in the electrical power grid in the western United States. You can download the paper from http://www.sciencemag.org/content/286/5439/509 . \n \nThey measure the degree (number of connections) of each node and compute P(k), the probability that a vertex has degree k; then they plot P(k) versus k on a log-log scale. The tail of the plot fits a straight line, so they conclude that it obeys a power law; that is, as k gets large, P(k) is asymptotic to k^(- \u03b3) , where \u03b3 is a parameter that determines the rate of decay. \n \nThey also propose a model that generates random graphs with the same property. The essential features of the model, which distinguish it from the model and the Watts-Strogatz model, are: \n \n Growth : Instead of starting with a fixed number of vertices, Barabasi and Albert start with a small graph and add vertices gradually. \n \n Preferential attachment : When a new edge is created, it is more likely to connect to a vertex that already has a large number of edges. This ``rich get richer'' effect is characteristic of the growth patterns of some real-world networks. \n \nFinally, they show that graphs generated by this model have a distribution of degrees that obeys a power law. Graphs that have this property are sometimes called scale-free networks; see http://en.wikipedia.org/wiki/Scale-free_network . That name can be confusing because it is the distribution of degrees that is scale-free, not the network. \n \nIn order to maximize confusion, distributions that obey a power law are sometimes called scaling distributions because they are invariant under a change of scale. That means that if you change the units the quantities are expressed in, the slope parameter, \u03b3 , doesn't change. You can read http://en.wikipedia.org/wiki/Power_law for the details, but it is not important for what we are doing here. \n \n Explanatory models \n \n  \nWe started the discussion of networks with Milgram's Small World Experiment, which shows that path lengths in social networks are surprisingly small; hence, ``six degrees of separation''. When we see something surprising, it is natural to ask ``Why?'' but sometimes it's not clear what kind of answer we are looking for. \n \nOne kind of answer is an explanatory model. The logical structure of an explanatory model is: \n \n In a system, S, we see something observable, O, that warrants explanation. \n We construct a model, M, that is analogous to the system; that is, there is a correspondence between the elements of the model and the elements of the system. \n By simulation or mathematical derivation, we show that the model exhibits a behavior, B, that is analogous to O. \n We conclude that S exhibits O because S is similar to M, M exhibits B, and B is similar to O. \n \nAt its core, this is an argument by analogy, which says that if two things are similar in some ways, they are likely to be similar in other ways. Argument by analogy can be useful, and explanatory models can be satisfying, but they do not constitute a proof in the mathematical sense of the word. \n \nRemember that all models leave out, or ``abstract away'' details that we think are unimportant. For any system there are many possible models that include or ignore different features. And there might be models that exhibit different behaviors, B, B' and B'', that are similar to O in different ways. In that case, which model explains O? \n \nThe small world phenomenon is an example: the Watts-Strogatz (WS) model and the (BA) model both exhibit small world behavior, but they offer different explanations: \n \n The WS model suggests that social networks are ``small'' because they include both strongly-connected clusters and ``weak ties'' that connect clusters. \n The BA model suggests that social networks are small because they include nodes with high degree that act as hubs, and that hubs grow, over time, due to preferential attachment. \n \nAs is often the case in young areas of science, the problem is not that we have no explanations, but too many. \n \n Questions \n \n[Think Complexity can be used as a textbook, so it includes exercises and topics for class discussion. Here are some ideas for discussion and further reading.] \n \nAre these explanations compatible; that is, can they both be right? Which do you find more satisfying as an explanation, and why? Is there data you could collect, or an experiment you could perform, that would provide evidence in favor of one model over the other? \n \nChoosing among competing models is the topic of Thomas Kuhn's essay, ``Objectivity, Value Judgment, and Theory Choice.'' You can download it here in PDF. What criteria does Kuhn propose for choosing among competing models? Do these criteria influence your opinion about the WS and BA models? Are there other criteria you think should be considered?"], "link": "http://allendowney.blogspot.com/feeds/8232137973385805329/comments/default", "bloglinks": {}, "links": {"http://www.sciencemag.org/": 1, "http://commonsenseatheism.com/": 1, "http://feedads.doubleclick.net/": 2, "http://greenteapress.com/": 1, "http://thinkcomplex.com/": 2, "http://1.blogspot.com/": 1, "http://allendowney.blogspot.com/": 3, "http://www.nature.com/": 1, "http://en.wikipedia.org/": 4, "http://shop.oreilly.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["My new book, Think Complexity , will be published by O'Reilly Media in March. For people who can't stand to wait that long, I am publishing excerpts here. If you really can't wait, you can read the free version at thinkcomplex.com . \n \nIn Part One I outline the topics in Think Complexity and contrasted a classical physical model of planetary orbits with an example from complexity science: Schelling's model of racial segregation. \n \nIn Part Two I outline some of the ways complexity differs from classical science. In this installment, I describe differences in the ways complex models are used, and their effects in engineering and (of all things) epistemology. \n \n A new kind of model \n \n \nComplex models are often appropriate for different purposes and interpretations: \n \nPredictive \u2192 explanatory: Schelling's model of segregation might shed light on a complex social phenomenon, but it is not useful for prediction. On the other hand, a simple model of celestial mechanics can predict solar eclipses, down to the second, years in the future. \n \nRealism \u2192 instrumentalism: Classical models lend themselves to a realist interpretation; for example, most people accept that electrons are real things that exist. Instrumentalism is the view that models can be useful even if the entities they postulate don't exist. George Box wrote what might be the motto of instrumentalism: ``All models are wrong, but some are useful.\" \n \nReductionism \u2192 holism: Reductionism is the view that the behavior of a system can be explained by understanding its components. For example, the periodic table of the elements is a triumph of reductionism, because it explains the chemical behavior of elements with a simple model of the electrons in an atom. Holism is the view that some phenomena that appear at the system level do not exist at the level of components, and cannot be explained in component-level terms. \n \n A new kind of engineering \n \nI have been talking about complex systems in the context of science, but complexity is also a cause, and effect, of changes in engineering and the organization of social systems: \n \nCentralized \u2192 decentralized: Centralized systems are conceptually simple and easier to analyze, but decentralized systems can be more robust. For example, in the World Wide Web clients send requests to centralized servers; if the servers are down, the service is unavailable. In peer-to-peer networks, every node is both a client and a server. To take down the service, you have to take down every node. \n \nIsolation \u2192 interaction: In classical engineering, the complexity of large systems is managed by isolating components and minimizing interactions. This is still an important engineering principle; nevertheless, the availability of cheap computation makes it increasingly feasible to design systems with complex interactions between components. \n \nOne-to-many \u2192 many-to-many: In many communication systems, broadcast services are being augmented, and sometimes replaced, by services that allow users to communicate with each other and create, share, and modify content. \n \nTop-down \u2192 bottom-up: In social, political and economic systems, many activities that would normally be centrally organized now operate as grassroots movements. Even armies, which are the canonical example of hierarchical structure, are moving toward devolved command and control. \n \nAnalysis \u2192 computation: In classical engineering, the space of feasible designs is limited by our capability for analysis. For example, designing the Eiffel Tower was possible because Gustave Eiffel developed novel analytic techniques, in particular for dealing with wind load. Now tools for computer-aided design and analysis make it possible to build almost anything that can be imagined. Frank Gehry's Guggenheim Museum Bilbao is my favorite example. \n \nDesign \u2192 search: Engineering is sometimes described as a search for solutions in a landscape of possible designs. Increasingly, the search process can be automated. For example, genetic algorithms explore large design spaces and discover solutions human engineers would not imagine (or like). The ultimate genetic algorithm, evolution, notoriously generates designs that violate the rules of human engineering. \n \n A new kind of thinking \n \nWe are getting farther afield now, but the shifts I am postulating in the criteria of scientific modeling are related to 20th Century developments in logic and epistemology. \n \nAristotelian logic \u2192 many-valued logic: In traditional logic, any proposition is either true or false. This system lends itself to math-like proofs, but fails (in dramatic ways) for many real-world applications. Alternatives include many-valued logic, fuzzy logic, and other systems designed to handle indeterminacy, vagueness, and uncertainty. Bart Kosko discusses some of these systems in Fuzzy Thinking . \n \nFrequentist probability \u2192 Bayesianism: Bayesian probability has been around for centuries, but was not widely used until recently, facilitated by the availability of cheap computation and the reluctant acceptance of subjectivity in probabilistic claims. Sharon Bertsch McGrayne presents this history in The Theory That Would Not Die . \n \nObjective \u2192 subjective: The Enlightenment, and philosophic modernism, are based on belief in objective truth; that is, truths that are independent of the people that hold them. 20th Century developments including quantum mechanics, Godel's Incompleteness Theorem, and Kuhn's study of the history of science called attention to seemingly unavoidable subjectivity in even ``hard sciences'' and mathematics. Rebecca Goldstein presents the historical context of Godel's proof in Incompleteness . \n \nPhysical law \u2192 theory \u2192 model: Some people distinguish between laws, theories, and models, but I think they are the same thing. People who use ``law'' are likely to believe that it is objectively true and immutable; people who use ``theory'' concede that it is subject to revision; and ``model'' concedes that it is based on simplification and approximation. \n \nSome concepts that are called ``physical laws'' are really definitions; others are, in effect, the assertion that a model predicts or explains the behavior of a system particularly well. I discuss the nature of physical models later in Think Complexity . \n \nDeterminism \u2192 indeterminism: Determinism is the view that all events are caused, inevitably, by prior events. Forms of indeterminism include randomness, probabilistic causation, and fundamental uncertainty. We come back to this topic later in the book. \n \nThese trends are not universal or complete, but the center of opinion is shifting along these axes. As evidence, consider the reaction to Thomas Kuhn's The Structure of Scientific Revolutions, which was reviled when it was published and now considered almost uncontroversial. \n \nThese trends are both cause and effect of complexity science. For example, highly abstracted models are more acceptable now because of the diminished expectation that there should be unique correct model for every system. Conversely, developments in complex systems challenge determinism and the related concept of physical law. \n \nThe excerpts so far have been from Chapter 1 of Think Complexity . Future excerpts will go into some of these topics in more depth. In the meantime, you might be interested in this timeline of complexity science (from Wikipedia):"], "link": "http://allendowney.blogspot.com/feeds/5850683576521043061/comments/default", "bloglinks": {}, "links": {"http://upload.wikimedia.org/": 1, "http://feedads.doubleclick.net/": 2, "http://greenteapress.com/": 1, "http://thinkcomplex.com/": 2, "http://allendowney.blogspot.com/": 2, "http://shop.oreilly.com/": 1, "http://amzn.to/": 3}, "blogtitle": "Probably Overthinking It"}, {"content": ["[This is an update of an article I wrote last year, \" Freshman hordes more godless than ever .\" There is a followup to this article here .] \n \nFor several years I have been following one of the most under-reported stories of the decade: the fraction of college freshmen who report no religious preference has more than tripled since 1985, from 8% to 25%, and the trend is accelerating. \n \nSimilarly, students reporting that in the last year they have never attended a religious service has grown from 8% to more than 27%. \n \n My analysis is based on survey results from the Cooperative Institutional Research Program (CIRP) of the Higher Education Research Insitute (HERI) . In 2011, more than 200,000 students at 270 colleges and universities completed the CIRP Freshman Survey, which includes questions about students\u2019 backgrounds, activities, and attitudes. \n \n \n In one question, students select their \u201ccurrent religious preference,\u201d from a choice of seventeen common religions, \u201cOther religion,\u201d or \u201cNone.\u201d \n \n \n Another question asks students how often they \u201cattended a religious service\u201d in the last year. The choices are \u201cFrequently,\u201d \u201cOccasionally,\u201d and \u201cNot at all.\u201d Students are instructed to select \u201cOccasionally\u201d if they attended one or more times. \n \n \n \n \nThi s figure shows students' responses over the history of the survey: \n \n \n \n It's clear that both measurements are increasing, and it looks like they might be accelerating. To make that more precise , I fit a parabola to each curve. This figure shows the data for \"No religion\" and a least squares fit: \n \n \n \n The red line shows the fitted model; the dark gray area shows the sample error of the model. The lighter gray area shows the sum of the sampling error and the residual error. R \u00b2 for this model is 0.95; the p-values for the model and the parameters are < 0.001. \n \n \n Similarly, here is the data for \"No attendance\" and a least squares fit: \n \n \n R \u00b2 for this model is 0.89, and again the p-values are effectively 0. [Note for stats geeks: as it happens, the coefficient of the linear term is close to zero, so it is not statistically significant. My first thought was to remove it from the model, but if I did that, I would understate the sampling error of the model. In this case, it is correct to keep an \"insignificant\" variable in the model; the fact that it is near zero doesn't mean we can ignore it, or the error associated with it.] \n \n \n To test more explicitly whether the growth is accelerating, I computed the change from one year to the next in percentage points. The following figure shows these changes and a least squares fit: \n \n \n Subtraction amplifies noise, so for this model R \u00b2 is only 0.24, but the p-value of the slope is 0.002, so this data provides strong evidence of acceleration. The slope is 0.035 percentage points per year. \n \n \n Based on this model, the predicted change for next year is 1 percentage point, so we expect the fraction of freshmen reporting no religious preference to be 26%. \n \n \n The gender gap \n \n \n Since the beginning of the CIRP survey, more men than women have reported no religious preference: \n \n \nAnd the gender gap seems to be growing. Here is the difference between men and women, in percentage points, and a least squares fit: \n \n \n R \u00b2 for this model is 0.44; the slope is 0.035 percentage points per year, with p-value < 0.0001. \n \n Discussion \n \n \n \n I first wrote about this in 2007, in this article for Free Inquiry magazine. There I wrote: \n  \n \n College students are hardly a random sample of the population. People with more education are less likely to believe in heaven, the devil, miracles, and the literal truth of the Christian Bible. However, contrary to many people\u2019s expectations, educated people are more likely to attend services. So, we expect the students in this sample to be less believing than the general population but also more observant. \n \n There is reason to think that the rate of secularization in the general population is faster than what we see in this sample. Over the lifetime of the CIRP survey, college education has democratized; the percentage of high-school graduates who enter college immediately after graduation has increased from roughly 50 percent in 1970 to 65 percent in 2003. Over this time, CIRP has included more poor students, more racial minorities, and more students from families with less education. These groups tend to be more religious than the general population, so we expect their participation to increase the religiosity in the sample. Thus, the observed decrease probably underestimates the trend in the general population. \n \n The theory of secularization\u2014that there is a global, long-term trend away from religion\u2014is controversial. Early sociologists, notably Max Weber, hypothesized that secularization is a predictable effect of rationalization\u2014the increasing tendency for social actions to be based on reason rather than emotion or tradition.  \n \n  \n \n \n In the 1950s and 1960s, many sociologists of religion defended strong theories of secularization, but, since then, several of them\u2014including Peter Berger and Harvey Cox\u2014have reversed their positions, arguing that religion is resurging in some areas, including the United States. \n \n The data presented here speak directly to this debate. The CIRP survey has posed almost the same questions to a large sample of a consistently defined group for almost forty years, and the results show a clear and consistent trend away from both identification with religious sects and participation in religious services. These data make a strong case for secularization in the United States that has, if anything, accelerated in the last decade. \n \n \n \n Data Source \n \nData from the 2011 CIRP Survey are reported i n The American Freshman: National Norms for Fall 2011 ,  Pryor, J. H., DeAngelo, L., Palucki Blake, L., Hurtado, S., & Tran, S ., Jan 2012. \n \n \n This and all previous reports are available from the HERI publications page ."], "link": "http://allendowney.blogspot.com/feeds/3812367456227674137/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://www.secularhumanism.org/": 2, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 3, "http://4.blogspot.com/": 1, "http://allendowney.blogspot.com/": 2, "http://2.blogspot.com/": 1, "http://www.ucla.edu/": 4}, "blogtitle": "Probably Overthinking It"}, {"content": ["My new book, Think Complexity , will be published by O'Reilly Media in March. For people who can't stand to wait that long, I am publishing excerpts here. If you really can't wait, you can read the free version at thinkcomplex.com . \n \nIn the previous installment I outlined the topics in Think Complexity and contrasted a classical physical model of planetary orbits with an example from complexity science: Schelling's model of racial segregation. In this installment I outline some of the ways complexity differs from classical science. \n \n Paradigm shift? \n \nWhen I describe this book to people, I am often asked if this new kind of science is a paradigm shift. I don't think so, and here's why. Thomas Kuhn introduced the term ``paradigm shift'' in The Structure of Scientific Revolutions in 1962. It refers to a process in the history of science where the basic assumptions of a field change, or where one theory is replaced by another. He presents as examples the Copernican revolution, the displacement of phlogiston by the oxygen model of combustion, and the emergence of relativity. \n \nThe development of complexity science is not the replacement of an older model, but (in my opinion) a gradual shift in the criteria models are judged by, and in the kinds of models that are considered acceptable. For example, classical models tend to be law-based, expressed in the form of equations, and solved by mathematical derivation. Models that fall under the umbrella of complexity are often rule-based, expressed as computations, and simulated rather than analyzed. Not everyone finds these models satisfactory. \n \nFor example, in Sync , Steven Strogatz writes about his model of spontaneous synchronization in some species of fireflies. He presents a simulation that demonstrates the phenomenon, but then writes: \n I repeated the simulation dozens of times, for other random initial conditions and for other numbers of oscillators. Sync every time. [...] The challenge now was to prove it. Only an ironclad proof would demonstrate, in a way that no computer ever could, that sync was inevitable; and the best kind of proof would clarify why it was inevitable. Strogatz is a mathematician, so his enthusiasm for proofs is understandable, but his proof doesn't address what is, to me, the most interesting part the phenomenon. In order to prove that ``sync was inevitable,'' Strogatz makes several simplifying assumptions, in particular that each firefly can see all the others. \n \nIn my opinion, it is more interesting to explain how an entire valley of fireflies can synchronize despite the fact that they cannot all see each other. Think Complexity discusses how this kind of global behavior emerges from local interactions. Explanations of these phenomena often use agent-based models, which explore (in ways that would be difficult or impossible with mathematical analysis) the conditions that allow or prevent synchronization. \n \nI am a computer scientist, so my enthusiasm for computational models is probably no surprise. I don't mean to say that Strogatz is wrong, but rather that people disagree about what questions to ask and what tools to use to answer them. These decisions are based on value judgments, so there is no reason to expect agreement. Nevertheless, there is rough consensus among scientists about which models are considered good science, and which others are fringe science, pseudoscience, or not science at all. \n \nI claim, and this is a central thesis of the book, that the criteria this consensus is based on change over time, and that the emergence of complexity science reflects a gradual shift in these criteria. \n \n The axes of scientific models \n \nI have described classical models as based on physical laws, expressed in the form of equations, and solved by mathematical analysis; conversely, models of complexity systems are often based on simple rules and implemented as computations. We can think of this trend as a shift over time along two axes: \n \nEquation-based \u2192 simulation-based \nAnalysis \u2192 computation \n \nThe new kind of science is different in several other ways: \n \nContinuous \u2192 discrete: Classical models tend to be based on continuous mathematics, like calculus; models of complex systems are often based on discrete mathematics, including graphs and cellular automata. \n \nLinear \u2192 non-linear: Classical models are often linear, or use linear approximations to non-linear systems; complexity science is more friendly to non-linear models. One example is chaos theory, which explores the dynamics of systems of non-linear equations. \n \nDeterministic \u2192 stochastic: Classical models are usually deterministic, which may reflect underlying philosophical determinism; complex models often feature randomness. \n \nAbstract \u2192 detailed: In classical models, planets are point masses, planes are frictionless, and cows are spherical (see http://en.wikipedia.org/wiki/Spherical_cow ). Simplifications like these are often necessary for analysis, but computational models can be more realistic. \n \nOne, two \u2192 many: In celestial mechanics, the two-body problem can be solved analytically; the three-body problem cannot. Where classical models are often limited to small numbers of interacting elements, complexity science works with larger complexes (which is where the name comes from). \n \nHomogeneous \u2192 composite: In classical models, the elements tend to be interchangeable; complex models more often include heterogeneity. \n \nThese are generalizations, so we should not take them too seriously. And I don't mean to deprecate classical science. A more complicated model is not necessarily better; in fact, it is usually worse. \n \nAlso, I don't mean to say that these changes are abrupt or complete. Rather, there is a gradual migration in the frontier of what is considered acceptable, respectable work. Some tools that used to be regarded with suspicion are now common, and some models that were widely accepted are now regarded with scrutiny. For example, when Appel and Haken proved the four-color theorem in 1976, they used a computer to enumerate 1,936 special cases that were, in some sense, lemmas of their proof. At the time, many mathematicians did not consider the theorem truly proved. Now computer-assisted proofs are common and generally (but not universally) accepted. \n \nConversely, a substantial body of economic analysis is based on a model of human behavior called ``Economic man,'' or, with tongue in cheek, Homo economicus . Research based on this model was highly-regarded for several decades, especially if it involved mathematical virtuosity. More recently, this model is treated with more skepticism, and models that include imperfect information and bounded rationality are hot topics. \n \nAt this point I have laid out a lot of ideas for one article, and explained them very briefly. Think Complexity gets into these topics in more detail, but I will stop here for now. Next time I will talk about related shifts in engineering and (a little farther afield) in ways of thinking."], "link": "http://allendowney.blogspot.com/feeds/1987200698901322045/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://greenteapress.com/": 1, "http://thinkcomplex.com/": 3, "http://allendowney.blogspot.com/": 1, "http://en.wikipedia.org/": 1, "http://shop.oreilly.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["Think Complexity \n \nOn Friday I turned in the manuscript for Think Complexity , which will be published by O'Reilly Media in March (or, at least, that's the schedule). For people who can't stand to wait that long, I am going to publish excerpts here. And if you really can't wait, you can read the free version at thinkcomplex.com . \n \nThe book is about data structures and algorithms, intermediate programming in Python, computational modeling and the philosophy of science: \n \n Data structures and algorithms: A data structure is a collection of data elements organized in a way that supports particular operations. For example, a Python dictionary organizes key-value pairs in a way that provides fast mapping from keys to values, but mapping from values to keys is slower. An algorithm is a mechanical process for performing a computation. Designing efficient programs often involves the co-evolution of data structures and the algorithms that use them. For example, in the first few chapters I present graphs, data structures that implement graphs, and graph algorithms based on those data structures. \n Python programming: Think Complexity picks up where Think Python leaves off. I assume that you have read that book or have equivalent knowledge of Python. I try to emphasize fundamental ideas that apply to programming in many languages, but along the way I present some useful features that are specific to Python. \n Computational modeling: A model is a simplified description of a system used for simulation or analysis. Computational models are designed to take advantage of cheap, fast computation. \n Philosophy of science: The experiments and results in this book raise questions relevant to the philosophy of science, including the nature of scientific laws, theory choice, realism and instrumentalism, holism and reductionism, and epistemology. \n \nThe book is also about complexity science, which is an interdisciplinary field---at the intersection of mathematics, computer science and natural science---that focuses on discrete models of physical systems. In particular, it focuses on complex systems, which are systems with many interacting components. \n \nComplex systems include networks and graphs, cellular automata, agent-based models and swarms, fractals and self-organizing systems, chaotic systems and cybernetic systems. Here is a framework for these topics (from http://en.wikipedia.org/wiki/Complex_systems ). \n \n  \n \n A new kind of science? \n \nIn 2002 Stephen Wolfram published A New Kind of Science where he presents his and others' work on cellular automata and describes a scientific approach to the study of computational systems. There's a chapter about cellular automata in the book, but for now I'm going to borrow his title for something a little broader. \n \nI think complexity is a ``new kind of science'' not because it applies the tools of science to a new subject, but because it uses different tools, allows different kinds of work, and ultimately changes what we mean by ``science.'' \n \nTo demonstrate the difference, I'll start with an example of classical science: suppose someone asked you why planetary orbits are elliptical. You might invoke Newton's law of universal gravitation and use it to write a differential equation that describes planetary motion. Then you could solve the differential equation and show that the solution is an ellipse. Voila! \n \nMost people find this kind of explanation satisfying. It includes a mathematical derivation---so it has some of the rigor of a proof---and it explains a specific observation, elliptical orbits, by appealing to a general principle, gravitation. \n \nLet me contrast that with a different kind of explanation. Suppose you move to a city like Detroit that is racially segregated, and you want to know why it's like that. If you do some research, you might find a paper by Thomas Schelling called ``Dynamic Models of Segregation,'' which proposes a simple model of racial segregation (you can download the paper here ). \n \nHere is an outline of the paper's primary experiment: \n \n The Schelling model of the city is an array of cells where each cell represents a house. The houses are occupied by two kinds of agents , labeled red and blue, in roughly equal numbers. About 10% of the houses are empty. \n At any point in time, an agent might be happy or unhappy, depending on the other agents in the neighborhood. In one version of the model, agents are happy if they have at least two neighbors like themselves, and unhappy if they have one or zero. \n The simulation proceeds by choosing an agent at random and checking to see whether it is happy. If so, nothing happens; if not, the agent chooses one of the unoccupied cells at random and moves. \n \nIf you start with a simulated city that is entirely unsegregated and run the model for a short time, clusters of similar agents appear. As time passes, the clusters grow and coalesce until there are a small number of large clusters and most agents live in homogeneous neighborhoods. \n \nThe degree of segregation in the model is surprising, and it suggests an explanation of segregation in real cities. Maybe Detroit is segregated because people prefer not to be greatly outnumbered and will move if the composition of their neighborhoods makes them unhappy.  \n \nIs this explanation satisfying in the same way as the explanation of planetary motion? Most people would say not, but why? \n \nMost obviously, the Schelling model is highly abstract, which is to say not realistic. It is tempting to say that people are more complex than planets, but when you think about it, planets are just as complex as people (especially the ones that have people). \n \nBoth systems are complex, and both models are based on simplifications; for example, in the model of planetary motion we include forces between the planet and its sun, and ignore interactions between planets. \n \nThe important difference is that, for planetary motion, we can defend the model by showing that the forces we ignore are smaller than the ones we include. And we can extend the model to include other interactions and show that the effect is small. For Schelling's model it is harder to justify the simplifications. \n \nTo make matters worse, Schelling's model doesn't appeal to any physical laws, and it uses only simple computation, not mathematical derivation. Models like Schelling's don't look like classical science, and many people find them less compelling, at least at first. But as I will try to demonstrate, these models do useful work, including prediction, explanation, and design. One of the goals of Think Complexity is to explain how. \n \nIn the next excerpt, I will present some of the ways I think complexity science differs from classical science. But in the meantime I welcome comments on Part One."], "link": "http://allendowney.blogspot.com/feeds/5221337829151532586/comments/default", "bloglinks": {}, "links": {"http://statistics.berkeley.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://greenteapress.com/": 3, "http://thinkcomplex.com/": 1, "http://upload.wikimedia.org/": 1, "http://en.wikipedia.org/": 1, "http://shop.oreilly.com/": 1, "http://www.wolframscience.com/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["I have a soft spot for crank science. Recently I visited Norumbega Tower , which is an enduring monument to the crackpot theories of Eben Norton Horsford , inventor of double-acting baking powder and faux history. But that's not what this article is about. \n \nThis article is about the Variability Hypothesis , which (quoth Wikipedia): \n \"originated in the early nineteenth century with Johann Meckel, who argued that males have a greater range of ability than females, especially in intelligence. In other words, he believed that most geniuses and most mentally retarded people are men. Because he considered males to be the 'superior animal,' Meckel concluded that females' lack of variation was a sign of inferiority.\" I particularly like the last part, because I suspect that if it turns out that women are actually more variable, Meckel would take that as a sign of inferiority, too. Anyway, you will not be surprised to hear that evidence for the Variability Hypothesis is mixed at best. \n \nNevertheless, it came up in my class recently when we looked at data from the CDC's Behavioral Risk Factor Surveillance System (BRFSS), specifically the self-reported height of adult American men and women. The dataset includes responses from 154407 men and 254722 women. Here's what we found: \n \n The average height for men is 178 cm; the average height for women is 163 cm. So men are taller, on average. No surprises so far. \n For men the standard deviation is 7.7 cm; for women is it 7.3 cm. So in absolute terms, men's heights are more variable. \n But to compare variability between groups, it is more meaningful to use the coefficient of variation (CV), which is the standard deviation divided by the mean. It is a dimensionless measure of variability relative to scale. For men CV is 0.0433; for women it is 0.0444. \n \nThat's very close, so we could conclude that this dataset provides no support for the Variability Hypothesis. But in keeping with the theme of this blog, I won't be happy until I have overthought the question. And along the way I want to use this problem to demonstrate Bayesian estimation in 2 dimensions. \n \n Bayesian estimation in 2-D \n \nIn previous articles I have presented Bayesian estimation for the number of species in a biological sample and the age of a renal tumor . But those are both 1-D problems. Estimating the mean and variance of a sample is a 2-D problem. There are analytic solutions for several forms of the prior distribution, but as usual I will demonstrate a computational approach. \n \nI'll present the code for the update first, and get back to the prior. Here's what the update looks like: \n \n \n def LogUpdate(suite, evidence): \n  for hypo in suite.Values(): \n   likelihood = LogLikelihood(evidence, hypo) \n   suite.Incr(hypo, likelihood) \n \n suite is a Pmf object that maps from each hypothesis to its probability. While we're performing the update, the probabilities are unnormalized, which is why they are called likelihoods. And the whole thing happens under a log transform, because otherwise the likelihoods get so small they get rounded off to zero (see underflow ). \n \nHere's the function that computes likelihoods: \n \n \n def LogLikelihood(evidence, hypo): \n  t = evidence \n  mu, sigma = hypo \n \n \n  total = Summation(t, mu) \n  return -len(t) * math.log(sigma) - total / 2 / sigma**2 \n \n evidence is the tuple of heights in centimeters; hypo is a tuple of hypothetical values for mu and sigma. Summation computes the sum of the squared deviations from mu; the return value is the log likelihood of getting this sample from a Gaussian distribution with the hypothetical parameters. \n \nI pulled Summation out into a separate function because it gets called repeatedly with the same parameters, so I can improve the run time (a lot) by caching previous results: \n \n \n def Summation(t, mu, cache={}): \n  try: \n   return cache[t, mu] \n  except KeyError: \n   total = sum((x-mu)**2 for x in t) \n   cache[t, mu] = total \n   return total \n \nThat's it for the update. Now we need a prior. One option is a uniform prior where mu can be any value and sigma can be any positive value. In practice, there is no point in evaluating values far from the true parameters, because their likelihoods will be vanishingly small. So we'll use the data to choose the range for the prior, then use the data to update the prior. \n \nThat might sound completely bogus, because we are using the same data twice. But choosing the range for the prior is just a computational convenience; it has no effect on the result, as long as the range is wide enough that the likelihood for values outside the range is effectively zero. \n \nTo choose the prior range for mu and sigma, I use the conventional estimators and their standard errors. Here's the code: \n \n \n def MakeUniformPrior(t, num_points, label, spread=3.0): \n  \n  # estimate mean and stddev of t \n  n = len(t) \n  xbar, S2 = thinkstats.MeanVar(t) \n  sighat = math.sqrt(S2) \n \n \n  # compute standard error for mu and the range of ms \n  stderr_xbar = sighat / math.sqrt(n) \n  mspread = spread * stderr_xbar \n  ms = numpy.linspace(xbar-mspread, xbar+mspread, num_points) \n \n \n  # compute standard error for sigma and the range of ss \n  stderr_sighat = sighat / math.sqrt(2 * (n-1)) \n  sspread = spread * stderr_sighat \n  ss = numpy.linspace(sighat-sspread, sighat+sspread, num_points) \n \n \n  # populate the PMF \n  pmf = Pmf.Pmf(name=label) \n  for m in ms: \n   for s in ss: \n    pmf.Set((m, s), 1) \n  return ms, ss, pmf \n \n Finally, here's the code that makes the prior, updates it, and normalizes the posterior: \n def EstimateParameters(t, label, num_points=31):  xs, ys, suite = MakeUniformPrior(t, num_points, label)  \n  suite.Log()  LogUpdate(suite, tuple(t))  suite.Exp()  suite.Normalize()  \n  return xs, ys, suite  \n Ok, that was more code than I usually put in these articles. You can download it all from thinkcomplex.com/bayes_height.py . \n Results \n The result is a map from pairs of mu and sigma to probabilities; one simple way to display the map is a contour plot. Here is the result for men:  \n And for women:  \n As expected, the central location is near the estimated parameters. But with this result in numerical form is that it is easy to compute the marginal distributions for mu and sigma, or their confidence intervals. We can also compute posterior distributions for CV, and compare the distributions CV for men and women:  \n Since there is no overlap between the distributions, we can conclude that the small observed difference is statistically significant; or, if we want to be more Bayesian about it, we can compute the probability that the CV for women is higher, which is pretty close to 1. \n Finally we conclude: The standard deviation of height for men is higher. \n But the coefficient of variation is slightly lower. \n Even though the difference is tiny, it is very unlikely to be due to chance. \n And therefore women are inferior. \n Hey, I told you I have a soft spot for crank science."], "link": "http://allendowney.blogspot.com/feeds/3735925895027847767/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://thinkcomplex.com/": 1, "http://4.blogspot.com/": 1, "http://en.wikipedia.org/": 5, "http://allendowney.blogspot.com/": 2, "http://www.cdc.gov/": 1}, "blogtitle": "Probably Overthinking It"}, {"content": ["My friend Ted Bunn wrote an article, \" Who knows what evil lurks in the hearts of men? The Bayesian doesn\u2019t care ,\" inspired in part by one of my posts, \" Repeated tests: how bad can it be? \" \n \nHe presents this scenario: \n \n Frank and Betsy are wondering whether a particular coin is a fair coin (i.e., comes up heads and tails equally often when flipped). Frank, being a go-getter type, offers to do some tests to find out. He takes the coin away, flips it a bunch of times, and eventually comes back to Betsy to report his results.  \u201cI flipped the coin 3022 times,\u201d he says, \u201cand it came up heads 1583 times. That\u2019s 72 more heads than you\u2019d expect with a fair coin. I worked out the p-value \u2014 that is, the probability of this large an excess occurring if the coin is fair \u2014 and it\u2019s under 1%. So we can conclude that the coin is unfair at a significance level of 1% (or \u201999% confidence\u2019 as physicists often say).\u201d And suggests that there are two ways Betsy can interpret this report (again, quoting Ted): \n \n \n Frank is an honest man, who has followed completely orthodox (frequentist) statistical procedure. To be specific, he decided on the exact protocol for his test (including, for some reason, the decision to do 3022 trials) in advance. \n Frank is a scoundrel who, for some reason, wants to reach the conclusion that the coin is unfair. He comes up with a nefarious plan: he keeps flipping the coin for as long as it takes to reach that 1% significance threshold, and then he stops and reports his results. \n \nFinally, Ted asks, \"What should Betsy conclude on the basis of the information Frank has given her?\" If you want to know Ted's answer, you have to read his article (and you should -- it is very interesting). \n \nBut I'm going to give you my answer: Frank is a scoundrel. Well, probably. \n \nI'll follow Ted by making one more assumption: \"Suppose that Betsy\u2019s initial belief is that 95% of coins are fair \u2014 that is, the probability P that they come up heads is exactly 0.5.\" Now we can evaluate the evidence that Frank is a scoundrel. \n \nIf Frank is a scoundrel, the probability that he reports a positive result is 1, provided that he is willing to keep flipping long enough, or 1-x, for small x, if we put a bound on the number of flips he is willing to do. So \n \nP(positive test | Frank is a scoundrel) = 1-x \n \nIf Frank is honest, then the probability of a positive result is \n \nP(fair coin) P(false positive | fair coin) + P(biased coin) * P(true positive | biased coin) \n \nBetsy believes that P(fair coin) is 95% and P(biased coin) is 5%. Since Frank's significance level is 1%, P(false positive | fair coin) is 1%. \n \nThe probability of a true positive is the power of the test, which depends on how biased the coin actually is. But I will make the approximation that 3022 flips is enough to detect any substantial bias, so I'll take P(true positive | biased coin) = 1-y, for small y. So, \n \nP(positive test | Frank is honest) = (0.95)(0.01) + (0.05)(1-y) \n \nAs x and y get small, the likelihood ratio is (1 / 0.0595), which is about 17. So that is fairly strong evidence that Frank is a scoundrel. \n \nI don't know about Betsy's prior beliefs about Frank, so you will have to fill in your own punchline about her posterior."], "link": "http://allendowney.blogspot.com/feeds/1982258704914428066/comments/default", "bloglinks": {}, "links": {"http://allendowney.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://blog.richmond.edu/": 2}, "blogtitle": "Probably Overthinking It"}, {"content": ["Way back in February I analyzed data from the National Survey of Family Growth (NSFG) and answered the question \" Are first babies more likely to be late? \" Now I am getting back to that data to look at the related question, \"Are first babies more likely to be light ?\" \n \nIn Think Stats (Chapter 7), I showed that the mean birth weight for first babies is lower than the mean for others by about 2 ounces, and that difference is statistically significant. But there is also a relationship between birth weight and mother's age, and the mothers of first babies tend to be younger. So the question is: if we control for the age of the mother, are first babies still lighter? \n \nSeveral students in my class are working on projects involving multivariate regression, so I want to use this question as an example. I will follow the steps I recommend to my students: \n \n1) Before looking at relationships between variables, look at each variable in isolation. In particular, characterize the distributions and identify issues like outliers or long tails. \n \n2) Look at the variables pairwise. For each pair, look at CDFs and scatterplots, and compute correlations and/or least squares fits. \n \n3) If there seem to be relationships among the variables, look for ways to separate the effects, either by breaking the data into subsets or doing multivariate regression. \n \nSo let's proceed. \n \n One variable at a time \n \nThe variables I'll use are \n Mother's age in years, \n Birth weight in ounces, and \n first , which is a dummy variable , 1 for first babies and 0 for others. \n For live births we have 9148 records with valid ages; here is the distribution: \n  It looks like there is a little skew to the right, but other than that, nothing to worry about. \n \nWe have 9038 records with valid weights, with this distribution: \n  The middle of the distribution is approximately normal, with some jaggies due to round-off. In the tails there are some values that are are certainly errors, but it is hard to draw a clear line between exceptional cases and bad data. It might be a good idea to exclude some extreme values, but for the analysis below I did not. \n \nThere are only two values for first , so it's not much of a distribution, but with categorical data, we should check that we have enough values in each bin. As it turns out, there are 4413 first babies and 4735 others, so that's just fine. \n \n One pair at a time \n \nTo characterize the relationship between weight and first , we compare the CDF of weights for first babies and others: \n  There is some space between the distributions, and the difference in means is 2 ounces. It's not obvious whether that difference is statistically significant, but it is, with p < 0.001. \n \nSimilarly we can compare the CDF of mother's age for first babies and others: \n  Here there is clearly space between the distributions. The difference in means is 3.6 years, which is significant (no surprise this time). \n \nIt is not as easy to see the relationship between age and birthweight. I often tell my students to start with a scatterplot: \n  But in this case it's not much help. If there is a relationship there, it is hard to see. An alternative is to break the age range into bins and compute the mean in each bin. Here's what that looks like with 2-year bins: \n  We can't take the first and last points too seriously; there are not many cases in those bins. And ideally I should represent the variability in each bin so we have a sense of whether the apparent differences are real. Based on this figure, it looks like there is a relationship, but it might be nonlinear. \n \nPearson's coefficient of correlation between age and birthweight is 0.07, which is small but statistically significant. Spearman's coefficient of correlation is 0.10; the difference between the two coefficients is another warning that the relationship is nonlinear. \n \nIf we ignore the non-linearity for now, we can compute a least squares fit for birthweight as a function of age. The slope is 0.28, which means that we expect an additional 0.28 ounces per year of age. Since first mothers are 3.6 years younger than others, we expect their babies to be 1.0 ounces lighter. In fact, they are 2.0 ounces lighter, so the linear model of weight vs age accounts for 50% of the observed difference. \n \n Multiple regression \n \nSo far I have been using the thinkstats libraries to compute correlation coefficients and least squares fits. But for multiple regression I am going to break out rpy , which is the Python interface to R , a \"language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories.\" To see how it works, you can download the code I used in this section: age_lm.py \n \n \n The first model I ran is the same linear model we were just looking at: \n \n \n weight = intercept + slope * age \n \n \n In R's shorthand, that's: \n \n \n \n weights ~ ages \n \n \nHere are the results: \n \n    Estimate Std. Error t value Pr(>|t|)  \n (Intercept) 109.28635 1.08775 100.470 < 2e-16 *** \n ages   0.27926 0.04258 6.559 5.72e-11 *** \n \n \n Multiple R-squared: 0.004738, Adjusted R-squared: 0.004628 \n \n \nThe intercept is 109 ounces; the slope is 0.28 ounces per year, which what we saw before---comparing my implementation to R makes me more confident that R is correct :) \n \nThe standard error provides a confidence interval for the estimates; plus or minus 2 standard errors is (roughly) a 95% confidence interval. \n \nThe last column is the p-value, which is very small, indicating that the slope and intercept are significantly different from 0. The t-value is the test statistic used to compute the p-value, but I don't know why it gets reported; it doesn't mean much. \n \nSince the p-values are so small, it might be surprising that R 2 is so low, only 0.0047. But there is no contradiction. We can say with high confidence that there is a relationship between these variables; nevertheless, birth weight is highly variable, and even if you know the age of the mother, that does not reduce the variability by much. \n \nTo understand adjusted R 2 , consider this: if you add more explanatory variables to a model, R 2 usually goes up even if there is no real relationship. The adjusted R 2 takes this into account, which makes it more meaningful to compare models with a different number of variables. \n \nNow let's add first as an explanatory variable, so the model looks like this: \n \n weights ~ first + ages \n \nHere are the results: \n \n    Estimate Std. Error t value Pr(>|t|)  \n (Intercept) 110.62791 1.24198 89.073 < 2e-16 *** \n first  -1.11688 0.49940 -2.236 0.0253 * \n ages   0.24708 0.04493 5.499 3.93e-08 *** \n \n \n Multiple R-squared: 0.005289, Adjusted R-squared: 0.005069 \n \nThe coefficient for first is -1.1, which means that we expect first babies to be 1.1 ounces lighter, controlling for age. The p-value for this estimate is 2.5%, which I consider borderline significant. \n \nThe coefficient for ages is about the same as before, and significant. And R 2 is a little higher, but still small. \n \nBut remember that the relationship between weight and age is non-linear. We can explore that by introducing a new variable: \n \n ages2 = ages^2 \n \nNow if we run this model: \n \n weights ~ ages + ages2 \n \nWe are effectively fitting a parabola to the weight vs age curve. \n \n    Estimate Std. Error t value Pr(>|t|)  \n (Intercept) 89.151237 4.407677 20.226 < 2e-16 *** \n ages   1.898151 0.346071 5.485 4.25e-08 *** \n ages2  -0.031002 0.006577 -4.714 2.47e-06 *** \n \n \n Multiple R-squared: 0.00718, Adjusted R-squared: 0.00696 \n \n Estimates for both variables are significant. The coefficient for ages2 is negative, which means that the parabola has downward curvature, as expected. And R 2 is a little bigger. \n Finally, we can bring it all together: \n weights ~ first + ages + ages2 \n With this model we get: \n    Estimate Std. Error t value Pr(>|t|)  (Intercept) 91.07627 4.56813 19.937 < 2e-16 *** first  -0.80708 0.50373 -1.602 0.109  ages   1.79807 0.35163 5.113 3.23e-07 *** ages2  -0.02953 0.00664 -4.447 8.80e-06 ***  \n Multiple R-squared: 0.007462, Adjusted R-squared: 0.007132  \n When we include the parabolic model of weight and age, the coefficient for first gets smaller, and the p-value is 11%. \n \nI conclude that the difference in weight for first babies is explained by the difference in mothers' ages. When we control for age, the difference between first babies and others is no longer statistically significant. It is possible that there is a small difference in weight for first babies, but this dataset provides little evidence for it."], "link": "http://allendowney.blogspot.com/feeds/2103279562654342252/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://greenteapress.com/": 2, "http://www.r-project.org/": 1, "http://www.gnu.org/": 1, "http://4.blogspot.com/": 3, "http://en.wikipedia.org/": 1, "http://allendowney.blogspot.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "Probably Overthinking It"}]