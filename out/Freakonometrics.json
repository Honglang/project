[{"blogurl": "http://freakonometrics.blog.free.fr/index.php\n", "blogroll": [], "title": "Freakonometrics"}, {"content": ["(bis repetita) Consider the following regression summary, Call : \n lm ( formula = y1 ~ x1 ) \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept )  3.0001  1.1247  2.667 0.02573 * \nx1    0.5001  0.1179  4.241 0.00217 ** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 1.237 on 9 degrees of freedom\nMultiple R - squared : 0.6665 , \tAdjusted R - squared : 0.6295 \nF - statistic : 17.99 on 1 and 9 DF , p - value : 0.00217  obtained from > data ( anscombe ) \n> reg1= lm ( y1 ~ x1 , data = anscombe ) Can we say something if we look (only) at that output ? The intercept is significatively non-null, as well as the slope, the is large (66%). It looks like we do have a nice model here. And in a perfect world, we might hope that data are coming from this kind of dataset,  But it might be possible to have completely different kinds of patterns. Actually, four differents sets of data are coming from Anscombe (1973) . And that all those datasets are somehow equivalent: the 's have the same mean, and the same variance > apply ( anscombe [ , 1 : 4 ] , 2 , mean ) \nx1 x2 x3 x4\n 9 9 9 9 \n > apply ( anscombe [ , 1 : 4 ] , 2 , var ) \nx1 x2 x3 x4\n 11 11 11 11  and so are the 's > apply ( anscombe [ , 5 : 8 ] , 2 , mean ) \ny1  y2  y3  y4\n 7.500909 7.500909 7.500000 7.500909 \n > apply ( anscombe [ , 5 : 8 ] , 2 , var ) \ny1  y2  y3  y4\n 4.127269 4.127629 4.122620 4.123249  Further, observe also that the correlation between the 's and the 's is the same > cor ( anscombe ) [ 1 : 4 , 5 : 8 ] \ny1   y2   y3   y4\nx1 0.8164205 0.8162365 0.8162867 - 0.3140467 \nx2 0.8164205 0.8162365 0.8162867 - 0.3140467 \nx3 0.8164205 0.8162365 0.8162867 - 0.3140467 \nx4 - 0.5290927 - 0.7184365 - 0.3446610 0.8165214 \n > diag ( cor ( anscombe ) [ 1 : 4 , 5 : 8 ] ) \n [ 1 ] 0.8164205 0.8162365 0.8162867 0.8165214 which yields the same regression line (intercept and slope) > cbind ( coef ( reg1 ) , coef ( reg2 ) , coef ( reg3 ) , coef ( reg4 ) ) \n [ , 1 ]  [ , 2 ]  [ , 3 ]  [ , 4 ] \n ( Intercept ) 3.0000909 3.000909 3.0024545 3.0017273 \nx1   0.5000909 0.500000 0.4997273 0.4999091 But there is more. Much more. For instance, we always have the standard deviation for residuals > c ( summary ( reg1 ) $ sigma , summary ( reg2 ) $ sigma , \n + summary ( reg3 ) $ sigma , summary ( reg4 ) $ sigma ) \n [ 1 ] 1.236603 1.237214 1.236311 1.235695 Thus, all regressions here have the same R2 > c ( summary ( reg1 ) $ r.squared , summary ( reg2 ) $ r.squared , \n + summary ( reg3 ) $ r.squared , summary ( reg4 ) $ r.squared ) \n [ 1 ] 0.6665425 0.6662420 0.6663240 0.6667073 Finally, Fisher's F statistics is also (almost) the same. + c ( summary ( reg1 ) $ fstatistic [ 1 ] , summary ( reg2 ) $ fstatistic [ 1 ] , \n + summary ( reg3 ) $ fstatistic [ 1 ] , summary ( reg4 ) $ fstatistic [ 1 ] ) \nvalue value value value\n 17.98994 17.96565 17.97228 18.00329  Thus, with the following datasets, we have the same prediction (and the same confidence intervals). Consider for instance the second dataset (the first one being mentioned above), > reg2= lm ( y2 ~ x2 , data = anscombe ) The output is here exactly the same as the one we had above > summary ( reg2 ) \n \nCall : \n lm ( formula = y2 ~ x2 , data = anscombe ) \n \nResiduals : \nMin  1Q Median  3Q  Max\n - 1.9009 - 0.7609 0.1291 0.9491 1.2691 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept )  3.001  1.125  2.667 0.02576 * \nx2    0.500  0.118  4.239 0.00218 ** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 1.237 on 9 degrees of freedom\nMultiple R - squared : 0.6662 , \tAdjusted R - squared : 0.6292 \nF - statistic : 17.97 on 1 and 9 DF , p - value : 0.002179  Here, the perfect model is the one obtained with a quadratic regression.  > reg2b= lm ( y2 ~ x2 + I ( x2 ^ 2 ) , data = anscombe ) \n > summary ( reg2b ) \n \nCall : \n lm ( formula = y2 ~ x2 + I ( x2 ^ 2 ) , data = anscombe ) \n \nResiduals : \nMin   1Q  Median   3Q  Max\n - 0.0013287 - 0.0011888 - 0.0006294 0.0008741 0.0023776 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept ) - 5.9957343 0.0043299  - 1385  < 2e - 16 *** \nx2   2.7808392 0.0010401  2674  < 2e - 16 *** \n I ( x2 ^ 2 )  - 0.1267133 0.0000571  - 2219  < 2e - 16 *** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 0.001672 on 8 degrees of freedom\nMultiple R - squared :  1 , \tAdjusted R - squared :  1 \nF - statistic : 7.378e + 06 on 2 and 8 DF , p - value : < 2.2e - 16 Consider now the third one > reg3= lm ( y3 ~ x3 , data = anscombe ) i.e. > summary ( reg3 ) \n \nCall : \n lm ( formula = y3 ~ x3 , data = anscombe ) \n \nResiduals : \nMin  1Q Median  3Q  Max\n - 1.1586 - 0.6146 - 0.2303 0.1540 3.2411 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept )  3.0025  1.1245  2.670 0.02562 * \nx3    0.4997  0.1179  4.239 0.00218 ** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 1.236 on 9 degrees of freedom\nMultiple R - squared : 0.6663 , \tAdjusted R - squared : 0.6292 \nF - statistic : 17.97 on 1 and 9 DF , p - value : 0.002176  This time, the linear model could have been perfect. The problem is one outlier. If we remove it, we have > reg3b= lm ( y3 ~ x3 , data = anscombe [ - 3 , ] ) \n > summary ( reg3b ) \n \nCall : \n lm ( formula = y3 ~ x3 , data = anscombe [ - 3 , ] ) \n \nResiduals : \nMin   1Q  Median   3Q  Max\n - 0.0041558 - 0.0022240 0.0000649 0.0018182 0.0050649 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept ) 4.0056494 0.0029242  1370  < 2e - 16 *** \nx3   0.3453896 0.0003206  1077  < 2e - 16 *** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 0.003082 on 8 degrees of freedom\nMultiple R - squared :  1 , \tAdjusted R - squared :  1 \nF - statistic : 1.161e + 06 on 1 and 8 DF , p - value : < 2.2e - 16 Finally consider > reg4= lm ( y4 ~ x4 , data = anscombe ) This time, there is an other kind of outlier, in 's, but again, the regression is exactly the same, > summary ( reg4 ) \n \nCall : \n lm ( formula = y4 ~ x4 , data = anscombe ) \n \nResiduals : \nMin  1Q Median  3Q Max\n - 1.751 - 0.831 0.000 0.809 1.839 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept )  3.0017  1.1239  2.671 0.02559 * \nx4    0.4999  0.1178  4.243 0.00216 ** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 1.236 on 9 degrees of freedom\nMultiple R - squared : 0.6667 , \tAdjusted R - squared : 0.6297 \nF - statistic :  18 on 1 and 9 DF , p - value : 0.002165  The graph is here  So clearly, looking at the summary of a regression does not tell us anything... This is why we do spend some time on diagnostic, looking at graphs with the errors (the graphs above could be obtained only with one explanatory variable, while errors can be studied in any dimension): everything can be seen on thise graphs. E.g. for the first dataset,  or the second one  the third one  or the fourth one,"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/31/De-l-importance-de-faire-des-dessins", "bloglinks": {}, "links": {"http://www.jstor.org/": 1, "http://inside-r.org/": 93}, "blogtitle": "Freakonometrics"}, {"content": ["Continuons la discussion de ce matin, avec l'exemple des capitales am\u00e9ricaines. On peut trouver une telle liste liste en ligne sur wikipedia , \n > capital= read.table ( \n + \"http://freakonometrics.free.fr/US-capital.csv\" , \n + header= TRUE , sep= \";\" ) \n > capital=capital [ - c ( 2 , 11 ) , ] \n On va enlever Hawa\u00ef et l'Alaska, qui forcent \u00e0 faire de trop grands\nd\u00e9tours. Dans TSP, sous R, il y a des listes de villes am\u00e9ricaines, avec\nles coordonn\u00e9es. En bricolant un peu, on va r\u00e9cup\u00e9rer les coordonn\u00e9es\nde la plupart des capitales, \n > library ( maps ) \n > library ( maptools ) \n > library ( sp ) \n > library ( TSP ) \n > Lcap= paste ( as.character ( capital [ , 1 ] ) , \n + as.character ( capital [ , 2 ] ) , sep= \", \" ) \n > M= as.matrix ( USCA312 ) \n > L= labels ( M ) [ [ 1 ] ] \n > Lcap=Lcap [ which ( Lcap %in% L ) ] \n > k= which ( L %in% Lcap ) \n > M=M [ k , ] \n > M=M [ , k ] \n > listeUSA= TSP ( M , L [ k ] ) \n Et cette fois on est pr\u00eat, \n > tour=solve_TSP ( listeUSA , method = \"nn\" ) \n on a lanc\u00e9 l'algorithme, et on peut faire un dessin, \n > COORD= data.frame ( USCA312_coords [ k , ] ) \n > COORD=COORD [ as.numeric ( tour ) , ] \n > map ( 'state' , fill= TRUE , col = \"yellow\" ) \n > lines ( COORD $ coords.x1 , COORD $ coords.x2 , \n + lwd= 3 , col = \"blue\" ) \n > points ( COORD $ coords.x1 , COORD $ coords.x2 , \n + pch = 19 , cex = 1.2 , col = \"red\" ) \n \n pas mal, n'est pas ? Bon, mais comme d'habitude, rien ne garantie que l'on ait trouv\u00e9 le trajet optimal... En fait, le soucis est que si on relance l'algorithme, on retombe sur un autre trajet, \n \n voire encore un autre si on lancer une nouvelle fois l'algorithme, \n \n C'est p\u00e9nible, n'est ce pas ? En fait, si on lance 1,000 fois\nl'algorithme on obtient la distribution (des distances optimales)\nsuivante \n \n autrement dit, on a encore beaucoup de variabilit\u00e9, avec une distance\noptimale qui peut varier d'environ 10% entre deux boucles\nd'algorithmes. Cela dit, cela reste faible par rapport \u00e0 un trajet au\nhasard, \n \n Et d'ailleurs, si on compare la distance obtenue en tirant les villes au hasard, on arrive \u00e0 un taux de l'ordre de 27% (qui peut \u00eatre compar\u00e9 \u00e0 ce que nous avions obtenu en tirant au hasard dans le carr\u00e9 unit\u00e9) \n > HAZARD=OPTIMAL=RATIO= rep ( NA , 500 ) \n > for ( s in 1 : 500 ) { \n + HAZARD [ s ] =tsp.longueur ( M , sample ( 1 : nrow ( M ) ) ) \n + tour=solve_TSP ( listeUSA , method = \"nn\" ) \n + OPTIMAL [ s ] =tsp.longueur ( M , as.numeric ( tour ) ) \n + RATIO [ s ] =HAZARD [ s ] / \n +    OPTIMAL [ s ] \n + } \n \n Bref, compte tenu de la (grande) dispersion des r\u00e9sultats optimaux obtenu, on pourrait dire que cet algorithme est assez mauvais. M\u00eame s'il est tr\u00e8s rapide. Un autre avantage est que l'on peut sp\u00e9cifier la\nville de d\u00e9part, par exemple. Ici, si on souhaite partir de Floride, on\npeut obtenir le trajet suivant \n > tour=solve_TSP ( listeUSA , method = \"nn\" , \n + control = list ( start = 41 ) ) \n \n Passionnant\nn'est-ce pas ? Et encore, \u00e7a ne va pas m'aider \u00e0 trouver le parcours\noptimal pour les enfants, car pour les enfants, il faut sp\u00e9cifier le point de d\u00e9part (voire le point d'arriv\u00e9e), il faut en plus rester\nsur la route (voire traverser aux passages pour pi\u00e9tons). On peut aussi vouloir faire une pause pipi au milieu de la promenade.... Bref, on a pas\nmal de contraintes ! Mais tout est expliqu\u00e9 dans le livre fabuleux de\nWilliam Cook, In Pursuit of the Traveling Salesman , qui reprend toute l'histoire des algorithmes les plus fins pour r\u00e9soudre ce joli probl\u00e8me de maths..."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/18/Quel-trajet-minimal-pour-r%C3%A9cup%C3%A9rer-le-maximum-de-bonbons-%C3%A0-l-Halloween-%28partie-2%29", "bloglinks": {}, "links": {"http://press.princeton.edu/": 1, "http://inside-r.org/": 38, "http://en.wikipedia.org/": 1}, "blogtitle": "Freakonometrics"}, {"content": ["Bon, ben cette fois \u00e7a y est, c'est l'Halloween... et comme tous les ans, mon r\u00f4le devrait se limiter \u00e0 creuser les citrouilles, d\u00e9corer la maison... et attendre \u00e0 la porte de la maison que les enfants des voisins viennent me d\u00e9valiser ma r\u00e9serve de bonbons ! Je pourrais \u00e0 la rigueur revendiquer un petit quelque chose dans le d\u00e9guisement de mon fils (on a pass\u00e9 pas mal de temps \u00e0 feuilleter les premiers tomes de Walking Deads , histoire de mieux saisir l'essence des personnages de morts-vivants, en plus de la Zombie Walk la fin de semaine pass\u00e9e, en centre-ville). \n Mais cette ann\u00e9e, pas de billet sur les zombies comme l'an pass\u00e9, mais un vrai probl\u00e8me pratique: comment optimiser son trajet, quand on sait o\u00f9 sont les bonnes maisons, o\u00f9 on a de fortes chances d'avoir plein de bonbons (en particulier chez les parents des amis) ? C'est assez proche de l'id\u00e9e de passer par un maximum de maisons, et ce \u00e0 une heure raisonnable (histoire qu'il reste des bonbons \u00e0 r\u00e9cup\u00e9rer). \n Pour tous ceux qui ont fait un peu de programmation dynamique, c'est un probl\u00e8me classique, connu sous le nom du \" voyageur de commerce \". Ce probl\u00e8me est n\u00e9 en juillet 1954 avec \" the shortest route for a traveling salesman \" paru dans Newsweek ci-dessous (que l'on retrouve en d\u00e9tails dans le livre 50 Years of Integer Programming ) \n \n Enfin, disons qu'il a \u00e9t\u00e9 popularis\u00e9 \u00e0 cette \u00e9poque, car Euler \u00e9voquait d\u00e9j\u00e0 un probl\u00e8me similaire quand il travaillait sur le probl\u00e8me des sept ponts de K\u00f6nigsberg, dans \" Solutio problematis ad geometriam situs pertinentis \", \n \n (m\u00eame si ce probl\u00e8me consiste \u00e0 \u00e9tudier l'existence d'un trajet sous des contraintes de passages par des n\u0153uds, et pas vraiment de l'optimisation). On retrouve aussi un exemple en 1832, en Allemagne, dans un livre intitul\u00e9 Der Handlungsreisende - wie er sein soll und was er zu thun hat, um Auftraege zu erhalten und eines gluecklichen Erfolgs in seinen Geschaeften gewiss zu sein - Von einem alten Commis-Voyageur o\u00f9 un trajet passant par 45 villes en Allemagne et en Suisse devait \u00eatre optimis\u00e9 \n \n (le sch\u00e9ma ci-dessus montre celle d\u00e9crite en 1832, pour un total de 1,285km, mais il est possible de faire une boucle de 1,248km). \n En fait le probl\u00e8me a explos\u00e9 en 1962 (\u00e0 cette \u00e9poque, il semble que les gens pouvaient encore penser que faire des maths pouvait \u00eatre le fun), lors que Procter & Gamble ont offert un prix de 10,000$ \u00e0 celui qui trouverait le chemin le plus court pour que Toody et Muldoom, les conducteurs de la voiture 54 dans une s\u00e9rie populaire \u00e0 l'\u00e9poque, passent par 33 villes, aux \u00c9tats-Unis. \n \n Cela dit, Carl Menger avait not\u00e9 d\u00e8s 1930 a not\u00e9 que ce probl\u00e8me devrait etre difficile \u00e0 r\u00e9soudre. Facile \u00e0 \u00e9crire, mais difficile \u00e0 r\u00e9soudre explicitement. Dans son livre, William Cooke rappelle qu'avec 33 villes, il y aurait 131,565,418,466,846,765,083,609,006,080,000,000 chemins possibles, dont il faudrait calculer la longueur. Et qu'avec l'ordinateur le plus puissant en 2009 (effectuant 1.5 million de milliards d'op\u00e9rations \u00e0 la seconde) il faudrait 28,000 milliards d'ann\u00e9e pour mener \u00e0 bien les calculs. M\u00eame si on essayait de tenir compte des progr\u00e8s de l'informatique dans les mille prochaines ann\u00e9es, \u00e7a risque de prendre du temps !.. \n En 1967, Jack Edmonds notait \" I conjecture that there is no good algorithm for the traveling salesman problem \" o\u00f9 \" good \" est une mani\u00e8re \u00e9l\u00e9gante pour dire un temps polynomial en , le nombre de villes. Ce qui s'appelle les probl\u00e8mes de la classe . Il semblerait que le probl\u00e8me du voyageur de commerce soit un probl\u00e8me et pas de la classe (cf le chapitre du livre de Sanjeev Arora et Boaz Barak, ou le chapitre 11 des notes de cours de Olivier Bournez, que j'ai pu d\u00e9couvrir gr\u00e2ce \u00e0 @ dmonniaux , ou formellement de la classe et pas , i.e. complet), voir aussi la discussion sur wikipedia ), \n Et depuis, l'institut Clay a propos\u00e9 $1,000,000 \u00e0 celui qui prouverait que les probl\u00e8mes ne sont pas de type (ou qui prouveraient qu'ils le sont... la conjecture reste ouverte, semble-t-il). Bref, les enjeux financiers derri\u00e8re ont consid\u00e9rablement augment\u00e9 (on pourra consulter Sipser (2007) pour la petite histoire) \n Bon, et si l'histoire est int\u00e9ressante, ce n'est pas une raison pour la lire en se croisant les bras... essayons de programmer un algorithme (simple) afin d'approcher une solution, car je ne pourrais me contenter de dire \u00e0 mes enfants \" c'est un probl\u00e8me complet, papa a capitul\u00e9 \". \nCommen\u00e7ons par tirer 5 points au hasard dans le carr\u00e9 unit\u00e9, > n = 5 \n> ( x = matrix ( runif ( 2 * n ) , nr=n ) ) \n [ , 1 ]   [ , 2 ] \n [ 1 , ] 0.4142621 0.35600725 \n [ 2 , ] 0.9477231 0.66938653 \n [ 3 , ] 0.2518656 0.19900873 \n [ 4 , ] 0.5314231 0.22954670 \n [ 5 , ] 0.5156522 0.09066741 et commen\u00e7ons par calculer la distance entre les points (i.e. une matrice, ou ici une matrice triangulaire inf\u00e9rieure) > ( d = dist ( x ) ) \n 1   2   3   4 \n 2 0.6186980 \n 3 0.2258786 0.8399243 \n 4 0.1723919 0.6056111 0.2812205 \n 5 0.2840514 0.7222195 0.2851688 0.1397719 On va mettre ce triangle sous forme de matrice (pleine), \u00e7a sera plus simple \u00e0 manipuler par la suite > ( d = as.matrix ( d ) ) \n 1   2   3   4   5 \n 1 0.0000000 0.6186980 0.2258786 0.1723919 0.2840514 \n 2 0.6186980 0.0000000 0.8399243 0.6056111 0.7222195 \n 3 0.2258786 0.8399243 0.0000000 0.2812205 0.2851688 \n 4 0.1723919 0.6056111 0.2812205 0.0000000 0.1397719 \n 5 0.2840514 0.7222195 0.2851688 0.1397719 0.0000000 Le principe, pour initialiser l'algorithme est de tirer au hasard un trajet, c'est \u00e0 dire une permutation de nos points, > ( o = sample ( 1 : n ) ) \n [ 1 ] 2 4 1 5 3 Il faut alors calculer le 4 distances entre ces 5 villes (on ne pr\u00e9voit pas de retour \u00e0 la ville de d\u00e9part, pour l'instant). Le plus simple est de jouer avec cette matrice de la r\u00e9arranger, > d [ o [ 1 : ( n - 1 ) ] , ] [ , o [ 2 : n ] ] \n 4   1   5   3 \n 2 0.6056111 0.6186980 0.7222195 0.8399243 \n 4 0.0000000 0.1723919 0.1397719 0.2812205 \n 1 0.1723919 0.0000000 0.2840514 0.2258786 \n 5 0.1397719 0.2840514 0.0000000 0.2851688 et de prendre les 4 valeurs sur la diagonale > diag ( d [ o [ 1 : ( n - 1 ) ] , ] [ , o [ 2 : n ] ] ) \n [ 1 ] 0.6056111 0.1723919 0.2840514 0.2851688 Ce sont les 4 distances que l'on cherche, entre les 5 villes. La distance totale parcourue est alors > sum ( diag ( d [ o [ 1 : ( n - 1 ) ] , ] [ , o [ 2 : n ] ] ) ) \n [ 1 ] 1.347223 L'id\u00e9e est ensuite de permuter deux des villes > ( i= sample ( 1 : n , 2 ) ) \n [ 1 ] 3 1 (ici la premi\u00e8re et la troisi\u00e8me de la liste, mais on pourrait d\u00e9cider de fixer la premi\u00e8re si on doit partir de l\u00e0) et de voir si la nouvelle distance est plus courte, ou pas, > os=o\n> os [ i ] =o [ rev ( i ) ] \n> sum ( diag ( d [ os [ 1 : ( n - 1 ) ] , ] [ , os [ 2 : n ] ] ) ) \n [ 1 ] 1.785391 La distance \u00e9tant plus longue, on ne va pas permuter. On pourrait tenter 1000 fois de permuter deux villes. Si on ne parvient pas \u00e0 r\u00e9duire la distance totale, c'est qu'on a atteint une valeur int\u00e9ressante (\u00e0 d\u00e9faut de pouvoir prouver qu'elle est optimale). Et si on peut am\u00e9liorer la distance, on permute. Le plus simple est donc de faire des petites fonctions. La premi\u00e8re reprend le calcul de la distance totale, sur la diagonale de la matrice permut\u00e9e, \u00e0 matrice de distance donn\u00e9e et \u00e0 permutation donn\u00e9e, > tsp.longueur= function ( matrice , ordres ) { \n + n= length ( ordres ) \n + sum ( diag ( matrice [ ordres [ 1 : ( n - 1 ) ] , ] [ , ordres [ 2 : n ] ] ) ) }  La seconde est juste une boucle, > tsp.optimal= function ( d , N= 1000 ) { \n + d= as.matrix ( d ) \n + n= ncol ( d ) \n + o= sample ( 1 : n ) \n + v=tsp.longueur ( d , o ) \n + k= 0 \n + while ( k < N ) { \n + i <- sample ( 1 : n , 2 ) \n + os=o\n + os [ i ] =o [ rev ( i ) ] \n + w=tsp.longueur ( d , os ) \n +  if ( w < v ) { \n +  v=w\n +  o=os\n +  k= 0 } else { k=k + 1 } } \n + list ( ordre=o , longueur=v ) } On peut se lancer maintenant, avec par exemple 30 points simul\u00e9s, sur le carr\u00e9 unit\u00e9, >  set.seed ( 1 ) \n > n= 30 \n > x= matrix ( runif ( 2 * n ) , nr=n ) \n > o= sample ( 1 : n ) \n >  plot ( x , xlim= 0 : 1 , ylim= 0 : 1 , xlab= \"\" , ylab= \"\" ) \n >  lines ( x [ o , ] , col = 'blue' ) \n > r=tsp.optimal ( dist ( x ) ) \n > os=r $ ordre\n >  lines ( x [ os , ] , col = 'blue' , lwd= 1.5 ) avec la premi\u00e8re trajectoire, \u00e0 gauche, qui reste en trait fin sur le dessin de droite,  \n  \n  Pas mal, non ? Bon, en fait le soucis est que si on joue plusieurs fois avec notre fonction, on obtient toujours un graphique diff\u00e9rent, \n  \n  \n  Autrement dit, je trouve (certes) des chemins pas trop long, mais je suis loin de trouver le chemin optimal. Et histoire de clot\u00fbrer le d\u00e9bat, notons qu'on peut faire un peu de simulation, afin de comparer la distance obtenue en faisant le trajet au hasard, et un trajet \" optimal \", \n > RATIO=HASARD=OPTIMAL= matrix ( NA , 500 , 10 ) \n > for ( m in 1 : 10 ) { \n + n= 5 * m\n + for ( s in 1 : 500 ) { \n + x= matrix ( runif ( 2 * n ) , nr=n ) \n + r=tsp.opt ( dist ( x ) ) \n + HAZARD [ s , m ] =tsp.longueur ( as.matrix ( dist ( x ) ) , 1 : n ) \n + OPTIMAL [ s , m ] =r $ longueur\n + RATIO [ s , m ] =HAZARD [ s , m ] / \n +    OPTIMAL [ s , m ] \n + } \n + } Si on compare la longueur moyenne du trajet (sur 500 sc\u00e9narios) avec un tour au hasard (en rouge) et ou tour optimis\u00e9 (en bleu), on a \n \n ce qui donne un ratio moyen (ou une moyenne des ratios, les deux sont repr\u00e9sent\u00e9s) \n \n Autrement dit, avec une dizaine de points \u00e9parpill\u00e9s (au hasard) dans le carr\u00e9 unit\u00e9, un tour au hasard ne sera que deux fois plus long... \u00e9tonnant non ? \n Mais on doit pouvoir aller plus loin, parce que sous R, il y a des fonctions (et des packages) pour faire des algorithmes plus compliqu\u00e9s... \u00e0 suivre avant la tourn\u00e9e des bonbons de demain soir !"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/06/09/Which-path-to-visit-friends-house-at-Halloween", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://math.dartmouth.edu/": 1, "http://inside-r.org/": 45, "http://fr.wikipedia.org/": 2, "http://montrealzombiewalk.com/": 1, "http://www.gatech.edu/": 2, "http://www.ac.uk/": 1, "http://www.princeton.edu/": 1, "http://freakonometrics.free.fr/": 1, "http://www.polytechnique.fr/": 1, "http://www.springerlink.com/": 1}, "blogtitle": "Freakonometrics"}, {"content": ["Pour r\u00e9pondre \u00e0 plusieurs questions, pour constituer sa propre base, il est important d'avoir diff\u00e9rentes variables pour un m\u00eame individu . Un individu peut \u00eatre une entit\u00e9 plus large, comme une r\u00e9gion, ou un quartier. Supposons que l'on s'int\u00e9resse aux votes lors d\u2019\u00e9lections. D\u00e9sol\u00e9 de reprendre des votes en France, mais les donn\u00e9es sont faciles d'acc\u00e8s. Par exemple, pour Paris, on a les donn\u00e9es de l'\u00e9lection pr\u00e9sidentielle de 2012, sur http://opendata.paris.fr/ \n > baseE= read.csv ( \n + \"http://freakonometrics.blog.free.fr/public/\ndata/election-paris-quartiers.csv\" , \n + header= TRUE , sep= \";\" ) \n J'ai mes les donn\u00e9es sur mon blog car c'est plus simple pour les lire. Les donn\u00e9es ressemblent \u00e0 \u00e7a, \n > baseE [ 1 : 6 , 1 : 5 ] \nARR    QUARTIERS GRD_QUART INSCRITS EMARGEMENTS\n 1  1 SAINTGERMAINLAUXERROIS 7510101  1075   854 \n 2  1     HALLES 7510102  5667   4529 \n 3  1    PALAISROYAL 7510103  1986   1610 \n 4  1   PLACEVENDOME 7510104  1762   1427 \n 5  2     GAILLON 7510205  799   613 \n 6  2    VIVIENNE 7510206  1929   1485 \n Les donn\u00e9es sont \" par arrondissement et par quartier \" comme on le lit dans le descriptif. Une ligne est un quartier. Maintenant, si on veut expliquer le taux de vote, ou le taux obtenu par tel ou tel candidat, il faut des variables explicatives. \n Pour cela, on peut aller sur http://www.recensement.insee.fr/ qui met en ligne beaucoup de donn\u00e9es. En particulier, avec le m\u00eame niveau de granularit\u00e9, par quartier ! \n > base1= read.table ( \n + \"http://freakonometrics.blog.free.fr/public/\ndata/IRIS-PARIS-ACTIV.csv\" , \n + header= TRUE , sep= \";\" ) \n Si on regarde dans la base, > base1 [ 1 : 15 , c ( 1 , 2 , 3 , 4 , 5 , 7 , 8 ) ] \nIRIS REG DEP UU2010 COM TRIRIS GRD_QUART\n 1 751010101 11 75  851 75101 750011  7510101 \n 2 751010102 11 75  851 75101 750011  7510101 \n 3 751010103 11 75  851 75101 750011  7510101 \n 4 751010104 11 75  851 75101 750011  7510101 \n 5 751010105 11 75  851 75101 750011  7510101 \n 6 751010199 11 75  851 75101 750011  7510101 \n 7 751010201 11 75  851 75101 750021  7510102 \n 8 751010202 11 75  851 75101 750021  7510102 \n 9 751010203 11 75  851 75101 750021  7510102 \n 10 751010204 11 75  851 75101 750021  7510102 \n 11 751010205 11 75  851 75101 750021  7510102 \n 12 751010206 11 75  851 75101 750021  7510102 \n 13 751010301 11 75  851 75101 750011  7510103 \n 14 751010302 11 75  851 75101 750011  7510103 \n 15 751010303 11 75  851 75101 750011  7510103 En fait, le niveau d'agr\u00e9gation est encore plus fin que les quartiers utilis\u00e9s pour les \u00e9lections. Mais on peut faire des sous-totaux, par quartier, > base1t= aggregate ( base1 [ , - ( 1 : 11 ) ] , \n + by = list ( GRD_QUART=base1 $ GRD_QUART ) , FUN= sum ) \n > base1t [ 1 : 5 , c ( 1 , 4 , 5 , 7 , 8 ) ] \nGRD_QUART P08_POP1564 P08_POP1524 P08_POP5564 P08_H1564\n 1  7510101   1278   222   250   662 \n 2  7510102   7420   1187   1061  3987 \n 3  7510103   2305   475   392  1171 \n 4  7510104   2111   376   308  1010 \n 5  7510205   875   111   136   426 On a enfin des bases construites sur les m\u00eames individus. Reste \u00e0 fusionner, > baseT= merge ( baseE , base1t , by = \"GRD_QUART\" ) On a bien d'autres bases que cette base de population, > base2= read.table ( \n + \"http://freakonometrics.blog.free.fr/public/\ndata/IRIS-PARIS-POPUL.csv\" , \n + header= TRUE , sep= \";\" ) \n > base3= read.table ( \n + \"http://freakonometrics.blog.free.fr/public/\ndata/IRIS-PARIS-LOGEMENT.csv\" , \n + header= TRUE , sep= \";\" ) \n > base4= read.table ( \n + \"http://freakonometrics.blog.free.fr/public/\ndata/IRIS-PARIS-FORMATION.csv\" , \n + header= TRUE , sep= \";\" ) qu'il faut encore agr\u00e9ger par quartier, > base2t= aggregate ( base2 [ , - ( 1 : 11 ) ] , \n + by = list ( GRD_QUART=base2 $ GRD_QUART ) , FUN= sum ) (etc) que l'on fusionne avec les autres bases > baseT= merge ( baseT , base2t , by = \"GRD_QUART\" ) On a alors enfin une base avec des donn\u00e9es individuelles pour mod\u00e9liser le taux de vote, par exemple, > plot ( ( baseT $ P08_FNSCOL15P_SUP / baseT $ P08_POP ) , \n + ( baseT $ VOTANTS / baseT $ INSCRITS ) )  On notera qu'il est possible d'extraire une sous-base, avec les variables d\u2019int\u00e9r\u00eat, > base = data.frame ( \n + Thollande= ( baseT $ HOLLANDE / baseT $ INSCRITS ) , \n + Tsarkozy= ( baseT $ SARKOZY / baseT $ INSCRITS ) , \n + T5pieces= ( baseT $ P08_RP_5PP / baseT $ P08_RP ) , \n + Tfemmes= ( baseT $ P08_POPF / baseT $ P08_POP ) , \n + Tpop2554= ( baseT $ P08_POP2554 / baseT $ P08_POP1564 ) , \n + Tenfants018= ( ( baseT $ P08_POP0002 + baseT $ P08_POP0305\n + + baseT $ P08_POP1117 + baseT $ P08_POP0610 ) / baseT $ P08_POP ) , \n + Tpop5564= ( baseT $ P08_POP5564 / baseT $ P08_POP1564 ) , \n + Tactifs= ( baseT $ P08_ACT5564 / baseT $ P08_POP1564 ) , \n + Tchomeur= ( baseT $ P08_CHOM1564 / baseT $ P08_POP1564 ) , \n + Tetudiant= ( baseT $ P08_ETUD1564 / baseT $ P08_POP1564 ) , \n + T65ansplus= ( baseT $ P08_POP65P / baseT $ P08_POP ) , \n + Npop=baseT $ INSCRITS ) et c'est parti pour faire un mod\u00e8le de r\u00e9gression..."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/30/Cr%C3%A9er-sa-propre-base-de-donn%C3%A9es", "bloglinks": {}, "links": {"http://www.insee.fr/": 1, "http://inside-r.org/": 22, "http://opendata.paris.fr/": 1}, "blogtitle": "Freakonometrics"}, {"content": ["Un billet rapide pour compl\u00e9ter et illustrer le passage au log dans un mod\u00e8le lin\u00e9aire (que l'on abordera cette semaine en cours). Le point de d\u00e9part est le mod\u00e8le lin\u00e9aire, o\u00f9 on suppose que, conditionnellement \u00e0 , suit une loi normale. Pour rappel, si on a une loi normale, , alors et . Les intervalles de confiance \u00e0 90% et 95% sont sym\u00e9triques par rapport \u00e0 la moyenne (qui est aussi la m\u00e9diane, soit dit en passant), \n \n Dans un mod\u00e8le Gaussien avec homosc\u00e9dasiticit\u00e9, i.e. alors que . On a alors les bandes de confiance suivantes, pour un mod\u00e8le de r\u00e9gression lin\u00e9aire, \n \n Bon, maintenant, que se passe-t-il si on prend l'exponentiel ? Pour la loi normale, rappelons que l'on obtient une loi lognormale, i.e. , les deux param\u00e8tres \u00e9tant li\u00e9s \u00e0 la loi normale sous jacente, car d\u00e9sormais \n \n alors que \n \n Graphiquement, on a la loi suivante, avec les intervalles de confiance \u00e0 90% et 95% repr\u00e9sent\u00e9s ci-dessous. Le point noir est alors que le point bleu est l'esp\u00e9rance de la loi lognormale. \n \n On notera que le quantile de la loi log-normale est l'exponentiel du quantile de la loi normale. En effet, si alors . En particulier, n'est pas la moyenne de , mais la m\u00e9diane (puisque \u00e9tait la m\u00e9diane de ). \n Mais il n'est pas rare de voir utilis\u00e9 un intervalle de confiance de la forme \n \n qui est la forme classique de l'intervalle de confiance Gaussien (sym\u00e9trique autour de la moyenne). Ici, on aurait les niveaux suivants \n \n Notons qu'il n'y a aucune raison ici d'avoir une probabilit\u00e9 d'\u00eatre dans l'intervalle de confiance obtenu avec les quantiles de la loi normale. \n Maintenant, si on prend l'exponentiel d'un mod\u00e8le lin\u00e9aire (i.e. le logarithme de la variable d\u2019int\u00e9r\u00eat est mod\u00e9lis\u00e9 par un mod\u00e8le lin\u00e9aire) on a \n \n avec une variance (conditionnelle) qui d\u00e9pend de la variable explicative \n \n L\u00e0 encore, le plus naturel est d'utiliser comme bornes de l'intervalle de confiance des quantiles associ\u00e9s \u00e0 la loi lognormale, \n \n mais il n'est pas rare de voir utilis\u00e9 des intervalles de type Gaussiens, \n \n On perd l\u00e0 encore en interpr\u00e9tation car les bornes n'ont plus rien \u00e0 voir avec les quantiles."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/23/Le-passage-au-log-dans-les-mod%C3%A8les-lin%C3%A9aires", "bloglinks": {}, "links": {}, "blogtitle": "Freakonometrics"}, {"content": ["Cet apr\u00e8s-midi avait lieu le premier examen d'actuariat 1, pr\u00e9paratoire pour l'examen P de la SOA. Comme pour l'examen P, il y avait des questions \u00e0 choix multiples, en un temps (tr\u00e8s limit\u00e9). Le sujet est en ligne, ainsi que des \u00e9l\u00e9ments de corrections (bient\u00f4t) ici . Si vous n\u2019\u00eates pas d'accord avec mes corrections, merci de me le dire rapidement, si possible avant que je n'attribue les notes..."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/29/ACT2121-premier-examen", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 2}, "blogtitle": "Freakonometrics"}, {"content": ["Poursuivons un peu la discussion d'hier. En fait, si on poursuit, on arrive \u00e0 regrouper des r\u00e9gions ensemble.... mais sans r\u00e9elle coh\u00e9rence car des r\u00e9gions proches au niveau \u00e9lectorale peuvent \u00eatre \u00e9loign\u00e9es g\u00e9ographiquement. \n \n Si on veut que les r\u00e9gions regroup\u00e9es soient proches, il faut une base avec des informations spatiales, comme savoir si deux d\u00e9partements se touchent, ou pas. Pour cela, on utilise la base suivante, qui contient \n le num\u00e9ro du d\u00e9partement i \n le num\u00e9ro du d\u00e9partement j \n une variable indicatrice indiquant si les d\u00e9partement se touche \n la distance (en km) entre les centro\u00efdes des d\u00e9partements \n \n > france= read.table ( \n + \"http://freakonometrics.blog.free.fr/public/\ndata/departements-france.csv\" , \n + header= TRUE , sep= \";\" )  On va nettoyer un peu, en enlevant les d\u00e9partements d'outre-mer, \n > france0=france ; i200= c ( 201 , 202 ) \n > france0=france0 [ - c ( which ( france0 $ depi %in% i200 ) , \n + which ( france0 $ depj %in% i200 ) ) , ] \n> france0 $ ij= 100 * france0 $ depi + france0 $ depj \n La base ressemble \u00e0 \u00e7a, \n > head ( france0 ) \ndepi depj Cij Dij ij\n 1  77  75  0 51 7775 \n 2  78  75  0 39 7875 \n 3  91  75  0 40 9175 \n 4  92  75  1 10 9275 \n 5  93  75  1 12 9375 \n 6  94  75  1 13 9475 \n Plusieurs strat\u00e9gies sont possible: regarde des d\u00e9partements voisins et voir si on peut les regrouper. Ou regarder des d\u00e9partements proches \u00e9lectoralement, et voir s'il sont proches g\u00e9ographiquement. Par exemple, si on ordonne les coefficients obtenus lors de la r\u00e9gression, \n > reg= lm ( X.VoixExpH ~ 0 + Dpt , weights =\n + Votants , data =sbelection ) \n > COEF= coefficients ( summary ( reg ) ) \n > dbcoef= data.frame ( COEF [ , 1 : 3 ] , Dpt= rownames ( COEF ) ) \n > rownames ( dbcoef ) = rank ( COEF [ , 1 ] ) \n > COEFtrie=dbcoef [ as.character ( 1 : 94 ) , ] \n > COEFtrie [ 1 : 6 , ] \nEstimate Std..Error t.value Dpt\n 1 35.69903 0.3246129 109.97414 DptD06\n 2 36.46840 0.3244806 112.39010 DptD67\n 3 36.67594 0.3896297 94.13025 DptD68\n 4 37.39283 0.3202112 116.77552 DptD83\n 5 39.92023 0.3921762 101.79156 DptD74\n 6 42.61811 0.6182921 68.92877 DptD10 \n On peut se demander si les quatre premiers d\u00e9partements pourraient \u00eatre regroup\u00e9s ensemble. Une des modalit\u00e9s sert de modalit\u00e9 de r\u00e9f\u00e9rence, et on regarde les trois autres par rapport \u00e0 cette derni\u00e8re \n > sbelection $ Dpt= relevel ( sbelection $ Dpt , \"D67\" ) \n > reg0= lm ( X.VoixExpH ~ Dpt , weights =Votants , data =sbelection ) \n> COEF0= coefficients ( summary ( reg0 ) ) \n > COEF0 [ c ( 1 , 7 , 67 , 82 ) , ] \nEstimate Std. Error  t value Pr ( >| t | ) \n ( Intercept ) 36.4684029 0.3244806 112.3900971 0.00000000 \nDptD06  - 0.7693736 0.4589784 - 1.6762740 0.09369320 \nDptD68  0.2075400 0.5070493  0.4093093 0.68231515 \nDptD83  0.9244243 0.4558759  2.0277980 0.04258819 \n On fait ensuite une analyse de la variance, \n > sbelection $ DptGroupe=sbelection $ Dpt\n> sblction $ DptGroupe [ sblction $ Dpt %in% c ( \"D06\" , \"D68\" , \"D83\" ) ] = \"D67\" \n> model0= lm ( X.VoixExpH ~ DptGroupe , weights =Votants , data =sbelection ) \n> model1= lm ( X.VoixExpH ~ Dpt , weights =Votants , data =sbelection ) \n> anova ( model0 , model1 ) \nAnalysis of Variance Table\n \nModel 1 : X.VoixExpH ~ DptGroupe\nModel 2 : X.VoixExpH ~ Dpt\nRes.Df  RSS Df Sum of Sq  F Pr ( > F ) \n 1 36114 2281072960 \n 2 36111 2280191044 3  881917 4.6556 0.002954 ** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n On rejette ici l'hypoth\u00e8se que les quatre d\u00e9partements se comportent de la m\u00eame mani\u00e8re. Mais si on tente de regrouper les trois premi\u00e8res, cette fois, on accepte l'hypoth\u00e8se \n > sbelection $ DptGroupe=sbelection $ Dpt\n > sblction $ DptGroupe [ sblction $ Dpt %in% c ( \"D06\" , \"D68\" ) ] = \"D67\" \n > model0= lm ( X.VoixExpH ~ DptGroupe , weights =Votants , data =sbelection ) \n> model1= lm ( X.VoixExpH ~ Dpt , weights =Votants , data =sbelection ) \n> anova ( model0 , model1 ) \nAnalysis of Variance Table\n \nModel 1 : X.VoixExpH ~ DptGroupe\nModel 2 : X.VoixExpH ~ Dpt\nRes.Df  RSS Df Sum of Sq  F Pr ( > F ) \n 1 36113 2280476723 \n 2 36111 2280191044 2  285679 2.2621 0.1041 \n Sauf que sur les trois d\u00e9partements, seulement deux se touchent, \n > france0 [ france0 $ ij %in% c ( 6768 , 667 , 668 ) , ] \ndepi depj Cij Dij ij\n 3510  6  67  0 533 667 \n 3552  67  68  1 95 6768 \n 3605  6  68  0 439 668 \n On peut essayer de parcourir les sorties de la r\u00e9gression, et regarder si on a des voisins. Pour chaque d\u00e9partement, on regarde si le voisin est un voisin g\u00e9ographique, \n > departement= as.numeric ( substr ( COEFtrie $ Dpt , 5 , 6 ) ) \n> deptij=departement [ 1 : ( nrow ( COEFtrie ) - 1 ) ] * 100 + \n> departement [ ( 2 ) : ( nrow ( COEFtrie ) ) ] \n> voisins=france0 [ france0 $ ij %in% deptij , ] \n > voisinage= tapply ( voisins $ Cij , factor ( voisins $ ij ) , max ) \n > COEFtrie $ voisin= NA \n> v=voisinage [ as.character ( deptij ) ] \n > COEFtrie $ voisin [ 1 : ( nrow ( COEFtrie ) - 1 ) ] =\n > v [ is.na ( v ) == FALSE ] \n > COEFtrie [ 1 : 10 , ] \nEstimate Std..Error t.value Dpt voisin\n 1 35.69903 0.3246129 109.97414 DptD06  0 \n 2 36.46840 0.3244806 112.39010 DptD67  1 \n 3 36.67594 0.3896297 94.13025 DptD68  0 \n 4 37.39283 0.3202112 116.77552 DptD83  0 \n 5 39.92023 0.3921762 101.79156 DptD74  0 \n 6 42.61811 0.6182921 68.92877 DptD10  0 \n 7 42.77423 0.4397101 97.27826 DptD01  0 \n 8 43.53395 0.4448212 97.86843 DptD84  0 \n 9 44.38759 0.3897553 113.88579 DptD85  0 \n 10 44.65361 0.4577710 97.54575 DptD51  1 \n On note qu'il y a plusieurs voisins. On peut ensuite faire un test de Fisher. S'il est positif, on fusionne les classes. Pour la r\u00e9gression, mais aussi dans la base des distances et des voisinages, \n > if ( COEFtrie $ voisin [ i ] == 1 ) { \n + sbelection $ Dpt [ sbelection $ Dpt == substr ( \n + COEFtrie $ Dpt [ i + vsn ] , 4 , 6 ) ] = substr ( COEFtrie $ Dpt [ i ] , 4 , 6 ) \n + france0 $ depi [ france0 $ depi == as.numeric ( \n +  substr ( COEFtrie $ Dpt [ i + vsn ] , 5 , 6 ) ) ] = as.numeric ( \n + substr ( COEFtrie $ Dpt [ i ] , 5 , 6 ) ) \n + france0 $ depi [ france0 $ depj == as.numeric ( \n + substr ( COEFtrie $ Dpt [ i + vsn ] , 5 , 6 ) ) ] = as.numeric ( \n + substr ( COEFtrie $ Dpt [ i ] , 5 , 6 ) ) \n Dans la base des d\u00e9partements, il y aura plusieurs paires. Si A et B ont fusionn\u00e9, il suffit que C touche A ou B pour que C soit voisin des r\u00e9gions fusionn\u00e9es. C'est pour cela que l'on utilise \n > voisinage= tapply ( voisins $ Cij , factor ( voisins $ ij ) , max ) \n pour savoir si les d\u00e9partements sont voisins. On peut alors r\u00e9p\u00e9ter plusieurs fois l'op\u00e9ration. \n Par exemple, on note que 28, 45 et 89 peuvent fusionner, \n > sbelection $ Dpt= relevel ( sbelection $ Dpt , \"D45\" ) \n + reg10= lm ( X.VoixExpH ~ Dpt , weights =Votants , data =sbelection ) \n + COEF10= coefficients ( summary ( reg10 ) ) \n + COEF10 [ c ( 1 , 28 , 88 ) , ] \nEstimate Std. Error  t value Pr ( >| t | ) \n ( Intercept ) 45.9303788 0.4170114 110.1417762 0.0000000 \nDptD28  0.5643212 0.6594028  0.8558065 0.3921105 \nDptD89  0.9423212 0.7016026  1.3430982 0.1792486 \n + sbelection $ DptGroupe=sbelection $ Dpt\n + sbelection $ DptGroupe [ sbelection $ Dpt %in% \n + c ( \"D28\" , \"D89\" ) ] = \"D45\" \n + model0= lm ( X.VoixExpH ~ DptGroupe , weights =Votants , data =sbelection ) \n + model1= lm ( X.VoixExpH ~ Dpt , weights =Votants , data =sbelection ) \n + anova ( model0 , model1 ) \nAnalysis of Variance Table\n \nModel 1 : X.VoixExpH ~ DptGroupe\nModel 2 : X.VoixExpH ~ Dpt\nRes.Df  RSS Df Sum of Sq  F Pr ( > F ) \n 1 36113 2280314007 \n 2 36111 2280191044 2  122963 0.9737 0.3777 \n G\u00e9ographiquement, ce sont effectivement des voisins \n \n Mais peu de d\u00e9partements voisins peuvent effectivement \u00eatre fusionn\u00e9s."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/27/Une-application-du-test-de-Fisher%2C-partie-2", "bloglinks": {}, "links": {"http://inside-r.org/": 80}, "blogtitle": "Freakonometrics"}, {"content": ["A nice post (found a couple of days ago) on an insteresting topic, I should perhaps discuss more\noften on this blog \n \n \n \"Why\ndo people love to say that correlation does not imply causation?\" http://www. slate.com/arti \u2026 \n \nAlso, this week, a series of posts and articles about L'Aquila trial, \n \n \n \"L'Aquila's\nearthquake-scarred streets see battle between science and politics\" http://www. guardian.co.uk/w /27/laquila-earthquake-battle-science-politics?CMP=twt_fd  \u2026 \n \n \"L\u2019Aquila:\nearthquake, verdict, and statistics\" http:// xianblog.wordpress.com/20 uila-earthquake-verdict-and-statistics/  \u2026 \n \n \"Complexity\nand the madness of crowds \u2013 lessons from disaster prevention\" on @ rszbt 's\nblog http:// reszatonline.wordpress.com/20 plexity-and-the-madness-of-crowds-lessons-from-disaster-prevention/  \u2026 \n \n \"Trial\nOver Earthquake in Italy Puts Focus on Probability\" http://www. nytimes.com/sci ence/04quake.html?_r=1&  \u2026 via @ statfr \n \n \"Italian\ncourt ruling sends chill through science community\" http://www. reuters.com/a1 0/22/us-science-manslaughter-italy-idUSBRE89L1IC20121022?feedType=RSS&feedName=scienceNews&utm_source=dlvr.it&utm_medium=twitter&dlvrit=309301  \u2026 \n \n \"Verdict\nof l\u2019Aquila Earthquake Trial Sends the Wrong Message\" about how to deal\nwith hazard assessment and mitigation http://www. wired.com/wir \u2026 \n \n \"Scientists\non trial: At fault?\" http://www. nature.com/1109 14/pdf/477264a.pdf  \u2026 (a detailed article) \n \n \"Italian\nscientists convicted of manslaughter\" http:// arstechnica.com/s 0/italian-scientists-convicted-of-manslaughter-for-earthquake-risk-report/  \u2026 \"No finding of elevated\nrisk in a report days before a fatal earthquake\" \n \nStill, a lot of very interesting posts found here, and there, \n \n \n \"The\nRole of Connections in Academic Promotions\u201d by Natalia Zinovyeva and\nManuel Bagues http:// ftp.iza.org/dp6821.pdf \n \n \"The\nStudent Debt Crisis\" http://www. americanprogress.org/iss education/report/2012/10/25/42905/the-student-debt-crisis/ \u2026 \n \n \"Integrals\ndon\u2019t have anything to do within discrete math, do they?\" http:// mathdl.maa.org/im \u2026 \n \n [ free ebook ]\n\"Think Bayes: Bayesian Statistics Made Simple\" by Allen B. Downey http://www. greenteapress.com/th \u2026 \n \n \"Benoit\nMandelbrot, the father of fractal geometry, pens a disturbing new\nmemoir on mathematics\u2014and survival\" http://www. tabletmag.com/jew d-culture/books/114766/a-math-genius-sad-calculus?all=1  \u2026 \n \n \"Milliseconds\nmatter\" http://www. washingtonpost.com/bu my/milliseconds-matter/2012/10/25/bedf8210-1ef9-11e2-9cd5-b55c38388962_graphic.html  \u2026 \"pricing strategies and\ndecision-making process involved in HF trading\" \n \n \n \n \n \"The Virtues and Vices of\nElection Prediction Markets\" http:// fivethirtyeight.blogs.nytimes.com/2 -23-the-virtues-and-vices-of-election-prediction-markets/  \u2026 by @ fivethirtyeight via http:// krugman.blogs.nytimes.com/2 \u2026 on nerds \n \n \"Communication\nabout science doesn\u2019t need to be time-consuming or distracting from\nresearch ...\" http:// blogs.nature.com/soapbox /2012/10/24/an-elevator-pitch-for-a-research-project?WT.mc_id=TWT_NatureBlogs&buffer_share=35adc  \u2026 \n \n \"21\nReasons Why You Should Never Date An Economist\" http:// inesad.edu.bo/dev \u2026 see also http://www. economistsdoitwithmodels.com/20 t-for-fun-reasons-not-to-date-an-economist-thanks-guys/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+economistsdoitwithmodels+%28Economists+Do+It+With+Models%29  \u2026 via @ adelaigue \n \n \"Fun\nwith tax: How taxation by government has changed\" http://www. economist.com/blo etail/2012/10/daily-chart-12  \u2026 \n \n nice\napplication http://www. visualthesaurus.com/app/view   for those who like words,\nand graphs \n \n \n \n \n \"Frankenstein\nEconomics is killing capitalism\" http://www. marketwatch.com/Sto int?guid=9E40EEA0-1C55-11E2-B624-002128040CF6  \u2026 \n \n \"Asset\nPricing with Garbage\" in the Journal of Finance, last year http:// onlinelibrary.wiley.com/do. 1540-6261.2010.01629.x/abstract  \u2026 \n \n \"The\nFuture of Computer Trading in Financial Markets\" http://www. bis.gov.uk/fore \u2026 \n \n \"Auction\nTheory: A Guide to the Literature\" by Paul Klemperer http://www. nuff.ox.ac.uk/us \u2026 \n \n \"Ethics\nand Finance: The Role of Mathematics\" http:// magic-maths-money.blogspot.ca/20 -and-finance-role-of-mathematics.html \u2026 \n \n \"How\nto find a perfect match for a Nobel\" http:// timharford.com/how-to -find-a-perfect-match-for-a-nobel/  \u2026 by @ timharford via @ nereidadin \n \n \"Who\nowns research data and the rights to publish it?\" http:// emckiernan.wordpress.com/who -owns-research-data-and-the-rights-to-publish-it/  \u2026 via @ mathewi \nsee also an article on databases\nproperty regulated by EU La \n w\n http:// ec.europa.eu/int t/copyright/prot-databases/index_en.htm  \u2026 \n \n \"We\u2019re\nprobably at the death of education\" http:// thenextweb.com/ins \u2026 via @ IanikMarcil \n \n \"Can\nhumans cause an earthquake?\" http:// arstechnica.com/scie 0/can-humans-cause-an-earthquake/  \u2026 see also http://www. nature.com/ng aop/ncurrent/full/ngeo1610.html  \u2026 \n \n \"Algorithms,\nArbitrage, and Overreaction on Intrade\" http:// rajivsethi.blogspot.ca/algori thms-arbitrage-and-overreaction.html  \u2026 \n \n \"Did\nthe financial blogosphere go away?\" http:// blogs.reuters.com/feli 012/10/23/did-the-financial-blogosphere-go-away/  \u2026 by @ felixsalmon via @ stanjourdan \n \n \"an XKCD-esque chart\" with # rstats  http:// drunks-and-lampposts.com/cle gg-vs-pleb-an-xkcd-esque-chart/  \u2026 answer to http:// stackoverflow.com/1267 5147/xkcd-style-graphs-in-r  \u2026 's challenge \n \n \"And\nthe winner for longest time on record between publication and\nretraction is\u2026\" http:// retractionwatch.wordpress.com/and -the-winner-for-longest-time-on-record-between-publication-and-retraction-is/  \u2026 via @ NGhoussoub \n \n \"Location,\nLocation, Location\" by Alexandra M. Lord http:// chronicle.com/Locati on-Location-Location/134264/  \u2026 via @ tomroud \n \n \"How\nProPublica\u2019s Message Machine Reverse Engineers Political Microtargeting\" http://www. propublica.org/ne \u2026 via @ propublica \nand http://www. propublica.org/ar e-machine-starts-providing-answers  \u2026 \n \n If\nyou're working in Academia \u201cit\u2019s your duty to be miserable\u201d http:// chronicle.com/Its-Yo ur-Duty-to-Be/135014/  \u2026 by @ Pannapacker via @ NGhoussoub @ m_m_campbell \n \n Forthcoming\nactuarial projects on life tables \"Average life span for Dwarves,\nHobbits and Men\" http:// lotrproject.com/st feexpectancy  \u2026 \n \n \"Scientific\nresearch bodies 'failing to engage public'\" http://www. scidev.net/fr/sc munication/news/les-organismes-de-recherche-scientifique-trop-peu-engag-s-avec-le-public-.html  \u2026 via @ collectifpapera \n \n \"Is\nit meaningful to talk about a probability of \u201c65.7%\u201d that Obama will\nwin the election?\" http:// andrewgelman.com/is-it- meaningful-to-talk-about-a-probability-of-65-7-that-obama-will-win-the-election/  \u2026 \n \n  \n \n \"Why\nIt is Essential That Criminal Bankers are Prosecuted\" http://www. nakedcapitalism.com/why-it -is-essential-that-criminal-bankers-are-prosecuted.html  \u2026 via @ michellaurence \n \n \"Perhaps\nthe whole \u2018don\u2019t put all your eggs in one basket\u2019 school of portfolio\nallocation is financial wisdom enough\" http:// timharford.com/when-s implicity-is-a-real-asset/  \u2026 \n \n damned,\neven the New York Times knows: \"You Don\u2019t Work as Hard as You Say You\nDo\" http:// economix.blogs.nytimes.com/you -dont-work-as-hard-as-you-say-you-do/  \u2026 \n \nAlso several documentaries, found, online \n \n \n [ video doc ] \" Money as Debt \" directed by Paul\nGrignon http:// youtu.be/e6LWqgohO4E   http:// youtu.be/lsmbWBpnCNk   http:// \nand youtu.be/f6uuAupT4AQ   via http://www. economicreason.com/d.. se/top-15-economic-documentaries/  \n \n [ video doc ]\n\" The Ascent of Money: A Financial\nHistory of The World \" by Niall Ferguson  http:// youtu.be/4Xx_5PuLIzc   via http://www. economicnoise.com/the -top-18-economic-documentaries/  \u2026 \n \n \n\n \n \n [ video doc ]\n\" 97% Owned \" http:// youtu.be/XcGh1Dex4Yo   via http://www. economicnoise.com/the -top-18-economic-documentaries/  \u2026 and @ clasicaliberal \n \n [ video doc ]\n\" Overdose: The Next Financial Crisis \" http:// youtu.be/4ECi6WJpbzE   via http://www. economicnoise.com/the -top-18-economic-documentaries/  \u2026 and @ clasicaliberal \n \nEt comme toujours, quelques articles en francais, \n \n \n Le\nKrach de 1929, discut\u00e9 15 jours plus tard dans le Journal des Finances http:// gallica.bnf.fr/bpt 6k5552712t.image  \u2026 via @ GallicaBnF \n \n \n \n \n \"\u00c9conomistes\n\u00e0 gages et m\u00e9dias complaisants\" par Renaud Lambert http://www. acrimed.org/ht ml  \u2026 (suite du livre de @ LaurentMauduit ) \n \n \"Ce\nque le blog apporte \u00e0 la recherche\" par Antoine Blanchard (a.k.a. @ Enroweb ) http:// press.openedition.org/172  \n \n Did I miss something ?         Follow @freakonometrics"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/27/Somewhere-else%2C-part-18", "bloglinks": {}, "links": {"https://twitter.com/": 25, "http://t.co/": 64}, "blogtitle": "Freakonometrics"}, {"content": ["Un petit (?) billet pour revenir sur un point \u00e9voqu\u00e9 mardi dernier en cours, sur l'utilisation du test de Fisher (et des techniques d'analyse de variance). Je vais juste \u00e9voquer rapidement un vieux billet sur l'histoire du 5% utilis\u00e9 lorsque l'on calcule la -value (ce qui compl\u00e8tera la r\u00e9ponse faite en classe mardi lorsque la question a \u00e9t\u00e9 pos\u00e9e). Mardi dernier, j'avais \u00e9voqu\u00e9 rapidement le test d'une contrainte lin\u00e9aire sur les param\u00e8tres. On se demande ici si les deux\nmod\u00e8les suivants sont vraiment diff\u00e9rents, entre \n \net sa version contrainte \n \nOn cherche alors \u00e0 tester l'hypoth\u00e8se \n \ncontre l'hypoth\u00e8se alternative \n \nPour \u00e7a, on peut utiliser ce qu'a propos\u00e9 Ronald Fisher,\n\u00e0 savoir d'utiliser la somme des carr\u00e9s des\nr\u00e9sidus (qui correspondent - \u00e0 un facteur multiplicatif\npr\u00e8s - \u00e0 l'estimateur de la variance du bruit), et il a\npos\u00e9 \n \nJe parlais tout \u00e0 l'heure de choix de mod\u00e8le... on notera\nque l'on en est pas tr\u00e8s loin car on peut r\u00e9\u00e9crire\ncette expression en faisant appara\u00eetre le des r\u00e9gressions, contraintes et noncontraintes. \n \nFisher a montr\u00e9 que sous l'hypoth\u00e8se H 0 , \n \ni.e. la loi de la statistique de Fisher est pr\u00e9cis\u00e9ment une loi de Fisher, Pour aller plus loin sur l'analyse de la variance (ANOVA, analysis of variance ), je renvois \u00e0 Faraway (2005) . J'avais dit oralement qu'une application int\u00e9ressant du test de nullit\u00e9 de plusieurs coefficient dans une r\u00e9gression \u00e9tait son utilisation pour des variables cat\u00e9gorielles, et pour faire du regroupement de modalit\u00e9. \n Consid\u00e9rons un cas d'\u00e9cole. On dispose des r\u00e9sultats des votes dans plusieurs villes, en France, lors des derni\u00e8res \u00e9lections pr\u00e9sidentielles (ces bases sont en ligne sur http://www.data.gouv.fr/) , opposant Fran\u00e7ois Hollande (avec un H) et Nicolas Sarkozy (avec un S), \n > election= read.table ( \n + \"http://freakonometrics.blog.free.fr/public/data/\n205cf26f15a43974fdb7d3b25397613d.csv\" , \n + sep= \";\" , header= TRUE , dec= \",\" ) \n > head ( election ) \nCode.du.dpt Inscrits Votants VoixH X.VoixExpH VoixS X.VoixExpS\n 1    1  592  506  195  41.05  280  58.95 \n 2    1  215  175  82  52.23  75  47.77 \n 3    1  8208  6447 3009  50.41 2960  49.59 \n 4    1  1152  949  340  38.16  551  61.84 \n 5    1  105  83  35  44.30  44  55.70 \n 6    1  1702  1461  630  46.46  726  53.54 \n On va nettoyer un peu la base \n > sbelection=election [ election $ Code.du.d\u00e9partement\n + %in% as.character ( 1 : 95 ) , ] \n car il y a des d\u00e9partements d'outre mers et la Corse, qui a une codification particuli\u00e8re. Bref, on retient les 95 d\u00e9partements m\u00e9tropolitain, moins le 20\u00e8me qui est la Corse. On va commencer par cr\u00e9er une variable de d\u00e9partement qui soit un facteur qualitatif. Pour l'instant, le d\u00e9partement est un num\u00e9ro, entre 1 et 95 mais ce n'est aucunement un nombre ! \n > sbelection $ Dpt= ( paste ( \"D\" , \n > sbelection $ Code.du.d\u00e9partement , sep= \"\" ) ) \n > I = as.numeric ( as.character ( sbelection $ Code.du.d\u00e9partement ) ) < 10 \n > sbelection $ Dpt [ I ] = ( paste ( \"D0\" , \n + sbelection $ Code.du.d\u00e9partement , sep= \"\" ) ) \n > sbelection $ Dpt= factor ( sbelection $ Dpt ) \n On peut alors faire une r\u00e9gression, o\u00f9 on peut essayer de mod\u00e9liser le taux de vote pour Fran\u00e7ois Hollande (par exemple) en fonction de variables explicatives, et en particulier, on veut voir s'il n'y a pas un effet spatial. Bref, on va r\u00e9gresser sur le d\u00e9partement. Et d'ailleurs que sur le d\u00e9partement, inutile de mettre d'autre variable pour illustrer. \n > reg= lm ( X.VoixExpH ~ 0 + Dpt , weights =\n + Votants , data =sbelection ) \n Par contre, je fais une r\u00e9gression pond\u00e9r\u00e9e, car les villes (et villages) ne font pas du tout la m\u00eame taille. C'est exactement la m\u00eame id\u00e9e que de faire une moyenne pond\u00e9r\u00e9e pour avoir le taux moyen dans une r\u00e9gion (on rediscutera des r\u00e9gressions pond\u00e9r\u00e9es en fin de cours). \n > COEF= coefficients ( summary ( reg ) ) \n La sortie ressemble \u00e0 \n > COEF\nEstimate Std. Error  t value Pr ( >| t | ) \nDptD01 42.77423 0.4397101 97.27826   0 \nDptD02 52.39423 0.4572016 114.59765   0 \nDptD03 56.89434 0.5468760 104.03518   0 \nDptD04 51.06544 0.7807748 65.40355   0 \nDptD05 50.70525 0.8403979 60.33481   0 \nDptD06 35.69903 0.3246129 109.97414   0 \nDptD07 53.44185 0.5564489 96.04089   0 \nDptD08 51.86319 0.6360901 81.53434   0 \nDptD09 64.67228 0.8013389 80.70528   0 \nDptD10 42.61811 0.6182921 68.92877   0 \n [...] \n DptD89 46.87270 0.5642231 83.07476   0 \nDptD90 50.47561 0.9063735 55.68964   0 \nDptD91 53.41748 0.3172310 168.38670   0 \nDptD92 49.54305 0.2850597 173.79888   0 \nDptD93 65.29062 0.3338954 195.54217   0 \nDptD94 56.48855 0.3196643 176.71211   0 \nDptD95 53.89382 0.3344373 161.14778   0 \n On peut d'ailleurs faire une pr\u00e9diction, par d\u00e9partement (en prenant comme pr\u00e9diction celle obtenue pour la premi\u00e8re ville du d\u00e9partement) \n > chg= which ( diff ( as.numeric ( as.character ( \n + sbelection $ Code.du.d\u00e9partement ) ) ) ! = 0 ) \n > newelection=sbelection [ 1 + c ( 0 , chg ) , ] \n > PredDept= predict ( reg , newdata=newelection ) \n On peut ensuite essayer de faire un petite dessin pour visualiser cette pr\u00e9diction, \n > chg= which ( diff ( as.numeric ( as.character ( \n + sbelection $ Code.du.d\u00e9partement ) ) ) ! = 0 ) \n > newelection=sbelection [ 1 + c ( 0 , chg ) , ] \n > PredDept= predict ( reg , newdata=newelection ) \n > nomdpt= read.table ( \n + \"http://freakonometrics.blog.free.fr/\npublic/data/departements.txt\" , sep= \";\" ) \n > nomdpt $ nom= tolower ( nomdpt $ V2 ) \n > dpt= as.character ( c ( paste ( \"0\" , \n + 1 : 9 , sep= \"\" ) , c ( 10 : 19 , 21 : 95 ) ) ) \n > noms= as.character ( nomdpt [ nomdpt $ V3 %in% dpt , 2 ] ) \n > couleurliste= rev ( rainbow ( n= 100 ) ) \n > library ( maps ) \n > francemap <- map ( database= \"france\" ) \n > dpt=noms\n > couleur=couleurliste [ trunc ( ( PredDept - 35 ) * 3 ) ] \n C'est un peu long (et compliqu\u00e9) car il faut identifier le bon d\u00e9partement sur la carte R (qui est cod\u00e9e en lettres). D'ailleurs la fonction suivante ne marche pas \n > match =match.map ( francemap , dpt ) \n > francemap $ names [ which ( is.na ( match.map ( \n + francemap , dpt [ 35 ] ) ) == FALSE ) ] \n [ 1 ] \"Indre-et-Loire\" \"Indre\" \n car en l'occurrence, seule l'Indre aurait du \u00eatre obtenue. On va faire la correspondance \u00e0 la main, \n > match = rep ( NA , length ( francemap $ names ) ) \n > for ( i in 1 : length ( francemap $ names ) ) { \n + if ( sum ( dpt == francemap $ names [ i ] ) > 0 ) \n + { match [ i ] = which ( dpt == francemap $ names [ i ] ) } \n + } \n Une fois les d\u00e9partements identifi\u00e9s, on colorie, \n > color=couleur [ match ] \n > map ( database= \"france\" , fill= TRUE , col =color ) \n C'est joli, mais comme toujours c'est un peu compliqu\u00e9 de faire des cartes... \n \n A titre d'information, les couleurs correspondent aux valeurs suivantes (avec en abscisse, le taux de vote obtenu par Fran\u00e7ois Hollande) \n \n Bref, on a beaucoup de modalit\u00e9 car il y a 95 d\u00e9partement. Mais on devrait pouvoir regrouper. Si on ne sait pas trop par o\u00f9 commencer, prenons un d\u00e9partement au hasard, disons le 35 (i.e. Rennes), \n > k= 34 \n On peut ensuite regarder les estimations des coefficients, ou mieux, les intervalles de confiances des diff\u00e9rents estimateurs, et voir ceux qui chevauche celui de Rennes (on va prendre des intervalles de confiance \u00e0 70% histoire d'avoir des chances que les deux coefficients soient vraiment proches) \n > alpha= .7 \n > CI.COEF= confint ( reg , level=alpha ) \n On peut visualiser ces intervalles, avec une bande horizontale pour Rennes, \n > plot ( c ( 1 : 19 , 21 : 95 ) , COEF [ , 1 ] , ylim= c ( 35 , \n + 60 ) , pch= 3 , cex= .5 ) \n > segments ( c ( 1 : 19 , 21 : 95 ) , CI.COEF [ , 1 ] , \n + c ( 1 : 19 , 21 : 95 ) , CI.COEF [ , 2 ] ) \n > lk= which ( ( CI.COEF [ , 1 ] < CI.COEF [ k , 2 ] ) & \n + ( CI.COEF [ , 2 ] > CI.COEF [ k , 1 ] ) ) \n > segments ( c ( 1 : 19 , 21 : 95 ) [ lk ] , CI.COEF [ lk , 1 ] , \n + c ( 1 : 19 , 21 : 95 ) [ lk ] , CI.COEF [ lk , 2 ] , col = \"red\" , lwd= 2 ) \n et en rouge, les intervalles qui la chevauchent \n  \n \u00c7a y est, on a une liste potentielle de six d\u00e9partements \u00e0 regrouper... On va prendre comme modalit\u00e9 de r\u00e9f\u00e9rence le d\u00e9partement qui nous int\u00e9resse, et voir si les coefficients des autres d\u00e9partements sont (ou pas) significatifs. \n > sbelection $ Dpt= relevel ( sbelection $ Dpt , \"D35\" ) \n > reg35= lm ( X.VoixExpH ~ Dpt , \n + weights =Votants , data =sbelection ) \n > COEF35= coefficients ( summary ( reg35 ) ) \n > COEF35 [ c ( 1 , lk [ lk < k ] + 1 , lk [ lk > k ] ) , ] \nEstimate Std. Error  t value Pr ( >| t | ) \n ( Intercept ) 55.66226653 0.3275407 169.9399801 0.0000000 \nDptD11  0.59043789 0.6272106  0.9413710 0.3465210 \nDptD32  0.96845567 0.7855154  1.2328920 0.2176241 \nDptD36  - 0.03007518 0.7414655 - 0.0405618 0.9676455 \nDptD62  0.56238714 0.4247897  1.3239189 0.1855384 \nDptD75  - 0.06226653 0.4098013 - 0.1519432 0.8792326 \nDptD81  - 0.11970183 0.6067040 - 0.1972986 0.8435950 \n On a confirmation que ces six d\u00e9partements sont non significativement (individuellement) diff\u00e9rents de celui de r\u00e9f\u00e9rence (et donc on accepte l'hypoth\u00e8se de nullit\u00e9 du coefficient). Mais peut-on pour autant les regrouper? C'est l\u00e0 que l'on peut faire un test de Fisher. Pour cela on fait deux r\u00e9gressions. La premi\u00e8re est celle avec un regroupement de d\u00e9partements en une modalit\u00e9 unique (les sept sont d\u00e9sormais ensemble) \n > sbelection $ DptGroupe=sbelection $ Dpt\n > sbelection $ DptGroupe [ sbelection $ Dpt%\n + in% substr ( rownames ( COEF [ lk , ] ) , 4 , 6 ) ] = paste ( \"D\" , k + 1 , sep= \"\" ) \n > model0= lm ( X.VoixExpH ~ DptGroupe , \n + weights =Votants , data =sbelection ) \n et la seconde est celle que l'ont vient de faire (qui sera notre hypoth\u00e8se alternative, i.e. il ne faut pas regrouper les d\u00e9partements ensemble car ils sont diff\u00e9rents - ou au moins l'un est diff\u00e9rent) \n > model1= lm ( X.VoixExpH ~ Dpt , \n + weights =Votants , data =sbelection ) \n On peut alors faire le test de Fisher (en continuant \u00e0 corriger de l'effet taille de la population, et en faisant des sommes pond\u00e9r\u00e9es) \n > SCR0= sum ( sbelection $ Votants * residuals ( model0 ) ^ 2 ) \n > SCR1= sum ( sbelection $ Votants * residuals ( model1 ) ^ 2 ) \n > dl0=model0 $ df.residual\n > dl1=model1 $ df.residual\n > ( F= ( SCR0 - SCR1 ) / ( dl0 - dl1 ) / SCR1 * dl1 ) \n [ 1 ] 0.917397 \n On obtient la statistique de Fisher, et le plus simple est alors de calculer la -value \n > 1 - pf ( F , dl0 - dl1 , dl1 ) \n [ 1 ] 0.4809411 \n On a fait (sans le savoir) une analyse de la variance, qui peut se faire plus simplement (?) en utilisant la fonction suivante \n > anova ( model0 , model1 ) \nAnalysis of Variance Table\n \nModel 1 : X.VoixExpH ~ DptGroupe\nModel 2 : X.VoixExpH ~ Dpt\nRes.Df  RSS Df Sum of Sq  F Pr ( > F ) \n 1 36117 2280538612 \n 2 36111 2280191044 6  347568 0.9174 0.4809 \n Et on accepte ici l'hypoth\u00e8se que ces sept d\u00e9partements peuvent \u00eatre regroup\u00e9 en un seul. \n Enfin, si statistiquement, il y a du sens \u00e0 regrouper les modalit\u00e9s, on peut se demander si ce regroupement de r\u00e9gions g\u00e9ographique \u00e0 du sens, \n > library ( maps ) \n > francemap <- map ( database= \"france\" ) \n > dpt=noms\n > couleur= \"red\" \n > match =match.map ( francemap , dpt ) \n > color=couleur [ match ] \n > map ( dataLa premi\u00e8re estbase= \"france\" , fill= TRUE , col =color ) \n \n Les r\u00e9gions que l'on souhaite associer \u00e0 Rennes sont fort \u00e9loign\u00e9es, et les regrouper n'a peut-\u00eatre pas de sens... (\u00e0 suivre donc)."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/26/Une-application-du-test-de-Fisher", "bloglinks": {}, "links": {"http://inside-r.org/": 92, "http://freakonometrics.free.fr/": 2, "http://www.gouv.fr/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Freakonometrics"}, {"content": ["Hier, on a \u00e9voqu\u00e9 en cours le th\u00e9or\u00e8me de Frisch-Waugh (\u00e9voqu\u00e9 sur ce blog il y a quelques mois). Derri\u00e8re, il y a l'estimation de mod\u00e8les sur des sous-ensembles de valeurs. Formellement, on a un mod\u00e8le de la forme \n \n Les \u00e9quations normales sont ici (cf transparents du cours, ou \u00e9quations \u00e9crites au tableau) , que l'on peut \u00e9crire sous la forme d'un syst\u00e8me, \n \n Le point int\u00e9ressant est que si les variables dans le premier ensemble et les variables dans le second sont orthogonales, i.e. et , alors les termes diagonaux du syst\u00e8me disparaissent, et on retrouve les \u00e9quations normales des r\u00e9gressions simples, i.e. \n \n On a ici une cons\u00e9quence du th\u00e9or\u00e8me de Frisch-Waugh: si les variables sont orthogonales, les estimateurs dans la r\u00e9gression multiple co\u00efncident avec ceux obtenus avec des r\u00e9gressions simples. \n Afin de mieux clarifier ce point, \u00e9crivons ce qui se passerait avec seulement deux variables. On va associer la constante \u00e0 la premi\u00e8re, i.e. \n \n Dans ce cas, les \u00e9quations normales sont \n \n Bref, il faut que et soient orthogonales pour que le terme sur la seconde diagonale disparaisse. C'est ce qu'on retrouve dans la plupart des livres d'\u00e9conom\u00e9trie, il suffit de chercher le th\u00e9or\u00e8me de Frisch-Waugh dans l'index (m\u00eame si le th\u00e9or\u00e8me de Frisch-Waugh va beaucoup plus loin puisqu'il explique quels ajustement faire pour lier les estimateurs dans une r\u00e9gression simple, et dans le cas multiple). \n Mais est-ce un r\u00e9sultat purement th\u00e9orique ou peut-on l'invoquer dans un cas pratique ? Autrement dit, si deux variables semblent orthogonales, que se passent-il num\u00e9riquement ? Si je simule des variables explicatives ind\u00e9pendantes, les estimateurs co\u00efncident-ils (comme le sugg\u00e8re la th\u00e9orie) ? \n Consid\u00e9rons un mod\u00e8le lin\u00e9aire avec deux variables explicative que l'on supposera ind\u00e9pendantes, mais ayant toutes deux un impact sur la variable explicative. Le plus simple est de consid\u00e9rer un triple de variables telles que \n \n o\u00f9 d\u00e9signe des valeurs non nulles. Par exemple, \n > set.seed ( 1 ) \n > n= 100 \n > S= matrix ( c ( 1 , .4 , .3 , .4 , 1 , 0 , .3 , 0 , 1 ) , 3 , 3 ) \n > S\n [ , 1 ] [ , 2 ] [ , 3 ] \n [ 1 , ] 1.0 0.4 0.3 \n [ 2 , ] 0.4 1.0 0.0 \n [ 3 , ] 0.3 0.0 1.0 \n > M= c ( 1 , 3 , 6 ) \n > Z=rmnorm ( n , M , varcov=S ) \n > Y=Z [ , 1 ] ; X1=Z [ , 2 ] ; X2=Z [ , 3 ] \n > base = data.frame ( Y , X1 , X2 ) \n > cor ( base ) \nY   X1   X2\nY 1.0000000 0.37796580 0.27859381 \nX1 0.3779658 1.00000000 - 0.05060539 \nX2 0.2785938 - 0.05060539 1.00000000 On a simul\u00e9 ici deux variables ind\u00e9pendantes, mais toutes deux corr\u00e9l\u00e9es avec la variable d'int\u00e9r\u00eat. On notera que num\u00e9riquement  (autrement dit, en moyenne, on simulera des donn\u00e9es ind\u00e9pendantes, mais dans ce cas pr\u00e9cis, les donn\u00e9es ne sont pas rigoureusement orthogonales). > sum ( X1 * X2 ) / n\n [ 1 ] 18.20194 \n > sum ( X1 ) * sum ( X2 ) / n ^ 2 \n [ 1 ] 18.25097 Mais un test rapide nous pousse \u00e0 accepter l'hypoth\u00e8se d'ind\u00e9pendance entre les deux variables (sans trop d'h\u00e9sitations). > cor.test ( X1 , X2 ) \n \nPearsons product - moment correlation\n \n data : X1 and X2\n t = - 0.5016 , df = 98 , p - value = 0.6171 \nalternative hypothesis : true correlation is not equal to 0 \n 95 percent confidence interval : \n - 0.2445923 0.1472766 \n sample estimates : \n cor \n - 0.05060539 Si on regarde maintenant la r\u00e9gression, > R= lm ( Y ~ X1 + X2 , data = base ) \n > summary ( R ) \n \nCall : \n lm ( formula = Y ~ X1 + X2 , data = base ) \n \nResiduals : \nMin  1Q Median  3Q  Max\n - 1.87885 - 0.57656 0.07957 0.52719 1.77407 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept ) 4.71180  0.28246 16.682 < 2e - 16 *** \nX1   0.37232  0.08441  4.411 2.66e - 05 *** \nX2   0.25975  0.07755  3.350 0.00115 ** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 0.7954 on 97 degrees of freedom\nMultiple R - squared : 0.2317 , \tAdjusted R - squared : 0.2159 \nF - statistic : 14.63 on 2 and 97 DF , p - value : 2.803e - 06 Quel est le lien avec les r\u00e9gressions simples, sur chacune des variables ? Si on r\u00e9gresse sur la premi\u00e8re, on obtient > R1= lm ( Y ~ X1 , data = base ) \n > summary ( R1 ) \n \nCall : \n lm ( formula = Y ~ X1 , data = base ) \n \nResiduals : \nMin  1Q Median  3Q  Max\n - 1.78973 - 0.54060 0.04036 0.44138 1.94389 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept ) 5.03165  0.27934 18.013 < 2e - 16 *** \nX1   0.35802  0.08859  4.041 0.000106 *** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 0.8358 on 98 degrees of freedom\nMultiple R - squared : 0.1429 , \tAdjusted R - squared : 0.1341 \nF - statistic : 16.33 on 1 and 98 DF , p - value : 0.0001058 et si on r\u00e9gresse sur la seconde > R2= lm ( Y ~ X2 , data = base ) \n > summary ( R2 ) \n \nCall : \n lm ( formula = Y ~ X2 , data = base ) \n \nResiduals : \nMin  1Q Median  3Q  Max\n - 1.8652 - 0.5507 0.1104 0.6109 2.1435 \n \nCoefficients : \nEstimate Std. Error t value Pr ( >| t | ) \n ( Intercept ) 5.85053  0.12495 46.824 < 2e - 16 *** \nX2   0.24244  0.08443  2.872 0.00501 ** \n --- \nSignif. codes : 0 \u2018 *** \u2019 0.001 \u2018 ** \u2019 0.01 \u2018 * \u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n \nResidual standard error : 0.867 on 98 degrees of freedom\nMultiple R - squared : 0.07761 , \tAdjusted R - squared : 0.0682 \nF - statistic : 8.246 on 1 and 98 DF , p - value : 0.005005 Si les donn\u00e9es ne sont pas rigoureusement orthogonales, alors dans le mod\u00e8le multiple est l\u00e9g\u00e8rement diff\u00e9rent de obtenu dans le mod\u00e8le simple. Si on regarde la distribution de nos estimateurs (n'oublions pas qu'il y a une marge d'erreur), ils sont proches, ou au moins statistiquement proches (avec \u00e0 gauche la distribution de et \u00e0 droite celle de )  \n  \n Si on g\u00e9n\u00e8re 100 bases de donn\u00e9es avec ce mod\u00e8le (avec 500 observations \u00e0 chaque fois) on obtient les estimateurs suivants (\u00e0 gauche les et \u00e0 droite les , avec en abscisse le mod\u00e8le multiple et en ordonn\u00e9es, le mod\u00e8le simple).  \n  \n (la valeur centrale \u00e9tant la vraie valeur, utilis\u00e9e pour les simulations). On pourrait se demander si les valeurs \u00e9loign\u00e9es du barycentre (ou de cette premi\u00e8re bissectrice) correspondent \u00e0 des cas o\u00f9 l\u2019hypoth\u00e8se d'ind\u00e9pendance entre les deux variables est rejet\u00e9e ? On peut mettre des couleurs diff\u00e9rentes en fonction de la -value: plus la -value est faible (rejet de l'hypoth\u00e8se d'ind\u00e9pendance) plus la couleur sera vive (et rouge), et plus la -value est forte (acceptation de l'hypoth\u00e8se d'ind\u00e9pendance) plus la couleur sera terne (et jaune, voire blanche), \n \nsur 1,000 simulations, on obtient  \n  \n \nLe code est ici > set.seed ( 1 ) \n > library ( mnormt ) \n > S= matrix ( c ( 1 , .4 , .3 , .4 , 1 , 0 , .3 , 0 , 1 ) , 3 , 3 ) \n > BX1=BX2= matrix ( NA , 1000 , 2 ) \n > P= rep ( NA , 1000 ) \n > for ( s in 1 : 1000 ) { \n + n= 500 \n + Z=rmnorm ( n , c ( 6 , 3 , 1 ) , varcov=S ) \n + Y=Z [ , 1 ] ; X1=Z [ , 2 ] ; X2=Z [ , 3 ] \n +  base = data.frame ( Y , X1 , X2 ) \n + R= lm ( Y ~ X1 + X2 , data = base ) \n + BX1 [ s , 1 ] = coefficients ( R ) [ 2 ] \n + BX2 [ s , 1 ] = coefficients ( R ) [ 3 ] \n + BX1 [ s , 2 ] = coefficients ( lm ( Y ~ X1 , data = base ) ) [ 2 ] \n + BX2 [ s , 2 ] = coefficients ( lm ( Y ~ X2 , data = base ) ) [ 2 ] \n + P [ s ] = cor.test ( X1 , X2 ) $ p.value\n + } \n > COL= ( heat.colors ( 100 ) ) \n > plot ( BX1 [ , 1 ] , BX1 [ , 2 ] , pch= 19 , cex= .6 , col =COL [ trunc ( 100 * P ) ] ) \n > abline ( a= 0 , b= 1 , lty= 2 ) Autrement dit le fait que l'estimation dans le mod\u00e8le multiple et les mod\u00e8les simples diff\u00e8rent beaucoup n'a rien a voir avec la -value d'un test d'ind\u00e9pendance entre les deux variables... \u00e9tonnant non ? En revanche, si on rajoute une observation pour rendre le mod\u00e8le parfaitement non-corr\u00e9l\u00e9, par exemple, > difference= sum ( X1 * X2 ) * ( n + 1 ) / sum ( X1 ) - sum ( X2 ) \n > X1= c ( X1 , 0 ) \n > X2= c ( X2 , difference ) \n > n=n + 1 \n > sum ( X1 * X2 ) / n\n [ 1 ] 18.02172 \n > sum ( X1 ) * sum ( X2 ) / n ^ 2 \n [ 1 ] 18.02172 On retrouve ici une corr\u00e9lation parfaitement nulle, > Y= c ( Y , 0 ) \n >  base = data.frame ( Y , X1 , X2 ) \n >  cor ( base ) \nY   X1   X2\nY 1.0000000 3.946091e - 01 2.923412e - 01\nX1 0.3946091 1.000000e + 00 - 4.817519e - 16\nX2 0.2923412 - 4.817519e - 16 1.000000e + 00 Si on regarde maintenant la r\u00e9gression multiple > R= lm ( Y ~ X1 + X2 , data = base ) \n >  coefficients ( R ) \n ( Intercept )   X1   X2\n - 1.5028358  0.3589776  0.2531358 \net les deux r\u00e9gressions simples , \n > R1= lm ( Y ~ X1 , data = base ) \n >  coefficients ( R1 ) \n ( Intercept )   X1\n 0.02847362 0.35897764 \n > R2= lm ( Y ~ X2 , data = base ) \n >  coefficients ( R2 ) \n ( Intercept )   X2\n - 0.4334011  0.2531358 on obtient exactement les m\u00eames coefficient. Moralit\u00e9 ? l'hypoth\u00e8se d'orthogonalit\u00e9 entre les variables explicatives n'est pas \u00e0 prendre \u00e0 la l\u00e9g\u00e8re: il faut une orthogonalit\u00e9 stricte pour que num\u00e9riquement, la r\u00e9gression multiple soit \u00e9quivalente aux r\u00e9gressions simples (comme le pr\u00e9dit la th\u00e9orie). Mais de mani\u00e8re un peu surprenante, si on est tr\u00e8s proche de l'ind\u00e9pendance, les valeurs num\u00e9riques des estimateurs peut diff\u00e9rer. Et ce, parfois de mani\u00e8re non n\u00e9gligeable, m\u00eame si statistiquement, on valide l'hypoth\u00e8se d'orthogonalit\u00e9 entre les variables explicatives."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/24/R%C3%A9gression-multiple-sur-des-variables-orthogonales", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1, "http://inside-r.org/": 105}, "blogtitle": "Freakonometrics"}, {"content": ["There have been a lot of interesting papers about the \" manslaughter trial \" of six seismologists and a government official in Italy, where justice pointed out that there was a failure to warn the population before the deadly earthquake in 2009, see e.g. \" Trial Over Earthquake in Italy Puts Focus on Probability and Panic \" on nytimes.com , \" Italian scientists convicted of manslaughter for earthquake risk report \" on arstechnica.com , \" Italian court ruling sends chill through science community \" on reuters.com , \"Scientists on trial: At fault?\" on nature.com or (probably the most interesting one) \" The Verdict of the l\u2019Aquila Earthquake Trial Sends the Wrong Message \" on wired.com . \n \n First of all, I started less than 15 months ago to work on earthquake series and models, and I am still working on the second paper, but so far, what I've seen is that those series are very noisy. When working on a large scale (say 500km), it is still very difficult to estimate the probability that there will be a large earthquake on a large period of time (from one year to a decade). Even including covariate such as foreshocks. So I can imagine that it is almost impossible to predict something accurate on a smaller scale, and on a short time frame. A second point is that I did not have time to look carefully at what was said during the trial: I just have been through what can be find in articles mentioned above. \n \n But as a statistician, I really believe, as claimed by Niels Bohr (among many others) that \" prediction is very difficult, especially about the future \". Especially with a 0/1 model (warning versus not warning). In that case, you have the usual type I and type II errors (see e.g. wikipedia.org for more details), \n type I is \" false positive error \" when you issue a warning, for nothing. A \" false alarm \" error. With standard \" test \" words, it is like when a pregnancy tests predict that someone is pregnant, but she's not. \n type II is \" false negative error \" failing to assert something, what is present.Here, it is like when a pregnancy tests predict that someone is not pregnant, while she actually is. \n \n The main problem is that statisticians wish to design a test with both errors as small as possible. But usually, you can't. You have to make a trade-off. The more you want to protect yourself against Type I errors (by choosing a low significance level), the greater the chance of a Type II error. This is actually the most important message in all Statistics 101 courses. \n Another illustration can be from the course I am currently teaching this semester, precisely on prediction and forecasting techniques . Consider e.g. the following series \n \n Here, we wish to make a forecast on this time series (involving a confidence interval, or region). Something like \n \n The aim of the course is to be able to build up that kind of graph, to analyze it, and to know exactly what were the assumptions used to derive those confidence bands. But if you might go to jail for missing something, you can still make the following forecast \n \n From this trial, we know that researchers can go to jail for making a type II error. So, if you do not want to go to jail, make frequent type I error (from this necessary trade-off). Because so far, you're more likely not to go to jail for that kind of error ( the boy who cried wolf kind). Then, you're a shyster, a charlatan, but you shouldn't spend six years in jail ! As mentioned on Twitter, that might be a reason why economist keep announcing crisis ! That might actually be a coherent strategy..."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/23/Predictions-and-errors", "bloglinks": {}, "links": {"http://arstechnica.com/": 1, "http://www.wired.com/": 1, "http://t.co/": 1, "http://en.wikipedia.org/": 1, "http://www.nytimes.com/": 1, "http://www.reuters.com/": 1}, "blogtitle": "Freakonometrics"}, {"content": ["Un petit compl\u00e9ment pour revenir sur un calcul fait hier, en classe, au tableau. \n \" Une police d\u2019assurance couvre des pertes X et Y qui sont des variables al\u00e9atoires continues fonction dont on donne la densit\u00e9 conjointes. Trouver l\u2019esp\u00e9rance du remboursement s\u2019il est de X + Y avec un d\u00e9ductible de 1. \" \nAu tableau, je m'\u00e9tais lanc\u00e9 dans des calculs un peu compliqu\u00e9 (notant que les variables \u00e9taient ind\u00e9pendantes, on pouvait utiliser la convolution pour trouver la loi de la somme facilement... mais on reviendra sur ce calcul plus tard). La solution la plus simple \u00e9tait la suivante: la loi de admet pour densit\u00e9 . Le remboursement est la variable al\u00e9atoire  On peut alors faire le calcul de  \n i.e.  soit  Voil\u00e0 pour la version simple (et finalement assez g\u00e9n\u00e9rale). Pour la version plus compliqu\u00e9e \n  \n autrement dit, on a obtenu que les deux composantes \u00e9taient ind\u00e9pendantes, et que est uniform\u00e9ment distribu\u00e9e sur . Aussi, pour la loi de la somme  et par ind\u00e9pendance,  i.e.  soit  Aussi  Premi\u00e8re chose, on note que  i.e.  J'avais alors sugg\u00e9r\u00e9 d'utiliser (en utilisant la propri\u00e9t\u00e9 que pour une variable positive, l'esp\u00e9rance est l'int\u00e9grale de la fonction de survie) que  \n Or cette expression n'est valide que pour une variable strictement positive ! Or ici, il y a de forte chance (1/3) que le remboursement soit nul. Le plus simple est alors de faire des calculs classiques d\u2019esp\u00e9rance. Notons que  donc un peu de calcul de d\u00e9riv\u00e9es permet d'\u00e9crire pour . On peut alors calculer l'esp\u00e9rance du remboursement  soit  Youpi, avec plein de calculs, on arrive au m\u00eame r\u00e9sultat... Faisons un peu de simulations pour v\u00e9rifier, > n= 1000000 \n > set.seed ( 1 ) \n > X= sqrt ( runif ( n ) ) \n > Y= runif ( n ) On peut v\u00e9rifier par exemple la densit\u00e9 de sur , > u= seq ( 1 , 2 , by = .01 ) \n > hist ( X + Y , probability= TRUE , col = \"light blue\" ) \n > lines ( u , u * ( 2 - u ) , lwd= 2 , col = \"red\" )  \nSi on regarde maintenant la prime pure, on retrouve le r\u00e9sultat obtenu par deux calculs diff\u00e9rents, > R= ( X + Y - 1 ) * ( X + Y > 1 ) \n > mean ( R ) \n [ 1 ] 0.2497528"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/23/ACT2121%2C-%C3%A9l%C3%A9ments-de-correction-%282%29", "bloglinks": {}, "links": {"http://inside-r.org/": 11}, "blogtitle": "Freakonometrics"}, {"content": ["This week, the most important news is undoubtly \n \n \n \"Randomly generated mathematics\nresearch papers!\" http:// thatsmathematics.com/b 102  \u2026 \"Mathgen paper accepted\" ,\nsee also \"Nonsense paper accepted by mathematics journal\" http:// marginalrevolution.com/ \u2026 via @ adelaigue \n \nI also read a very interesting post \n \n \n \"Publication\nincentives\" http://www. quantumforest.com/public ation-incentives/  \u2026 by @ zentree (following http://www. quantumforest.com/academ ic-publication-boycott/  \u2026 ) \n \n among many others \n \n \n \"Challenging\nthe integrity of research\" http:// blogs.nature.com/201 2/10/17/challenging-the-integrity-of-scientist?WT.ec_id=NATUREjobs-20121018 \u2026 \n \n \"We\nneed a method of assessing the support of research if we want to change\nthe \u2018publish or perish\u2019 culture\" http:// blogs.lse.ac.uk/impa ciences/2012/10/17/voytek-fishy-citations-brainscanr/  \u2026 \n \n [ notes ]\n\"R for SAS and SPSS users\" https:// science.nature.nps.gov/im/datamgmt/st atistics/R/documents/R_for_SAS_SPSS_users.pdf  \u2026 \n \n \"status\nand math in economics\" http:// orgtheory.wordpress.com/sta tus-and-math-in-economics/  \u2026 see also the paper http:// mpra.ub.uni-muenchen.de/MPRA_p aper_41363.pdf  \u2026 \n \n \"What\nis math, and why should we use it in economics?\" http:// noahpinionblog.blogspot.ca/what-i s-math-and-why-should-we-use-it.html  \u2026 \n \n [ video\ncourse ] \"Beyond Computation: The P vs NP Problem\" by Michael\nSipser http:// youtu.be/msp2y... \n \n [ free\nebook ] \"Bayesian Reasoning and Machine Learning\" by David Barber http:// web4.cs.ucl.ac.uk/staff/ /textbook/090310.pdf  \u2026 via @ DataJunkie \n \n \"What\nJP Morgan\u2019s release of VaR has in common with sex and computer viruses\" http://www. nickdunbar.net/what- jp-morgans-release-of-var-has-in-common-with-sex-and-computer-viruses/  \u2026 \n \n \"forecasting\nthe Presidential election using regression, simulation, or dynamic\nprogramming\"  http:// punkrockor.wordpress.com/or ecasting-the-presidential-election-using-regression-simulation-or-dynamic-programming/  \u2026 via @ lamclay \n \n \"Where\nWill The Next Pandemic Come From? And How Can We Stop It?\" http://www. popsci.com/science/s e/2012-08/out-wild?single-page-view=true \u2026 \n \n \"How\nclose are pairwise and mutual independence?\" http:// legacy.lclark.edu/pairw ise2.pdf  \u2026 via @ statfr \n \n \"Homogeneous\nrecord of Atlantic hurricane surge threat since 1923\" http://www. pnas.org/content/early/ 2012/10/10/1209542109.full.pdf+html  \u2026 via http://www. newscientist.com/article/dn2238 2-tidal-records-expose-surge-in-hurricanes.html  \u2026 \n \n \"Blogging\" http://www. guardian.co.uk/higher-educati on-network/blog/2012/oct/15/blog-action-day-power-of-we  \u2026 \" Too often dismissed as narcissistic\necho-chambers, blogs are the ultimate form of collegiality \" \n \n \"All\nyou scientists frustrated by the rejection of your papers from\njournals, relax. Rejection is rare\" http:// news.sciencemag.org/scienceinsider /2012/10/scientists-may-feel-rejected-but.html?ref=em  \u2026 \n \n \"Beware,\nwin or lose: Domestic violence and the World Cup\" http:// onlinelibrary.wiley.com/ 13.2012.00606.x/abstract  \u2026 in @ signmagazine \n \n \"what\nis the optimal way to find a parking spot?\" http:// punkrockor.wordpress.com/wha t-is-the-optimal-way-to-find-a-parking-spot/  \u2026 by @ lamclay  \n \nwith several articles early this week on the Nobel price (in Economics) \n \n \n \"I\nnever, never in my life took a course in economics\" (Lloyd Shapley) http:// larspsyll.wordpress.com/i-n ever-never-in-my-life-took-a-course-in-economics/  \u2026 \n \"A\nNobel for work that affects your daily life\" http://www. cbsnews.com/830 2-57532993/a-nobel-for-work-that-affects-your-daily-life/ \u2026 \n \n \"To\nwin the Nobel Prize in Economics, it helps to wield math. Lots of it\"\n(posted before the annoncement) http:// qz.com/to-win-t he-nobel-prize-in-economics-it-helps-to-wield-math-lots-of-it/  \u2026 via @ karlfisch \n \navec plusieurs articles en francais \n \n \n \"Alvin\nRoth et Lloyd Shapley, Nobels d'Economie 2012\" joli papier d' @ adelaigue sur http:// blog.francetvinfo.fr/classe-eco/ 2/10/15/alvin-roth-et-lloyd-shapley-nobels-deconomie-2012.html \u2026 \n \n \"Le\nPrix Nobel d\u2019\u00e9conomie 2012 : ni prix, ni Nobel, ni \u00e9conomie ?\" http:// legizmoblog.blogspot.fr/le-pri x-nobel-deconomie-2012-ni-prix-ni.html  \u2026 par @ blogizmo \n \nmais aussi \n \n \n \"Le\ndouloureux calcul de la valeur de la vie\" http://www. slate.fr/story/63485/qa ly-calcul-valeur-vie  \u2026 \n quand\nun \n probl\u00e8me\nclassique pour les actuaires int\u00e9resse maintenant les \u00e9conomistes\ncf http://www. nber.org/11405... ,\n http:// ideas.repec.org/v 21y2002i2p253-270.html  \u2026 pour une m\u00e9ta analysis ou http://www. ncbi.nlm.nih.gov/P MC1650128/  \u2026 \n quand\nun brillant statisticien (Paul Deheuvels) prend la parole sur l'\u00e9tude\nde S\u00e9ralini, http:// leplus.nouvelobs.com/c 61194-l-etude-de-seralini-sur-les-ogm-pomme-de-discorde-a-l-academie-des-sciences.html  \u2026 via @ GolumModerne  @ virg_garin \n \n \"Ne\ncomptez pas trop sur les JT et les cha\u00eenes d'info pour d\u00e9livrer un\ndiscours critique sur l'\u00e9conomie\" sur http:// television.telerama.fr/po... rquoi-ma-tele-est-elle-liberale,87925.php#xtor=RSS-18  \n \n \"Ces\n\u00e9conomistes qui monopolisent (toujours) les d\u00e9bats\" http://www. acrimed.org/article3904 ml  \u2026 \n \n \u00abChocolatine\nou Pain au chocolat ?\u00bb (en France) http:// blog.adrienvh.fr/car tographie-des-resultats-de-chocolatine-ou-pain-au-chocolat/  \u2026 via @ AdrienneAlix  @ le_luk  @ romainmenard  @ blogizmo  @ tomroud \n \n \n \n \n \"Universit\u00e9s\ndu Qu\u00e9bec: le spectre du sous-financement, ou quand la quantit\u00e9\nremplace la qualit\u00e9 http://www. iris-recherche.qc.ca/blo tre-du-sous-financement  \u2026 \"\nRT @ Mlle_Titam \n \n \"idiots\" http:// dirtydenys.net/in /2012/10/10/idi \u2026 par Denys Bergrave via @ Vicnent  @ pegobry  @ bzavier  @ adelaigue \n \n (en\nmoyenne) \"Twitter est une jeune femme am\u00e9ricaine\" http://www. ledevoir.com/soc /361401/twitter-est-une-jeune-femme-americaine  \u2026 \n \n  Did I miss something ?         Follow @freakonometrics"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/20/Somewhere-else%2C-part-17", "bloglinks": {}, "links": {"https://twitter.com/": 21, "https://t.co/": 1, "https://twitter.com/Blog": 2, "http://t.co/": 38}, "blogtitle": "Freakonometrics"}, {"content": ["Pour la troisi\u00e8me s\u00e9rie d'exercices, les exercices sont en ligne ici . \n \n Je rappelle que lundi 29 octobre aura lieu le premier examen (qui compte pour 33,33% de la note finale, comme annonc\u00e9 dans le plan de cours, en ligne ici ). Il y aura 30 questions, en fran\u00e7ais, du m\u00eame type que celles vues toutes les semaines. La correction se fera suivant le principe de l'examen P de la SOA \n \" A candidate\u2019s score will be based on the number of questions answered correctly. No credit will be given for omitted answers, and no credit will be lost for wrong answers; therefore, a candidate should answer all questions, even if the candidate needs to guess. \""], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/19/Actuariat-1%2C-ACT2121%2C-troisi%C3%A8me-cours", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 2}, "blogtitle": "Freakonometrics"}, {"content": ["L'examen intra du cours ACT6420 m\u00e9thodes de pr\u00e9visions de cet automne aura lieu (comme promis ) le premier mercredi de novembre. La forme sera proche de l'examen donn\u00e9 cet hiver , avec 40 questions: une partie de questions \" th\u00e9oriques \", portant sur la compr\u00e9hension du cours, et une seconde partie portant sur l'analyse de sortie et de mod\u00e9lisation de donn\u00e9es (avec uniquement des questions sur la r\u00e9gression sur donn\u00e9es individuelles cette fois). Pour l'examen, la partie de mod\u00e9lisation portera sur la base suivante. \n > base = read.table ( \n + \"http://freakonometrics.free.fr/examen-act6420-A.txt\" , \n + sep= \";\" ) \n Il s'agit de donn\u00e9es collect\u00e9e lors de l'examen de cet hiver. \n > tail ( base ) \nNOTE PRED SEXE AGE MAT3080\n 36 57.5 65.0  F 25.0  61.5 \n 37 62.5  NA  F 21.4  95.0 \n 38 50.0  NA  H 24.6  52.5 \n 39 65.0 75.0  H 27.2  74.5 \n 40 57.5 62.5  H 28.8  57.5 \n 41 47.5 70.0  F 23.4  61.5 \n On dispose de 41 observations, avec la note obtenue \u00e0 l'examen final (choix multiple, note sur 100), du sexe de l'\u00e9tudiant ou de l'\u00e9tudiante, de son age au moment de passer l'examen, de sa note (ou un proxy de sa note, la lettre \u00e9tant ici convertie arbitrairement en chiffres) \u00e0 l'examen de statistique MAT3080 (qui est un pr\u00e9requis \u00e0 ce cours), et une pr\u00e9diction de la note, faite par chaque \u00e9tudiant au moment de remettre sa copie (cette question \u00e9tait libre, et sans cons\u00e9quence sur la note... m\u00eame si un bonus \u00e9tait accord\u00e9 \u00e0 toute personne qui pr\u00e9dirait parfaitement son nombre de bonnes r\u00e9ponses). L'examen intra, des questions porteront sur la mod\u00e9lisation de la note des \u00e9tudiants, voire de l'\u00e9cart de perception entre la note obtenue, et la note esp\u00e9r\u00e9e."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/17/ACT6420-examen-%28et-mise-en-abyme%29", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 2, "http://inside-r.org/": 4}, "blogtitle": "Freakonometrics"}, {"content": ["A few years ago, I went to listen to Roger Nelsen who was giving a talk about copulas with fractal support . Roger is amazing when he gives a talk (I am also a huge fan of his books, and articles), and I really wanted to play with that concept (that he did publish later on, with Gregory Fredricks and Jos\u00e9 Antonio Rodriguez-Lallena). I did mention that idea in a paper , writen with Alessandro Juri, just to mention some cases where deriving fixed point theorems is not that simple (since the limit may not exist). \n The idea in the initial article was to start with something quite simple, a the so-called transformation matrix, e.g. \n Here, in all areas with mass, we spread it uniformly (say), i.e. the support of is the one below, i.e. th of the mass is located in each corner, and is in the center. So if we spread the mass to have a copula (with uniform margin,)we have to consider squares on intervals , and ,\n \n Then the idea, then, is to consider , where is the tensor product (also called Kronecker product) of with itself. Here, the support of is\n \n \n \nThen, consider , where is the tensor product of with itself, three times. And the support of is\n \n Etc. Here, it is computationally extremely simple to do it, using this Kronecker product. Recall that if , then\n \n So, we need a transformation matrix: consider the following matrix, > k= 4 \n > M= matrix ( c ( 1 , 0 , 0 , 1 , \n +    0 , 1 , 1 , 0 , \n +    0 , 1 , 1 , 0 , \n +    1 , 0 , 0 , 1 ) , k , k ) \n > M\n [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] \n [ 1 , ]  1  0  0  1 \n [ 2 , ]  0  1  1  0 \n [ 3 , ]  0  1  1  0 \n [ 4 , ]  1  0  0  1 Once we have it, we just consider the Kronecker product of this matrix with itself, which yields a matrix, > N= kronecker ( M , M ) \n > N [ , 1 : 4 ] \n [ , 1 ] [ , 2 ] [ , 3 ] [ , 4 ] \n [ 1 , ]  1  0  0  1 \n [ 2 , ]  0  1  1  0 \n [ 3 , ]  0  1  1  0 \n [ 4 , ]  1  0  0  1 \n [ 5 , ]  0  0  0  0 \n [ 6 , ]  0  0  0  0 \n [ 7 , ]  0  0  0  0 \n [ 8 , ]  0  0  0  0 \n [ 9 , ]  0  0  0  0 \n [ 10 , ]  0  0  0  0 \n [ 11 , ]  0  0  0  0 \n [ 12 , ]  0  0  0  0 \n [ 13 , ]  1  0  0  1 \n [ 14 , ]  0  1  1  0 \n [ 15 , ]  0  1  1  0 \n [ 16 , ]  1  0  0  1 And then, we continue, > for ( s in 1 : 3 ) { N= kronecker ( N , M ) } After only a couple of loops, we have a matrix. And we can plot it simply to visualize the support, > image ( N , col = c ( \"white\" , \"blue\" ) ) As we zoom in, we can visualize this fractal property,"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/17/Fractals-and-Kronecker-product", "bloglinks": {}, "links": {"http://www.caltech.edu/": 1, "http://inside-r.org/": 8, "http://legacy.lclark.edu/": 1, "http://projecteuclid.org/": 1}, "blogtitle": "Freakonometrics"}, {"content": ["Petit compl\u00e9ments pour les \u00e9tudiants qui utilisent mac. Le fonctionnement est (beaucoup) plus simple que sous windows. Sur le fonctionnement, je vois essentiellement deux diff\u00e9rences. La premi\u00e8re est sur la localisation, > getwd ( ) \n [ 1 ] \"/home/charpentier\" Il suffit d'utiliser des slashs, qui est naturel sous mac (comme sous linux d'ailleurs) > setwd ( \"/home/charpentier/code-r/\" ) La seconde est sur l'installation de packages, qui peut se faire en ligne de commandes, > install.packages ( \"lmtest\" , dependencies= TRUE ) A part ces deux points, je crois que tout le reste est identique... Sinon, il est possible (sous mac, windows et linux) d'utiliser RStudio . Comme RStudio n'est pas install\u00e9 dans les salles informatiques, je ne l'utiliserais pas, mais le fonctionnement est relativement simple,"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/16/R-avec-un-mac", "bloglinks": {}, "links": {"http://rstudio.org/": 1, "http://inside-r.org/": 3}, "blogtitle": "Freakonometrics"}, {"content": ["Un peu de code pour le cours de demain, \n US= read.table ( \"http://freakonometrics.free.fr/US.txt\" , \nheader= TRUE , sep= \";\" ) \nabreviation= read.table ( \n \"http://freakonometrics.blog.free.fr/public/data/etatus.csv\" , \nheader= TRUE , sep= \",\" ) \nUS $ USPS= rownames ( US ) \nUS= merge ( US , abreviation ) \nUS $ state= tolower ( US $ NOM ) \nGV= read.table ( \n \"http://freakonometrics.blog.free.fr/public/data/governor.csv\" , \nheader= TRUE , sep= \";\" ) \netat= strsplit ( as.character ( GV $ State ) , \"-\" ) \nlisteetat= rep ( NA , nrow ( GV ) ) \n for ( i in 1 : nrow ( GV ) ) { \nlisteetat [ i ] =etat [ [ i ] ] [ 1 ] \n } \nindice= which ( is.na ( listeetat ) == FALSE ) \nbasegv= data.frame ( state = tolower ( listeetat [ indice ] ) , \n party =GV $ Party [ indice ] ) \n base = merge ( US , basegv ) Je mets aussi une petite fonction pour faire des graphiques, \n \n library ( maps ) \nVL0= strsplit ( map ( \"state\" ) $ names , \":\" ) \nVL=VL0 [ [ 1 ] ] \n for ( i in 2 : length ( VL0 ) ) { VL= c ( VL , VL0 [ [ i ] ] [ 1 ] ) } \nETAT= match ( VL , US $ state ) \n library ( RColorBrewer ) \ncarte= function ( V=US $ Murder , titre=\n \"Taux d'homicides aux Etats-Unis\" ) { \nvariable= as.numeric ( as.character ( cut ( V , \n quantile ( V , seq ( 0 , 1 , by = 1 / 6 ) ) , labels = 1 : 6 ) ) ) \nniveau=variable [ ETAT ] \ncouleur= rev ( brewer.pal ( 6 , \"RdBu\" ) ) \nnoml= levels ( cut ( V , quantile ( V , seq ( 0 , 1 , by = 1 / 6 ) ) ) ) \nmap ( \"state\" , fill = TRUE , col =couleur [ niveau ] ) ; \n legend ( - 78 , 34 , legend =noml , fill=couleur , \ncex= 1 , bty= \"n\" ) ; \n title ( titre ) } \ncarte ( US $ Murder , titre=\n \"Taux d'homicides aux Etats-Unis\" )"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/16/R%C3%A9gresion-multiple", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1, "http://inside-r.org/": 45}, "blogtitle": "Freakonometrics"}, {"content": ["La plupart des calculs pouvant se faire sans calculs (trop) complexes avec une calculatrice, je vais revenir sur un exercice (l'exercice 11 de la seconde s\u00e9rie) pour proposer une correction. \n \" Le petit Nestor collectionne les cartes de joueurs de Baseball dans les paquets de gommes \u00e0 m\u00e2cher. Il y a en tout 20 cartes diff\u00e9rentes (r\u00e9parties al\u00e9atoirement, une par paquet). Combien de paquets de gommes Nestor devrait-il s\u2019attendre \u00e0 avoir \u00e0 acheter pour obtenir la collection compl\u00e8te ? \"\n \nLa bonne strat\u00e9gie \u00e9tait d'ordonner les cartes par ordre d'apparition, pour la premi\u00e8re fois, et de mod\u00e9liser le nombre de paquets entre deux premi\u00e8res apparitions, comme indiqu\u00e9 sur le dessin ci-dessous Si d\u00e9signe le nombre total de paquets \u00e0 acheter, on note le nombre de paquets \u00e0 acheter, entre l'apparition de la \u00e8me nouvelle carte, et la \u00e8me (avec la convention que vaut 1, i.e. \u00e0 l'achat du premier paquet, on a notre premi\u00e8re carte). On notera que et \u00e0 partir de l\u00e0, les calculs sont simples, puisque \n \n soit (par lin\u00e9arit\u00e9 de l'esp\u00e9rance) \n \n \n i.e. \n \n  ou encore \n  La bonne r\u00e9ponse \u00e9tait alors (il faut sommer les vingt termes) \n > sum ( 20 / ( 1 : 20 ) ) \n [ 1 ] 71.95479 Mais cette sommation de vingt termes n'est pas triviale, aussi, en cours, j'avais sugg\u00e9r\u00e9 que\n \n  \n > log ( 20 ) * 20 \n [ 1 ] 59.91465 qui diff\u00e8re du r\u00e9sultat num\u00e9rique car il manquait la constante d'Euler \n  i.e. \n  Num\u00e9riquement, on obtient, en prenant pour 0.57721 (cf Google ) \n > ( log ( 20 ) + 0.57721 ) * 20 \n [ 1 ] 71.45885"], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/15/ACT2121%2C-%C3%A9l%C3%A9ments-de-correction", "bloglinks": {}, "links": {"https://www.google.ca/": 1, "http://inside-r.org/": 3, "http://fr.wikipedia.org/": 1}, "blogtitle": "Freakonometrics"}, {"content": ["Les transparents qui devraient couvrir la fin du cours sont en ligne . On y parle de diagnostique, de tests d'hypoth\u00e8ses, et d'extensions du mod\u00e8le lin\u00e9aire."], "link": "http://freakonometrics.blog.free.fr/index.php?post/2012/10/14/Mod%C3%A8les-de-r%C3%A9gression%2C-derniers-transparents", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1}, "blogtitle": "Freakonometrics"}]