[{"blogurl": "http://bpchesney.org\n", "blogroll": [], "title": "bpchesney.org"}, {"content": ["Suppose you have 3 sensors: A, B, C. Each has a set of readings that goes into a column of a matrix, b : \n  \n A matrix A is constructed such that b A , b B , b C represent compressed measurements in some domain. \u00a0 A is Gaussian, since this is a universal sensing matrix for compressive sensing (CS), due to its poor correlation to anything other than itself. [1] \n  \n The solution for each x i (i \u2208 A,B,C) represents the data recovered from these compressive measurements and can be found with min || x i || 1 , or some other sparsity-promoting technique. \n I am interested in the condition when all sensors (A,B,C) agree. \u00a0 Consensus , agreement among multiple sensors, is important for many applications. \u00a0One example is safing functions. Safing is checking for agreement among, sometimes redundant, sensors before engaging a drastic action, for example, deploying the airbags in the event of an automobile crash. When all sensors agree (or agree enough), then the dangerous function of deploying an airbag towards the driver and passengers can be performed. \n In this example, I am interested in the case when each sensor finds some nonzero sparse solution. Previously , I developed the notion of the diverse solution to a set of linear equations that require a positive integer solution. \u00a0The most diverse solution is the one that has values of roughly similar value. \u00a0In another post, I showed that the diverse solution can be used to mitigate risk . \n \n For the safing function we want to mitigate the risk that our sensor data is in error, indicating that we should erroneously employ our drastic action. \u00a0We use multiple redundant sensor to mitigate this risk. \u00a0The\u00a0agreement among sensors is maximized when diversity is maximized across all sensors. Even though we are seeking a sparse solution for each individual sensor (for example, min || x i || 1 ),\u00a0we seek to diversify the number of sensors that have witnessed a significant event, that is, have found a nonzero sparse solution. So the algorithm is a sparsity-based\u00a0linear program wrapped inside a diversity-based linear program. \u00a0When all sensors agree, a peak-to-sum ration (PSR) is minimized, if the sensitivity and units of each sensor are normalized. \u00a0Since we are nesting two linear programs together, it is important that the innermost program, recovering the compressively-sensed (CS) data, seeks a sparse solution, since this can be computationally efficient. \n The l1-norm minimization routine is used as an example of a CS recovery routine and exists inside the maximum diversity routine. \n  \n Where x (:,i) indicates the i th column of x . \u00a0The objective function value of the l1-norm minimization of each column of x is stored in a vector, u . \u00a0This is the innermost routine. \u00a0The outermost routine seeks to maximize the diversity of u by minimizing its peak-to-sum ratio. \n Let\u2019s look at some example solutions. \u00a0We\u2019ll just look at types of solutions that we can reach with this algorithm, independent of the b and A that actually generated them. \n  \n In the first example, above, all sensors seem to be in agreement that something happened (of magnitude 10). \u00a0This is indicated by the low PSR, which has a lower bound of 1/N (= 1/3, in this case). \u00a0There must be a little bit of noise in the readings, since the agreement isn\u2019t perfect, but this could be considered close enough to declare consensus. \u00a0Notice, also, that the sensors don\u2019t get exactly the same reading; the readings are basically shifted by one index. \u00a0If the columns of x represent samples in time, then each sensor is phase-shifted with respect to the others. \u00a0In this algorithm, we don\u2019t have to explicitly time-align the data \u2014 which can be a computationally-intensive step. \u00a0We just accept the data as is, even with small difference between sensors. \n  \n In the second example, above, 2 of the 3 sensors seem to agree. \u00a0One sensor appears to be broken, however, this has not completely destroyed our PSR. \u00a0We could still limp along with 2 functional sensors for a little while, until we are in a position to address the potentially defective sensor. \u00a0The algorithm has mitigated the risk of one sensor dying by using a diversity-promoting strategy. \n  \n In the third example, above, something is clearly busted . \u00a0One sensor has detected a significant event, but the majority have not. \u00a0A simple majority voting scheme would not be suspicious of this data set, but our nested diversity/sparsity linear program is. \u00a0Notice that the PSR is getting closer to its upper bound, which is 1. \u00a0At this level of PSR, our algorithm diagnoses our system of sensors as broken and can take action accordingly, instead of making a bad decision. \n The strategy of reaching a consensus among sensors by using both sparsity- and diversity-promoting techniques have made this system more robust. \u00a0However, the way the actual computation is performed hasn\u2019t been made clear yet. \u00a0Sparsity-based recovery techniques have been well covered and recently, I posted about how to solve a diversity-based integer program . \u00a0Next I\u2019ll look at how to nest these linear programs. \n \n References \n [1] Baraniuk, Richard. \u201cCompressive Sensing,\u201d IEEE Signal Processing Magazine . July 2007."], "link": "http://bpchesney.org/?p=1040&utm_source=rss&utm_medium=rss&utm_campaign=nested-linear-programs-to-find-agreement-among-sensors", "bloglinks": {}, "links": {"http://bpchesney.org/": 9}, "blogtitle": "bpchesney.org"}, {"content": ["In an earlier post , we saw the benefits of finding the solution to a set of equations with the most diversity , which was found by minimizing the peak-to-sum ratio (PSR). \u00a0This post discusses ways to formulate the linear program to arrive at that solution. \n Consider the following system of equations. \n  \n The A and b matrices for this system of equations are \n  \n We focus on finding an initial solution for the linear program using two methods. \n \n Method 1: Elimination and Back-substitution \n The first method for solving for the minimum PSR solution starts with Gaussian elimination and Jordanian back-substitution to find the initial solution. From there, the linear program iterates to find a feasible solution with the minimum PSR, that is also an integer solution, greater than 0. \u00a0With elimination and back-substitution, [ A b ] turns into: \n  \n This means that\u00a0 \n is a solution to the system of equations, if non-integer solutions are allowed . \u00a0However, in this problem, they are not. \u00a0The first method takes this solution and then finds the nearest integer vector \u2014 not necessarily a solution \u2014 and then proceeds to the minimum PSR solution in the feasible set, considering only integer vectors. \n For an underdetermined system of equations, using elimination and substitution will not yield a solution, at all. \u00a0Instead it will describe the system of equations in terms of certain variables for which values need to be chosen, called \u201cfree variables.\u201d \u00a0From the choice of free variables, the others are determined and the linear program can iteratively make better and better choices of free variables. \u00a0One common initial choice for the the free variables is to set them all to 0. \u00a0This, unfortunately, will yield the most sparse solution, which we saw earlier is the opposite of the most diverse solution. \u00a0Starting with the sparsest possible solution means the linear program will have to work for as long as possible to get to the most diverse solution. \n \n \u00a0Method 2: The Intersection of a Boundary Condition and the Minimum PSR Line \n Let\u2019s modify the system of equations slightly to look at them from a different angle. \u00a0We\u2019ll turn the \u201cgreater-than\u201d to \u201cless-than\u201d and add in another constraint. \u00a0This looks more like the investing example from earlier . \n  \n Note also that PSR is bounded for nonnegative integers. \n  \n The second method for finding the initial solution for the maximum diversity integer program exploits this fact: the PSR of an N-element, nonnegative, integer vector can never be below 1/N. \u00a0So if one of our equations has the form: \n  \n The minimum PSR integer program can reduce to: \n  \n \n So you can use a sparsity-promoting integer program to find the minimum PSR, maximum diversity solution once you subtract off the evenly distributed vector . Note that this vector does not need to be in the feasible solution space \u2013 it can be used as an initial value. Now the minimum PSR integer program can take advantage of the computational efficiency of and the extensive analysis devoted to sparsity-promoting integer programs. \u00a0We are looking for a vector that is not very different from the evenly distributed vector, just like with sparsity-promoting programs we look for a vector that is not very much different from 0 . \n \n Making Sure All the Units are the Same is a Big Deal \n If N >> S, then subtracting off the evenly distributed vector doesn\u2019t do much and we\u2019re pretty much back to trying to solve a system of equations by minimizing the first norm. \u00a0This only works if\u00a0 x \u00a0is sparse, otherwise, its a terrible approximation to the solution of\u00a0 Ax<=b . \u00a0However, N >> S is telling us that there isn\u2019t much to distribute, compared to the number of bins we are distributing to. \u00a0So we may not get a good, diverse answer anyway. \n Another way of looking at the constraint \n  \n is that it relates all of the elements of x to one another. \u00a0This equation requires that all of the units of x are the same, for the equation to make any sense. \u00a0Referring back to the investing example again, the A that gives the system of equations for this example, overall, is actually pretty sparse. \u00a0Without the two row vectors of A that are not mostly made up of zeroes, 1 T and p T , the possible values of x could take on wildly different values, if the values of b varied wildly. \u00a0For instance, x 2 could be on the order of 1000 and x 3 could be on the order of 1,000,000. \u00a0This would make it very difficult for an integer program that minimizes the first norm, as given in method 2, to arrive at a good answer. \n So the choice of starting with a dense row of A and finding its most diverse solution as an initial vector is not arbitrary. \u00a0For well-formed problems that seek a most diverse solution, it can actually be a great starting point. \u00a0If your system of equations for a problem does not have such a vector then you may not really need the most diverse solution or maybe you have not fully captured the problem. \u00a0Usually, a system of equations has most, if not all dense rows. \u00a0Then it becomes a question of finding the best one for finding an initial solution to the maximum diversity integer program, which I\u2019m sure is a whole investigation unto itself."], "link": "http://bpchesney.org/?p=994&utm_source=rss&utm_medium=rss&utm_campaign=solving-the-maximum-diversity-integer-program", "bloglinks": {}, "links": {"http://bpchesney.org/": 13}, "blogtitle": "bpchesney.org"}, {"content": ["Minimize risk by maximizing diversity. \u00a0Whether you\u2019re a trader deciding which stocks to invest in, an investor deciding which companies to invest in, or a manager trying to decide which projects to support, you can minimize your risk by investing in a portfolio of products. \u00a0This approach fits very nicely with the notion of diversity developed earlier . \n Suppose you have 5 companies seeking investment. An investor can purchase up to 30% of a company. Each company must project its value at the end of the investment period and the investor has this information, along with the price per share for each company. Different companies have different prices per share and different projected valuations at the end of the investment period. \n The investor cashes out and earns a share of the value of the company when the investment period is complete, proportional to the amount of the company the investor owns. For example, if the investor owns 3000 of the company\u2019s 10000 shares, 30% of the valuation of the company at the end of the investment period goes to the investor. \n The investor can only purchase whole shares of the company (no fractional shares, this is like a minimum investment). The investor may not \u2019short\u2019 a company, that is, sell shares of a company that the investor does not own. So the number of shares an investor chooses to purchase must either be 0 or some positive integer less than or equal to the number of shares for sale. \n Companies may or may not meet their valuation projections. Companies may go under, resulting in a valuation of 0 at the end of the investment period. The investor does not know how likely a company is to meet its goal or fold altogether. The goal is to avoid modeling this unknown and to develop a robust strategy based on the only two things the investor knows for sure: price per share and the projected valuation. \u00a0The investor seeks a 10% return and has $100,000 to invest. \n Let\u2019s write some equations to figure out what the investor should do. \u00a0We\u2019ll use x to represent the amount of money invested in each company. \n  \n The restriction on how much of each company the investor can own amounts to a restriction on the amount of money the investor can invest in each company. \u00a0Let\u2019s say that for company 1, the maximum investment is 50000, for company 2, its 40000, and for company 3 its 60000. \n  \n Each company expects to turn the investment they receive into a return to the investor. \u00a0We\u2019ll use p to represent the percentage return on investment each company claims they will return to the investor at the end of the investment period. \u00a0If a company claims a 10% return on their investment, then their value for p is 1.1, since they will return the initial investment, plus another 10% on top of that. \u00a0The investor has a goal of 10% return on total investment, so weighted sum of p , weighted by the amount invested, must be greater than 110000. \n  \n That last equation can be rewritten as: \n  \n \n So now we have our equations in the form Ax<=b , where \n  \n \n To find the most diverse solution, the optimization problem is \n  \n As opposed to the most common way of solving this problem, which is to focus\u00a0on maximizing return . \u00a0One way of maximizing return is to ignore the constraint of getting at least 10% on the investment and just get as much as possible. \u00a0That linear program is \n  \n where \n  \n Another way of maximizing the return is to leave the constraint as a minimum bound on the return. \u00a0That just tells the solver of the linear program to quit once 10% return is reached. \u00a0That kind of violates the spirit of maximizing your return and is not very illustrative when comparing it to maximizing diversity. \u00a0We\u2019ll treat max return as truly getting as much return as possible, without regard for how difficult the problem is for the solver, and drop the minimum return requirement for max return. \n Let\u2019s look at the two strategies: max diversity and max return. \u00a0Let\u2019s say that as a company offers more return, it\u2019s price per share is higher and, even accounting for differences in the number of shares available, that this translates into a higher maximum investment. \u00a0So the companies with a higher maximum investment are promising a bigger return. \u00a0Let\u2019s say p is: \n  \n The max return solution is: \n  \n This solution has a projected return of 14.1%. \u00a0This solution is 3-sparse for N=5, or 60% sparse. \u00a0It concentrates as much investment as possible into the highest returning company. \u00a0Once that investment is maxed out, then the rest goes into the second highest returning company until the total investment limit is reached. \n The most diverse solution is: \n  \n It has a PSR of 0.20834 and a return of 10.00016%. \n What happens if one of the companies fails? \u00a0If it\u2019s the high-return company, 4, the max return solution, since it is sparse, only returns 33.6% of the investor\u2019s money ($33600). \u00a0 However, the diverse solution is much more robust and still returns over 86% of the investor\u2019s money ($860411) if company 4 fails and the others meet their targets. \u00a0For a 4% gain on return, the risk that you would lose over 50% of your money \u00a0is clearly not worth it. \n Maximizing return could give a sparse solution, even when you don\u2019t specifically seek it. The sparse solution is not robust to the risk of total failure, in this case, zero return. The minimum PSR solution sacrifices return to deliver a diverse solution which is more robust to failure, that is, it is much more likely to have an acceptable return. Notice that I am comparing max return vs. the diverse solution as strategies . Allowing the sparse solution is what makes max return less robust. \u00a0When considering how to invest, the sparse solution is focusing your efforts, which could promise more return, but is less robust. \u00a0The diverse solution is hedging your bets. \u00a0Once the minimum return goal is met the diverse solution mitigates risk by spreading it out across a portfolio of investments."], "link": "http://bpchesney.org/?p=936&utm_source=rss&utm_medium=rss&utm_campaign=diversity-in-investing", "bloglinks": {}, "links": {"http://bpchesney.org/": 12}, "blogtitle": "bpchesney.org"}, {"content": ["We know about sparsity for vectors, which is the property of having a lot of 0\u2032s (or elements close enough to 0). \u00a0The 0\u2032s in a sparse matrix or vector, generally, make computation easier. \u00a0This has been a boon to sampling systems in the form of compressive sampling, which allows recovery of sampled data by using computationally-efficient algorithms that exploit sparsity. \n The opposite of sparsity is density. \u00a0A vector whose entries are mostly nonzero is said to be \u201c dense ,\u201d both in the sense that it is not sparse, as well as being slow and difficult to muddle through calculations on this vector if it is really large. \u00a0In general, dense vectors are not very useful. \u00a0Or are they? \u00a0In this post, I introduce the notion of diverse vectors, a subset of dense vectors, as a more interesting foil to sparsity than simply dense vectors. \n This post explores a measure\u00a0of diversity called the peak-to-sum ratio (PSR). We can use PSR to find the most diverse solution in a linear program, but what can it be used for? We\u2019ll find this linear program optimally distributes quantized, positive units, such as currency or genes. Maximizing diversity, by minimizing PSR, is desirable in several applications: \n \n Investing \n Workload Distribution \n Product Distribution \n Sensor Data Fusion and Machine Learning \n \n In contrast to some systems which allow dense vectors, these systems actually desire the most diverse solution. \n Consider the following system of equations: \n x 1 \u00a0+\u00a0x 2 \u00a0+\u00a0x 3 \u00a0+\u00a0x 4 \u00a0=\u00a012 \n x 2 \u00a0-\u00a0x 3 \u00a0\u00a0\u00a0\u00a0 =\u00a00 \n x 4 \u00a0=\u00a00 \n The following table gives a partial list of possible solutions (for x 1 , x 2 , and x 3 , since x 4 is already given). \n \n \n \n \n \n \n \n x3 \n x2 \n x1 \n \n \n 0 \n 0 \n 12 \n \n \n 1 \n 1 \n 10 \n \n \n 2 \n 2 \n 8 \n \n \n 3 \n 3 \n 6 \n \n \n 4 \n 4 \n 4 \n \n \n 5 \n 5 \n 2 \n \n \n 6 \n 6 \n 0 \n \n \n 7 \n 7 \n -2 \n \n \n \n The sparsest solution is highlighted in green, (12, 0, 0, 0). \u00a0It has the most zeroes in it. \u00a0It has everything concentrated in one element of the vector, the first element. \u00a0All other elements are zero, making several computations on that vector faster and easier. \n Notice the solution highlighted in yellow, (4,4,4,0). \u00a0It has very few zeros in it, in fact, none except the one variable that was explicitly set to 0. \u00a0But there are other solutions that have only one zero, or are, as they say, 1-sparse. \u00a0For instance, (10,1,1,0) is also 1-sparse. \u00a0The vector (4,4,4,0) is highlighted and (10,1,1,0) is not because (4,4,4,0) is more diverse . \u00a0What makes it more diverse? \u00a0The (4,4,4,0) vector has the most equitable distribution among its elements, which is, conceptually, the opposite of being sparse. \u00a0Whereas the most sparse solution has its energy concentrated in the fewest elements, the most diverse solution has its energy distributed to the most elements. \u00a0In fact, (10,1,1,0) could be considered to be more sparse than (4,4,4,0) as it is closer to having its energy concentrated in one vector (first posited in [1]). \n So if we\u2019re talking about more or less diverse/sparse, this implies that we can quantify it. \u00a0One way to quantify it is to look at the peak-to-sum \u00a0ratio (PSR). \u00a0The peak to sum ratio is defined as the ratio of the largest element in x to the sum of all elements of x . \n  \n PSR can also be expressed as the ratio of two norms, the infinity norm and the first norm. \u00a0Let\u2019s see what this ratio looks like for the solutions we considered above, plus a few more. \n \n \n \n \n \n \n \n \n x3 \n x2 \n x1 \n PSR \n \n \n 0 \n 0 \n 12 \n 1.000 \n \n \n 1 \n 1 \n 10 \n 0.833 \n \n \n 2 \n 2 \n 8 \n 0.667 \n \n \n 3 \n 3 \n 6 \n 0.500 \n \n \n 4 \n 4 \n 4 \n 0.333 \n \n \n 5 \n 5 \n 2 \n 0.417 \n \n \n 6 \n 6 \n 0 \n 0.500 \n \n \n 7 \n 7 \n -2 \n 0.438 \n \n \n 8 \n 8 \n -4 \n 0.400 \n \n \n 9 \n 9 \n -6 \n 0.375 \n \n \n 10 \n 10 \n -8 \n 0.357 \n \n \n 11 \n 11 \n -10 \n 0.344 \n \n \n 12 \n 12 \n -12 \n 0.333 \n \n \n 13 \n 13 \n -14 \n 0.325 \n \n \n \n The (4,4,4,0) solution appears to have the lowest PSR, until we proceed to solution (13,13,-14,0). \u00a0As we proceed past (13,13,-14,0), PSR becomes vanishingly small. \u00a0In fact, we see that (4,4,4,0) was just a local minima for PSR. \n  \n This is unsatisfying, for a couple of reasons. \u00a0The first is linear programming. \u00a0We can\u2019t use a linear program to minimize PSR and arrive at the most diverse solution because PSR is not convex , that is, we can\u2019t be sure we\u2019re on a path to smaller and smaller PSR because we could encounter a local minima, like we did at (4,4,4,0). \n The second way this is unsatisfying is that (13,13,-14,0) does not seem more diverse than (4,4,4,0). \u00a0If we\u2019re considering the elements of x to be bins of energy, or some other thing that is to be distributed, then having an element that is -14 does not make any sense. \u00a0Instead of distributing, or deciding how things are to be allocated, we are actually taking them away, which defeats the whole purpose of what we are trying to do. So let\u2019s add in a constraint that x has to be greater than 0 . \n For this particular problem, we can proceed in a linear fashion from the first candidate solution (12,0,0,0) to the most diverse one (4,4,4,0). However, in general, is this problem convex? That is, could we choose another set of equations and be able to march towards the most diverse answer? \n Consider the definition of convexity for a feasible set of solutions. \u00a0A set is convex if, when proceeding from one point to the next, you stay in that set [2]. \u00a0It helps to rewrite our problem in terms of linear algebra. \u00a0We seek the most diverse solution to a set of linear equations, \n  \n \n where Ax=b represents our equations we\u2019re trying to solve. \u00a0Since x cannot be negative, we know that, \n  \n Is this convex? \u00a0The shaded region in the following graph shows the x\u2019s that satisfy the quantity (PSR) to be minimized when it\u2019s less than or equal to 1. \n  \n \n In the example in the graph above, there are only two elements in x. \u00a0The possible values of x outside of the shaded region, when either x1 or x2 or both are negative, are not in the set of feasible solutions. \u00a0So do the answers in this shaded region form a convex set? Yes, because we can pick two points in the shaded region, and draw a straight line between them that has to stay in the shaded region. \n  \n Notice if we restrict values of the elements of x to the integers, we still have a convex set! \u00a0This result is satisfying because we\u2019ll want to distribute units of energy, currency, etc\u2026, anyway. \n In the language of linear algebra, \n  \n where Z represents the set of integers. \u00a0So the two constraints we\u2019ve added to our convex optimization problem to find the most diverse solution: \n \n Must not contain a negative distribution (x greater than or equal to 0) \n Must have a base unit of distribution (x must contain integers) \n \n We can now use an integer program to perform the optimization. \u00a0Restricted to positive, integer values, when we minimize PSR, our linear program will march algorithmically, inexorably to the most diverse solution to a system of linear equations. \n \n References \n [1] \u00a0Zonoobi et al. \u201cGini Index as Sparsity Measure for Signal Reconstruction from Compressive Samples,\u201d \u00a0 IEEE Journal of Selected Topics in Signal Processing . \u00a0vol 5, no. 5, Sept. 2011. \n [2] \u00a0Strang, Gilbert. \u00a0 Computational Science and Engineering . \u00a0Wellesley, MA: Wellesley-Cambridge Press, 2007."], "link": "http://bpchesney.org/?p=894&utm_source=rss&utm_medium=rss&utm_campaign=diversity-mathematically", "bloglinks": {}, "links": {"http://bpchesney.org/": 8, "http://en.wiktionary.org/": 1}, "blogtitle": "bpchesney.org"}, {"content": ["This post is a framework for diagnosing diseases with artificial intelligence. \u00a0It draws its inspiration heavily from a transcript of Henry Cohen \u2018s excellent lecture in 1943, \u201cThe Nature, Method and Purpose of Diagnosis.\u201d \u00a0I liked reading Cohen\u2019s lecture because it is clear and concise and seemed to fit well with an artificial intelligence approach to diagnosing diseases. \u00a0My interest in this subject is in developing algorithms to make diagnoses. \n There are no diseases, only disease. \n [1] \n This kind of sums the whole thing up and is a great place to start. \u00a0This quote reinforces the idea that we\u2019re not looking to exhaustively search innumerable avenues. \u00a0We\u2019re looking to find what, at its root, is bothering the patient. \n Why artificial intelligence to diagnose diseases? \u00a0The reason is to provide a consistently high quality of patient care, a quality of care that is repeatable and reliable. \u00a0Cohen argues that one of the main problems with patient care is consistency. \u00a0The same patient could get different diagnoses from different doctors. \u00a0Doctors are human, after all, and clearly differ. \u00a0How do doctors differ? \n \n Observational prowess \n Knowledge of symptoms, signs, and syndromes of disease \n Interpretive ability \n Use different labels \n \n [1] \n Creating and deploying an artificial intelligence based system with the same observational and interpretive abilities, and a consistent taxonomy would relieve the confusion of conflicting medical advices. \n The literature on using artificial intelligence to algorithmically make medical diagnoses is surprisingly timid. \u00a0Usually, attempts at automatically diagnosing diseases are couched in words like, \u201cassist\u201d and \u201cconsult\u201d and rarely, if ever, take full responsibility for making the diagnosis. \u00a0One author suggested a reason for this: trepidation about encroaching on the doctor patient relationship. \u00a0Usually, algorithmic diagnostic systems are tailored to diagnosing specific disease. \u00a0The task of this post is to outline a framework for diagnosing disease. \n \n Observation \n The first stage in the diagnosing disease is the recognition of simple quantitative deviations from normal. \n [1] \n This sentence from Cohen provides a great start to our diagnostic engine. \u00a0\u201dFirst stage\u201d implies that a simple state machine structure for the main diagnostic engine is appropriate, with the first stage seeking to gather information and compare it to what is considered normal for that patient. \n \n  \n The inputs to the observation stage are the patient\u2019s medical history, their testimony about why they are seeking diagnosis and a physical examination. \u00a0Processing the patient\u2019s testimony can exist on a spectrum with one end anchored by using natural language processing (NLP) to parse and comprehend the patient\u2019s testimony and the other by letting the patient choose from a drop down menu. \u00a0The former is much more sophisticated, involved, but more accurately answers the task as it allows the patient to consult the device when they don\u2019t know what is wrong, which makes the device infinitely more usable. \u00a0The latter essentially boils down to a choose-your-own adventure approach, is trivial to implement, but does not significantly move the needle beyond simple internet searches, leaving the responsibility of diagnosis or deciding whether or not to consult a doctor in the hands of the patient. \u00a0The NLP approach, or something close to it is preferred. \n The NLP lies outside of the main diagnostic engine so that different algorithms can be swapped in and out seamlessly. \u00a0The diagnostic flow should not depend on the specifics of the NLP used to parse the patient\u2019s testimony. \u00a0The NLP outputs certain key words gleaned from the patient\u2019s testimony ordered and weighted in terms of \u201cimportance.\u201d \u00a0Clearly, the NLP will need to know what the observation stage of the diagnostic engine considers important, but does not need to be embedded in the diagnostic engine. \n The patient\u2019s testimony, their medical record and any vital signs recorded are collected in the observation stage and passed to the next stage. \u00a0The observation stage may and probably will order or perform certain biometric tests on the patient and wait for those results before proceeding to the next stage, Interpretation. \u00a0For example, the observation stage may perform and pass on the results of an electrocardiogram (ECG) test based on certain watchwords figuring prominently in the patient\u2019s testimony. \n The observation stage looks at physical exam measurements and compares them to expected values given the patient\u2019s data form their medical records (age, height, weight, gender\u2026). \u00a0It also performs additional tests based on simple keyword matching from the patient\u2019s testimony. \u00a0The observation stage writes back to the patient\u2019s file and passes everything onto the interpretation stage. \u00a0The observation stage is like the nurse taking your vital signs and the interpretation stage is like the doctor who comes to make a medical diagnosis. \n The output of the observation stage is an up-to-date medical record of the patient. \u00a0The ability to index the patient\u2019s record temporally is important as this allows the Interpretation stage to analyze how a particular condition has changed over time. \n \u00a0 \n Interpretation \n The interpretation stage can ask the patient questions directly to obtain additional information. \u00a0It is better to keep this ability directly in the interpretation stage instead of going back to the observation stage because this new information will probably be directly linked to branching in this stage of the diagnosis. \u00a0To simplify things, the query/response format in the interpretation stage does not need to be as open-ended, from a linguistic standpoint, as in the observation stage. \u00a0In the observation stage, natural language processing is needed because we may not be sure what we are trying to figure out yet, so we have to derive meaning from a very complex set of possibilities. \u00a0However, in the interpretation stage, we are seeking specific, targeted information, so the response options should be limited. \u00a0This fits well into a dropdown box, an example is below. \n Have you felt chest pain? \n \n Yes \n No \n I don\u2019t know \n I don\u2019t understand the question \n It\u2019s complicated \n \n \n The last three, \u201cI don\u2019t know\u201d, \u201cI don\u2019t understand the question,\u201d and \u201cIt\u2019s complicated\u201d allow the response/query routine to improve its chances for getting a useful answer out of the patient. \n The AI for medical diagnosis will need to\u00a0 reason anatomically , that is, it will have to move from one part of the body to the other in search for interpretations that fit the existing data. \u00a0Cohen considered the \u201cfundamental tripod of medicine\u201d to be anatomy, physiology and pathology. \u00a0Of these, anatomy lends itself well to being described as a connectivity graph. \u00a0The AI could have different graphs for different systems in the body such as circulatory, respiratory, endocrine, \u2026. each describing how different parts of the body are connected together. \u00a0A simple 1, 0 (connected, not connected) would probably do as the AI is simply looking for what to try next, that is, once it traverses the graph from, say, the heart to the liver, it is using \u201cliver\u201d as a keyword to lookup potential next steps. \n What about a Bayesian approach to interpretation? \u00a0I would stay away from it because it relies on \u201cmodels that are subjective, and the resulting inference depends greatly on the model selected.\u201d [2] \u00a0We are seeking a framework that can be used for diagnosing a wide range of diseases, not tuned to specific diseases. \u00a0The framework must be general and its reasoning mathematical. \u00a0The reasoning itself cannot have a subjective foundation. \n The output of the interpretation stage is a provisional medical decision about which steps to take next. \u00a0If the algorithm does not have enough information to make a decision, it does not need to do so. \u00a0It can order more tests or suggest a therapy to alleviate the condition and have the patient report back. \u00a0When the patient reports back, they start again in the observation phase. \n \n Symbolization, Corrective Action and Evaluating an Algorithm \n Even after the diagnostic engine reaches the heart of the matter, what\u2019s wrong with the patient, there is still much more work to do. \u00a0First it must encode the diagnosis in a manner that will allow it to treat the same disease the same way, every time, from patient to patient. \u00a0The Oxford American Dictionary defines syndrome as \n syndrome n. 1) a group of concurrent symptoms of a disease \n If the list of syndromes for a disease is complete enough, it will uniquely identify a disease. \u00a0Cohen assesses syndromes as the\u00a0site, functional disturbances and cause of disease [1]. This should be enough information to universally encode the disease. \u00a0Notice we have not included any prescriptive remedy in the encoding as this will vary from patient to patient as patients with the same disease at the same site may need different courses of action based on age, gender\u2026. \n Second, we must figure out the cause of the disease and its implications. \n Too frequently we have been content with a diagnostic label without investigating its implications. \n [1] \n Causation implies a search for antecedents, and not for the ultimate \u2014 the final \u2014 cause of all things. \u00a0This means not a single antecedent or even a chain of antecedents, but a whole interlacing network \u00a0of them. \n [1] \n This points directly to graph theory for reasoning through the causes and implications for the disease. \u00a0Somehow we\u2019d need to map corporal function to a manifold and be able to traverse it. \u00a0This is significantly more complicated than the simple graph traversal in the Interpretation stage, as there, we are simply seeking clues to help us along our decision making tree. \u00a0We\u2019ve already mapped the several systems of the body to graphs: circulatory system, skeletal, respiratory system, and are simply looking them up. \u00a0In this stage we\u2019d likely need to do the mapping on the fly based on what we figured out from the previous stage. \n In addition to affixing a label to the diagnosis, the output of this stage is to recommend a corrective action. \n The main aim of diagnosis, that of providing the rational basis for treatment and prognosis\u2026 \n [1] \n The main implementation decision to make here is: do we spend more time/energy investigating causes and implications and make the treatment recommendation and prognosis estimate simpler or vice versa. \u00a0For instance, if the algorithm is good at figuring out causation and implication, maybe the treatment and prognosis can be a simple look up table. \u00a0If causation/implication is simple, then we\u2019ll want to do something more complicated for treatment/prognosis. \u00a0I prefer the former. \u00a0Because they are so tightly coupled, causation/implication and prognosis/treatment I consider them part of the same stage of diagnosis, even though they may have separate artificial intelligence approaches. \n \n Evaluating a Medical Diagnosis Algorithm \n \u2018\u2026common sense pressed for time accepts and acts on acceptance.\u2019 \u00a0We physicians are often confronted by a situation in which we have to give a provisional verdict on the admittedly inadequate available evidence. \n [1] \n For any algorithm, execution time will be crucial. \u00a0The algorithm will have to provide feedback quickly to the patient, even if it does not have a final diagnosis. \u00a0The user experience aspect of keeping the patient informed as the algorithm works through its reasoning will help the patient become comfortable with seeking \u00a0diagnosis from a machine, as opposed to a human doctor. \u00a0Time is of the essence; in fact, the algorithm should not get bogged down in spending clock cycles on getting every corner case right in exchange for reaching most common conclusions quickly. \n The New England Journal of Medicine published the results of case records presented to clinicians and discussant groups and whether or not they were able to correctly diagnose the disease in the case studies. \u00a0Of the 43 case studies, individual clinicians were correct 65% of the time and the discussant groups were correct 80% of the time. \u00a0The study asked the participants to assess the confidence level of their diagnosis as well. \n \n \n \n \n \n \n \n \n \n Clinicians \n Discussant \n \n \n Correct, definite \n 23 \n 29 \n \n \n Correct, tentative \n 5 \n 6 \n \n \n Total \n 43 \n 43 \n \n \n \n [3] \n It seems that being right 80% is good enough, at least it was the state of the art in 1969. \u00a0That\u2019s another important thing to remember when testing medical diagnostic AI: what are we comparing it against? \u00a0If medical diagnostic AI can approach 80% success rate then it will be a viable alternative to seeing a doctor. \u00a0Even at roughly 50% success rate, its a reasonable alternative. \u00a0For some purposes such as the Tricorder X-Prize, this should be good enough since the potential use of the Tricorder X-Prize device would be to help people decide if they should go see a doctor. \n \n References \n [1] Cohen, Henry. \u201cThe Nature, Method and Purpose of Diagnosis,\u201d The Skinner Lecture, 1943 . Cambridge, UK: University Press, 1943. \n [2] Hogg, Robert and Allen Craig. \u00a0Introduction to Mathematical Statistics. \u00a05th ed, NJ: Prentice-Hall, 1995. \n [3] Case Records of the Massachusetts General Hospital (Case 30-1969). \u00a0 New England Journal of Medicine . \u00a01969; 281: 206-213."], "link": "http://bpchesney.org/?p=855&utm_source=rss&utm_medium=rss&utm_campaign=the-artificial-intelligence-of-diagnosing-diseases", "bloglinks": {}, "links": {"http://bpchesney.org/": 2, "http://en.wikipedia.org/": 1}, "blogtitle": "bpchesney.org"}, {"content": ["The Tricorder X-Prize is a $10M competition to foster innovation in medical diagnostics. \u00a0The goal of the competition is to create a medical device that can diagnose 15 diseases. \u00a0The competition\u00a0 guidelines do not state which 15 diseases the tricoder will need to diagnose, however, it seems the competition guidelines will be refined in September. \u00a0Maybe they will be announced at that point. \u00a0Maybe they won\u2019t be announced before the competition. \u00a0For now, it\u2019s fun to speculate which disease should be included. \n In 2008, the Center for Disease Control and Prevention did a survey of ambulatory care in the US. They summarized the most prevalent diagnoses at office visits for nearly a million participants. \u00a0The most common of all diagnoses was\u00a0 essential hypertension .The fourth most common diagnosis was diabetes mellitus. \u00a0 Each\u00a0of these\u00a0medical conditions has a fairly well-understood decision tree for diagnosis. \n \n \n \n \n \n \n \n \n Primary Diagnosis \n \u00a0 \u00a0 \u00a0 Number of Visits \n \u00a0 \u00a0 Percentage \n \n \n Essential hypertension \n 45,969 \n 4.81% \n \n \n Routine infant or child health check \n 43,178 \n 4.52% \n \n \n Acute upper respiratory infections, excluding pharyngitis \n 29,296 \n 3.06% \n \n \n Arthropathies and related disorders \n 28,404 \n 2.97% \n \n \n Diabetes mellitus \n 25,365 \n 2.65% \n \n \n Spinal disorders \n 24,376 \n 2.55% \n \n \n Normal pregnancy \n 22,140 \n 2.32% \n \n \n General medical examination \n 20,913 \n 2.19% \n \n \n Malignant neoplasms \n 19,770 \n 2.07% \n \n \n Rheumatism, excluding back \n 18,757 \n 1.96% \n \n \n Specific procedures and aftercare \n 18,372 \n 1.92% \n \n \n Follow up examination \n 17,652 \n 1.85% \n \n \n Heart disease, excluding ischemic \n 17,017 \n 1.78% \n \n \n Gynecological examination \n 16,140 \n 1.69% \n \n \n Otitis media and eustachian tube disorders \n 15,812 \n 1.65% \n \n \n Disorders of lipoid metabolism \n 15,274 \n 1.60% \n \n \n Ischemic heart disease \n 14,448 \n 1.51% \n \n \n Chronic sinusitis \n 12,506 \n 1.31% \n \n \n Acute pharyngitis \n 11,729 \n 1.23% \n \n \n Allergic rhinitis \n 9,966 \n 1.04% \n \n \n All other diagnoses \n 528,885 \n 55.32% \n \n \n TOTAL \n 955,969 \n 100.00% \n \n \n \n Table 1: Primary Diagnosis Groups from NAMCS 2008 Survey [1] \n \n My understanding \u2014 and I am not a doctor \u2014 is that hypertension is diagnosed primarily with a high blood pressure reading. \u00a0You do have to make sure that the reading is repeatable and not primarily influenced by external factors, such as the presence of a doctor. \u00a0Overall, it sounds like diagnosing hypertension boils down to getting consistently high blood pressure readings for the patient\u2019s profile (gender, age, etc\u2026). \u00a0Blood pressure is not difficult too measure non-invasively \u2014 you see blood pressure monitoring machines in grocery stores. The main design consideration for the Tricorder competition would be is there an even less non-invasive way to do it? \u00a0One that does not involve requiring the patient to strap a band around themselves. \u00a0Even using a traditional approach, for the price of a blood pressure monitor, a device could diagnose nearly 5% of all office visits in the US. \n Diabetes mellitus is #5 with 2.7% of office visit diagnoses. \u00a0Again, my understanding is that the decision tree is pretty simple: blood glucose readings outside of the norm for a patients profile. \u00a0However, blood glucose is traditionally measured very invasively, by taking a small blood sample. \u00a0While the Tricorder X-Prize guidelines do not rule out devices that use invasive techniques, they strongly encourage noninvasive techniques. \u00a0In fact, a medical doctor on our board at Chesney Research , described noninvasive blood glucose monitoring to me as one of the \u201c holy grails \u201d of medical device technology. \u00a0Since one of the stated goals of the competition is to drive sensor technology, I think diagnosing diabetes has to be one of the diseases in the competition. \n Another holy grail is characterizing bacterial versus viral upper respiratory tract infection . \u00a0This disease is the third most prevalent diagnosis in office visits, according the NAMCS survey. \u00a0Right now, there\u2019s no real way to tell the difference other than waiting; bacterial infections tend to last 7-10 days and viral only 2. \u00a0However, the course of treatment is very different for each: antibiotics for the bacteria, but not for the virus, since they do not respond to antibodies. \n Further down the list is heart disease , the non-ischemic variety, that is, not due to low blood volume. \u00a0Heart disease is a pretty broad category. \u00a0However, there are analog integrated circuits on the market aimed at measuring electrocardiogram (ECG) signals. \u00a0For the price of this chip (typically around $20) and the appropriate interface with the patient, a medical device could take a big step towards diagnosing heart disease. \u00a0There is also a wealth of information on the links between heart disease and hypertension and heart disease and diabetes. With an ECG, a blood pressure monitor, a glucose meter and some fancy AI, a team may be well on its way to gobbling up a significant portion of heart diseases diagnoses. \u00a0In fact, those three, hypertension, diabetes and heart disease, would get you nearly one out of every ten (9.24%) of all office visit diagnoses. \n If we look at the NACMS top 20 again, and take out routine follow-ups, checkups and pregnancy, we are left with 14 diseases. \u00a0They are given in Table 2. \u00a0They accounted for nearly one third of all office visits in 2008. \n \n \n \n \n \n \n \n \n \n Rank \n Primary Diagnosis \n Number of Visits \n Percentage \n \n \n 1 \n Essential hypertension \n 45,969 \n 4.81% \n \n \n 3 \n Acute upper respiratory infections, excluding pharyngitis \n 29,296 \n 3.06% \n \n \n 4 \n Arthropathies and related disorders \n 28,404 \n 2.97% \n \n \n 5 \n Diabetes mellitus \n 25,365 \n 2.65% \n \n \n 6 \n Spinal disorders \n 24,376 \n 2.55% \n \n \n 9 \n Malignant neoplasms \n 19,770 \n 2.07% \n \n \n 10 \n Rheumatism, excluding back \n 18,757 \n 1.96% \n \n \n 13 \n Heart disease, excluding ischemic \n 17,017 \n 1.78% \n \n \n 15 \n Otitis media and eustachian tube disorders \n 15,812 \n 1.65% \n \n \n 16 \n Disorders of lipoid metabolism \n 15,274 \n 1.60% \n \n \n 17 \n Ischemic heart disease \n 14,448 \n 1.51% \n \n \n 18 \n Chronic sinusitis \n 12,506 \n 1.31% \n \n \n 19 \n Acute pharyngitis \n 11,729 \n 1.23% \n \n \n 20 \n Allergic rhinitis \n 9,966 \n 1.04% \n \n \n \n \n TABLE TOTAL \n 288,689 \n 30.20% \n \n \n \n TOTAL DIAGNOSES \n 955,969 \n 100.00% \n \n \n \n \u00a0Table 2: Top 14 Diseases, Including Chronic Conditions from NAMCS 2008 Survey Data \n \n The competition\u2019s 15 diseases will need to be diagnosed on 30 different patients and the Tricoder will be evaluated for its effectiveness and ease of use by a panel of judges. \u00a0The devices should be able to tell the patient if they need to go see a doctor or not. \u00a0These 14 diseases are a good place to start. \n \n References \n [1] National Ambulatory Medical Care Survey: 2008 Summary Tables. \u00a0The Center for Disease Control and Prevention. \u00a0 http://www.cdc.gov/nchs/ahcd.htm"], "link": "http://bpchesney.org/?p=839&utm_source=rss&utm_medium=rss&utm_campaign=which-diseases-to-diagnose-for-tricorder-x-prize", "bloglinks": {}, "links": {"http://www.chesneyresearch.org": 1, "http://www.qualcommtricorderxprize.org/": 1, "http://www.cdc.gov/": 2}, "blogtitle": "bpchesney.org"}, {"content": ["A couple of weeks ago, Qualcomm announced they are sponsoring an\u00a0 X-prize to come up with a health care device . \u00a0The prize is $10 M and the device must diagnose 15 diseases \u2014 it doesn\u2019t say which ones. \u00a0The goal is to make a consumer device, \u00a0dubbed a tricorder,\u00a0that people can use in their homes to provide medical care, which requires advances in sensor technology, medical diagnostics and artificial intelligence, among other things, according to the site. \n In reading the overview , there is a section about the potential trade-offs a design team in the competition will have to make. \u00a0One of those mentioned is the placement of the AI engine . \u00a0I think a more important concern is what kind of artificial intelligence would be in the device and how it would interact with the sensors and the user. \u00a0From that the best placement of the AI engine would probably become pretty obvious. \n The authors of the overview are right to draw attention to the AI in the tricorder, as diagnoses, seems to be the true intent of the device. \u00a0The emphasis is not so much on the accuracy and resolution of the sensors themselves that are used \u2014 their resolution will be determined by what is just good enough to make an accurate diagnosis. \u00a0Instead the real focus of the competition is in the diagnoses, the intelligence, and this is ripe for machine learning . \n One of the most important machine learning functions in the device will be its natural language processing . \u00a0Telling the doctor your symptoms is still a major aspect of any patients experience with any medical conditions. \u00a0We\u2019re not yet at the stage where people can just submit to series of measurements and get a diagnosis. \u00a0From both standpoints of the patients comfort level with the device as well as our own understanding of medicine, any effective diagnosis machine will have to be able to understand a person\u2019s description of why they are consulting the device for medical help in the first place. \u00a0A major aspect of any medical diagnosis is how the patient feels, how much pain they are experiencing and how the condition for which they are seeking help is affecting them. \u00a0Without a sensor that can accurately measure pain, we have to rely on the patients words. \n The effective tricorder will have to navigate the language as well as incorporate information from sensors to arrive at an effective diagnosis. \u00a0I think this is the first consideration when designing the tricorder: how will it parse the patient\u2019s description of the problem. \u00a0Then blend in the information from the sensors to complete the story."], "link": "http://bpchesney.org/?p=830&utm_source=rss&utm_medium=rss&utm_campaign=tricorder-x-prize-an-opportunity-for-machine-learning", "bloglinks": {}, "links": {"http://www.qualcommtricorderxprize.org/": 2}, "blogtitle": "bpchesney.org"}, {"content": ["Over at the Chesney Research website , I posted an introduction to compressive sensing . \u00a0There\u2019s already lots of good information out there on what is compressive sensing. \u00a0This introduction is a little different because it gives an overview of compressive sensing without getting too far into the math and stays focused on why you would want to use it."], "link": "http://bpchesney.org/?p=828&utm_source=rss&utm_medium=rss&utm_campaign=introduction-to-compressive-sensing", "bloglinks": {}, "links": {"http://chesneyresearch.org/": 1, "http://www.chesneyresearch.org/": 1}, "blogtitle": "bpchesney.org"}, {"content": ["I\u2019ve been working on clustering as it pertains to graph partitioning. \u00a0A brief introduction is up at\u00a0 http://www.chesneyresearch.org/cluster.html ."], "link": "http://bpchesney.org/?p=824&utm_source=rss&utm_medium=rss&utm_campaign=clustering", "bloglinks": {}, "links": {"http://www.chesneyresearch.org/": 1}, "blogtitle": "bpchesney.org"}, {"content": ["I was thinking of the immutability of eigenvectors and the immutability of certain vectors when projected and realized these two qualities are one in the same. \u00a0Eigenvectors can be viewed and explained in terms of a projection matrix, which may be a more intuitive and easier way to understand eigenvectors than is commonly taught. \u00a0Certainly it relies on much less math \u2014 only the concept of rows or columns as vectors and a basic understanding of the fundamental and canonical equation that defines eigenvectors and eigenvalues. \n Projecting one vector onto another gives a scalar length of the first vector on the second. \u00a0It is the amount that the second vector \u201ccontributes\u201d to the first. \u00a0This contribution is easily calculated by taking the dot product. \n  \n In the above example, the vectors are given as column vectors, and since their dot product must result in a scalar quantity, it is calculated as a T b . \n  \n The projection of\u00a0 b \u00a0onto a creates another vector, let\u2019s call it\u00a0 p , that is in the same direction as\u00a0 a , but whose length is determined by the length and direction of\u00a0 b . \n  \n Strang\u2019s Introduction to Linear Algebra book gives a great explanation of projection. \u00a0In it, he rearranges the terms of p to get \n  \n \n P a \u00a0is a rank 1 projection matrix, which makes sense because we are projecting onto the scalar\u2019s one-dimensional space [1]. \u00a0The projection matrix, P a , is a transform that projects a vector onto a . \u00a0The result of this transform is a scaled version of the vector a . \n  \n The lambda parameter gives the amount of scaling on a . \u00a0The answer gives how much of a is in b , sometimes referred to as the component of a in b . \n Projection is a way of moving one vector onto another, that is, the projection changes the direction of a vector to line up with another. \u00a0More generally, an n-by-n matrix is a transform that changes an n-element vector into another n-element vector, possibly pointing in a different direction. \n The vectors that are able to survive this transform unmoved are called eigenvectors. \u00a0They may be scaled differently as a result of the transform, but they are pointing in their same original direction before the transform was applied. \u00a0The amount that the transform scales the vector is the eigenvalue. \n  \n This is the familiar linear algebra definition for eigenvalues and eigenvectors. \u00a0The vector x is an eigenvector of A , if the above equation holds for some nonzero x . \u00a0Lambda gives the eigenvalue for that eigenvector, x . \n Numerically eigenvectors and eigenvalues look like this, \n  \n Where [1;2] is an eigenvector of [1,3;4,5] with eigenvalue 7. \u00a0The vector [1;2] survives the transform pointed in the same direction, but with a different scale value (7). \n Some vectors do not survive the transform without a change in direction. \u00a0For instance, \n  \n The vectors that are able to survive this transform unmoved, i.e. pointed in the same direction, are the eigenvectors of that transform. \u00a0The eigenvalues are the scale values resulting from the transform. \n \n \n [1] Strang, Gilbert. \u00a0 Introduction to Linear Algebra . \u00a0Wellesley, MA: Cambridge-Wellesley Press, 2009."], "link": "http://bpchesney.org/?p=794&utm_source=rss&utm_medium=rss&utm_campaign=projections-and-eigenvectors", "bloglinks": {}, "links": {"http://bpchesney.org/": 8, "http://math.mit.edu/": 1}, "blogtitle": "bpchesney.org"}]