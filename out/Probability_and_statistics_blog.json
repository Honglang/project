[{"blogurl": "http://www.statisticsblog.com\n", "blogroll": [], "title": "Probability and statistics blog"}, {"content": ["\u201c[I]f you have performed any statistical analysis that is more complex than calculating the mean and the standard deviation, you should perform the same analysis on noise to make sure that whatever effect you observe is indeed a unique feature of your data and not an artefact of the analysis.\u201d \n Found this one over at Stefan\u2019s sieste blog . I couldn\u2019t agree more, especially now that computers and big data sets entice us to make ever more complex models. Oh, and that\u2019s not a bad thing! As I\u2019ve argued, we\u2019ll need to give up on simple, easy to interpret models in order to get more predictive power . \n I\u2019d go even more meta than Stefan and argue that you should re-test your entire model-creating process on noise (perhaps he meant this with his quote). If you started with a data set, then ran a stepwise variable selection algorithm, then added in a new non-linear term to get a better fit, do the same on noise, trying to get the best fit. Are you able to get a statistically significant result? Better still, run the same procedure on different types of noise, not just Gaussian White (I know, sounds like something you\u2019d load into a syringe. Normality, the gateway drug?)."], "link": "http://www.statisticsblog.com/2012/10/recommendation-of-the-week/", "bloglinks": {}, "links": {"http://sieste.wordpress.com/": 1, "http://www.statisticsblog.com/": 1}, "blogtitle": "Probability and statistics blog"}, {"content": ["Lately I\u2019ve been thinking about how to measure the fatness of the tails of a distribution. After some searching, I came across the Pareto Tail Index method. This seems to be used mostly in economics. It works by finding the decay rate of the tail. It\u2019s complicated, both in formula and in it\u2019s R implementation (I couldn\u2019t get \u201cawstindex\u201d to run, which supposedly can be used to calculate it). The Index also has the disadvantage of being a \u201ccurve fitting\u201d approach, where you start by assuming a particular distribution, then see which parameter gives the best fit. If this doesn\u2019t seem morally abhorrent to you, perhaps you have a future as a high-paid econometrician . \n In the past I\u2019ve looked at how to visualize the impact of the tails on expectation , but what I really wanted was a single number to measure fatness. Poking around the interwebs, I found a more promising approach. The Mean Absolute Deviation (or MAD, not to be confused with the Median Absolute Distribution, or MAD) measures the average absolute distance between a random variable and it\u2019s mean. Unlike the Standard Deviation (SD), the MAD contains no squared terms, which makes it less volatile to outliers. \n As a result, we can use the MAD/SD ratio as a gauge of fat-tailedness. The closer the number is to zero, the fatter the tails. The closer the number is to 1 (it can never exceed 1!), the thinner the tails. For example, the normal distribution has a MAD/SD ratio of 0.7970, which happens to be the square root of 2 over pi (not a coincidence, try proving this if you rock at solving integrals). \n The graph at the beginning of this post shows a Monte Carlo estimation of the MAD/SD ratio for the Student T distribution as it goes from very high Degrees of Freedom (1024) to very low (1/4). You may know that the T distro converges to the Normal at high degrees of freedom (hence the result of nearly .8 for high DF), but did you know that the T distro on 1 Degree of Freedom is the same as the infamously fat-tailed Cauchy? And why stop at 1? We can keep going into fractional DFs. I\u2019ve plotted the ratio all the way down to 1/4. As always, code in R is at the end of the post. \n One more thing: there is at least one continuous distribution for which the MAD/SD ratio reaches it\u2019s maximum possible value of one. First person to guess this maximally thin-tailed distribution gets a free copy of the comic I worked on . \n\n # Start with a Normal, move to a Cauchy \ndfs = 2 ^ ( 10 :- 2 ) \nresults = c ( ) \n for ( i in dfs ) { \n\tx = rt ( 1000000 ,i ) \n\tresults = c ( results, mean ( mean ( abs ( x ) ) / sd ( x ) ) ) \n } \n \n # Note the wonky x-axis limit and order \n plot ( rev ( - 2 : 10 ) , results, col = \"blue\" , pch = 20 , xlim = rev ( range ( - 2 : 10 ) ) , xlab = \"Degrees of Freedom (binary log scale)\" , ylab = \"MAD/SD ratio\" )"], "link": "http://www.statisticsblog.com/2012/10/how-fat-are-your-tails/", "bloglinks": {}, "links": {"http://ftp.ucla.edu/": 1, "http://www.statisticsblog.com/": 3}, "blogtitle": "Probability and statistics blog"}, {"content": ["I recently finished work on the first issue of a graphic novel. It\u2019s in the form of a fictional first person narrative. The story isn\u2019t directly about statistics, but there are a few digressions on the subject. Here are some samples, make sure to click on the images for a larger view: \n  \n  \n If you\u2019re interested, head over to sunfalls.com and pick up a copy. Here\u2019s the order page . The comic comes with a full money-back guarantee, including shipping. You don\u2019t even have to send back your copy to claim the refund."], "link": "http://www.statisticsblog.com/2012/10/comic-with-stats-discussion/", "bloglinks": {}, "links": {"http://sunfalls.com/": 1, "http://sunfalls.com": 1, "http://www.statisticsblog.com/": 2}, "blogtitle": "Probability and statistics blog"}, {"content": ["Image found out there on The Internets. If it doesn\u2019t hurt your brain, you\u2019re not thinking about it hard enough."], "link": "http://www.statisticsblog.com/2012/10/if-you-choose-an-answer-to-this-question-at-random-what-is-the-chance-you-will-be-correct/", "bloglinks": {}, "links": {"http://www.statisticsblog.com/": 1}, "blogtitle": "Probability and statistics blog"}, {"content": ["Let\u2019s say your goal is to observe all known species in a particular biological category. Once a week you go out and collect specimens to identify, or maybe you just bring your binoculars to do some spotting. How long will it take you to cross off every species on your list? \n I\u2019ve been wondering this lately since I\u2019ve begun to hang out on the Mushrooms of Qu\u00e9bec flickr group. So far they have over 2200 species included in the photos. At least one of the contributors has written a book on the subject, which got me wondering how long it might take him to gather his own photos and field observations on every single known species. \n My crude model (see R code at the end of this post) assumes that every time Yves goes out, he has a fixed chance of encountering every given species. In other words, if there were 1000 species to find, and he averages 50 species per hunt, then every species is assigned a 1/20 chance of being found per trip. Thus the total found on any trip would have a Poisson distribution with parameter 50. \n This model is unrealistic for lots of reasons (can you spot all the bad assumptions?), but it does show one of the daunting problems with this task: the closer you get to the end, the harder it becomes to find the last few species. In particular, you can be stuck at 1 for a depressingly long time. Run the simulation with different options and look at the graphs you get. I\u2019m calling this \u201cThe unicorn problem,\u201d after Nic Cage\u2019s impossible-to-rob car in the movie Gone in 60 Seconds. \n Do you have a real-life unicorn of one sort or another? \n\n \nspecies = 1000 \nfindP = 1 / 20 \ntrials = 100 \ntriesToFindAll = rep ( 0 , trials ) \n \n \n \n for ( i in 1 : trials ) { \n\ttriesNeeded = 0 \n \n\tleftToFind = rep ( 1 , species ) \n\tleftNow = species\n\tnumberLeft = c ( ) \n \n\t while ( leftNow > 0 ) { \n \n\t\tfound = sample ( c ( 0 , 1 ) , 1000 , replace = TRUE, prob = c ( ( 1 - findP ) ,findP ) ) \n \n\t\tleftToFind = leftToFind - found\n \n\t\tleftNow = length ( leftToFind [ leftToFind > 0 ] ) \n \n\t\tnumberLeft = c ( numberLeft, leftNow ) \n \n\t\ttriesNeeded = triesNeeded + 1 \n \n\t } \n \n\t if ( i == 1 ) { \n\t\t plot. ts ( numberLeft, xlim = c ( 0 , 2 * triesNeeded ) , col = \"blue\" , ylab = \"Species left to find\" , xlab = \"Attempts\" ) \n\t } else { \n\t\t lines ( numberLeft, col = \"blue\" ) \n\t } \n \n\ttriesToFindAll [ i ] = triesNeeded\n }"], "link": "http://www.statisticsblog.com/2012/10/the-unicorn-problem/", "bloglinks": {}, "links": {"http://www.flickr.com/": 1, "http://www.statisticsblog.com/": 1}, "blogtitle": "Probability and statistics blog"}, {"content": ["A small one in terms of words, but lots of thought has gone into this addition : \n \n Correlation proves compatibility . \nNegative correlation implies incompatibility. \n \n As Ned Ryerson would ask, \u201cAm I right or am I right?\u201d"], "link": "http://www.statisticsblog.com/2012/06/manifesto-update-2/", "bloglinks": {}, "links": {"http://www.statisticsblog.com/": 1}, "blogtitle": "Probability and statistics blog"}, {"content": ["Naomi Robbins looks at using pie charts to represent women and men in publishing. Her piece is here . The charts in question are here . Warning: Don\u2019t click if you hate pie charts, you just might have a meltdown or wonder why all the info couldn\u2019t be put into a single bar chart. \n Discussion at Cross Validated about How best to communicate uncertainty with way too few responses. My feeling is that as news filters from scientific realms to pop culture outlets, it becomes more and more \u201ccertain\u201d in terms of how it\u2019s presented. Can that be fixed? \n Master of data visulization Hans Rosling made Time magazines list of 100 most influential people in the world ."], "link": "http://www.statisticsblog.com/2012/05/early-may-link-roundup/", "bloglinks": {}, "links": {"http://www.forbes.com/": 1, "http://stats.stackexchange.com/": 1, "http://www.time.com/": 1, "http://www.vidaweb.org/": 1, "http://www.ted.com/": 1}, "blogtitle": "Probability and statistics blog"}, {"content": ["Just added another statement to my manifesto . Here is the full text: \n Interpret or predict. Pick one. There is an inescapable tradeoff between models which are easy to interpret and those which make the best predictions. The larger the data set, the higher the dimensions, the more interpretability needs to be sacrificed to optimize prediction quality. This puts modern science at a crossroads, having now exploited all the low hanging fruit of simple models of the natural world. In order to move forward, we will have to put ever more confidence in complex, uninterpretable \u201cblack box\u201d algorithms, based solely on their power to reliably predict new observations. \n Since you can\u2019t comment to WordPress pages, you can post any comments about my latest addition here. First, though, here is an example that might help explain the difference between interpreting and predicting. Suppose you wanted to say something about smoking and its effect on health. If your focus is on interpretability, you might create a simple model (perhaps using a hazards ratio) that leads you to make the following statement: \u201cSmoking increases your risk of developing lounge cancer by 100%\u201d. \n There may be some broad truth to your statement, but to more effectively predicts whether a particular individual will develop cancer, you\u2019ll need to include dozens of additional factors in your model. A simple proportional hazards model might be outperformed by an exotic form of regression, which might be outperformed by a neural network, which would probably be outperformed by an ensemble of various methods. At which point, you can no longer claim that smoking makes people twice as likely to get cancer. Instead, you could say that if Mrs. Jones \u2014a real estate agent and mother of two, in her early 30s, with no family history of cancer \u2014 begins smoking two packs a day of filtered cigarettes, your model predicts that she will be 70% more likely to be diagnosed with lounge cancer in the next 10 years. \n The shift taking place right now in how we do science is huge, so big that we\u2019ve barely noticed. Instead of seeing the world as a set of discrete, causal linkages, this new approach sees rich webs of interconnections, correlations and feedback loops. In order to gain real traction in simulating (and making predictions about) complex systems in biology, economics and ecology, we\u2019ll need to give up on the ideal of understanding them."], "link": "http://www.statisticsblog.com/2012/05/may-manifesto-addendum/", "bloglinks": {}, "links": {"http://www.statisticsblog.com/": 1}, "blogtitle": "Probability and statistics blog"}, {"content": ["We often speak implicitly of different types of randomness but neglect to name or categorize them. Consider this post to be a kind of RFC or rough draft on the division of randomness into five categories. If you start using these distinctions explicitly, even if only in your own head, I think you will find them highly useful, as I have. \n Type 0: Fixed numbers or known outcomes \n Type 0 randomness is the special case of randomness where the data are already known. Any known outcome, regardless of the process that generated it, is Type 0 randomness. Once known, it has become a constant. In terms of information conveyed, all Type 0 randomness has zero informational entropy, and all messages with zero entropy are examples of Type 0 randomness. \n Type 1: Pseudo random. \n Most computers generate random numbers by a deterministic process. An initial \u201cseed\u201d is picked using some environmental factor, like the microsecond timing of the CPU, and from there onward every number that follows is fully determined by the algorithm. These algorithms can be very good, in terms of producing sequences of numbers that have desirable qualities. Yet, if you know that the sequence comes from some variation of, say, the Mersenne Twister , then a single number or short sub-sequence might be enough to predict all the subsequent numbers. \n Even if you can\u2019t figure out the algorithm used to create a sequence, all algorithmically generated random numbers will eventually loop. The sequence can be quite long, but it\u2019s always finite and it always loops. Once you\u2019ve seen the whole thing, all future numbers will be known exactly, and you will have Type 0 randomness. \n Computer software isn\u2019t the only source of Type 1 randomness. Card shuffling machines, if sufficiently precise in their operation, map each unique ordering of playing cards to a single final ordering. Learn how the machine works, and you will know how each initial ordering is transformed. \n The key to Type 1 randomness is that it is fully reducible to Type 0, in principle . The data source is known to be determinate, but the code is yet to be cracked. With enough time, attention, or technical sophistication, the sequence can be fully mapped. \n Type 2: Non-fully reducible \n Most real world randomness, and in general the most interesting sources of randomness, are of Type 2. Data streams of Type 2 randomness are conditionally random in the sense that we are able to reduce the uncertainty related to them, but only up to a certain point. Our model predicts the value of some response variable based on the other data, and this prediction can be quite good. But with Type 2 randomness there will always be some uncertainty left over, conditional on us making the best prediction we can. \n A typical example of Type 2 randomness would be predicting whether certain individuals will develop heart disease within the next 10 years. Without knowing any specifics about the individuals, it\u2019s very hard to make accurate predictions. Once we know some basic data, such as age, sex, and weight, we can make a better prediction. Even more fine-grained detail \u2014 history of smoking, diet, exercise patterns \u2014 allow us to make even better predictions. Each study or experiment we do, if of sufficient quality, improves the predictions we are able to make. Yet the randomness is non-fully reducible in the sense that, no matter how good our prior information or model, we will never be able to predict with 100% certainty whether a person will develop heart disease. \n Regression curves are attempts to understand Type 2 randomness by separating signal (the model, or conditionally determinant, part) and noise. Often this noise part is modeled with some maximum entropy distribution, like the Gaussian. This is our way of recognizing that beyond some limit, we can no longer reduce the randomness. There will always be some Type 3 randomness left over. \n Type 3: Martingale random \n One way to think about Type 3 randomness is to imagine a fair bet. If the true probability of an event happening is 1/2, then 1 to 1 odds make it martingale random. There\u2019s nothing you can do to improve your expected return to above zero; nor is there anything you can do to decrease your exception to below zero. In a series of independent fair bets, strategy is irrelevant to expectation. Importantly, this doesn\u2019t prevent you from adjusting the probability distribution for payoffs, if you are able to vary wager amounts and stopping times. For example, you could try the martingale betting strategy , which offers a high probability of making small gains in exchange for a small chance of catastrophic loss. \n Martingale randomness implies that there is no disconnect between the \u201cadvertised\u201d distribution and the true (or revealed) distribution. The theoretical \u201cfair coin\u201d you meet in textbooks is martingale random. Of course you have to be very careful in how you interpret the results of a real coin toss in terms of informational content. Maybe it isn\u2019t martingale random after all! \n Type 3 randomness is not limited to situations in which you have two equally probable outcomes. Anytime you are unable to reduce randomness beyond a particular limit of predictability, what\u2019s left over is martingale randomness. In fact, through a process of \u201cwhitening\u201d, signals that generate non-uniform randomness can be converted into uniform randomness. The opposite can be accomplished as well (though I\u2019ve never heard it called \u201cblackening\u201d). \n Type 4: Real randomness. \n This is the real thing: baked-in, irreducible randomness. For a data source to be Type 4 random, it must be martingale random and it must come from a sequence that is not only unknown, but a priori unknowable . If Type 4 randomness exists, then God plays dice; randomness is \u201cbaked in\u201d to the universe. \n I suspect that if Type 4 randomness really does exist, then it will be impossible to prove. \n General thoughts on types and some examples \n The most important thing to note about these categorizations is that the type of randomness depends on your perspective. The cards you hold in your hand are Type 0 randomness to you, but to the person sitting across the poker table from you, they are Type 2 randomness. Your opponent can use any number of tools to try and do better than pure chance at guessing your hand (how much you bet, the look in your eyes, and of course the cards they hold). The type of randomness you perceive is a function of what you know. \n All degenerate random variables (i.e. the indicator function for the entire sample space, which is always 1) are Type 0 randomness. \n Most of the games we play have some element of Type 2 randomness. Kids will play games with Type 1 randomness, like War, which is deterministic for any given card shuffling, and could in theory be mapped out. Type 1 randomness can still be surprising to you, but if there is any skill involved it would have to be Type 2 randomness: entropy reducible in theory. \n The concept of Type 3 randomness is connected with two important statistical concepts: sufficiency and coherence. Once you know the sufficient statistics from a data source (and, vitally , assuming your model about the data is correct), threre\u2019s nothing more you can do to improve your confidence intervals or ability to make predictions. For example, if you know that your data source has a Poisson distribution and the points are uncorrelated, then once you know the mean, there\u2019s no other piece of information that can improve your ability to predict new values from the distribution. Broadly speaking, martingale randomness satisfies the de Finetti conditions of coherence, in that odds assignments must match up with known probabilities, and internal consistency needs to be maintained. \n If your dice are loaded, then you\u2019ve got a generator of Type 2 randomness. Over time, you can make better and better predictions about how often the different numbers will come up. But you still won\u2019t be able to predict, with certainty, the results of any given roll. If somehow you knew the exact probabilities for each face, then you could use these dice as a generator of Type 3 randomness. \n In a sense, the very first number generated by your computer\u2019s random number algorithm is martingale random. There\u2019s no way, unless you know how the seed is generated from the CPU\u2019s timing and can \u201csee\u201d the microseconds tick by, to predict the range in which that number will fall with greater accuracy than would be expected by chance alone. On the other hand, it could be argued that the decimal part of the CPU\u2019s clock isn\u2019t really uniformly distributed. There will be some slight bias towards lower numbers, which is natural for any distribution of numbers that \u201cgrows\u201d in size, even if it cycles (see Benford\u2019s Law, and note that it applies not just to the first digit or a number, but to secondary digits as well). With enough careful investigation, you might be able to convert that first seeded random number into a case of Type 2 randomness. \n Type 3 randomness is the holy grail of randomization. Casinos want dice which are perfectly symmetric in weighting, and resistant to wear and tear that might cause bias. Assignments to treatment in a clinical trial should strive for martingale randomness. Failure to achieve martingale randomness, when it is required, can have highly negative consequences. \n \u201cBeating the house\u201d at a casino involves turning Type 3 randomness into Type 2 randomness, with enough usable signal left to overcome the casino\u2019s inherent advantage. Strategies like analyzing roulette spins to find bias, and most famously counting cards, have been successfully used. One group of geeks was able to turn the randomness of a Vegas lottery machine which followed a Type 1 sequence into Type 0. They made the tactical mistake of hitting two huge jackpots in a row, tipping off the casino that they had successfully cracked the code. From the casino\u2019s perspective, you have an inverse classification problem: given how well players are doing, what can you infer about the type of randomness they are detecting? Those jackpot-winning geeks could have taken a lesson from code breakers in WWII, who thought carefully about how to use the information contained in the cracked messages without showing the Germans that their code had been broken and that the allies understood their messages as more than the white noise of Type 3 randomness. \n Because what separates the Types is our knowledge, randomness can come from a generation process that is completely deterministic and known to others (Type 0). As far as I\u2019ve been able to tell , the digits of pi (beyond the ones you have memorized) are martingale random. If I give you a sequence of digits, and tell you they come from somewhere after the trillionth digit of pi, and let you use any tools you want short of a computer, there\u2019s nothing you could do in a single lifetime to predict additional digits with an accuracy greater than 1/10th. Note that while the tail digits of pi appear to be a source of martingale randomness, not all irrational (or even transcendental) numbers have unpredictable digits. As counterexamples, see\u00a0 Louisville\u2019s or Chapperhorn\u2019s numbers . It appears (though I don\u2019t have a proof for this!), that any data source of Type 2 or greater must be incompressible, in the sense that if the sequence has infinite length, no finite-length description of it can exist. \n I\u2019ve tried to make these categorizations as clear as possible, but there are still edge cases which are hard to place. As is always the case, the closer you look, the fuzzier things get . However, I think you will still find this categorization of randomness to be quite useful, especially as a tool to discuss edge cases. Consider for a moment Chaitin\u2019s Omega , which is the probability (weighted by string length) that a given computer program, run in a fixed computing environment, halts. The first few digits of Omega, determined by very short programs which instantly halt or loop, are easy to figure out. But we know from Turing that the halting problem is, in general, undecidable. So at some point, the digits of Omega become unknown and unknowable. Nor can we know when they become unknowable! The digits of Omega make the transition from Type 0 randomness (known) to Type 1 randomness (we just need to run the programs and see if they halt or loop), to Type 2 randomness (we may be able to set upper and lower bounds for the next few digits, or make a likelihood prediction based on reasoning and past experience), to Type 3 randomness (only God could predict the 10,000th digit with better than chance accuracy) and perhaps, almost frighteningly, to Type 4 randomness (God\u2019s in a back alley, shaking up the dice right now)."], "link": "http://www.statisticsblog.com/2012/02/a-classification-scheme-for-types-of-randomness/", "bloglinks": {}, "links": {"http://mathworld.wolfram.com/": 1, "http://sprott.wisc.edu/": 1, "http://www-personal.umich.edu/": 1, "http://www.statisticsblog.com/": 4}, "blogtitle": "Probability and statistics blog"}, {"content": ["Ronald Aylmer Fisher, statistics badass. Illustration by Rachelle Scarf\u00f3 for a project I was working on."], "link": "http://www.statisticsblog.com/2012/01/r-a-fisher-illustration/", "bloglinks": {}, "links": {"http://www.statisticsblog.com/": 1, "http://www.rachellescarfo.com/": 1}, "blogtitle": "Probability and statistics blog"}]