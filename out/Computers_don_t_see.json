[{"blogurl": "http://www.computersdontsee.net\n", "blogroll": [], "title": "Computers don't see"}, {"content": ["WTF, WTF, w00t !  A quick note for something that I couldn't find anywhere else, and that cost me a few hours (and a bunch of white hair). \n\n\n I'm currently trying to setup a MacOS graphical app to test my current thesis algorithms.\nThe GUI is developed using the native Mac tools, namely the latest Xcode in Objective-C . \n\n\n Some standard, ugly hacks first \n\n\n Since the algorithms are developed in C++ with OpenCV, I first had to switch the project compilation language setting from According to the file extension to Obj-C++ and I had to modify the pre-compiled include file to ensure that the file opencv.hpp was included before any Cocoa related file.\nThese tricks are well known: \n \n the first one forces Xcode to link the resulting binary against the standard C++ library. It's not possible to use the file-type setting because of trick #2 \n the second trick aims at solving a collision in the definition of the MAX(a,b) function/macro. Both OpenCV and Cocoa define it but in slightly different ways, resulting in errors at compile-time. \n \n By including OpenCV first in the .pch file, the MAX(a,b) symbol is already defined via a macro and the complier gets quiet.\nThe drawback to this ugly hack is that it leads to the inclusion of opencv.hpp inside all the files of a project including pure Objective-C ones, hence you have to force the compiler to Obj-C++. \n\n\n Now, the big problem \n\n\n My algorithms are developed in C++ with OpenCV, and compiled to a static library from the command line, using Cmake to generate the compiling infrastructure. \n\n\n In spite of the previously mentioned precautions, I kept having undefined symbol errors at link time when trying to use my code inside the graphical app.\nThese errors were typically bound to the C++ standard part, since the missing functions were part of the std \u00a0C++ namespace.\nHowever, I checked and checked again (and banged my head against the walls) and the linker invocation seemed perfectly fine... \n\n\n And the solution! \n\n\n Exploring the various linker settings in Xcode, I came across one called C++ standard library .\nYes ! In Xcode, you can choose between 2 C++ standard libraries.\nThe first one (used by default) is clang's C++11 standard library libc++ .\nThe second one (used by the command line process) is the GNU's libstdc++ . \n\n\n  \n\n\n And things became clear:\n Since I was not linking agains the same standard C++ library, the linker couldn't find standard C++ symbols ! \n\n\n So, I switched to GNU's libstdc++, and it did the trick."], "link": "http://www.computersdontsee.net/index.php/post/2012/10/14/C-lib%2C-Cmake%2C-Obj-c", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 1, "https://developer.apple.com/": 2}, "blogtitle": "Computers don't see"}, {"content": ["Parallel Freaks !  Wow !\nMy 1-bit LBDs reconstruction algorithm is working now [ 1 ] \n\n\n The problem... \n\n\n My goal now is to produce enough reconstruction results for a journal paper.\nHence, I wrote a small Python script that : \n \n gets all the image files from a directory \n generates some experiments by creating different sets of parameters \n launches the reconstruction routine for each image with each set of parameter in a shell with the @os.system@@ module \n \n my_command = str ( ... )  os . system ( command )  \n\n\n There is an obvious parallelism in the big experiment loop, since each reconstruction process is independent from the others. \n\n\n In C/C++, I would have used OpenMP on a carefully chosen loop, and voil\u00e0 !\nHowever, there is no such thing in Python . \n\n\n ... has a simple solution ! \n\n\n Since I'm new with Python , I had to do some googling for some time, until I found the multiprocessing module. \n\n\n This module introduces the handy concept of Pool , that groups together a bunch of processes.\nBy default, it creates the same number of processes as the number of CPUs on the host machine. \n\n\n Then, you can add function calls to your pool either with the command Pool.apply(func) , which blocks until the function returns (hence not really interesting...) or asynchronously using Pool.apply_async(func) .\nWhen I have enqueued a large number of processes, I close the Pool and wait for the processes to complete. \n\n\n Happy ending \n\n\n Right now, after writing a few lines of code, I have 10 cores frenetically computing LBDs reconstructions !\nI'll keep you updated about the results once the paper is submitted. \n\n\n Post-scriptum: Joblib (10.09.12) \n\n\n Ga\u00ebl , in the comments section below, gives us a link to another Python module called joblib .\nAs the title of joblib 's page reads Embarrassingly parallel for loops , it sure looks like a good candidate for the task I've exposed in this post !\nI especially like the possibility to pass a parameter claiming how many CPUs should be ignored by the process pool, thus allowing one to continue working on something else in a transparent manner. It would be quite useful for me since I work at the same time on an 8-CPU workstation and a 2-CPU laptop, hence specifying the number of busy CPUs is less useful for me than specifying the number of idle cores. \n\n\n If you don't want to miss the next post, you can register to the blog's feed or follow me on Twitter ! \n Note \n [ 1 ] I will keep the primer of the results for my ICPR talk. So, be there or be patient !"], "link": "http://www.computersdontsee.net/index.php/post/2012/09/07/Easily-spawning-parallel-tasks-with-python", "bloglinks": {}, "links": {"http://docs.python.org/": 1, "https://twitter.com/": 1, "feed://www.computersdontsee.net/index.php/feed/atom": 1, "https://iapr.papercept.net/": 1, "http://gael-varoquaux.info": 1, "http://packages.python.org/": 1, "http://www.computersdontsee.net/": 3}, "blogtitle": "Computers don't see"}, {"content": ["> make post  So, we have seen first that using Matlab and CUDA together was not always straightforward, especially on a Mac.\nThen, I showed a simple project to demonstrate how to do it in practice.\nIn this post, we're going to see how to compile this project using a Makefile . \n\n\n Final part : the Makefile ! \n\n\n Overview of the compilation process \n\n\n Let's recap the different steps needed to compile our mex file : \n \n get the name and location of the various libraries, scripts and compilers involved \n compile the CUDA kernels with the NVIDIA nvcc compiler \n compile any additional C/C++ file (not required here) \n make a library of these object files \n use the mex script provided by Matlab to compile the mexFunction(), and link it with the library obtained above. \n \n\n Anatomy of the Makefile \n\n CUDA_INSTALL_PATH := / usr / local / cuda   CUDA := $ ( CUDA_INSTALL_PATH )  SDK := / Developer / GPU\\ Computing / C   INC  := -I$ ( CUDA ) / include -I$ ( SDK ) / common / inc -I. LIB  := -L$ ( CUDA ) / lib -L$ ( SDK ) / lib   # Mex script installed by Matlab  MEX = / Applications / MATLAB_R2012a.app / bin / mex   # Flags for the CUDA nvcc compiler  NVCCFLAGS := -O = 4 -arch =sm_11 --ptxas-options =-v -m 64  #THIS IS FOR DEBUG !!! -g -G  # IMPORTANT : don't forget the CUDA runtime (-lcudart) !  LIBS  := -lcudart -lcusparse -lcublas   # Regular C++ part  CXX = g++  CFLAGS = -Wall -c -O2 -fPIC $ ( INC )  LFLAGS = -Wall  AR = ar   all: dataloop mex   kernels:   $ ( CUDA ) / bin / nvcc dataloop.cu -c -o dataloop.cu.o $ ( INC ) $ ( NVCCFLAGS )   main.o:  main_dataloop.cpp   ${CXX} $ ( CFLAGS ) $ ( INC ) -o main.o main_dataloop.cpp   dataloop:  kernels main.o   ${CXX} $ ( LFLAGS ) -o demo_dataloop main.o dataloop.cu.o $ ( LIB ) $ ( LIBS )   dataloop.a:  kernels   ${AR} -r libdataloop.a dataloop.cu.o   mex:  dataloop.a   ${MEX} -L. -ldataloop -v mex_dataloop.cpp -L$ ( CUDA ) / lib $ ( LIBS )   install_name_tool -add_rpath / usr / local / cuda / lib mex_dataloop.mexmaci64   clean:   rm * .o a.out * .a * .mexmaci * * ~ \n\n\n (Be careful and refrain from copy/pasting, it seems that the CMS has lost the spaces / tabs alignment.) \n\n\n Setting up the environment \n\n\n In the first part, we specify the path and names of various tools, including of course nvcc and g++ [ 1 ] \nSome additional files that were required by the software consultant's code are installed in the NVIDIA GPGPU SDK, so we had to give its path too. \n\n\n Finally, we need to give the full path to Matlab's mex script, because MacTeX (a TeX distribution for Mac) already installs an executable with the same name in /usr/local , hence it's detected by the terminal instead of the Matlab's script. \n\n\n The various flags and options are standard and you should already now most of them (and if you don't, don't hesitate to ask in the comments below !) : \n \n -O for the optimization level (code speed vs. debugability) \n -arch=sm_11 to force nvcc to compile for a compute capability of 1.1 (the CC of our target card). If you don't require that, then I understand from the doc that the CUDA code could be compiled for a higher CC than the one of your card, and the runtime would do the necessary translations when the kernel is running \n -m 64 to enforce the creation of a 64 bits object file instead of the default 32 bits. This is mandatory if we want to link then with a 64 bits C++/mex executable, otherwise we get an architecture mismatch error from the linker \n \n\n Compiler and linker options \n\n\n The creation of object files with nvcc and g++ or llvm is simple, using the -c option.\nWe keep the .cu part in the filename to keep in mind that we created dataloop.cu.o with nvcc , but it could be removed. \n\n\n Since we want to perform static linking , we call the archive builder ar and tell it to append both object files (.o). \n\n\n Then, we calling mex , we don't forget to indicate the path to our build results with -L. (we are in the same directory) and to link against them with the -l option.\nIf you've never seen this before [ 2 ] , you have to know that for historical reasons gcc (and the compilers that use its syntax) automatically append the lib prefix to the argument of -l , hence our call -ldataloop is correct. \n\n\n One more thing... \n\n\n I've already explained before that you can't safely rely on variables such as LD_LIBRARY_PATH / DYLD_LIBRARY_PATH to find the external libraries when working on a Mac.\nHowever, you still have to provide the location of the dynamically linked functions ! \n\n\n This can be achieved by modifying the final output (the compiled mex) with install_name_tool() .\nWe add /usr/local/cuda/lib to the list of the directories explored at launch (variable rpath ) because it is a non-standard location used by our CUDA install. \n\n\n Note that we could do it to the standalone executable (target dataloop of the Makefile), but it was unnecessary because it's launched from a terminal, and I had already added /usr/local/cuda/lib to the DYLD_LIBRARY_PATH variable in the .profile . \n\n\n How to use it ? \n\n\n Once you are in the correct directory, just invoke the different targets : \n \n > make kernels allows you to build the CUDA code \n > make dataloop builds the kernels (if needed) and creates a command line executable to test them \n > make mex creates a Matlab-compatible command, that you can call with mex_dataloop(single(my_vector)) . \n \n\n Conclusion \n\n\n I hope you enjoyed this mini-series !\nIt's short, but yet it gets you started correctly for Matlab-CUDA-Mac joy and happiness interactions.\nYou can download an archive with this project in order to test and modify it. \n\n\n Don't hesitate to drop a word or ask questions in the comment form below, it helps keeping me motivated and the updates coming ! \n\n\n If you don't want to miss the next post, you can register to the blog's feed or follow me on Twitter ! \n Notes \n [ 1 ] More precisely, on our system it's a symlink to llvm-g++-4.2 \n [ 2 ] This was the case of our Windows consultant."], "link": "http://www.computersdontsee.net/index.php/post/2012/08/31/A-simple-CUDA-kernel-mex-file-%28part-2", "bloglinks": {}, "links": {"http://tug.org/": 1, "http://www.computersdontsee.net/": 8, "https://twitter.com/": 1, "feed://www.computersdontsee.net/index.php/feed/atom": 1}, "blogtitle": "Computers don't see"}, {"content": ["A := B, really ?  In the previous post , I introduced the various problems that I have encountered when I had to install some external CUDA library on our Mac workstations.\nIn this post and the next, I will take a simple example (moving data from array A to array B through the GPU) to show how I finally achieved this installation process.\nWhile the application is not really useful, it demonstrates all the various tips and tricks that we applied. \n\n\n The big picture \n\n\n So, let's say we want to copy array A to array B, through the GPU and from Matlab.\nWhat exactly do we need, how do we organize the code ? \n\n\n File layout \n\n\n As mentioned previously , our goal is to isolate the CUDA-specific part (that will be compiled with CUDA's nvcc ) and the Matlab-specific part (that will be compiled using the mex script provided by Mathworks).\nHence, we embed all the CUDA function calls inside C-wrappers, and we build a small library out of that.\nThen, we pass this library to the linkage option of mex, and voil\u00e0 ! \n\n\n (At least, it's how it should work...) \n\n\n So, we're going to have : \n \n for the C+CUDA part : a file dataloop.h that contains only C-compliant function declarations, and a file dataloop.cu that will implement these functions and the CUDA kernels, eventually compiled into a static library (.a, see below) \n for the Matlab part : a mex_dataloop.cpp file that implements the gateway function required by Matlab and links against the said library \n for sanity checks : a main.cpp file that executes the code of dataloop.cu outside Matlab, just to be sure that everything works. \n \n\n \n\n Dynamic vs. Static linking ? \n\n\n A first Mac caveat here. \n\n\n MacOS does not have exactly the same X-server model as Linux systems have ; there is no kind of \"default\" behind-the-scene terminal like the one you access with the Ctrl-Alt-F7 keys.\nGraphical applications can be (and are usually) launched without reading the environment values that may be set in your .profile .\nFurthermore, the latest Matlab releases [ 1 ] seem to not launch an Xterm and an X11 instance anymore on Mac platforms, hence we're almost sure that environment variables set in the .profile are not read. \n\n\n Since I wasn't sure that Matlab would be able to find the necessary dynamic libraries (even when using standard locations such as /usr/local/lib or modifying the DYLD_LIBRARY_PATH [ 2 ] variable) I preferred static linkage over dynamic linkage for our task, and we added the appropriate linker command (with ar ) in the Makefile. \n\n\n Finally : the code explained \n\n\n The kernel \n\n\n The CUDA kernel is really simple and just copies the data from one memory position to another : \n __global__ void dataloop ( float * src, float * dest )  {   int tid = blockIdx. x ;   dest [ tid ] = src [ tid ] ;  }  \n\n\n As stated in the previous post, all the CUDA-specific calls are embedded in a regular C wrapper, so that we do not expose any CUDA additional syntax in the public interface : \n  void process_data_with_cuda ( float * host_src, float * host_dest, int N )  {   float * d_src = NULL ;   float * d_dest = NULL ;    memset ( host_dest, 0 , N * sizeof ( float ) ) ;    // Allocate on device   cudaMalloc ( ( void ** ) & d_src, N * sizeof ( float ) ) ;   cudaMalloc ( ( void ** ) & d_dest, N * sizeof ( float ) ) ;    // Transfer src to device   cudaMemcpy ( d_src, host_src, N * sizeof ( float ) , cudaMemcpyHostToDevice ) ;    // Launch kernel   dataloop <<< N, 1 >>> ( d_src, d_dest ) ;    // Fetch data back   cudaMemcpy ( host_dest, d_dest, N * sizeof ( float ) , cudaMemcpyDeviceToHost ) ;    // Release memory   cudaFree ( d_src ) ;   cudaFree ( d_dest ) ;  }  \n\n Again, this code is fairly straightforward to understand (and not really useful) : \n \n we allocate memory on the GPU with cudaMalloc , \n we transfer the data from the CPU to the GPU with cudaMemcopy + cudaMemcpyHostToDevice , \n we use the CUDA kernel to copy this data from the source GPU slot to the target GPU memory slot, \n we fetch the data back to the CPU memory ( cudamemcpy again), \n finally, we gently release the memory we allocated on the device with cudaFree . \n \n\n Anatomy of the mex file \n\n  #include <iostream>  #include <cassert>  #include \"dataloop.h\"  #include \"mex.h\"  #include \"matrix.h\"   int const kNDims = 2 ;   //---------------//  // Gateway //  //---------------//  void mexFunction ( int nlhs, mxArray * plhs [ ] , int nrhs, mxArray const * prhs [ ] )  {  std :: cerr << __FUNCTION__ << \" with args: \\n \" ;  std :: cerr << \"Left: \" << nlhs << \" | \" << plhs << std :: endl ;  std :: cerr << \"Right: \" << nrhs << \" | \" << prhs << std :: endl ;    //-------------------------------------------------------------------------  std :: cerr << \"Reading input... \\n \" ;  mxArray const * src = prhs [ 0 ] ;  float * p_src = ( float * ) mxGetData ( src ) ;  mwSize const * srcSize = mxGetDimensions ( src ) ;  int N = ( srcSize [ 0 ] == 1 ? srcSize [ 1 ] : srcSize [ 0 ] ) ;   std :: cerr << \"Src: \" << srcSize [ 0 ] << \"x\" << srcSize [ 1 ] << std :: endl ;  std :: cerr << \"N = \" << N << std :: endl ;    //-------------------------------------------------------------------------  std :: cerr << \"Configuring output... \\n \" ;   plhs [ 0 ] = mxCreateNumericArray ( kNDims, srcSize, mxSINGLE_CLASS, mxREAL ) ;  float * p_dest = ( float * ) mxGetData ( plhs [ 0 ] ) ;    //-------------------------------------------------------------------------  std :: cerr << \"CUDA calling ! (There's a tribute here...) \\n \" ;   process_data_with_cuda ( p_src, p_dest, N ) ;   std :: cerr << \"Back from the war. Check your PTSD syndrom. \\n \" ;   std :: cerr << std :: endl ;  for ( int i = 0 ; i < N ; ++ i )   std :: cerr << \"(\" << i << \") \\t @in = \" << * ( p_src + i ) << \" \\t @out = \" << * ( p_dest + i ) << std :: endl ;  }  \n\n\n The file dataloop.h contains our public interface, i.e. only the declaration of the process_data_with_cuda() function.\nThe files mex.h and matrix.h declare the functions and structures that Matlab uses, and the prototype of mexFunction() is also imposed by Matlab. \n\n\n Then, we log some information to the standard error stream (it should happen in read in your Matlab command-line window).\nThis information includes the size of the input matrix.\nNote that the code handle both rows and column vectors by setting N to the greatest value between the number of rows and columns of the input. \n\n\n Then, we allocate memory for the output vector, using the size read from the input data.\nNote that we create an array of floating-point values by passing mxSINGLE_CLASS to the mxCreateNumericArray function : while Matlab natively uses double values (8 bytes per real number), most GPUs still only handle float types (4 bytes).\nOn the other side, in the Matlab call to our mex file, you also need to convert the input data do float with the command single() . \n\n\n When everything is setup, we call our main function process_data_with_cuda() that performs the actual computation on the GPU.\nThis function takes as inputs the pointers to the source and destination data, and knows nothing about Matlab, while nothing about CUDA is exposed to Matlab and the mex script in this file. \n\n\n And the Makefile ? \n\n\n Since this post is already long enough, I postpone the explanation of the Makefile to the next post , coming very soon. \n\n\n If you already now about Makefile syntax, CUDA compiling and mex script usage, then you can probably guess almost everything that's inside this file.\nThere are however a couple of caveats that are Mac-platform specific, that I will detail in this future post. \n\n\n If you don't want to miss the next post, you can register to the blog's feed or follow me on Twitter ! \n Notes \n [ 1 ] We installed the R2012a. \n [ 2 ] Mac's LD_LIBRARY_PATH"], "link": "http://www.computersdontsee.net/index.php/post/2012/08/29/A-simple-CUDA-kernel-mex-file-%28part-1%29", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 6, "https://twitter.com/": 1, "feed://www.computersdontsee.net/index.php/feed/atom": 1}, "blogtitle": "Computers don't see"}, {"content": ["Learning lessons the hard way.  I've done a lot of casual (unfortunately, non-thesis-writing related) stuff this summer, including a lot of code.\nAmong these, we had the chance to host a software consultant during a few days, to test and deploy some CUDA codes that they had been optimizing during the last year. \n\n\n As the official Nerd PhD Student [ 1 ] (and given my previous experience with OpenCL for my ICIP'11 paper on optical flow), I was picked up by a post-doc to assist the consultant integrating their CUDA stuff on our Mac line. \n\n\n And this is how I went through 3 days of complete madness, anger, and cries. \n\n\n Spoiler alert: starting with the end \n\n\n If you're in a hurry, here is the solution to all our problems : we bought a brand new desktop PC, not a Mac [ 2 ] , with a decent and recent NVIDIA GPU. \n\n\n On my personal side, I was able (after some steps described below) to compile and run all the external CUDA code and to execute it from Matlab .\nHowever, the results were always garbage : we suspect it come from some hardware incompatibility issues (the compute capability of the card is much too low : 1.1). \n\n\n We strongly suspect the hardware capability because I managed to have a simple CUDA kernel executed correctly from a mex-file.\nIn a follow-up post, I'll comment this code and how to compile it with a Makefile. \n\n\n Lessons learned \n\n\n Cross platform development \n\n\n Let's start with something easy : the company's code was developed and run only on windows .\nIt consists in a bunch of files, ultimately compiled into a DLL , and the said DLL is then used by Matlab through the loadlib and calllib functions, and a lot of complicated boilerplate code to put the data in form and correctly hook into the DLL. \n\n\n A DLL is not something mysterious : it's just the windows version of a dynamic library !\nYes, just like a libfoo.so or a libfoo.dylib on linux or mac platforms.\nHowever, as a DLL, the lib object should export some symbols using DLL-specific syntax, which is not recognized by our compiler (Apple's version of llvm ). \n\n\n The fix was easy : protect the windows only part of the include files (*.h) using the pre-processor.\nI chose to use : \n  #if !_defined(_WIN32) && !_defined(_WIN64)  \n\n to isolate the windows-only from the mac/linux-only parts. \n\n\n It's 2012. Let's write a Makefile ! \n\n\n Of course, VC++ project files (used by the consultants) don't work outside their original world [ 3 ] .\nSo, we needed a way to build the project.\nAnd here comes the make tool and Makefile editing. \n\n\n If you did it before, then \n \n either you've pursued doing it every day, and you feel quite comfortable with it \n or (like me and our consultant) you learned it years ago at school, then copied/pasted/modified very carefully a skeleton Makefile (usually a compacted one created by someone else) that you always had with you. \n \n\n So, we had some 30 or 60 minutes of fun editing an old Makefile to adapt it to our project.\nThis part was a bit tedious, but with the help of emacs [ 4 ] to fill the indentations with the correct number of tabs and spaces. \n\n\n \n\n Calling CUDA from a mex-function \n\n\n In order to easily call (and compile !) the CUDA part from Matlab, I wrote a mex file as an interface between these 2 worlds.\nI will provide an example later in another post, but here is a rough description : \n \n the mex file is a vanilla C/C++ file that respects the gateway syntax expected by matlab, for example super_mex.cpp \n it includes a vanilla C .h file, for example foo.h , that again does not exhibit any CUDA peculiarity \n all the CUDA code is placed inside C wrappers in foo.cu : CUDA kernel calls happen only inside C/C++ functions \n foo.h is included in both foo.cu and super_mex.cpp . \n \n\n Then, to compile, proceed as follow : \n \n use nvcc to compile the .cu file to an object file (.o), make a library from the results \n use the mex compiler to compile the .cpp part and link it with the output of the previous step. \n \n\n That's all... for now ! \n\n\n In a next post, I will detail the content of the Makefile.\nWhile most of its content is standard, it contains a few lines that are really Mac-specific and that can hardly be guessed if you've never seen them before.\nSo, stay tuned and don't miss the next post ! \n\n\n Reference \n\n\n Fast TV-L1 Optical Flow for Interactivity \nD'Angelo, Emmanuel; Paratte, Johan; Puy, Gilles; Vandergheynst, Pierre \nIEEE International Conference on Image Processing (ICIP) 2011, Brussels, Belgium, September 11-14, 2011 \n\n\n If you don't want to miss the next post, you can register to the blog's feed or follow me on Twitter ! \n Notes \n [ 1 ] Our lab does not have a tech guy. Hence, PhD students play this role and fight with LDAP, servers, compiler weirdnesses... \n [ 2 ] We're not a Mac-only lab anymore \n [ 3 ] Or at least I don't know how to do it. \n [ 4 ] It's included by default by MacOS X, you can launch it from Terminal.app."], "link": "http://www.computersdontsee.net/index.php/post/2012/08/28/CUDA-%3A-lessons-from-the-trenches", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 9, "https://twitter.com/": 1, "feed://www.computersdontsee.net/index.php/feed/atom": 1, "http://llvm.org": 1}, "blogtitle": "Computers don't see"}, {"content": ["A summer of sun, and cr\u00eapes, and code !  Wow ! A long period without any updates.\nHere is some why's, and also some good news about what's in the pipeline. \n\n\n A summer of cr\u00eapes \n\n \n I've been on vacation Abroad, hence without 3G coverage [ 1 ] \n Vacation consequence : I'm going to blog (probably in French, but I can translate it to English if you, readers, are interested) about the Prehistory museum of Carnac (Brittany). While the menhir fields are well presented, this museum is a complete failure. \n \n\n A summer of code \n\n \n It was not an idle vacation ! Remember the previous post about Eugene Khvedchenya's OpenCV+iOS tutorial ? I wrote an implementation of TV diffusion based on FISTA , the now standard algorithm of Beck and Teboulle. You can find the source code on my github (a bit outdated) or on Evgeny's github (up-to-date). \n While my iOS implementation is slow [ 2 ] , it parallelizes the various loops with Grand Central Dispatch [ 3 ] , which is an interesting technique per se , so I'm preparing a blog post about that. \n \n\n Visits ! \n\n \n We had a visit at the lab by a software specialist to deploy some GPU (CUDA) libraries, and integrate them with Matlab. As the geeky PhD student I had to do all the dirty setup-compiling-installing work, and while the outcome was clearly not what we had hoped I learned a lot of interesting things , that I'm also going to blog about (2 posts are on the schedule). \n We had another summer visit ! Laurent Jacques from UC Louvain (Belgium) came for a couple of days. We had an interesting discussion about binary descriptors reconstruction . Laurent suggested two-and-a-half algorithm for that task. I have already implemented 1.5 of them, unfortunately without success, but I'm trying hard, so we have a breakthrough soon ! \n \n\n And software again ! \n\n \n On a more mundane side, the dotclear platform that powers this blog got an update during the summer, and the scheduled post publication functionality is back ! I usually write the posts in advance then program their publication. I'm relieved to come back to my usual workflow (and it should prevent long periods of silence like this summer). \n \n\n What a program ! So, stay tuned, either using the blog's feeds or on Twitter , and be sure not to miss any of these future updates ! \n Notes \n [ 1 ] Roaming fees are just... If you're a carrier, please take into account the fact that I do not use 3G when traveling because of the cost (and you earn 0), while I would use it a little bit with cheaper fees (let's say 50% less than now, and you would earn something). \n [ 2 ] I suspect the floating-point computations and the size of t the test images. \n [ 3 ] An Apple-backed technology for easy parallel (massively or asynchronous) programming."], "link": "http://www.computersdontsee.net/index.php/post/2012/08/28/Radio-silence", "bloglinks": {}, "links": {"http://perso.uclouvain.be/": 1, "https://twitter.com/": 1, "https://github.com/": 2, "http://www.computersdontsee.net/": 7, "feed://www.computersdontsee.net/index.php/feed/atom": 1, "http://en.wikipedia.org/": 2, "http://dotclear.org": 1}, "blogtitle": "Computers don't see"}, {"content": ["What about coding summer vacation ?  A short post to let you know about this interesting initiative from the blog Computer Vision Talks . \n\n\n The blogger (and augmented reality software developer) Eugene Khvedchenya has started a tutorial series on using OpenCV in the iOS world.\nThis is really useful, since it can be problematic to make both worlds work together.\nFurthermore, CV and Image Processing people are usually not really aware of software design issues, and developing nice iOS demos of our cool algorithms can be a really painful process without this knowledge. \n\n\n The code can be cloned from github .\nI have already forked the repo and implemented TV minimization with FISTA \n\n\n If you are interested by a more in-depth coverage of this topic, Eugene is a co-author of a book called OpenCV 2 Hotshot , about Augmented Reality, Computer Vision and the iOS platform.\nA preview of the book (expected to be complete in October 2012) can be found here ."], "link": "http://www.computersdontsee.net/index.php/post/2012/08/03/OpenCV-iOS-tutorial", "bloglinks": {}, "links": {"https://github.com/": 1, "http://www.packtpub.com/": 1, "http://computer-vision-talks.com/": 3}, "blogtitle": "Computers don't see"}, {"content": ["Unknown symbol for architecture...  Just lost a few hours ripping my hair apart [ 1 ] . \n\n\n I'm currently doing some tests compiling CUDA code as object files or as a C/C++ library, in order to call it from the Dreaded Matlab. \n\n\n Wait a minute... did you say external Cuda lib + Matlab ? \n\n\n Yes, I did say that.\nI am aware of the existence of Matlab's Parallel Computing Toolbox .\nHowever, two important things make me avoid it : \n \n the toolbox requires Cuda devices with compute capability at least 1.3 . Except for a few (2 or 3) cards in our lab, we have devices with capabilities 1.1 . This comes from the fact that we are a Mac-only lab, and our workstations ( Mac Pro's ) are either old or embedding Radeon cards [ 2 ] ; \n the code may eventually be released as an external (open-sourced) library, and it's good to have it as-loosely-as-possible tied to a specific, expensive software, because not every lab will have it, or the correct version, etc. \n \n\n Back to the topic : nvcc on mac \n\n\n Anyway, back to the topic. \n\n\n Nvcc on the mac \n\n\n I had previously experienced that there was an architecture mismatch when mixing C/C++ libraries and Cuda code on my mac, producing undefined symbols.\nThe solution was quickly identified : nvcc compiles by default (at least on my machine) for 32 bits architecture, while gcc [ 3 ] compiles for the native arch , which is x86_64 on my Core 2 Duo (64 bits). \n\n\n So, I was careful to invoke nvcc with the -m 64 option, and gcc with the -arch x86_64 option, to control exactly which parameters were used.\nI compiled my files in three steps, following e.g. this procedure on Stack Overflow. \n\n\n Still undefined symbols \n\n\n Furthermore, to avoid any linkage weirdness, and because Cuda is originally C-only compatible, I wrote everything in standard vanilla C, avoiding C++ features, and compiled my object files by invoking gcc .\nHowever, I still had some undefined symbols at the link stage (e.g. _main ) ! \n\n\n I tried a quick fix : I compiled with g++ instead, and everything worked !\nI can't really explain why this happened.\nThe Cuda compiler backend has maybe moved to C++, and some extern \"C\" {} 's are maybe required in my include files. \n\n\n Anyway, I have something working now, and I can tackle the Next Big Step : linking my lib with a mex file... \n Notes \n [ 1 ] Which is why I like wearing it short: no grip. \n [ 2 ] A list of Cuda compute capabilities can be found here \n [ 3 ] Actually, llvm-gcc ."], "link": "http://www.computersdontsee.net/index.php/post/2012/07/12/CUDA-weirdness", "bloglinks": {}, "links": {"http://www.mathworks.com/": 1, "http://www.apple.com/": 1, "http://developer.nvidia.com/": 1, "http://stackoverflow.com/": 1, "http://www.computersdontsee.net/": 6, "http://en.wikipedia.org/": 1}, "blogtitle": "Computers don't see"}, {"content": ["RTFM ...  A short post about the method cv::Mat::reshape(int channels, int rows) . \n\n\n A few days ago, I had some strange results when analyzing some high-dimensional data by PCA in OpenCV .\nFor compatibility with another library, I was forcing the allocation of contiguous memory for my images, simply by creating them with only 1 row and \\(m \\times n\\) columns [ 1 ] , then reshaping into the actual size of \\(m \\times n\\) pixels.\nHowever, the corresponding principal values were actually very very low, ad furthermore the results was not stable at all . \n\n\n Looking at my output more carefully, I suddenly realized that my PCA output had size \\(1 \\times 1\\) pix, which is clearly not high dimensional ! \n\n\n And I found the answer, and my bug, in the manual of course !\nLet's have a look at the complete signature of the method : \n cv :: Mat & cv :: Mat :: reshape ( int channels, int rows ) const  \n\n\n You've probably guessed the bug by now.\nAs written above, the method is a const member of cv::Mat , i.e. it does not change the object that calls it .\nInstead, it returns a reference to another instance wrapping the same data but with the new size, stride, etc... \n\n\n Hence, the fix was simple : replace each occurrence of myMatrix.reshape() by myMatrix = myMatrix.reshape() . \n Note \n [ 1 ] Recall that C arrays are stored in a row-major order, hence any additional alignment stride is introduced between rows and not columns.."], "link": "http://www.computersdontsee.net/index.php/post/2012/07/11/Reshaping-a-matrix-with-OpenCV", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 2, "http://opencv.org": 1}, "blogtitle": "Computers don't see"}, {"content": ["Getting angry, feeling desperate.  The SNR situation is getting worse, or at the very least more and more intriguing . \n\n\n Less artifacts, and good deblurring \n\n\n I've modified my implementations to minimize boundary effects, added an SSIM function to have another reference, made more experiences (denoising and deblurring + denoising), but still I don't really understand what's going on. \n\n\n Visually , the results are still good, as can be seen in these Figures .\nFurthermore, when doing deblurring , both SNR and SSIM measures match the visual feeling, and show that the algorithm improves the input image . \n\n\n \n \n \n\n\n Bad denoising !?! \n\n\n However, when doing pure denoising , SNR and SSIM measures are lower for the output of the algorithm !\nThe following Figures show the SSIM map for the noisy image , for the denoised image , and a plot where the denoised SSIM is greater than the noisy one. \n\n\n \n \n \n \n \n \n \n\n\n The histogram point of view \n\n\n By looking at the corresponding image histograms, one can see the probable cause of the bad SNR/SSIM outputs. \n\n\n The original image histogram exhibits a strong curvature, and there are few intermediate values on its second half.\nSince I added white noise, the shape of this histogram is not modified [ 1 ] . \n\n\n On the other hand, the output of the algorithm has a much flatter histogram , although the 2 main modes are more or less respected. \n\n\n \n \n \n \n\n\n Yet, I have no idea of the origin of this behavior (regularization maybe too strong ?), and no idea of a solution [ 2 ] \n Notes \n [ 1 ] Except for the values that fall outside the original dynamic range. \n [ 2 ] Except for the very ugly one of matching the histograms before computing the quality measures."], "link": "http://www.computersdontsee.net/index.php/post/2012/07/06/The-SNR-strikes-back", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 18}, "blogtitle": "Computers don't see"}, {"content": ["Show me more code !  Some updates \n\n\n I'm quietly updating the github depot associated with this blog. \n\n\n If you sync the code regularly, you may have already noticed the apparition of movies a few weeks ago.\nThey will be useful when the optical flow code will be online \n\n\n Today's update brings hard and soft-thresholding , and a demo on how to use them for image denoising (in the DCT domain). \n\n\n Being original \n\n\n The implementation of soft-thresholding may seem a bit awkward to people familiar with it : the soft-thresholding is implemented via a shrinkage operation on the original coefficient, without explicit sign extraction.\nThis formula comes from Matthieu Kowalski's papers for block-based thresholding, on which I rely for my work now.\nAt first, I tried to have an implementation close to his equations, and finally I decided that I liked it and kept it as is when using the code to github. \n\n\n You can visit Matthieu Kowalski's homepage here . \n\n\n More to come \n\n\n As said before, optical flow is scheduled to be the next big update in the code depot.\nIn the meanwhile, since I'm doing a lot of structured sparsity for my thesis, I will probably add small bits of transforms, thresholdings, etc. regularly. \n\n\n If you feel that I'm getting too slow to push the optical flow code, don't hesitate to drop a comment to keep me motivated"], "link": "http://www.computersdontsee.net/index.php/post/2012/07/05/Code-repo-updated", "bloglinks": {}, "links": {"https://github.com/": 1, "http://en.wikipedia.org/": 1, "http://webpages.supelec.fr/": 1}, "blogtitle": "Computers don't see"}, {"content": ["In this post, we brutally and ruthlessly linearize our LBPs.  LBP model \n\n\n An LBP can be decomposed into two tiers: \n \n first, a real description vector, obtained by convolution-then-difference \n then the quantization (binarization) operation. \n \n\n Mathematically, the real i-th component of the descriptor is computed with the formula:\n\\[ {\\mathcal L}(p)_i = \\langle{\\mathcal G}_{x_i, \\sigma_i} , p \\rangle - \\langle G_{x_i', \\sigma_i'} p\\rangle, \\]\nwhere \\({\\mathcal G}_{x_i, \\sigma_i}\\) and \\({\\mathcal G}_{x'_i, \\sigma'_i}\\) are two Gaussians. \n\n\n The variety of the LBP family comes from the choice of these Gaussians : they can have a fixed size but random positions (a la BRIEF ), fixed sizes and positions (a la BRISK ) [ 1 ] ...\nThe choice in FREAK was : \n \n the size of the Gaussians is fixed but depends on their position with respect to the center of the image patch (a coarser spatial resolution is used when moving away from the center) \n the spatial layout of the Gaussians is fixed \n the pairs of Gaussians used in the difference process are fixed, but instead of being given a priori they are produced by a learning stage . \n \n\n Linearization \n\n\n How can we turn an LBP into a linear operator ?\nWell, we simply forgot or neglected the non-linear operation, i.e. we keep only the real difference vecto r \n\n\n OK, this is really really brutal, but there is some some intuition behind. \nFirst, this work started as a proof-of-concept. So, the first step was logically to see if we had enough information to reconstruct the original patch from the real vector.\nSecond, by choosing the \\(\\ell_1\\)-norm, we are not completely bound by the data term and gain some robustness against the quantization, what will be demonstrated in some experiments. \n\n\n This is clearly the main limitation of the current paper, but it simplifies things greatly: if we forget about the quantization (more on this later), then we are left with an operator made of a convolution composed with a projection. \n\n\n Considering the reshaping of an image patch as a column vector, applying this non-quantized operator can be obviously written as a matrix-vector operation, and we have a final problem very close to the standard space varying image deconvolution problem. \n\n\n The next figures clearly show the process of forming this convoluted operator.\nWe start with in the image domain, using for example two pairs of Gaussians:\n \n\n\n We form a row-vector from an image patch by stacking the lines of the patch in lexicographic order.\nApplying this same procedure to each pair of Gaussians, we naturally obtain a matrix such as the one shown in the following figure:\n \n\n\n Moving to binary descriptors \n\n\n To move to a proved and reliable 1-bit reconstruction process, our intuition is to turn to 1-bit Compressed Sensing techniques . \n\n\n It is not completely clear however if this framework is fully applicable here: recall that by design FREAK is not random, because the retina-like structure was imposed and then the best pairs were selected.\nHence, it is difficult to prove useful properties [ 2 ] like RIP of the LBP operator in this case. \n\n\n Where to go next ? \n\n\n Upcoming post \n\n\n In the next post of the series, we will talk (at last!) about implementation tricks . \n\n\n If you don't want to miss the next posts, please subscribe to the blog !\nYou can also follow me on Twitter. \n\n\n References for further reading \n\n \n The FREAK home page \n [DAV12] Reconstructing FREAKs: Beyond Bits: Reconstructing Images from Local Binary Descriptors , accepted at ICPR 2012 \n [CLSF10] Calonder, M., Lepetit, V., Strecha, C., & Fua, P. (2010). BRIEF: Binary Robust Independent Elementary Features. Computer Vision\u2013ECCV 2010 \n [LCS11] Stefan Leutenegger, Margarita Chli and Roland Siegwart, BRISK: Binary Robust Invariant Scalable Keypoints, to appear in Proceedings of the IEEE International Conference on Computer Vision (ICCV) 2011. \n \n Notes \n [ 1 ] BRISK was the basis of the work on FREAK. \n [ 2 ] In the sense that they give us some recovery guarantees."], "link": "http://www.computersdontsee.net/index.php/post/2012/07/04/Linearized-model-of-LBPs", "bloglinks": {}, "links": {"https://twitter.com/": 1, "feed://www.computersdontsee.net/index.php/feed/atom": 1, "http://www.computersdontsee.net/": 8, "http://www.icpr2012.org/": 1, "http://www.ivpe.com/": 1, "https://infoscience.epfl.ch/": 1}, "blogtitle": "Computers don't see"}, {"content": ["Drowning by numbers...  I'm not as much on FREAKs reconstruction and blogging as I wished, because I have one really important target for the next 6 months : submitting my thesis before the end of the year. \n\n\n The dreaded SNR measure \n\n\n We've come up with a nice equation in the past few months (hence, this work is completely unpublished) and after managing to implement it I'm now testing it to show that it is actually outstanding.\nOr at least that it works.\nWell, I'd be happy if it just worked \n\n\n I'm quite pleased by the current visual state of our work.\nEven texture deconvolution seems OK, sharpness recovered with few artifacts. \n\n\n But then, when I want to comfort my opinion by SNR measures, I am always disappointed to get a lower score than the input degraded image. \n\n\n (Click on the image to see an animation with the deblurring result)\n \n\n\n As they say in the movies, something's going on here , and I'd better to find out what. Quickly."], "link": "http://www.computersdontsee.net/index.php/post/2012/07/02/SNR%2C-why-have-you-forsaken-me", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 3}, "blogtitle": "Computers don't see"}, {"content": ["Setting up the stage, and the script, before the Big Show !  In a previous post , I have briefly introduced our FREAK descriptor, which belongs to the more general family of the LBPs .\nIn this post, I will state mathematically the problem of the reconstruction of an image patch given its descriptor, i.e. answering the question: \n\n\n Can you guess which image part created this particular description vector ? \n\n\n Related work \n\n\n As far as we have seen, there is only one recent article [WJP11] addressing the same question: \n\n\n In this work, Weinzaepfel and his co-authors first learn the relationship between SIFT descriptors and image patches in a database.\nIt's like considering that the SIFT descriptor is the key used to sort a dictionary of image parts .\nThe core of the inversion process is a lookup operation in this database given an observed descriptor. \n\n\n Our approach \n\n\n We chose a somewhat more brutal approach, by considering instead the case where no external information is available .\nSince it is an ill-posed problem [ 1 ] , embracing the reconstruction task then naturally calls for a regularized inverse problem approach. \n\n\n Ingredients of the regularized inverse problem \n\n\n This approach is quite common among image processing tasks, hence I will directly give its formulation (see the paper for the details). \n\n\n Regularization function \n\n\n We chose as regularizer the Total Variation , i.e. the norm of the gradient (and not its squared norm as in Tikhonov).\nIt seemed to be a good fit because : \n \n it promotes sharp, piecewise-flat results, which is a good model for small image patches (typically 32x32 pixels); \n it is expressed in the image domain (a.k.a. analysis) , not in a coefficient space (a.k.a. synthesis, e.g. wavelet domain), like the LBP that we want to invert. \n \n\n Data term \n\n\n We chose the \\(\\ell_1\\)-norm for the data term.\nThis norm is known to be robust to salt-and-pepper noise, hence it will show some robustness to misestimations of the pixel-wise error, which is good in our case (recall that in the limit we only have access to the sign of the difference between pixels, and not its value). \n\n\n Furthermore, although it's not differentiable, the proximal mapping of its Fenchel-Legendre conjugate is easy to compute. \n\n\n Primal-dual solver \n\n\n The regularization term and the data term are completed by a set indicator function to guide the minimization process, and finally we have an initial equation which is convex, but non-differentiable, because of the \\(\\ell_1\\)-norm in the data term and in the Total Variation:\n\\[\n\\hat{x} = \\underset{x}{\\operatorname{argmin}}~\n\\underbrace{ \\lambda \\| A_{\\mathcal L}x - g\\|_1 }_{ \\text{data term} }\n+\n\\underbrace{ \\| p \\|_\\text{TV} + \\delta_{\\mathcal S}(x) }_{\\text{regularization} },\n\\]\nwhere \\(x\\) and \\(g\\) are respectively the unknown and the observation, \\(A\\) is the LBP, and \\(\\mathcal{S}\\) is a set of acceptable solutions. \n\n\n After a bit of cooking [ 2 ] , this problem boils down to :\n\\[ \\underset{x}{\\operatorname{argmin}}~F(Kx) + G(x), \\]\nwith \\( K = \\left(\\begin{array}{c} A_{\\mathcal L} \\\\ \\nabla \\end{array}\\right) \\) and \\(Kx = (y, z)^T = (A_\\mathcal{L}x, \\nabla x)^T\\).\n\\(G\\) is the remaining part of the initial constraints, i.e. the indicator function of \\(\\mathcal{S}\\). \n\n\n This formula is interesting because we have 2 independent constraints inside \\(F\\) :\n\\[ F(Kx) = F_1(y) + F_2(z) = \\lambda \\| y - g \\|_1 + \\| z \\|_1.\\] \n\n\n Furthermore, the different members of the final expression may not be differentiable, but it is convex and we know how to compute efficiently their proximal mapping (or equivalently, the proximal mapping of their Fenchel-Legendre conjugate).\nHence, we can apply the primal-dual solver presented in [CP10] to solve it. \n\n\n Where to go next ? \n\n\n Upcoming post \n\n\n In the next post of the series, I'll explain how we modeled the LBPs to create a linear operator \\(A_\\mathcal{L}\\), before moving on to the implementation. \n\n\n If you don't want to miss the next posts, please subscribe to the blog !\nYou can also follow me on Twitter. \n\n\n References for further reading \n\n \n The FREAK home page \n [DAV12] Reconstructing FREAKs: Beyond Bits: Reconstructing Images from Local Binary Descriptors , accepted at ICPR 2012 \n [WJP11] Weinzaepfel, P., Jegou, H., & P\u00e9rez, P. (2011). Reconstructing an image from its local descriptors (pp. 337\u2013344). Presented at the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). \n [CP10] Chambolle, A., & Pock, T. (2010). A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging. Journal of Mathematical Imaging and Vision, 40(1), 120\u2013145. \n \n Notes \n [ 1 ] The description is given by a projection, hence we're losing information. \n [ 2 ] See the paper for more hints, or come at the ICPR conference to have a chat !"], "link": "http://www.computersdontsee.net/index.php/post/2012/06/28/FREAK-reconstruction-as-an-inverse-problem", "bloglinks": {}, "links": {"https://twitter.com/": 1, "feed://www.computersdontsee.net/index.php/feed/atom": 1, "http://www.icpr2012.org": 1, "http://www.google.ch/": 1, "http://www.computersdontsee.net/": 8, "http://www.icpr2012.org/": 1, "http://www.ivpe.com/": 1, "https://infoscience.epfl.ch/": 1}, "blogtitle": "Computers don't see"}, {"content": ["Towards infinity... and beyond!  Integrating our lab's FREAK in your vision workflow is getting easier and easier ! \n\n\n While there was already a public github , I've checked out revision 8739 from OpenCV's trunk svn, and I've noticed a nice new line in the change log: FREAK is now a part of the Features2D module ! It's been there for a few days, but I was offline during this period. \n\n\n According to the commit messages, there have been a few problems on Windows and Android, but they have already been fixed!\nSo now, let's enjoy FREAK together !"], "link": "http://www.computersdontsee.net/index.php/post/2012/06/25/FREAK-makes-it-into-OpenCV-trunk", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 1, "https://github.com/": 1}, "blogtitle": "Computers don't see"}, {"content": ["Freak, c'est chic ! Let's dance.  OK, starting to mix posts in English and French since the English version of the blog is still buggy. \n\n\n Introduction \n\n\n Hello everyone ! \n\n\n Let's start this English part with good news for our lab. FREAK , our new Local Binary Pattern [ 1 ] had a lot of success this week: \n \n the authors of the original paper received the Best Open-Source Code Award at CVPR 2012 ! You can find the code here . \n our paper showing preliminary results of LBP reconstruction (including FREAK of course) has been accepted for an oral presentation at ICPR 2012 next November! [ 2 ] \n \n\n In a series of follow-up posts, I'll talk more about the reconstruction paper (and especially the choices in the implementation), so let's have a quick look at what's all this about. \n\n\n Forward transform \n\n\n The forward transform is quite natural: the goal here is to describe an image patch by an as-compact-as-possible-yet-highly-efficient descriptor. \n\n\n But what's an LBP ? \n\n\n In the case of LBP's , the descriptor is a binary vector, i.e. a vector of 0's and 1's, where each bit tells us the sign of the difference between the integral of two subregions [ 3 ] . \n\n\n  \n\n\n Hence, LBPs are very compact: they store 1 bit where other descriptors store 1 floating-point value (4 bytes). Furthermore, they are computationally very efficient: \n \n to compute them, one just needs integral images (which are fast and easy to compute), then a subtraction and a sign test. Even modest hardwares (e.g. smartphones) can easily compute LBPs. \n to compare 2 LBPs, the natural distance is the Hamming distance (the number of 1's in the result of a XOR b ), which is also very fast to compute (there may be some CPU optimized versions available, depending on your compiler). \n \n\n LBPs are compact and efficient image patch descriptors. Associated with easy-to-compute keypoints such as FAST, they will probably become ubiquitous in mobile object recognition and image matching softwares in the near future. \n\n\n What makes FREAK so special ? \n\n\n FREAKs have 2 particular points: \n \n a special structure is imposed on the averaging areas that will be used as inputs to the difference-then-quantize operator. This structure is inspired by the human retina : the spatial resolution is fine near the center, and becomes coarser when moving away from it (see figure below). \n the pairs that are used in the difference process are not random. They have been retained after assessing their performance in a matching task on an image database. \n \n\n \n \n\n\n And that's all for this short introduction to FREAKs !\nIf you want more detail, see thereferences below or feel free to use the comment form. \n\n\n Backward transform \n\n\n Inspired by the work of Weinzaepfel et al. [WJP11] , we asked ourselves wether it was possible or not to infer the original image patches from the LBP. \n\n\n However, while Weinzaepfel's paper describes an algorithm where the correspondences descriptor - patches are first learned from an image database, we decided to directly address the reconstruction task by an inverse problem approach . \n\n\n Coming soon ! \n\n\n Since this post is already too long, I will keep the description of the reconstruction algorithm for a follow-up post. \n\n\n The implementation won't be online very soon, because I still have some work to do with it [ 4 ] and I want to release a final and complete code only. \n\n\n Still, I will add a post next week to describe my solution to the most difficult parts of the implementation: while the algorithm is almost standard now ( primal-dual solver of Chambolle and Pock), the transpose of the LBP operator needs to be coded carefully to avoid the saturation of the memory. \n\n\n Stay tuned, and don't miss the next posts by subscribing to this blog's feed or by following me on Twitter ! \n\n\n \n\n References \n\n \n The FREAK home page \n [AOV12]Alahi, A., Ortiz, R., Vandergheynst, P., Fast Retina Keypoint , presented at CVPR 2012 . \n The original code on github \n [DAV12] Reconstructing FREAKs: d'Angelo, E., Alahi, A., Vandergheynst, P., Beyond Bits: Reconstructing Images from Local Binary Descriptors , accepted at ICPR 2012 \n [WJP11] Weinzaepfel, P., Jegou, H., & P\u00e9rez, P. (2011). Reconstructing an image from its local descriptors. (pp. 337\u2013344). Presented at the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). \n [CP10] Chambolle, A., & Pock, T. (2010). A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging. Journal of Mathematical Imaging and Vision, 40(1), 120\u2013145. \n \n Notes \n [ 1 ] Well, I prefer the name Local Binary Descriptor, which I find more accurate. \n [ 2 ] And I'm quite happy to go back to Japan \n [ 3 ] After normalization by the area. \n [ 4 ] Mostly about the 1-bit reconstruction"], "link": "http://www.computersdontsee.net/index.php/post/2012/06/20/Introducing-FREAKs", "bloglinks": {}, "links": {"https://twitter.com/": 1, "feed://www.computersdontsee.net/index.php/feed/atom": 1, "http://www.computersdontsee.net/": 17, "https://github.com/": 1, "http://www.icpr2012.org/": 2, "http://www.cvpr2012.org/": 1, "http://www.ivpe.com/": 1, "https://infoscience.epfl.ch/": 2}, "blogtitle": "Computers don't see"}, {"content": ["Gn\u00e9... ?  Si, comme moi, vous avez des difficult\u00e9s \u00e0 compiler OpenCV sur Mac depuis les versions post-2.4.0, voici un petit truc qui a march\u00e9 pour moi. \n\n\n A la compilation (avec make ), j'avais un tas d'erreurs d\u00e8s le d\u00e9part, du genre <sys/types.h>, <unistd.h> introuvables, et \u00e9chec d\u00e8s la tentative de compilation de la zlib [ 1 ] .\nJ'ai trouv\u00e9 ce matin comment contourner le probl\u00e8me : dans la configuration de cmake , il faut laisser le champ CMAKE_OSX_ARCHITECTURES vide. \n\n\n J'avais l'habitude de compiler en universal binaries pour 32 et 64 bits, en 2 passes (1 pour chaque architecture, puis utilisation de la commande lipo pour fusionner les 2 r\u00e9sultats).\nJe sp\u00e9cifiais donc \u00e0 chaque compilation l'architecture d\u00e9sir\u00e9e, chose qui est d\u00e9sormais impossible.\nA noter que m\u00eame fixer l'architecture de compilation \u00e0 l'architecture native de la machine (x86_64 pour mon Core 2 Duo) bloque la compilation ; il faut absolument laisser le champ vide. \n\n\n Comme cette solution me satisfait, je n'ai pas cherch\u00e9 plus loin les causes pr\u00e9cises de ce probl\u00e8me : toutes les machines auxquelles j'ai acc\u00e8s sont en 64 bits, et j'abandonne peu \u00e0 peu les UB sur cette plate-forme. \n Note \n [ 1 ] Avec Xcode 4.3.2 et Xcode 4.3.3."], "link": "http://www.computersdontsee.net/index.php/post/2012/06/15/Quickfix-%3A-compiler-le-svn-d-OpenCV-sur-Mac", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 2, "http://fr.wikipedia.org/": 1}, "blogtitle": "Computers don't see"}, {"content": ["Oui, je sais, l'introduction est un peu longue. Les gens press\u00e9s peuvent sauter directement \u00e0 la fin.  On commence par mes plus plates excuses : la version anglaise du blog est toujours bugg\u00e9e, mais malheureusement je n'ai pas le temps de m'y consacrer . \n\n\n Ouverture synth\u00e9tique \n\n\n Pas le temps ? Eh oui, j'ai fini par succomber \u00e0 la pression [ 1 ] et je fait turbiner le Mac Pro de mon bureau [ 2 ] sur de la synth\u00e8se d'ouverture optique . \n\n\n Comme en SAR , le principe est de combiner les images de plusieurs cam\u00e9ras (ou plusieurs observations d'une m\u00eame sc\u00e8ne, \u00e7a revient au m\u00eame) pour simuler, math\u00e9matiquement, une cam\u00e9ra \"plus grande\".\nDans le cas du radar, on simule une plus grande antenne pour am\u00e9liorer la pr\u00e9cision des mesures.\nDans le cas optique, on simule une pupille d'entr\u00e9e (ouverture) plus grande, dans mon cas de 60 ou 70 cm, alors que la focale des objectifs utilis\u00e9s est de 6 mm. \n\n\n Les fans de photographie auront tout de suite devin\u00e9 l'int\u00e9r\u00eat de ces syst\u00e8mes : comme la profondeur de champ est inversement proportionnelle \u00e0 la taille de l'ouverture, on peut transformer des images enti\u00e8rement nettes en images o\u00f9 le plan de nettet\u00e9 est tr\u00e8s mince, sans avoir \u00e0 payer le prix d'une optique de grande ouverture et en ayant le choix a posteriori du plan de nettet\u00e9. \n\n\n Pipe-line de traitement \n\n\n Donc, un labo voisin m'a transmis 15 Go de donn\u00e9es brutes de capteurs (fichiers images raw), qui passent par une savante moulinette : \n \n d\u00e9mosa\u00efquage (standard, celui d' OpenCV pour l'instant) ; \n d\u00e9bruitage (filtre bilat\u00e9ral) car les images sont horriblement bruit\u00e9es ; \n calibration (un algo maison \u00e0 base de d\u00e9tection de coins et de points d'int\u00e9r\u00eat, car le point est assez d\u00e9cal\u00e9 sur certaines cam\u00e9ras et le flou engendr\u00e9 perturbe la d\u00e9tection de coins) ; \n synth\u00e8se d'ouverture (youpi), \u00e0 ex\u00e9cuter sur les 36 000 images d'entr\u00e9e pour chaque distance de mise au point d\u00e9sir\u00e9e. \n \n\n Le r\u00e9sultat de tout \u00e7a, c'est un dossier contenant environ 4000 images (les trames individuelles de la s\u00e9quence vid\u00e9o) pour chaque distance de mise au point synth\u00e9tique, que je vais ensuite transformer en fichier vid\u00e9o gr\u00e2ce \u00e0 mencoder en ligne de commande. \n\n\n La commande mencoder \n\n\n Comme mencoder est assez peu document\u00e9, j'en arrive donc au coeur de ce billet, \u00e0 savoir la (longue) commande que j'ex\u00e9cute : \n mencoder mf: // ${SRC_DIR/*.png} -mf w = 1024 : h = 768 : fps = 30 : type =png -ovc -lavcopts vcodec =mpeg4: mbd = 2 -oac copy -o $DEST_MOVIE  \n\n\n La premi\u00e8re partie de la commande d\u00e9crit les donn\u00e9es d'entr\u00e9e : des images png, dans le dossier SRC_DIR, de taille 1024x768 pixels \u00e0 30 images-seconde, qu'il faut ensuite encoder en vid\u00e9o ( -ovc ) avec le codec mpeg4, la partie audio \u00e9tant simplement copi\u00e9e (-oac copy@@) depuis le fichier d'origine, donc ici ignor\u00e9e.\nLe tout \u00e9tant ensuite \u00e9crit dans le fichier DEST_MOVIE. \n\n\n Mais au fait, pourquoi... ? \n\n\n Oui, mais pourquoi s'emb\u00eater avec cette commande ?\nDeux raisons : \n \n je n'ai pas trouv\u00e9 de clicodrome satisfaisant pour cette t\u00e2che assez simple \n je peux continuer \u00e0 bosser depuis chez moi en lan\u00e7ant les conversions via ssh, en ligne de commande, depuis mon t\u00e9l\u00e9phone. Oui, bon, d'accord, cette derni\u00e8re raison n'est peut-\u00eatre pas la meilleure... \n \n\n  \n Notes \n [ 1 ] Et aussi par retrouver un peu d'esprit positif. \n [ 2 ] D'ailleurs, aucune annonce de nouveaux Mac Pro \u00e0 la WWDC de cette semaine, je sens l'esprit n\u00e9gatif et r\u00e2leur qui repointe le bout du nez..."], "link": "http://www.computersdontsee.net/index.php/post/2012/06/14/Ligne-de-commande-mencoder", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 5, "http://code.opencv.org": 1, "http://fr.wikipedia.org/": 2, "http://www.mplayerhq.hu/": 1}, "blogtitle": "Computers don't see"}, {"content": ["Men at work.  Un bref billet pour pr\u00e9venir que le blog risque de subir des perturbations dans les jours \u00e0 venir : \n \n je vais enlever la partie index.php des url, pour les rendre plus petites et lisibles. C\u00f4t\u00e9 blog c'est pas grand chose, mais \u00e7a risque de casser certains liens externes. \n une version en anglais du blog est en pr\u00e9paration. Pour rigoler, vous pouvez d'ores et d\u00e9j\u00e0 essayer de visiter l'adresse http://en.computersdontsee.net/ mais le site est vide, \u00e0 part une page d'accueil. \n \n\n Et demain, un billet pour voir si les Sombreros de l'Android ont r\u00e9ussi leur pari . Autant dire que c'est mal barr\u00e9, puisqu'\u00e0 12 heures de l'\u00e9ch\u00e9ance fatidique seulement 3 000$ (sur 50 000 vis\u00e9s) ont \u00e9t\u00e9 r\u00e9colt\u00e9s. Mais bon, on sait jamais, si un m\u00e9c\u00e8ne passer par l\u00e0..."], "link": "http://www.computersdontsee.net/index.php/post/2012/06/07/Mises-%C3%A0-jour-%C3%A0-venir", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 1, "http://en.computersdontsee.net/": 1, "http://www.kickstarter.com/": 1}, "blogtitle": "Computers don't see"}, {"content": ["Runner 5, you'd better run fast... now!  Avec le retour du beau temps, j'ai essay\u00e9 l'application Zombies, run! dont j'avais d\u00e9j\u00e0 parl\u00e9 . \n\n\n L'histoire \n\n\n Vous \u00eates Runner 5 , le survivant d'un crash d'h\u00e9licopt\u00e8re, isol\u00e9 en plein campagne infest\u00e9e par les zombies.\nHeureusement, un camp de survivants situ\u00e9 \u00e0 proximit\u00e9 vous a rep\u00e9r\u00e9s et vous guide, par radio, vers la s\u00e9curit\u00e9.\nToutefois, rien n'\u00e9tant gratuit en ce bas monde, ils vont en profiter pour vous demander de faire des petits d\u00e9tours, comme visiter un h\u00f4pital pour r\u00e9cup\u00e9rer des pansements, retrouver un \u00e9quipement... le tout avec la Horde aux trousses! \n\n\n Les principes de jeu \n\n\n Le jeu se compose d'\u00e9pisodes sonores de quelques minutes, espac\u00e9s entre eux de 5 \u00e0 10 minutes.\nCes dialogues vous pr\u00e9sentent l'intrigue , ou servent \u00e0 vous motiver \u00e0 courir en vous donnant des indications, comme La Horde est \u00e0 vos trousses! , On vous voit \u00e0 la jumelle, vous vous rapprochez! , ou bien SVP, faites un d\u00e9tour par l'h\u00f4pital \u00e0 c\u00f4t\u00e9 pour ramasser des pilules. .\nEntre deux dialogues, le jeu passe de la musique issue d'une playlist de votre choix. \n\n\n Pendant que vous courez , le jeu vous attribue des r\u00e9compenses sous la forme de provisions, m\u00e9dicaments, v\u00eatements... qui serviront \u00e0 agrandir et \u00e9quiper votre base , une fois de retour (si vous en revenez!). \n\n\n Au fur et \u00e0 mesure, l'intrigue se d\u00e9veloppe et de nouveaux profils de mission (ravitaillement vs storyline) se d\u00e9bloquent. \n\n Impressions \n\n \n L'ambiance sonore est vraiment tr\u00e8s r\u00e9ussie, on s'y croirait! Je dois reconna\u00eetre que \u00e7a aide bien \u00e0 passer le temps pendant la course. \n Je ne sais pas si c'est d\u00fb au hasard, mais le jeu avait l'air de choisir des morceaux plus rapides quand l'histoire acc\u00e9l\u00e9rait (par exemple, quand les zobies se rapprochent, ou au moment du sprint final). \n Petit b\u00e9mol, la narration adh\u00e8re strictement \u00e0 la dur\u00e9e choisie pour la mission (30 ou 60 minutes). Si vous \u00eates adeptes des dur\u00e9es interm\u00e9diaires (45 minutes ?) ou si vous vous entra\u00eenez si des distances fixes, l'application n'est pas tr\u00e8s adapt\u00e9e (m\u00eame s'il est possible de mettre en pause une mission pour la reprendre plus tard)."], "link": "http://www.computersdontsee.net/index.php/post/2012/05/28/Zombies%2C-run%21", "bloglinks": {}, "links": {"http://www.computersdontsee.net/": 1, "https://www.zombiesrungame.com/": 1, "http://www.computersdontsee.net/blog": 1}, "blogtitle": "Computers don't see"}]