[{"blogurl": "http://dataminingblog.com\n", "blogroll": [], "title": "Data Mining Research"}, {"content": ["Today, I propose an interview with Marcel Baumgartner, Demand Planning Specialist at Nestl\u00e9. Thanks to Marcel for detailing his activities and sharing his experience with us. \n Data Mining Research: Could you introduce yourself? What is your journey in Analytics? \n Marcel Baumgartner: My name is Marcel Baumgartner. I am one of these Swiss Germans who moved across the \u201cR\u00f6stigraben\u201d to live in the French speaking part of Switzerland. I graduated from EPFL, the \u201clittle\u201d sister of the ETHZ, in mathematical engineering, and then finished a Masters in Statistics at Purdue University in the US. My first job was at Nestl\u00e9, and as for many others, Nestl\u00e9 has remained my employer. I started my career (1995) first in Nestl\u00e9\u2019s R&D organization, in its heart at the Nestl\u00e9 Research Center near Lausanne. In my role as a consultant in all kinds of statistical matters, I had the privilege to more or less apply what I was taught in school. We basically explained our fellow researchers (biologist, nutritionists, chemist, food engineers, \u2026) what to do with all the data they gather and generate day in, day out. One of the strengths of this still existing group is the use of experimental designs, and making sense of high-dimensional data using multivariate analysis methods like Principal Components or Decision Trees . \n In 2001 I needed a change, and two colleagues from EPFL told me that Supply Chain Management needs a bit more statistics. A job opening was available in Vevey, in the headquarter of Nestl\u00e9, and I joined the Demand & Supply Planning team. In Vevey, we are responsible for corporate guidelines, best practices, appropriate tools and vision. I don\u2019t do myself any operational planning. \n Around 2003, Nestl\u00e9 got access to systems from SAP to support the work of Demand Planners . Their role is to foresee the future orders of our customers, as we very rarely can produce \u00a0based on orders: we need to build stock, therefore we need accurate forecasts. Through SAP and their module Advanced Planning and Optimization (SAP APO) , we suddenly had exponential smoothing routines within our toolbox. So I jumped on this occasion, learned all about it, provided training material, gave courses, and step by step we have learned how we can use these statistical methods to our advantage. \n DMR: What is demand planning about? What is the difference between forecasting and planning? \n MB: Demand Planning is about ensuring that the right product is at the right place, with the right volume, at the right time. The goal is to have enough so that we deliver exactly what has been ordered (internally we say that \u201cany unfulfilled order is a crime\u201d), but naturally not too much, as inventory is costly: it ties up capital, uses space, and products get old and eventually become \u201cbad goods\u201d. Freshness of our products is key: a chocolate consumed a few weeks after production is so much better than if it is already 6 months old. \n Forecasting is about a honest outlook of what will happen under the current assumptions. For example, assuming that the trend continues, that the seasonality is stable, and that the investment in the forthcoming marketing campaign will really materialize in more demand. Basically, we want to be able to identify gaps to our targets. Planning is then more about closing this gap: if our forecasts say that we are missing 2% growth, then we plan the actions that we need to close the gap. This becomes our plan. \n DMR: What are your current challenges at Nestl\u00e9? Anything you can share? \n MB: The challenge is that we need to become more efficient. Our typical Demand Planner is still doing way too much manual work, and therefore cannot focus on his or her value added tasks. A value added task is to have a very good relationship with the Sales team, and always stay informed about what\u2019s going on with promotions and other customer related activities. A non-value added task is to come up with a forecast for a product that can be forecasted well statistically. This is the transformation we are going through. \n We have now a very good understanding about the part of our portfolio can be forecasted statistically, or as we also call it, analytically. We now need to translate this into action ! What we will do is to create analytical competence centers that will provide these statistical forecasts as a service, so that Demand Planners don\u2019t need to worry about the details. The members of these analytical competence centers will be called Demand Analysts , and they will use state-of-the-art forecasting software. They will then provide reliable forecasts, and ensure that the Demand Planners adopt them widely. \n Another challenge is to find the right blend of people, brains\u00a0 that are good in statistics (not experts, simply people who know what can, and more importantly, what cannot be done with statistical methods), that are good in data management and finally individuals that can talk and live business. The new name for such talents is Data Scientist . All this in a context where hiring new people is very difficult. \n DMR: Do you have an advice for people involved in business forecasting? \n MB: There\u2019s a huge potential of forecasting performance improvements, both on accuracy but often more on efficiency. Today\u2019s forecasting products (e.g. SAS Forecast Server, ForecastPro, \u2026) are amazing in performance, scalability and automation. The challenge is to identify what part of your portfolio can take advantage of such tools. Your historical demand volatility is a key driver: low volatile products (typically measured through a coefficient of variation, adjusted for trend and seasonality) will have high performance, high volatile products need more business insight. \n Then don\u2019t underestimate the difficulties you will face to ensure adoption of this approach. If your company does not have a strong analytical mindset, you will need to spend a lot of time to gain the trust. Fortunately, you have the right tool: facts ! In forecasting, it is straightforward to show how good you are: simply eliminate a few periods of your history, simulate the performance of your methods, and you will have a very honest and unbiased estimate of your future performance. With these facts, you will convince all your managers not only of the power of these statistical, sorry analytical , methods, but also of their beauty ! \n Marcel Baumgartner can be joined at marcel.baumgartner@nestle.com"], "link": "http://www.dataminingblog.com/data-mining-interview-marcel-baumgartner/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 1}, "blogtitle": "Data Mining Research"}, {"content": ["To continue our series on forecasting , let\u2019s discuss one of the varying factors: the evaluation criteria. In classification, the percentage of accuracy is often used. It is obvious and easy to interpret. In the case of regression (e.g. forecasting), this is more complex. \n Whatever the application and the prediction method used, at one point, performances need to be evaluated. One motivation to evaluate results is to choose the most appropriate forecasting algorithm. Another one is to avoid overfitting. Thus, choosing the right criterion for your problem is a key step. In this post, we will focus on three accuracy measures. \n The Root Mean Square Error (RMSE) is certainly the most used measure. It is mainly due to its simplicity and usage in other domains. Its equation is given below: \n  \nThe main drawback of RMSE is to be scale dependent. It is thus not possible to compare two different time series. The second one is the Mean Absolute Percentage Error (MAPE). It is scale independent: \n  \nIts main issue is to be undefined when the denominator is null. This may happen often with intermittent data. The third error measure is the Mean Absolute Scaled Error (MASE). The na\u00efve forecast (last value) can be used as the denominator: \n  \nThe measure is scale independent and if below 1, better than na\u00efve forecast (a good benchmark). \n What error measure do you use and why? Post a comment to share your opinion."], "link": "http://www.dataminingblog.com/forecasting-evaluation-criteria/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 4}, "blogtitle": "Data Mining Research"}, {"content": ["Do you know HTML, the common standard for web pages? PMML is the same idea applied to data mining. PMML stands for Predictive Model Markup Language. It is a standard using XML notation to deploy data mining model in the industry. PMML standard comprises data pre-processing and modelling. \n The book contains a good (but short) introduction to the standard. Only a few schemas about the various steps of the process are given. Most of the book focuses on the different parts of the standard, illustrated with code snippets.\u00a0 The main objective of this standard is to reduce time to market when deploying data mining solutions. \n The book is really useful if you want to deploy your solution using PMML. If you are interested to know more about this standard, this is not the right book (but maybe the only one?). It contains a lot of code, but no case study. This would have been a plus, since most of the material can certainly be found on the web. \n PMML in Action (2nd Edition): Unleashing the Power of Open Standards for Data Mining and Predictive Analytics"], "link": "http://www.dataminingblog.com/data-mining-book-review-pmml-in-action/", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 2}, "blogtitle": "Data Mining Research"}, {"content": ["End of last year, I was invited to present at BaQMAR 2011. This exciting event was organized by a group of Belgium Analytics fans. Back in Switzerland, I felt the need for such an association. With the help of friends, experts in areas such as analytics, data mining, machine learning, web analytics and statistics, we created the Swiss Association for Analytics . \n  \nThe association, currently managed on LinkedIn, can be summarized as follows: \n \u201c The main objective of the Swiss Association for Analytics (SAA) is to spread the word about analytics in Switzerland. The goal is also to show added value of predictive analytics, data mining and machine learning to Swiss companies. Allowing analytics practitioners to exchange ideas and experiences is also our objective. The Swiss Association for Analytics provides latest updates, trends, news and original content to analytics practitioners. \u201d \n Are you interested in Analytics? \nLearn. Practice. Exchange. Join. \n http://www.swiss-analytics.ch"], "link": "http://www.dataminingblog.com/swiss-association-for-analytics/", "bloglinks": {}, "links": {"http://www.swiss-analytics.ch": 2, "http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 1}, "blogtitle": "Data Mining Research"}, {"content": ["To continue the first post of a series on forecasting, let\u2019s discuss standard methods used for predicting time series. The two firsts methods can be used as benchmark for comparing with more advances models: \n Mean value: the mean of the time series used for training is used as the forecast for all values in the test time series. \n  Last value: the last value of the time series is used as the forecast for the next one. \n  Simple Moving Average (SMA): the last m values are averaged to predict the next one. \n  Weighted Moving Average (WMA): the last m values are averaged, with a more important weight for recent values, to predict the next one. \n Other approaches exist such as Exponential Moving Average (EMA), ARIMA, Neural Network (NN) and Support Vector Regression (SVR). Do you use other techniques for forecasting? Which one is better according to your experience?"], "link": "http://www.dataminingblog.com/forecasting-standard-methods/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 5}, "blogtitle": "Data Mining Research"}, {"content": ["Mike Biere is the author of the book reviewed today on Data Mining Research. This recent book (2011) focuses on building and deploying BI in the company. The subtitle of the book is misleading: \u201cUsing Analytics to Achieve a Global Competitive Advantage\u201d. Let\u2019s be clear on the fact that the book deals with BI and not at all with analytics (statistics, data mining, etc.). One can only read a few general words about text mining. If I could suggest a new subtitle, it would be \u201cA Personal Journey\u201d since this is really what the book is about (without negative connotation). Several personal comments are really interesting. \n Although the book is easy to read, the style is sometimes awkward. The use of terms such as \u201cfolks\u201d regarding the reader is strange. The book is mainly a succession of paragraphs that are not always clearly linked together. My main concern is about the several copy/paste from Wikipedia within the book. Why not just adding a link to Wikipedia as a reference? \n As it is often the case with BI experts, Mike Biere considers BI as the center point, surrounded by data mining, data quality, master data management and so on. The author makes a clear distinction between BI and data warehouse. One of the most interesting part of the book (chapter 9) describes key influencers in the enterprise. Several excellent checklists for BI planning are given at the end of the book. \n To conclude, Biere is clearly an expert in BI and his book proves it. The style of writing may be surprising but personal comments are really interesting. Not directly related to analytics and full of Wikipedia contents, this book is still a good entrance door to the field of business intelligence. \n The New Era of Enterprise Business Intelligence: Using Analytics to Achieve a Global Competitive Advantage"], "link": "http://www.dataminingblog.com/data-mining-book-review-the-new-era-of-enterprise-business-intelligence/", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 1}, "blogtitle": "Data Mining Research"}, {"content": ["It\u2019s my pleasure to present you two blogs that I have added to the list of data mining blogs : \n \n Deep Data Mining Blog : written by Jay Zhou, the blog focuses on technical aspects of data mining (it started in May 2012). Examples of function used for data mining are given using SAS, Oracle and R. \n Predictive Analytics for Everyone : this business oriented blog is managed by Tom Fuyala from New Zealand. The blog discusses several topics related to data mining and contains news and discussions. Among other, it covers topics such as CRM, personalization and privacy. \n \n I wish you a nice reading!"], "link": "http://www.dataminingblog.com/new-data-mining-blogs-3/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 1, "http://predictive-analytics-for-everyone.blogspot.ch/": 1, "http://www.deep-data-mining.com/": 1}, "blogtitle": "Data Mining Research"}, {"content": ["Have you ever worked with time series? This topic is particular for several reasons. I will discuss them in a series of posts on Data Mining Research. In this post, I explain varying factors that may influence forecasting results. \n Time series is an ordered sequence of values of a variable at equally spaced time intervals. Time series prediction (forecasting) uses past data to predict future time series values. These forecasts can be used to support decisions in a company (planning). Examples of applications include electricity demand, call volume and stock requirements. One of the main difficulty when dealing with time series analysis is the number of parameters to tune. Here are examples: \n \n The time scale must be chosen carefully: daily, monthly, yearly? At the day level, although you have precise data, it may be difficult to highlight patterns. Daily prediction may also be too detailed, for example for managers. In certain situations, there may simply be no daily data available. \n The forecast horizon is critical: should you predict for 1,\u00a05 or 20 weeks ahead? This question highly depends on your business problem. It also depends on the available data: the longest the forecast horizon, the more data you need to build and test your model. \n The time window size is the number of time events you consider in the past data for building your model. The bigger the time windows the more information you include in your model. As for the forecast horizon, a bigger time window size will need more data. \n Additional data can be included in your time series model. For example, external data from various sources can be incorporated. It is possible to add any time event data that may influence your time series values. \n Several prediction methods are available for forecasting. It goes from the simple moving average (and its extension such as weighted and exponential) to ARIMA and more advanced techniques such as neural networks and support vector regression. There is no best choice and it clearly depends on your data and experience with these algorithms. \n The evaluation criterion is key since it allows you to estimate your performance, compare prediction techniques and time series between each others. Choosing the wrong evaluation criterion may lead to incorrect comparison and inexact conclusions. \n \n As you can read, there are more varying factors with forecasting than with most other data mining domains. I will describe certain of these varying factors in upcoming posts, so stay tuned!"], "link": "http://www.dataminingblog.com/forecasting-varying-factors/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 1}, "blogtitle": "Data Mining Research"}, {"content": ["I\u2019m back from Analytics 2012 in Cologne (Germany). It was an excellent 2-days conference organized by SAS. More than 400 participants from 32 countries. Keynote talks discussed topics such as big data (classical nowadays), high-performance analytics (SAS-oriented), social network mining (so trendy) and text analytics. Some keynotes were very interesting, but the added value was from\u00a0the standard talks. \n \u00a0 \n  \n \u00a0 \n Four parallel tracks were organized during the two days. Three were focused on specific industry applications, while the last one was oriented towards SAS tools. Among the topics covered by the conference, one can mention forecasting (demand, electricity, etc.), retail loss prevention and supply chain optimization. Particularly the forecasting topic was very well described by experts in the field. I really appreciated the talks from \u00a0 Sven Crone (Lancaster University) and Marcel Baumgartner (Nestl\u00e9). They both showed their expertise in the field of time series prediction. \n Although the conference was organized by SAS, most of the talks were oriented towards applications and the way to solve them, rather than\u00a0(SAS) tools. One of the presenter even showed a screenshot from SPSS: the best proof it was not a SAS-restricted event (!) Nothing to say about the organization, food and location. As usual with SAS, everything is perfect. To conclude, Analytics 2012 certainly was the analytical industry-oriented event in Europe. I learned a lot during these two days, thanks to all presenters and SAS!"], "link": "http://www.dataminingblog.com/analytics-2012-feedback-from-the-conference/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 1}, "blogtitle": "Data Mining Research"}, {"content": ["If you are using Microsoft data mining tools, this book is a must have. Written by MacLennan, Tang and Crivat, it describes how to perform data mining using SQL Server 2008. The book is huge \u2013 more than 630 pages \u2013 but it is normal since authors give detailed explanation for each data mining function. The book covers topics such as general data mining concepts, DMX, Excel add-ins, OLAP cubes, data mining architecture and many more. The seven data mining algorithms included in the tool are described in separate chapters. \n The book is well written, so it can be read from A to Z or by selecting specific chapters. Each theoretical concept is explained through examples. Using screenshots, each step of a given method is presented in details. It is thus more a user manual than a book explaining data mining concepts. Don\u2019t expect to read any detailed algorithms or equations. A good surprise of the book are the case studies. They are present in most chapters and show real examples and how to solve them. It really shows the experience of the authors in the field. \n The book also contains a lot of code for SQL Server programmers to directly implement the proposed methods. The book explains how to add your own code (plug-in) and use existing models (PMML). To conclude, this book is a comprehensive user manual about data mining functionalities of Microsoft SQL Server. More than that, it describe each method in details and propose very interesting data mining case studies. \n Data Mining with Microsoft SQL Server 2008"], "link": "http://www.dataminingblog.com/data-mining-book-review-data-mining-with-microsoft-sql-server-2008/", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.addtoany.com/": 1, "http://www.dataminingblog.com/": 1}, "blogtitle": "Data Mining Research"}]