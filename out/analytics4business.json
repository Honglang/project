[{"blogurl": "http://analytics4business.wordpress.com\n", "blogroll": [], "title": "analytics4business"}, {"content": ["You\u2019re working on the MAIN MODEL. The one that leverages half the company\u2019s assets, and on which your paycheck and that of many others depends. You\u2019ve already run through a stepwise, forward, and backward search of the variables, their interactions, and possible curvatures. What are the most productive things to do next? \n Here are a couple of ideas revolving around the ideas of relationship consistency and complex variable interactions. \n 1. COMPLEX VARIABLE INTERACTIONS \u2013 Predictive variables sometimes aren\u2019t: \u00a0It\u2019s a funny statement, but it represents a common problem that\u2019s usually ignored. We\u2019ve all seen variable interactions that change the significance, curvature, and even the sign of an important predictor. It\u2019s not uncommon. I think we can also agree that virtually no dataset contains all the data we\u2019d like it to, so it only stands to reason that there are many unavailable interacting variables. That\u2019s to say, many unidentified situations when our predictors don\u2019t predict the way we believe, or just aren\u2019t predictive at all. \n While we\u2019ll never have all the data we\u2019d like, it\u2019s possible to look for situations in which our predictive variables aren\u2019t behaving well, or in which normally unpredictive variables are useful. \n Example: I was once predicting stock price movements and could graphically see long trends in prices, but none of a variety of trend calculations showed much promise of predicting future prices. There were a lot of issues at play: Trends had to be calculated from prior highs and lows not just from a fixed time interval, Some time periods were noticeably more volatile than others, Down trends were usually more volatile than up trends, etc. That\u2019s when it occurred to me that the solution rested not in showing that the trend was or wasn\u2019t predictive, but upon determining WHEN it was predictive. As a result I began to create descriptive statistics about the trend calculations. These proved to be invaluable in illustrating when the trend did predict the future and when it did not. \n Interestingly, while it is easy to see and show the value of these interactions once they are known, they aren\u2019t detected by techniques such as stepwise regression or CART. This is because while the trend calculation is predictive in specific situations, neither the trend, nor its descriptive statistics are predictive individually. Thus they aren\u2019t identified as valuable by most algorithms. \n 2. THE UNEXPECTED IMPACT OF MISSING: When a variable is included, or taken out of a model, it impacts the parameters of the other variables. That same thing happens when a variable contains missing values, and imputing the variable\u2019s average doesn\u2019t fix it. \n Example: I was once predicting credit card transaction revenue for a bank. Two of the predictors were customer income and customer age, but the bank only had the incomes for about half its clients. The presence or absence of income had a strong impact on how the customer age was modeled. When income was present in the model, age appeared to act like a proxy for willingness to adapt technology, with more card usage for younger customers and DECREASING with age, given the same income. However, when income was missing and represented by an average value, the age variable had a completely different relationship. In the absence of income, age acted as a proxy for income with card transactions INCREASING with age, until retirement where they dropped again. In that case, I ended up building a model for when income was available and another when it wasn\u2019t. \n Depending upon the assets being leveraged this type of solution might become worthwhile long before the percentage of missing gets high."], "link": "http://analytics4business.wordpress.com/2010/11/17/ideas-for-improving-already-good-models/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "analytics4business"}, {"content": ["THE GOAL: \nWhen segmenting markets the objective is to find distinct groups of consumers who are similar to each other on multiple variables of \u201cinterest\u201d. If that lofty goal is realized, then products and marketing programs can be designed to appeal to desirable consumer groups so that when CONSUMERS select between you and your competitors, what your company offers will be a better fit in their eyes. \n WHAT CLUSTER ANALYSIS DOES FOR YOU: \n- Cluster Analysis groups observations based on their similarity across multiple variables. (That sounds \u201ckind of\u201d like the goal.) \n WHAT CLUSTER ANALYSIS DOESN\u2019T DO FOR YOU: \n- Determine what similarities are useful. \n- Adjust for different measurement scales. (Prices from $12,000 to $25,000 represent 13,000 measurement units while a 1 \u2013 7 agree/disagree survey questions represent 7 units of measurement.) \n TRAP\u00a01 \nlot of people\u2019s knee jerk reaction to different measurement scales is to standardize the data to mean zero and standard deviation one. That changes things, but does it fix anything? Now the original distribution of $12,000-$25,000 dollars is about equal to a seven point scale survey question. Is that appropriate? Maybe, but I could also imagine scenarios where the willingness to pay double the price might be worth 9 or 10 times more than agreement to a survey question. In effect, the scale problem and the usefulness problem are inextricably linked. \n The second problem with standardization is that the data that WAS on the same scale is now corrupted. A survey question showing large consumer differences across the whole 7-point range is deflated and a question that all respondents answered as either 6 or 7 is inflated so that they both end up with a standard deviation of one. Consequently, the procedure blurs a lot of the consumer distinctiveness you set out to look for. \n TRAP 2 \nAnother common practice is to run a Factor or Correspondence Analysis prior to the Cluster Analysis. Many of these procedures standardize the data automatically (in SAS it\u2019s automatically standardized unless you specify otherwise), but apart from that there is a second problem. Some of the factors represent large sources of variation (a.k.a. differences between consumers), while others represent trivial ones. Cluster Analysis just sees the units of measurement that you give it, so if you do not rescale the factors in accordance with their importance, this procedure will also wipe away data patterns you\u2019d like to identify. \n TRAP 3 \nUnlike many other statistical techniques, there is nothing \u201cunbiased\u201d about using all the data you have. Let\u2019s say you\u2019ve got one pricing variable from your database and twelve survey questions about different advertising themes. Cluster Analysis measures the distances you give it, so 12 measures of advertising is implicitly about 12 times more distance than one. Second, irrelevant data sums together just as easily as relevant data, so including everything available is likely to reduce the separation of more important concepts. \n SOLUTION \nAlthough there are several issues to watch for, all of the above problems can be solved by scaling the data based on its importance to your goals. It\u2019s very much akin to creating a dependent variable. You could over or under weight something, by accident or on purpose, but if the scaling is done with a good faith effort, the results should move you a lot closer to your goals. \n In Cluster Analysis, the science takes you part way, but it reminds me of one of those \u201cmathematical word problems\u201d from the 5th grade. It\u2019s up to you to pick which numbers have to be added together to get the desired answer. The described approach, I call \u201cConcept Availability Scaled Segmentation\u201d (CASS). I hope you find it useful."], "link": "http://analytics4business.wordpress.com/2010/11/17/tips-for-using-cluster-analysis-to-segment-markets-cass/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "analytics4business"}, {"content": ["One of the challenges of Non-Linear Regression modeling is coming up with a formula that easily converges, uses a minimum of parameters, and has the right balance of flexibility and inflexibility to model the problem without over fitting it. \n For me, and I suspect for others, that can be quite a challenge. Let\u2019s all share some of the formulas that we\u2019ve found to be useful building blocks and learn from each other. Here are some of examples I\u2019ve picked up here and there. \n\u2014\u2013 \n Problem: Forcing a parameter to stay within a specific range. \n Solution: \u00a0For that I usually use the logit function and multiply\u00a0it by a fixed scalar to get the desired parameter range. Logit\u00a0function = exp(A) / (1+exp(A)) Since the logit function always results in a number from 0 to 1, multiplying this by a scalar gets you a \u201cnet\u201d parameter of any fixed range desired. The simplicity of this example probably reflects my own lamentable skill level at this. \n\u2014\u2014 \n Problem: Modeling a multiple filtering system like a drug passing through a body or a new product being adopted by \u201cEarly Adopters\u201d and later by \u201cFollowers\u201d. \n Solution: Compartment model. From what I\u2019ve read it\u2019s a standard model in medicine. \nC(t) = B1\u00a0exp(-B2t) + B3\u00a0exp(-B4t), All B\u2019s >0, where C(t) is the drug concentration at time t. \n\u2014\u2014 \n Problem: Competition for control of something like a ball between competing teams or major accounts between competing salesmen. \n Solution: Logistic regression framework adding parameters that subtract from one another. \nProb. Team A beats Team B = exp(A-B) / (1+exp(A-B)) \nProb. Team A beats Team C = exp(A-C) / (1+exp(A-C)) \nProb. Team B beats Team C = exp(B-C) / (1+exp(B-C)) \nEstimated across games / sales encounters. A, B, C are interpreted as Team Strengths/ Salesmen persuasiveness. I used such a model to good effect for Soccer matches, but it might need to be adjusted a little for the salesmen example to account for more than two salesmen competing for the same client. \n\u2014\u2013 \n WHAT FORMULAS HAVE YOU FOUND TO BE USEFUL?"], "link": "http://analytics4business.wordpress.com/2010/11/17/useful-formulas-for-non-linear-regression/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "analytics4business"}, {"content": ["LEGOS: \n1. Square red pieces can be stacked to build towers. \n2. Long skinny pieces can be used as roads. \n3. Long skinny pieces on top of square red towers can build bridges. \n4. Curved skinny pieces can make turning roads or bridges. \n5. Tubes, motors, wheels, etc. can make pretty much anything you can imagine. \n STATISTICS: \n1. Univariate statistics can be used to clean errors out of your data. \n2. Correlations of past data with current data can predict future events. \n3. Linear Regression can combine many correlations to predict even better. \n4. Logistic Regression can predict curved relationships. \n5. Simultaneous equations, Non-Linear Regression, etc. can be applied to improve pretty much any business process you can imagine. \n THOUGHT PROVOKING QUESTION \nWhich of the following best describes how is your company is organized? \n A. We hire really creative builders who have access to all the pieces. \n B. We hire creative people who have only ever seen the red square pieces to tell the builders what to build. \n C. We hire builders who have a big sack of Legos with lots of different pieces, but they\u2019re not too interested in building anything they haven\u2019t seen before. \n D. We use robots to automatically build \u201cstuff\u201d that we use as bridges. The bridges usually work, but sometimes we fall through. \n E. We don\u2019t have any Legos."], "link": "http://analytics4business.wordpress.com/2010/11/17/statistical-modeling-is-like-building-with-legos/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "analytics4business"}, {"content": ["All the hard work we put into the \u201cmodel\u201d on the right hand side of the model equation is only as accurate as the dependent variable was to start with in reflecting the business problem at hand. Yet\u2026. No statistics class I ever took said the first word about the dependent variable and, in practice as well, it is often taken \u201cAS IS\u201d with all that that implies. \n Since Dependent Variable definitions are highly situation specific I think it would help us all to contribute our anecdotal stories about good things we\u2019ve done when defining the \u201cmodel\u2019s goal\u201d and let everyone take away what he/she can to their own problems. \n Some of my own stories: \n No Dependent Available \nTargeting for a new model car: Once I worked on a targeting project for a new car that had, at that point, never been sold. In other words there was NO sales history. A group of managers and myself judgmentally determined how similar the new car was to competitive cars that did have a sales history and then we modeled our similarity sales history as the dependent. The mix of science and judgment worked quite well in predicting new sales. \n Bad Dependent Available \nCustomer Attrition: This is an area that is often modeled poorly, because the initial temptation is to take everyone within a time period and define ALL those who later leave the company as the attritors. While this sounds okay at first pass, it works poorly because many customers don\u2019t just lease, they phase out little by little. The problem is that having a model that says everyone who has quickly drawn down their bank balance to $5 will soon leave the bank isn\u2019t very insightful and more importantly it is too late. A better definition is to count all these ghost accounts as another form of attrition. This is sometimes resisted by modelers because it will drop their \u201cstated accuracy\u201d (R2 or whatever) like a rock, but it is clearly more useful for the business offer an actionable prediction with a low R2 than a prediction with a high R2 that can\u2019t be used. \n Many Results but Little History \nSoccer Modeling: I modeled European Soccer outcomes for betting purposes for several years. One of the challenges was that the primary betting result of (Home Win / Draw / Away Win) had very little granularity, but the predictions had to be very accurate in order to beat the odds consistently enough to make money over the long run. One thing that helped a lot was to do a multi-stage estimation so that first you estimated each teams ability in terms: of Shots Taken, Corners, Fouls, Cards, etc and then used those estimates as predictors of who would win. It was an effective way to take advantage of both game history and the data structure to get more finely tuned results."], "link": "http://analytics4business.wordpress.com/2010/11/17/the-most-important-and-least-thought-about-variable-the-dependent/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "analytics4business"}]