[{"blogurl": "http://www.r-bloggers.com\n", "blogroll": [], "title": "R-bloggers"}, {"content": ["(This article was first published on  is.R() , and kindly contributed to R-bloggers)  \n \n \n So, where does ggplot get its colors? If you\u2019ve ever asked ggplot to color on the basis of a factor, you might have beeen surprised by the default color choices. \u00a0The fact is, ggplot colors factors on the basis of finding evenly spaced colors around the HSL color circle, shown below: EDIT : ggplot2 actually uses HCL space , which is not the same as the color circle below, but the basic idea of equally-spaced colors is valid. \n \n So, for one color, the default is a salmon/pink color; two colors sees the addition of blue, which is found 180 degrees from the first color. For three colors, the spacing must remain even so each color is now only 120 degrees from one another, and so on. Here\u2019s a plot that shows the\u00a0color\u00a0palette\u00a0ggplot automatically chooses for between 1 and 8 factor levels; enjoy! \n https://gist.github.com/3941162 \n This is the output: \n \n \n To leave a comment for the author, please follow the link and comment on his blog: is.R() . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/ggtutorial-day-3-introduction-to-colors/", "bloglinks": {}, "links": {"http://feedburner.google.com/": 1, "http://cscheid.github.com/": 1, "http://is-r.tumblr.com/": 2, "http://www.r-project.org/": 1, "http://www.r-bloggers.com/": 20, "https://gist.github.com/": 1}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  MPK Analytics \u00bb R , and kindly contributed to R-bloggers)  \n \n \n Edmonton has made a name for itself as the City of Champions , The Gateway to the North and the most northern city in North America with a population of\u00a0 over 1 million . Recently, however, Edmonton has added yet another achievement to this list, it has become the most northerly outpost is North America having an official R useR Group. The Edmonton R useR Group is the first official R useR group in Alberta and its aim is to bring together practitioners from industry and academia alike to exchange knowledge and experience in solving data analysis and statistical problems by using R.\u00a0Our HQ is located at the University of Alberta \u00a0and out next meeting is scheduled for Friday November 9 @ 3pm (see details here ). \n We are hoping that the Edmonton R User Group will follow in the footsteps of other successful R User Groups and to this end we are aiming to have workshops and presentations both by members and invited guests. So, if you live in or around Edmonton and are an R aficionado (or wannabe) we encourage you to join us. Although we are just starting up this is a great opportunity \u00a0to engage in discussions about what the Edmonton R User Group should do and how it can best serve its members and the R community at large. Hope to see you there. \n \n This is from the blog of MPK Analytics ( www.mpkanalytics.com ). In the business of helping clients transforming data into insight through the power of R. \n Filed under: Announcements , R Tagged: Alberta , Edmonton , University of Alberta        \n \n To leave a comment for the author, please follow the link and comment on his blog: MPK Analytics \u00bb R . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/edmonton-r-user-group-is-going-live/", "bloglinks": {}, "links": {"http://www.mpkanalytics.com/": 1, "http://feedburner.google.com/": 1, "http://blog.revolutionanalytics.com/": 1, "http://maps.google.com/": 3, "http://www.r-project.org/": 1, "http://mpkanalytics.com/": 7, "http://feeds.wordpress.com/": 7, "http://en.wikipedia.org/": 2, "http://www.r-bloggers.com/": 20, "http://canadabulldog.com/": 1, "http://www.meetup.com/": 2}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  Research tips \u00bb R , and kindly contributed to R-bloggers)  \n \n Make is a marvellous tool used by programmers to build software, but it can be used for much more than that. I use make whenever I have a large project involving R files and LaTeX files, which means I use it for almost all of the papers I write, and almost of the consulting reports I produce. \n If you are using a Mac or Linux, you will already have make installed. If you are using Windows and have Rtools installed, then you will also have make . Otherwise, Windows users will need to install it. One implementation is in GnuWin . \n A typical project of mine will include several R files containing code that fit some models, and generate tables and graphs. I try to set things up so I can re-create all the results by simply running the R files. Then I will have a LaTeX file which contains the paper or report I am writing. The tables and graphs produced by R are pulled in to the LaTeX file. Consequently, all I need to do is run all the R files, and then process the tex file, and the paper/report is generated. \n Make relies on a Makefile to determine what it must do. Essentially, a Makefile specifies what files must be generated first, and how to generate them. So I need a Makefile that specifies that all the R files must be processed first, and then the LaTeX file. \n The beauty of a Makefile is that it will only process the files that have been updated. It is smart enough not to re-run code if it has already been run. So if nothing has changed, running make does nothing. If only the tex file changes, running make will re-compile the tex document. If the R code has changed, running make will re-run the R code to generate the new tables and graphs, and then re-compile the tex document. All I do is type make and it figures out what is required. \n A Makefile for LaTeX \n It is easy to tell if the latex document needs compiling \u2014 make simply has to check that the pdf version of the document is older than the tex version of the document. Here is a simple Makefile that will just handle a LaTeX document. \n\n  TEXFILE = paper\n$ ( TEXFILE ) .pdf: $ ( TEXFILE ) .tex\n\trubber --pdf $ ( TEXFILE )  \n\n The first line specifies the name of my file, in this case paper.tex . The second line specifies that the pdf file must be created from the tex file, and the last line explains how to do that. If you don\u2019t have rubber (which is a unix program for compiling a LaTeX document), you could use pdftexify (which is part of MikTeX for Windows). \n To use the above Makefile , copy the code into a plain text file called Makefile and store it in the same directory as your tex file. Change the first line so the name of your tex file (without the extension) is used. Then type make from a command prompt within the same directory as the tex file, and it should do whatever is necessary to convert your tex to pdf. \n Of course, you wouldn\u2019t normally bother with a Makefile if that is all it did. But throw in a whole lot of R files, and it becomes very worthwhile. \n A Makefile for R and LaTeX \n We need a way to allow make to be able to tell if an R file has been run. We could specify all the outputs of the R file, but that is messy. Instead, we will create empty files of the form file.Rdone whenever file.R is run. That way, make only has to check if file.Rdone is older than file.R . \n I also like to strip out all the white space from the pdf figures created in R before I put them in a LaTeX document. There is a nice command pdfcrop which does that. (You should already have it on a Mac or Linux, and also on Windows provided you are using MikTeX.) So I also want my Makefile to crop all images if they have not already been done. Once an image is cropped, an empty file of the form file.pdfcrop is created to indicate that file.pdf has already been cropped. \n OK, now we are ready for my marvellous Makefile . \n\n  # Usually, only these lines need changing \n TEXFILE = paper\n RDIR = . / figs\n FIGDIR = . / figs\n \n # list R files \nRFILES := $ ( wildcard $ ( RDIR ) /* .R ) \n # pdf figures created by R \nPDFFIGS := $ ( wildcard $ ( FIGDIR ) /* .pdf ) \n # Indicator files to show R file has run \nOUT_FILES:= $ ( RFILES:.R=.Rdone ) \n # Indicator files to show pdfcrop has run \nCROP_FILES:= $ ( PDFFIGS:.pdf=.pdfcrop ) \n \nall: $ ( TEXFILE ) .pdf $ ( OUT_FILES ) $ ( CROP_FILES ) \n \n # May need to add something here if some R files depend on others. \n \n # RUN EVERY R FILE \n$ ( RDIR ) /% .Rdone: $ ( RDIR ) /% .R $ ( RDIR ) / functions.R\n\tRscript $ < && touch $ @ \n \n # CROP EVERY PDF FIG FILE \n$ ( FIGDIR ) /% .pdfcrop: $ ( FIGDIR ) /% .pdf\n\tpdfcrop $ < $ < && touch $ @ \n \n # Compile main tex file and show errors \n$ ( TEXFILE ) .pdf: $ ( TEXFILE ) .tex $ ( OUT_FILES ) $ ( CROP_FILES ) \n\trubber --pdf $ ( TEXFILE ) \n\trubber-info $ ( TEXFILE ) \n \n # Run R files \nR: $ ( OUT_FILES ) \n \n # View main tex file \nview: $ ( TEXFILE ) .pdf\n\tevince $ ( TEXFILE ) .pdf & \n \n # Clean up stray files \nclean:\n\t rm -fv $ ( OUT_FILES ) \n\t rm -fv $ ( CROP_FILES ) \n\t rm -fv * .aux * .log * .toc * .blg * .bbl * .synctex.gz\n\t rm -fv * .out * .bcf * blx.bib * .run.xml\n\trubber --clean $ ( TEXFILE ) \n\t rm -fv $ ( TEXFILE ) .pdf\n \n.PHONY: all clean  \n\n Download the file here. For most projects I copy this file into the main directory of my project, then all I have to do is modify the first few lines. RDIR specifies where the R files are kept and FIGDIR specifies where the figures are kept. Normally I keep these together, but sometimes they might be in separate directories. \n Now make will do everything necessary \u2014 run the R files, crop the pdf graphics, and process the latex document. But it won\u2019t do any steps that don\u2019t need doing. \n make R will only process the R files. \n make view will run the pdf viewer, after updating the pdf file if necessary. \n make clean will delete all the files generated by latex or by make, so that the entire process must be run again at the next make command. \n Notice that my R files all depend on functions.R . This is a file that contains project-specific functions. If this file is updated, all the other R files will need updating also. \n For many projects, some R files will depend on some others having already run. For example, read.R may read in the data and reformat it for analysis, while plot.R might produce some graphs assuming that read.R has already run. To ensure make knows about this dependency, we need to add a line \n\n  $ ( RDIR ) / plot.Rdone: $ ( RDIR ) / plot.R $ ( RDIR ) / functions.R $ ( RDIR ) / read.R\n\tRscript $ < ; && touch $ @  \n\n This should be inserted where I have the comment # May need to add something here if some R files depend on others. \n This Makefile works on Linux. Mac and Windows users will need to replace evince by whatever pdf viewer they prefer. \n\n \n To leave a comment for the author, please follow the link and comment on his blog: Research tips \u00bb R . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/makefiles-for-rlatex-projects/", "bloglinks": {}, "links": {"http://feedburner.google.com/": 1, "http://www.r-bloggers.com/": 20, "http://www.gnu.org/": 1, "http://www.r-project.org/": 1, "http://robjhyndman.com/": 4, "http://launchpad.net/": 1, "http://gnuwin32.sourceforge.net/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  Revolutions , and kindly contributed to R-bloggers)  \n \n\n Tim Gasper (Product Manager at Big Data platform Infochimps ) has an informative article at TechCrunch that provides an overview of five open-source technologies\u00a0 trending now for Big Data applications . They are: \n \n Storm and Kafka (for processing stream data) \n Drill and Dremel (for ad-hoc queries of big data) \n R (for data science with big data) \n Gremlin and Giraph (for graph analysis, e.g. of social networks) \n SAP HANA (for in-memory analytics). HANA isn't an open-source tool though, so perhaps the fifth slot should really go to ... \n Honourable mention D3 , for web-based visualization. \n \n Regarding R, Gasper says that it is \"incredibly powerful\", \"the new standard for statistics\", and that \"the R community is one of the most thrilling places to be in Big Data right now\".\u00a0He also mentions the RHadoop project (\"R work very well with Hadoop\") and the up-and-coming Julia project . \n You can read Gasper's complete overview of R and the other trending big-data technologies at link below. \n TechCrunch:\u00a0 Big Data Right Now: Five Trendy Open Source Technologies \n\n \n To leave a comment for the author, please follow the link and comment on his blog: Revolutions . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/r-among-techcrunchs-5-trendy-open-source-techs-for-big-data/", "bloglinks": {}, "links": {"http://www.infochimps.com/": 1, "http://feedburner.google.com/": 1, "http://blog.revolutionanalytics.com/": 4, "http://techcrunch.com/": 2, "http://www.r-project.org/": 1, "http://www.r-bloggers.com/": 20, "http://www.revolutionanalytics.com/": 1}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  Xi'an's Og \u00bb R , and kindly contributed to R-bloggers)  \n \n \n  A short visit to ISU but and therefore a busy and proftable day! About ten appointments in Snedecor Hall after a nice morning run , a highly attended Zyskind Lecture , and many interesting discussions all over the day: e.g., I had a great time discussing using null recurrent Markov chains for integral approximations with Krishna Athreya and Vivek Roy, following Vivek\u2019s seminar last week, ABC for spatial point processes with Alicia Carriquiry and Kristian Schmidt, SMC and ABC with [fellow blogger] Jarad Niemi , empirical likelihood with Song Chen, and hierarchical Bayes modelling and model checking with Mark Kaiser. I also met an impressive PhD student, Yihui Xie , who seems to have an endless pool of energy as he develops R packages by the dozen, such as animation , formatR , and knitr such as animation , formatR , and knitr , the later being an alternative to sweave, works on a book and seems to be contributing a lot to community sites like RPubs, in addition to maintaining his own blog \u2026 I actually took the opportunity to ask him a problem that bugged me for a while, namely how to include R code within beamer so that when I give a class/talk I can click on the code and see the output coming on the slide\u2026 \n Filed under: pictures , R , Running , Statistics , Travel , University life Tagged: Ames , George Snedecor , Iowa State University  \n \n To leave a comment for the author, please follow the link and comment on his blog: Xi'an's Og \u00bb R . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/visit-to-isu/", "bloglinks": {}, "links": {"http://feedburner.google.com/": 1, "http://xianblog.wordpress.com/": 14, "http://yihui.name/": 6, "http://www.r-project.org/": 1, "http://niemiconsulting.com/blog": 1, "http://feeds.wordpress.com/": 1, "http://www.r-bloggers.com/": 20, "http://www.iastate.edu/": 2, "http://cran.r-project.org/": 2}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  me nugget , and kindly contributed to R-bloggers)  \n \n     I finally got around to reproducing the DINEOF method ( Beckers and Rixon, 2003 ) for optimizing EOF analysis on gappy data fields - it is especially useful for remote sensing data where cloud cover can result in large gaps in data. Their paper gives a nice overview of some of the various methods that have been used for such data sets. One of these approaches, which I have\u00a0written about before ,\u00a0 involves deriving EOFs from a covariance matrix as calculated from available data. Unfortunately, as the author's point out, such covariance matrices are no longer positive-definite , which can lead to several problems. The DINEOF method seems to overcome several of these issues.  Read more \u00bb \n \n To leave a comment for the author, please follow the link and comment on his blog: me nugget . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/dineof-data-interpolating-empirical-orthogonal-functions/", "bloglinks": {}, "links": {"http://feedburner.google.com/": 1, "http://menugget.blogspot.com/": 3, "http://www.r-project.org/": 1, "http://menugget.blogspot.de/": 1, "http://en.wikipedia.org/": 1, "http://www.r-bloggers.com/": 20, "http://4.blogspot.com/": 1, "http://modb.ac.be/": 1, "http://dx.doi.org/": 1}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  4D Pie Charts \u00bb R , and kindly contributed to R-bloggers)  \n \n I\u2019m writing a book on R for O\u2019Reilly, and I need interesting datasets for the examples. Any data that you provide will get you a mention in the book and in the publicity material, so it\u2019s a great opportunity to publicise your work or your organisation. \n Datasets from any area or industry are suitable; the only constraint is that it can be analysed with a few pages of R code to provide a result that a general reader might go \u201cooh\u201d. There\u2019s a chapter on data cleaning, so even dirty data is suitable! \n All the data will be provided in an R package to accompany the book, so you need to be willing to make it publically available. I can help you anonymise the data, or strip out commercially sensitive parts if you require. \n If you can provide anything, or you know someone who might be able to, then drop me an email at richierocks AT gmail DOT com. Thanks. \n Tagged: book , data , r , stats  \n \n To leave a comment for the author, please follow the link and comment on his blog: 4D Pie Charts \u00bb R . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/make-your-data-famous/", "bloglinks": {}, "links": {"http://feedburner.google.com/": 1, "http://feeds.wordpress.com/": 1, "http://4dpiecharts.com/": 6, "http://www.r-project.org/": 1, "http://www.r-bloggers.com/": 20}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  Blag's bag of rants , and kindly contributed to R-bloggers)  \n \nThis is a presentation that I did on the Community Theatre at SAP TechEd Las Vegas 2012.   Happy sap hana friends from Alvaro Tejada  In this presentation, Blagbert helps his friends Nerdbert to set up his first SAP HANA project using different technologies like: Python to feed up SAP HANA with some random data. R to consolidate the data PowerBuilder to consume the data and present it on a graphical style. If you didn't attend TechEd in Las Vegas, here's your chance to see my presentation. If you attend TechEd in Las Vegas, but you didn't attend my presentation...well...shame on you! It was a nice presentation -:) Feel free to comment -;) Greetings, Blag. \n \n To leave a comment for the author, please follow the link and comment on his blog: Blag's bag of rants . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/happy-sap-hana-friends/", "bloglinks": {}, "links": {"http://feedburner.google.com/": 1, "http://www.slideshare.net/": 2, "http://www.r-project.org/": 1, "http://blagrants.blogspot.com/": 2, "http://www.r-bloggers.com/": 20}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  united states government survey data by anthony damico , and kindly contributed to R-bloggers)  \n \nnhanes is this fascinating survey where doctors and dentists accompany survey interviewers in a little mobile medical center that drives around the country. while the survey folks are interviewing people, the medical professionals administer laboratory tests and conduct a real doctor's examination. the blood work and medical exam allow researchers like you and me to answer tough questions like, \"how many people have diabetes but don't know they have diabetes?\" conducting the lab tests and the physical isn't cheap, so a new nhanes data set becomes available once every two years and only includes about twelve thousand respondents. since the number of respondents is so small, analysts often pool multiple years of data together. the replication scripts below give a few different examples of how multiple years of data can be pooled with r. the survey gets conducted by the centers for disease control and prevention (cdc) , and generalizes to the united states non-institutional, non-active duty military population. most of the data tables produced by the cdc include only a small number of variables, so importation with the foreign package's read.xport function is pretty straightforward. but that makes merging the appropriate data sets trickier, since it might not be clear what to pull for which variables. for every analysis, start with the table with 'demo' in the name -- this file includes basic demographics, weighting, and complex sample survey design variables. since it's easy to download the files directly from the cdc's ftp site, there's no massive ftp download automation script (yet). this new github repository contains four scripts: 2009-2010 interview only - download and analyze.R download, import, save the demographics and health insurance files onto your local computer load both files, limit them to the variables needed for the analysis, merge them together perform a few example variable recodes create the complex sample survey object, using the interview weights run a series of pretty generic analyses on the health insurance questions 2009-2010 interview plus laboratory - download and analyze.R download, import, save the demographics and cholesterol files onto your local computer load both files, limit them to the variables needed for the analysis, merge them together perform a few example variable recodes create the complex sample survey object, using the mobile examination component (mec) weights perform a direct-method age-adjustment and match figure 1 of this cdc cholesterol brief  replicate 2005-2008 pooled cdc oral examination figure.R download, import, save, pool, recode, create a survey object, run some basic analyses replicate figure 3 from this cdc oral health databrief - the whole barplot  replicate cdc publications.R download, import, save, pool, merge, and recode the demographics file plus cholesterol laboratory, blood pressure questionnaire, and blood pressure laboratory files match the cdc's example sas and sudaan syntax file's output for descriptive means match the cdc's example sas and sudaan syntax file's output for descriptive proportions match the cdc's example sas and sudaan syntax file's output for descriptive percentiles  click here to view these four scripts for more detail about the national health and nutrition examination survey (nhanes), visit: the cdc's nhanes homepage the national cancer institute's page of nhanes web tutorials  notes: nhanes includes interview-only weights and interview + mobile examination component (mec) weights. if you only use questions from the basic interview in your analysis, use the interview-only weights (the sample size is a bit larger). i haven't really figured out a use for the interview-only weights -- nhanes draws most of its power from the combination of the interview and the mobile examination component variables. if you're only using variables from the interview, see if you can use a data set with a larger sample size like the current population (cps) , national health interview survey (nhis) , or medical expenditure panel survey (meps) instead. confidential to sas, spss, stata, sudaan users: why are you still riding around on a donkey after we've invented the internal combustion engine? time to transition to r. :D \n \n To leave a comment for the author, please follow the link and comment on his blog: united states government survey data by anthony damico . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/analyze-the-national-health-and-nutrition-examination-survey-nhanes-with-r/", "bloglinks": {}, "links": {"http://feedburner.google.com/": 1, "https://github.com/": 1, "http://usgsd.blogspot.com/": 5, "http://www.r-project.org/": 1, "http://riskfactor.cancer.gov/": 1, "http://www.r-bloggers.com/": 20, "http://stat.ethz.ch/": 2, "http://www.cdc.gov/": 8}, "blogtitle": "R-bloggers"}, {"content": ["(This article was first published on  Milano R net , and kindly contributed to R-bloggers)  \n \n This is the last post about the course. As places are limited, please register as soon as possible! \n Milano R net, in collaboration with Quantide, organizes \n \"Advanced R\" Course \nNovember 15-16, 2012 \n \n Course description \nThis course is designed for those already using R and willing to gain a more in depth perspective of R working mechanism along with an overview of several advanced topics ranging from \"S4 programming frame\" to parallel computation. \n Course outline \nHow R works \nInside R Functions \nVectorized Calculation \nMethods and classes \nDebugging and Exceptions \nInvoking R \nParallel Computation \nSystem and foreign language interfaces \nCreating R packages \n Course location \nHotel Michelangelo; Via Scarlatti, 33; Milano (near Central Railway Station) \n Course language \nCourse will be held in Italian. Course material will be in English. \n Course price \nCommercial: 680,00 EUR (plus VAT). No profit and academic: 480,00 EUR (plus VAT) \nPrice includes coffee breaks and lunches. \n10% discounts for 2 participants of the same company (commercial attendees only) \n20% discounts for 3 participants of the same company (commercial attendees only) \nFor 4 or more participants of the same company or no profit institution, please contact us \n Course time \nNovember 15-16, 2012 from 9.00 to 17.30. \n Course material \nThe course material will be provided during the course. \n Information and bookings :\u00a0 training@quantide.com \n\n \n To leave a comment for the author, please follow the link and comment on his blog: Milano R net . \n \n R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: visualization ( ggplot2 , Boxplots , maps , animation ), programming ( RStudio , Sweave , LaTeX , SQL , Eclipse , git , hadoop , Web Scraping ) statistics ( regression , PCA , time series , ecdf , trading ) and more..."], "link": "http://www.r-bloggers.com/advanced-r-course-november-15-16-2012-3/", "bloglinks": {}, "links": {"http://www.milanor.net/blog": 2, "http://feedburner.google.com/": 1, "http://www.r-project.org/": 1, "http://www.r-bloggers.com/": 20}, "blogtitle": "R-bloggers"}]