[{"blogurl": "http://www.cerebralmastication.com\n", "blogroll": [], "title": "Cerebral Mastication"}, {"content": ["There\u2019s a charming little brain teaser that\u2019s going around the Interwebs. It\u2019s got various forms, but they all look something like this: \n This problem can be solved by pre-school children in 5-10 minutes, by programer \u2013 in 1 hour, by people with higher\u00a0education \u2026 well, check it yourself!\u00a0 \n 8809=6 \n7111=0 \n2172=0 \n6666=4 \n1111=0 \n3213=0 \n7662=2 \n9313=1 \n0000=4 \n2222=0 \n3333=0 \n5555=0 \n8193=3 \n8096=5 \n7777=0 \n9999=4 \n7756=1 \n6855=3 \n9881=5 \n5531=0 \n2581=? \n SPOILER ALERT\u2026 \n The answer has to do with how many circles are in each number. So the number 8 has two circles in its shape so it counts as two. And 0 is one big circle, so it counts as 1. So 2581=2. Ok, that\u2019s cute, it\u2019s an alternative mapping of values with implied addition. \n What bugged me was how might I solve this if the mapping of values was not based on shape. So how could I program a computer to solve this puzzle? I gave it a little thought and since I like to pretend I\u2019m an\u00a0econometrician, this looked a LOT like a series of equations that could be solved with an OLS regression. So how can I refactor the problem and data into a trivial OLS? I really need to convert each row of the training data into a frequency of\u00a0occurrence\u00a0chart. So instead of 8809=6 I need to refactor that into something like: \n 1,0,0,0,0,0,0,0,2,1 = 6 \n In this format the independent variables are the digits 0-9 and their value is the number of times they occur in each row of the training data. I couldn\u2019t figure out how to do the freq table so, as is my custom, I created a\u00a0concise simplification of the problem and put it on StackOverflow.com which\u00a0\u00a0yielded a great solution. Once I had the frequency table built, it was simple a matter of a linear regression with 10 independent variables and a dependent with no intercept term. \n My whole script, which you should be able to cut and paste into R, if you are so inclined, is the following: \n \n \n ## read in the training data \n ## more lines than it should be because of the https requirement in Github \ntemporaryFile <- tempfile ( ) \n download.file ( \"https://raw.github.com/gist/2061284/44a4dc9b304249e7ab3add86bc245b6be64d2cdd/problem.csv\" , destfile=temporaryFile , method= \"curl\" ) \nseries <- read.csv ( temporaryFile ) \n\n ## munge the data to create a frequency table \nfreqTable <- as.data.frame ( t ( apply ( series [ , 1 : 4 ] , 1 , function ( X ) table ( c ( X , 0 : 9 ) ) - 1 ) ) ) \n names ( freqTable ) <- c ( \"zero\" , \"one\" , \"two\" , \"three\" , \"four\" , \"five\" , \"six\" , \"seven\" , \"eight\" , \"nine\" ) \nfreqTable $ dep <- series [ , 5 ] \n\n ## now a simple OLS regression with no intercept \nmyModel <- lm ( dep ~ 0 + zero + one + two + three + four + five + six + seven + eight + nine , data =freqTable ) \n round ( myModel $ coefficients ) \n \n \n Created by Pretty R at inside-R.org \n The final result looks like this: \n \n > round(myModel$coefficients) \n zero \u00a0 one \u00a0 two three \u00a0four \u00a0five \u00a0 six seven eight \u00a0nine \n \u00a0 \u00a01 \u00a0 \u00a0 0 \u00a0 \u00a0 0 \u00a0 \u00a0 0 \u00a0 \u00a0NA \u00a0 \u00a0 0 \u00a0 \u00a0 1 \u00a0 \u00a0 0 \u00a0 \u00a0 2 \u00a0 \u00a0 1 \n \n So we can see that zero, six, and nine all get mapped to 1 and eight gets mapped to 2. Everything else is zero. And four is NA because there were no fours in the training data. \n There. I\u2019m as smart as a preschooler. And I have code to prove it."], "link": "http://www.cerebralmastication.com/2012/03/solving-easy-problems-the-hard-way/", "bloglinks": {}, "links": {"http://www.inside-r.org/": 1, "http://inside-r.org/": 14, "http://stackoverflow.com/": 1}, "blogtitle": "Cerebral Mastication"}, {"content": ["I had someone ask me about fitting a beta distribution to data drawn from a gamma distribution and how well the distribution would fit. I\u2019m not a \u201cclosed form\u201d kinda guy. I\u2019m more of a \u201cnumerical simulation\u201d type of fellow. So I whipped up a little R code to illustrate the process then we changed the parameters of the gamma distribution to see how it impacted fit. An exercise like this is what I call building a \u201ctoy model\u201d and I think this is invaluable as a method for building intuition and a visceral understanding of data. \nHere\u2019s some example code which we played with: \n \n \n \n set.seed ( 3 ) \nx <- rgamma ( 1e5 , 2 , .2 ) \n plot ( density ( x ) ) \n \n # normalize the gamma so it's between 0 & 1 \n # .0001 added because having exactly 1 causes fail \nxt <- x / ( max ( x ) + .0001 ) \n \n # fit a beta distribution to xt \n library ( MASS ) \nfit.beta <- fitdistr ( xt , \"beta\" , start = list ( shape1= 2 , shape2= 5 ) ) \n \nx.beta <- rbeta ( 1e5 , fit.beta $ estimate [ [ 1 ] ] , fit.beta $ estimate [ [ 2 ] ] ) \n \n ## plot the pdfs on top of each other \n plot ( density ( xt ) ) \n lines ( density ( x.beta ) , col = \"red\" ) \n \n ## plot the qqplots \n qqplot ( xt , x.beta ) \n \n \n Created by Pretty R at inside-R.org \n \n It\u2019s not illustrated above, but it\u2019s probably useful to transform the simulated data (x.beta) back into pre normalized space by multiplying by max( x ) + .0001 . (I swore I\u2019d never say this but I lied) I\u2019ll leave that as an exercise for the reader. \n Another very useful tool in building a mental road map of distributions is the graphical chart of distribution relationships that John Cook introduced me to ."], "link": "http://www.cerebralmastication.com/2011/05/fitting-distribution-x-to-data-from-distribution-y/", "bloglinks": {}, "links": {"http://inside-r.org/": 17, "http://www.johndcook.com/": 1, "http://www.cerebralmastication.com/": 1, "http://www.inside-r.org/": 1}, "blogtitle": "Cerebral Mastication"}, {"content": ["Lately I\u2019ve been doing some work with creating ad-hoc clusters of EC2 machines. My ultimate goal is to create a simple way to spin up a cluster of EC2 machines for use with Bryan Lewis\u2019s very cool doRedis backend for the R foreach package . But that\u2019s a whole other post. What I was scratching my head about today was that I\u2019d really just like to, with a single command, spin up an EC2 instance, wait for it to come up, and then ssh into it. I do this iteration about 20 times a day when I\u2019m testing things, so it seemed to make sense to shell script it. \nTo do this, one needs the EC2 command line tools installed on your workstation. In Ubuntu that\u2019s as easy as `sudo apt-get ec2-api-tools` \n So here\u2019s a short shell script to spin up an instance, wait 30 seconds, then connect: \n \n If you\u2019re reading this through an RSS reader, you can see the script over at github . \n Obviously you\u2019ll need to change the parameters at the top of the script to suit your needs. But since this was a bit of a pain in the donkey hole for me to figure out, I thought I would share. \n If you want to help out, I\u2019d love you to enlighten me on how to have the script figure out if an instance has finished booting so I could eliminate the sleep step."], "link": "http://www.cerebralmastication.com/2011/05/shell-scripting-ec2-for-fun-and-profit/", "bloglinks": {}, "links": {"http://www.thinkgeek.com/": 1, "https://gist.github.com/": 1, "http://cran.r-project.org/": 2}, "blogtitle": "Cerebral Mastication"}, {"content": ["In 2005 I was interviewing for a job as Risk Manager with Genworth Financial . I was working a gig up in Armonk, NY so I hopped a car to the GNW office and met with Mark Griffin, at that point the Chief Risk Office (CRO) for GNW. After some small talk, Mark asked me the single most interesting interview question I\u2019ve ever been asked. I don\u2019t recall the exact wording, but the gist was: \n If you could go back and work more on one project from your past, what would it be and why? \n This immediately struck me as a good question. Like all really good interview questions, there is no right answer, but any answer tells a LOT about the person answering it. I talked about a few projects I had really enjoyed from my past: fuel hedging dashboard for an international airline, data mining government program data, but said that the one thing I wish I could work more on was reinsurance ceding strategies for insurance companies. Naturally he responded, \u201cWhy so?\u201d So I explained the challenge and how I felt that if I had a little more time and a little more data I could numerically optimize reinsurance strategies and when I last worked on the problem it was 2001 and now, four years later, the computing power was better and I thought I could really get it right. \n I\u2019m pretty sure I didn\u2019t explain very well. Mark was obviously fishing around to see if I got a little OCD about analytical challenges and if I loved digging. I thought about Mark\u2019s question a lot three years later when I left Genworth to go work in reinsurance, optimizing reinsurance strategies."], "link": "http://www.cerebralmastication.com/2011/04/the-best-interview-question-ive-ever-been-asked/", "bloglinks": {}, "links": {"http://www.google.com/": 1}, "blogtitle": "Cerebral Mastication"}, {"content": ["In a previous post I discussed my frustrations with trying to get Dropbox or Spideroak to perform BOTH encrypted remote backup and AND fast two way file syncing. This is the detail of how I set up for two machines, both Ubuntu 10.10, to perform two way sync where a file change on either machine will result in that change being replicated on the other machine. \n I initially tried running Unison on BOTH my laptop and the server and had the server Unison set to sync with my laptop back through an SSH reverse proxy. After testing this for a while I discovered this is totally the wrong way to do it. The problem is that the Unison process makes temp directories and files in the file system of the target. So my Unison job on the laptop would be trying to syn files and, in the process, create temp files which would kick off a Unison sync on the sever which would make temp files on the laptop\u2026 I think you can see how convoluted this gets. \n So a much better solution is to only run Unison from one machine (I chose my laptop) and have the other machine (server in my case) send an SSH command (over the aforementioned reverse proxy) to the laptop asking the laptop to kick off a Unison sync. This way all of the syncs happen from the laptop. \n So, in short, both machines run lsyncd which monitors files for changes. I keep up an SSH tunnel with reverse port forwarding which forwards a remote machine port back to my laptop\u2019s port 22 (SSH). Unison need be installed ONLY on my laptop. When a change happens on my laptop, lsyncd fires off a Unison sync from my laptop that syncs it with the server. When a file changes on the server, the lsyncd job on the server makes a connection to my laptop via ssh and fires off a Unsion sync between my laptop and the server. \n Here\u2019s an example of my lsyncd config scripts: \n Laptop: \n settings = { \nlogfile = \u201c/home/jal/lsyncd/laptop/lsyncd.log\u201d, \nstatusFile = \u201c/home/jal/lsyncd/laptop/lsyncd.status\u201d, \nmaxDelays = 15, \n\u2013nodaemon = true, \n} \n runUnison2 = { \nmaxProcesses = 1, \ndelay = 15, \nonAttrib = \u201c/usr/bin/unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonCreate = \u201c/usr/bin/unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonDelete = \u201c/usr/bin/unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonModify = \u201c/usr/bin/unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonMove = \u201c/usr/bin/unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \n} \n sync{runUnison2, source=\u201d/home/jal/Documents\u201d} \n Server: \n settings = { \nlogfile = \u201c/home/jal/lsyncd/server/lsyncd.log\u201d, \nstatusFile = \u201c/home/jal/lsyncd/server/lsyncd.status\u201d, \nmaxDelays = 15, \n\u2013nodaemon = true, \n} \n runUnison2 = { \nmaxProcesses = 1, \ndelay = 15, \nonAttrib = \u201cssh localhost -p 5432 unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonCreate = \u201cssh localhost -p 5432 unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonDelete = \u201cssh localhost -p 5432 unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonModify = \u201cssh localhost -p 5432 unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \nonMove = \u201cssh localhost -p 5432 unison -batch /home/jal/Documents ssh://12.34.56.78//home/jal/Documents\u201d, \n} \n sync{runUnison2, source=\u201d/home/jal/Documents\u201d} \n Keep in mind that I am using version 2 of lsyncd which can be downloaded here: http://code.google.com/p/lsyncd/ \n The version of lsyncd available in the Ubuntu repo is version 1.x which does not use the same config format as I illustrate above. However, if you run into dependency issues with v2, the easiest thing to do is install the repo version which will install dependencies and then manually download and install v2 from the above URL. \n My reverse port forwarding set up looks like this: \n autossh -2 -4 -X -R 5432:localhost:22 12.34.56.78 \n the -R bit forwards remote port 5432 to my laptop\u2019s port 22 which is the ssh. So on my server if I run ssh localhost -p 5432 what actually happens is I am sshing from the remote machine to my laptop. \n Notes: \n \n The IP address of my server in this example is 12.34.56.78. \n Don\u2019t try and sync the directories where the lsyncd logs are kept. That will results in an endless sync cycle as each machine keeps noticing changes endlessly. Don\u2019t ask me how I know this. \n The command to start the sync on the laptop is \u201clsyncd /home/jal/lsyncd/laptop/configfile\u201d where configfile is the above lsyncd configuration file. \n lsyncd could, conceivably, tell Unison to sync only the part of the directory tree that changed. I have not been able to make that feature work right, however. And it only takes Unison a few seconds to sync, so I\u2019ve not worried about it. \n \n This has greatly sped up my RStudio based workflow when doing analysis with R. Now when I change files on my server using RStudio they are immediately (well it waits 15 seconds) replicated to my local machine and vice versa! \n Good luck and if you have any suggestions please post a comment!"], "link": "http://www.cerebralmastication.com/2011/04/details-of-two-way-sync-between-two-ubuntu-machines/", "bloglinks": {}, "links": {"http://rstudio.org": 1, "http://www.cerebralmastication.com/": 2, "http://code.google.com/": 1}, "blogtitle": "Cerebral Mastication"}, {"content": ["I love the portability of a laptop. I have a 45 min train ride twice a day and I fly a little too, so having my work with me on my laptop is very important. But I hate doing long running analytics on my laptop when I\u2019m in the office because it bogs down my laptop and all those videos on The Superficial get all jerky and stuff. \n I get around this conundrum by running much of my analytics on either my work server or on an EC2 machine (I\u2019m going to call these collectively \u201cmy servers\u201d for the rest of this post). The nagging problem with this has been keeping files in sync. RStudio Server has been a great help to my workflow because it lets me edit files in my browser and they run on my servers. But when a long running R job blows out files I want those IMMEDIATELY synced with my laptop. That way I know when I undock my laptop to run to the train station that all my files will be there for me to spill Old Style beer on as I ride the Metra North line. \n I experimented with Dropbox and I gotta say, it\u2019s great. It really is well engineered, fast, and drop dead simple. I love that with Dropbox I could pull up most any file from my Dropbox on my iPad or iPhone. That\u2019s a very handy feature. And it\u2019s fast. If I created a small text file on my server, it would be synced with my laptop in a few seconds. Perfect! Wel\u2026 almost. Dropbox has a huge limitation: encryption. Dropbox encrypts for transmission and may even store files encrypted on their end. However, Dropbox controls the key. So if a rogue employee, a crafty Russian hacker, or a law enforcement officer with a subpoena gained access to Dropbox, they could get access to my files without my knowledge. As a risk manager I can\u2019t help but see Dropbox\u2019s security as a huge, targeted, single point of failure. It\u2019s hard to say which would be a bigger payday: cracking GMail, or cracking Dropbox. But I\u2019m suspicious it\u2019s Dropbox. There are some workarounds to try and shoehorn file encryption into Dropbox, and they all suck. \n So Dropbox can\u2019t really give me what I want (what I really really want). But I stumbled into Spideroak who are like the smarter, but lesser known cousins of Dropbox. Their software does everything Dropbox does (including tracking all revisions!) but they have a \u201ctrust no one\u201d model which encrypts all files before leaving my computer using, and this is critical, MY key which they don\u2019t store. Pretty cool, eh? Spideroak also has a iPad/iPhone app and offers a neat feature that allows emailing any file in my Spideroak \u201cbucket\u201d to anyone using my iPhone without having to upload the file to my iPhone first. They do this by sending a special link to the email recipient that allows them to open only the file you wanted them to have. This could be a huge bacon saver on the road. \n So Spideroak\u2019s the panacea then? Well\u2026 um\u2026 no. They have two critical flaws: 1) They depend on time stamps on files to determine most recent file. 2) Syncs are slow, sometimes taking more than 5 minutes for very small files. The time stamp issue is an engineering failure, plain and simple. I\u2019ve talked to their tech support and been assured that they are going to change this and index using server time, not system time in the future. But as of April 6, 2011, Spideroak uses local system time. For most users this is no big deal. For my use case this is painful. My server and my laptop were 6 seconds different and that time difference was enough for me to get Spideroak confused about which files were the freshest. This is a big deal when syncing two file systems with fast changing files. The other issue, slow sync, was actually more painful but probably the result of their attempt to be nice with CPU time and also encryption. When jobs on my server finished, I expected those files to start syncing within seconds and the only delay I expected was bandwidth constraints. With Spideroak syncs might take 5 minutes to start and then it would go out for coffee, come back jittery and then finally complete. Even if SPideroak fixed the time sync issue (or I forced my laptop to set its time based on my server), it still would not work for my sync because of the huge lags. \n So looking at Dropbox and Spideroak I realized that I liked everything about Spideroak except its sync. It\u2019s a great cloud backup tool that seems to properly do encryption, it\u2019s multiplatform (win, linux, mac), has an iPad/iPhone app for viewing/sending files, it\u2019s smart about backups and won\u2019t upload the same file twice (even if the file is on two different computers). For my business use, I just can\u2019t use Dropbox. The lack of \u201ctrust no one\u201d encryption is a deal killer. So what I really need is a sync solution to use along side Spideroak. \n There are some neat projects out there for sync. Projects like Sparkleshare look really promising but they are trying to do all sorts of things, not just sync. I\u2019ve already settled on letting Spideroak do backup and version tracking so I don\u2019t really need all those features\u2026 OK, OK, I can hear you muttering, \u201cjust use rsync and be done with it already.\u201d Yeah, that\u2019s a good idea. But rsync is single directional and does a lot of things well, but can also be a bit of an asshole if you don\u2019t set all the flags right and rub its belly the right way. If you google for \u201cbidirectional sync\u201d you\u2019re going to see this problem has plagued a lot of folks. This blog post has already gone on long enough so I\u2019ll cut to the chase. Here\u2019s the stack of tools I settled on for cobbling together my own secure, real-time, bidirectional sync between two Ubuntu boxes (one of which changes IP address and is often behind a NAT router): \n 1) Unison \u2013 Fast sync using rsync-esque algos and really fast caching/scanning \n 2) lsyncd \u2013 Live (real-time) sync daemon \n 3) autossh \u2013 ssh client with a nifty wrapper that keeps the connection alive and respawns the connection if dropped \n I\u2019ll do another post with the nitty-gritty of how I set this up, but the short version is that I installed Unison and lsyncd on both the laptop and the server. Single direction sync from my laptop to the server is pretty straight forward: lsyncd watches files, if one changes it calls unison which syncs the files with the server. The tricky bit was getting my server to be able to sync with my laptop which is often behind a NAT router. The solution was to open an ssh connection from my laptop to my server using autossh and reverse port forward port 5555 from the server back to my laptop\u2019s port 22. That way an lsyncd process on the server can monitor the file system and when it sees a change can kick off a unison job that syncs the server to ssh://localhost:5555//some/path which is forwarded to my laptop! Autossh makes sure that connection does not get dropped and respawns if it does get dropped. So with a little shell scripting to start the lsyncd daemon on both machines, some config of lsyncd, and a local shell script to fire off the autossh connection, I\u2019ve got real-time bidirectional sync! \n In a follow up post I\u2019ll put of the details of this configuration. Stay tuned. (EDIT: Update posted !) \n If you\u2019ve solved sync a different way and you like your solution, please comment. I\u2019ve not settled that this is my long-term solution. It\u2019s just a solution that works. Which is more than I had yesterday."], "link": "http://www.cerebralmastication.com/2011/04/fast-two-way-sync-in-ubuntu/", "bloglinks": {}, "links": {"http://linux.die.net/": 1, "http://www.upenn.edu/": 1, "http://rstudio.org/": 1, "https://spideroak.com/": 1, "http://www.sparkleshare.org/": 1, "http://www.cerebralmastication.com/": 4, "http://code.google.com/": 1, "http://www.thesuperficial.com/": 1, "https://www.dropbox.com/": 1}, "blogtitle": "Cerebral Mastication"}, {"content": ["It\u2019s been pointed out to me that I haven\u2019t had any blog posts in a while. It\u2019s true. I\u2019m fairly slack. But in the last few months I\u2019ve changed jobs (same firm, new role), written an R abstraction on top of Hadoop, been to China, and managed to stay married. While that sounds pretty awesome, I\u2019m nothing compared to Hideaki Akaiwa . \n And you may have heard that the R Cookbook by Chicago\u2019s own Paul Teeter has been printed! Way to go Paul! And for a limited time you can get the book 50% off direct from O\u2019Reilly . \n And let it be known: I\u2019ve double dog dared you to find a stats or programming book with any better back cover quotes:"], "link": "http://www.cerebralmastication.com/2011/03/where-the-heck-has-jd-been/", "bloglinks": {}, "links": {"http://www.badassoftheweek.com/": 1, "http://oreilly.com/": 1}, "blogtitle": "Cerebral Mastication"}, {"content": ["I\u2019ve been messing around with using Amazon Web Services for a while. I\u2019ve had some projects where I wanted to upload files to S3 or fire off EMR jobs. I\u2019ve been controlling AWS services using a hodgepodge of command line tools and the R system() function to call the tools from the command line. This has some real disadvantages, however. Using the command line tools means each tool has to be configured individually which is painful on a new machine. It\u2019s also much harder to roll my R code up into a CRAN package because I have to check dependencies on the command line tools and ensure that the user has properly configured each tool. Clearly a pain in the ass. \n So I was looking for more simple/elegant solutions. After thinking the Boto library for Python might be helpful, I realized that the easiest way to use that would be with rJython which meant having to interact with R, Python, AND Java. Considering I don\u2019t program in Python or Java, that seemed like a fair bit of complexity. Then I realized that the canonical implementation of the AWS API was the AWS Java SDK. The rJava package makes interacting with Java from R a viable option. \n Since I\u2019ve never written a single line of Java code in my pathetic life, this was somewhat harder than it could have been. But with some help from Romain Francois I was able to cobble together \u201csomething that works.\u201d The code below gives a simple example of interfacing with S3. The example will look to see if a given bucket exists on S3, if not it will create the bucket. Then it will upload a single file from your PC into that bucket. You will have to download the SDK , unzip it in the location of your choice, and then change the script to reflect your configuration. \n If you are running R in Ubuntu, you should install rJava using apt-get instead of using install.packages() from inside of R: \n sudo apt-get install r-cran-rjava \n Here\u2019s the codez. And a direct link for you guys reading this through an RSS reader: \n \n I realize that Duncan Temple Lang has created the RAmazonS3 package which can easily do what the above code sample does. The advantage of using rJava and the AWS Java SDK is the ability to apply the same approach to ALL the AWS services. And since Amazon maintains the SDK this guarantees that future AWS services and features will be supported as well."], "link": "http://www.cerebralmastication.com/2010/11/controlling-amazon-web-services-using-rjava-and-the-aws-java-sdk/", "bloglinks": {}, "links": {"http://aws.amazon.com/": 1, "http://www.omegahat.org/": 1, "http://www.rforge.net/": 1, "http://romainfrancois.free.fr/": 1, "http://rjython.r-project.org/": 1, "http://code.google.com/": 1, "https://gist.github.com/": 1}, "blogtitle": "Cerebral Mastication"}, {"content": ["I\u2019m a huge O\u2019Reilly Media fan boy. I can\u2019t hide it. I hear Tim O\u2019Reilly speak at conferences and I think to myself, \u201cScrew being president, I want to be Tim O\u2019Reilly.\u201d I\u2019ve been a subscriber to their online book services called Safari Books Online for years. Every month I see the bill for $43 come through and I think to myself, \u201cSelf, that\u2019s the best $43 you spent all month.\u201d But the real downside of Safari Books Online is that it is, as the name implies, an online service. I spend 90 minutes each day on a train and I would LOVE to spend a huge chunk of that time reading O\u2019Reilly books. My iPad is not the 3g model so reading Safari Books Online is not an option for me. Then earlier this week I read that they had released the O\u2019Reilly Safari to Go app for the iPad. I was stoked and excited! I got so breathless that I even tweeted my excitement and then was re-tweeted by @OreillyMedia as you can see from the image in the upper left corner. \n I immediately downloaded the app and started playing with it. The fit and finish was not too good, but this is a first release product so I was cutting it some slack. It was a little slow and the screens visibly flashed when I changed screens. Typing was so sluggish that the cursor would lag behind my typing for 5-6 letters. This was all annoying but I was so excited to have these books on my train ride. After struggling a little to figure out how to get books into my offline book-bag I loaded 6 books into the bag and then left the app up and iPad running so they could download while I worked. When I got on the train I was dismayed to discover that I had no books at all in my off line book back. Odd. I know I put 6 in there. After tucking my daughter into bed I spent 3 hours fighting with the app. My final conclusion is that the app is complete and utter shit. It\u2019s poorly designed, poorly executed, and horrible to use. And the UI is nothing like an iPad app. It has zero redeeming value. The offline book bag is so buggy that it takes me ~8 tries to get a single book in the book bag. Often this after waiting for > 5 minutes for the book to download only to have it fail and I have to start over. For online book reading on the iPad the mobile version of the Safari Books website is far superior to the iPad app. Most of my time with the app was reading error messages like the one to the left. What I found bemusing was that I really did feel mad at O\u2019Reilly for this app. It wasn\u2019t the mad that I feel when I get ripped off, it was the mad that I feel when my 3 year old dumps her plate out on the table like a baby. It was a feeling of being let down by someone who I know can do better. And it appears I\u2019m not the only one. The pissed off comments on the Safari Books Online official blog are down right angry. So I did a little soul searching and asked myself why I felt so angry about my experience with the app. What I uncovered I tried to capture in a response post I made to CJ Rayhill, SVP Product Management & Technology. You can see my response here . And here\u2019s the same text for your easy reading enjoyment: \n CJ, I know you and your team have to be in pain over this app. It\u2019s terrible. You know that. And now you have a sunk cost problem, a vendor issue, and a \u201cpissed off geeks with pitchforks\u201d problem. Many of us have been there. There are bound to be multiple come-to-Jesus meetings over this. I\u2019ve sat in meetings like that. I\u2019ve led meetings like that. It sucks for every single person at the table. \n I\u2019m not sure if the vitriol in the tone of the comments above makes sense to you or your leadership team. Some folks reading this blog might think that the responses are a little over the top. Let me take a shot at helping this make sense through a personal anecdote. \n I love O\u2019Reilly Publishing. Recently I was invited to be a tech reviewer for _R Cookbook_ and I was over the moon to be asked by O\u2019Reilly to be a reviewer because I love O\u2019Reilly and I have a ton of positive feelings about those fantastic animal clad book covers. So, it\u2019s an understatement to say I\u2019m a fan. And I have this very personal device, my iPad, which I also love. This device is so intimate that I bring it to bed with me and my wife sometimes feels jealousy toward the time and attention I give to this device. So I invited O\u2019Reilly, who I love and trust, to come join me for a shared experience on this very personal device. And when O\u2019Reilly came over, in the form of the Safari to Go app, it was like having a trusted friend over who then decides to rub their muddy shoes on my suede couch while yelling \u201cF*ck your couch! F*ck your couch!\u201d The app is shockingly bad and totally inconsistent with the rest of my experience with O\u2019Reilly. Hours which I could have spent kicking ass were spent being mocked by this poorly coded and dysfunctional app now hogging the resources of my most intimate personal companion. \n You can see this level of hurt and frustration in the blog comments above. The relationship O\u2019Reilly has with its customers is special. You help us kick ass each and every day. When we want to learn something we go to you and you teach us through your books, your blogs, and your magazines. We\u2019re the ones who download IT Conversations podcast and scan through the playlist deleting Dr. Moira Gunn in order to move Tim O\u2019Reilly higher up in the playlist. When we daydream about being rock stars, we don\u2019t think about which model of Fender we\u2019ll play, we think about which animal the editors will pick to go on the cover of our book. And we hope to god they don\u2019t pick some overly cuddly critter or a 3 toed sloth. We want to be like Randal Schwartz and have our book known simply by the animal on the cover. \n CJ, you\u2019re an ass kicker too. You graduated from the Navel [sic] Academy, for crying out loud. You\u2019re a trail blazer and the Safari to Go app is a trailblazer. But I (and many others) think this project has lost its way. It seems the trail you tried to blaze was creating a multi-platform reader. Please allow me to be so bold as to suggest this is not the right goal. A better goal is to thrill your rock star fans with the best possible mobile off line Safari reading experience that helps them kick serious ass. You\u2019ve got some hard choices to make about your vendor, your technology stack, and your implementation strategy. They are hard choices. But hard choices are the cost of being a trailblazer. If it was easy, someone else would have already done it. \n I believe that the Safari mobile initiative could revolutionize not only technical books, but also text books. But the core of the platform has to be solid. Not only is the current core not solid, it\u2019s unusable. But I know you can fix it. I\u2019m glad Safari Books Online has you at the helm of their ship. Fix this thing, CJ, so we can all be rock stars with you. We\u2019re mad because were disappointed. But we want so much to be thrilled. \n -JD Long \n@CMastication \n If the couch reference is not entirely obvious, then you should brush up on your Dave Chappelle:"], "link": "http://www.cerebralmastication.com/2010/11/the-oreilly-safari-books-online-app-broke-my-heart/", "bloglinks": {}, "links": {"http://safaribooksonline.wordpress.com/": 2, "http://www.cerebralmastication.com/": 2}, "blogtitle": "Cerebral Mastication"}, {"content": ["A few months ago I switched my laptop from Windows to Ubuntu Linux. I had been connecting to my corporate SQL Server database using RODBC on Windows so I attempted to get ODBC connectivity up and running on Ubuntu. ODBC on Ubuntu turned into an exercise in futility. I spent many hours over many days and never was able to connect from R on Ubuntu to my corp SQL Server. \n Joshua Ulrich was kind enough to help me out by pointing me to RJDBC which scared me a little (I\u2019m easily spooked) because it involves Java. The only thing I know about Java is every time I touch it I spend days trying to get environment variables loaded just exactly the way it wants them. But Josh assured me that it was really not that hard. Here\u2019s the short version: \n Download the RJDBC driver from Microsoft . There\u2019s Win and *nix versions, so grab which ever you need. Unpack the driver in a known location (I used /etc/sqljdbc_2.0/). Then access the driver from R like so: \n require(RJDBC)\ndrv <- JDBC(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n\u00a0 \"/etc/sqljdbc_2.0/sqljdbc4.jar\")\u00a0\n\u00a0 conn <- dbConnect(drv, \"jdbc:sqlserver://serverName\", \"userID\", \"password\")\n#then build a query and run it\nsqlText <- paste(\"\n\u00a0\u00a0 SELECT * FROM myTable\n\u00a0 \", sep=\"\")\nqueryResults <- dbGetQuery(conn, sqlText) \n I have a few scripts that I want to run on both my Ubuntu laptop and my Windows Server. To accommodate that I made my scripts compatible with both by doing the following to my drv line: \n if (.Platform$OS.type == \"unix\"){\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 drv <- JDBC(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"/etc/sqljdbc_2.0/sqljdbc4.jar\")\n} else {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 drv <- JDBC(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"C:/Program Files/Microsoft SQL Server JDBC Driver 3.0/sqljdbc_3.0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /enu/sqljdbc4.jar\")\n } \n Obviously if you unpacked your drivers in different locations you\u2019ll need to molest the code to fit your life situation. \n EDIT: A MUCH better place to put the JDBC drivers in Ubuntu would be the /opt/ path as opposed to /etc/ which I used above. In Ubuntu the /opt/ directory is where one should put user executables and /etc/ should be reserved for packages installed by apt. I\u2019m not familiar with all the conventions in Ubuntu (or even Linux in general) so I didn\u2019t realize this until I got some reader feedback. \n Be forewarned, RJDBC is pretty damn slow and it appears to no longer be in active development. For my use case, RODBC was clearly faster. But RJDBC works for me in Ubuntu and that was my biggest need."], "link": "http://www.cerebralmastication.com/2010/09/connecting-to-sql-server-from-r-using-rjdbc/", "bloglinks": {}, "links": {"http://www.microsoft.com/": 1, "http://www.fosstrading.com/": 1, "http://www.cerebralmastication.com/": 1, "http://www.rforge.net/": 1, "http://stackoverflow.com/": 1}, "blogtitle": "Cerebral Mastication"}]