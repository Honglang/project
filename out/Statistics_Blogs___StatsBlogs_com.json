[{"blogurl": "http://www.statsblogs.com\n", "blogroll": [], "title": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Systematic Investor \u00bb R , and syndicated at StatsBlogs .)\n \n Regime Detection comes handy when you are trying to decide which strategy to deploy. For example there are periods (regimes) when Trend Following strategies work better and there are periods when Mean Reversion strategies work better. Today I want to show you one way to detect market Regimes. \n To detect market Regimes, I will fit a Hidden Markov Regime Switching Model on the set of simulated data (i.e. Bull / Bear market environments) I will use the excellent example from the Markov Regime Switching Models in MATLAB post and adapt it to R. \n The idea behind using the Regime Switching Models to identify market states is that market returns might have been drawn from 2 or more distinct distributions. As a base case, for example, we may suppose that market returns are samples from one normal distribution N(mu, sigma) i.e. \n \nReturns = mu + e, e ~ N(0, sigma)\n \n Next we may suppose that market returns are samples from two normal distributions (i.e. returns during Bull market may be ~ N(mu.Bull, sigma.Bull) and returns during Bear market may be N(mu.Bear , sigma.Bear) i.e. \n \nReturns = mu + e, e ~ N(0, sigma) \nmu = mu.Bull for Bull regime and mu.Bear for Bear regime and\nsigma= sigma.Bull for Bull regime and sigma.Bear for Bear regime\n \n Fortunately we do not have to fit regimes by hand, there is the RHmm package for Hidden Markov Models at CRAN that uses the Baum-Welch algorithm to fit Hidden Markov Models. \n Next, let follow the steps from the Markov Regime Switching Models in MATLAB post. \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\t#*****************************************************************\n\t# Generate data as in the post\n\t#****************************************************************** \n\tbull1 = rnorm( 100, 0.10, 0.15 )\n\tbear = rnorm( 100, -0.01, 0.20 )\n\tbull2 = rnorm( 100, 0.10, 0.15 )\n\ttrue.states = c(rep(1,100),rep(2,100),rep(1,100))\n\treturns = c( bull1, bear, bull2 )\n\n\n\t# find regimes\n\tload.packages('RHmm')\n\n\ty=returns\n\tResFit = HMMFit(y, nStates=2)\n\tVitPath = viterbi(ResFit, y)\n\n\t#Forward-backward procedure, compute probabilities\n\tfb = forwardBackward(ResFit, y)\n\n\t# Plot probabilities and implied states\n\tlayout(1:2)\n\tplot(VitPath$states, type='s', main='Implied States', xlab='', ylab='State')\n\t\n\tmatplot(fb$Gamma, type='l', main='Smoothed Probabilities', ylab='Probability')\n\t\tlegend(x='topright', c('State1','State2'), fill=1:2, bty='n')\n \n  \n The first chart shows states (1/2) determined by the model. The second chart shows the probability of being in each state. \n Next, let\u2019s generate some additional data and see if the model is able to identify the regimes \n \n\t#*****************************************************************\n\t# Add some data and see if the model is able to identify the regimes\n\t#****************************************************************** \n\tbear2 = rnorm( 100, -0.01, 0.20 )\n\tbull3 = rnorm( 100, 0.10, 0.10 )\n\tbear3 = rnorm( 100, -0.01, 0.25 )\n\ty = c( bull1, bear, bull2, bear2, bull3, bear3 )\n\tVitPath = viterbi(ResFit, y)$states\n\n\t#*****************************************************************\n\t# Plot regimes\n\t#****************************************************************** \n\tload.packages('quantmod')\n\tdata = xts(y, as.Date(1:len(y)))\n\n\tlayout(1:3)\n\t\tplota.control$col.x.highlight = col.add.alpha(true.states+1, 150)\n\tplota(data, type='h', plotX=F, x.highlight=T)\n\t\tplota.legend('Returns + True Regimes')\n\tplota(cumprod(1+data/100), type='l', plotX=F, x.highlight=T)\n\t\tplota.legend('Equity + True Regimes')\n\t\n\t\tplota.control$col.x.highlight = col.add.alpha(VitPath+1, 150)\n\tplota(data, type='h', x.highlight=T)\n\t\tplota.legend('Returns + Detected Regimes')\t\t\t\t\n \n  \n The first 300 observations were used to calibrate this model, the next 300 observations were used to see how the model can describe the new infromation. This model does relatively well in our toy example. \n To view the complete source code for this example, please have a look at the bt.regime.detection.test() function in bt.test.r at github . \n  \n \n \n Please comment on the article here:  Systematic Investor \u00bb R \n \n The post Regime Detection appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/0coaJBL7jWw/", "bloglinks": {}, "links": {"https://github.com/": 1, "http://blogs.mathworks.com/": 2, "http://systematicinvestor.wordpress.com": 1, "http://r-forge.r-project.org/": 1, "http://www.statsblogs.com/": 1, "http://www.statsblogs.com": 2, "http://feeds.wordpress.com/": 1, "http://systematicinvestor.wordpress.com/": 3}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Statistical Modeling, Causal Inference, and Social Science , and syndicated at StatsBlogs .)\n \n Bayesian inference, conditional on the model and data, conforms to the likelihood principle. But there is more to Bayesian methods than Bayesian inference. See chapters 6 and 7 of Bayesian Data Analysis for much discussion of this point. \n It saddens me to see that people are still confused on this issue. \n\n \n \n Please comment on the article here:  Statistical Modeling, Causal Inference, and Social Science \n \n The post It not necessary that Bayesian methods conform to the likelihood principle appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/9vT4ABmqp2w/", "bloglinks": {}, "links": {"http://www.statsblogs.com": 2, "http://www.statsblogs.com/": 1, "http://andrewgelman.com": 1, "http://errorstatistics.com/": 1, "http://andrewgelman.com/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Error Statistics Philosophy \u00bb Statistics , and syndicated at StatsBlogs .)\n \n  U-Phil: I would like to open up this post, together with  Gandenberger\u2019s  (Oct. 30, 2012), to reader U-Phils, by December 5, (< 1000 words) for posting on this blog (please see # at bottom of post).\u00a0 Where Gandenberger claims, \u201cBirnbaum\u2019s proof is valid and his premises are intuitively compelling,\u201d I have shown that if Birnbaum\u2019s premises are interpreted so as to be true, the argument is invalid.\u00a0 If construed as formally valid, I argue, the premises contradict each other. Who is right?\u00a0 Gandenberger doesn\u2019t wrestle with my critique of Birnbaum, but I invite you (and Greg!) to do so. I\u2019m pasting a new summary of my argument below. \n \n \u00a0The main premises may be found on pp. 11-14. While these points are fairly straightforward (and do not require technical statistics), they offer an intriguing logical, statistical and linguistic puzzle. The following is an overview of my latest take on the Birnbaum argument. See also \u201cBreaking Through the Breakthrough\u201d posts: Dec. 6 and Dec 7, 2011 .  \n Gandenberger also introduces something called the methodological likelihood principle. A related idea for a U-Phil is to ask: can one mount a sound, non-circular argument for that variant?\u00a0 And while one is at it, do his methodological variants of sufficiency and conditionality yield plausible principles? \n Graduate students and others invited! \n ______________________________________________________ \n New Summary of Mayo Critique of Birnbaum\u2019s Argument for the SLP \n Deborah Mayo  \nSee also a (draft) of the full\u00a0 PAPER corresponding to this summary. Yet other links to the Strong Likelihood Principle SLP: Mayo 2010 ; Cox & Mayo 2011 (appendix). \n Please alert me to corrections, not all the symbols transferred so well. \n \u00a01. (SLP): For any two experiments E\u2019 and E\u201d with different probability models f\u2019, f\u201d but with the same unknown parameter \u03b8, if the likelihood of outcomes x \u2019* and x \u201d* (from E\u2019 and E\u201d respectively) are proportional to each other, then x \u2019* and x \u201d* should have the identical evidential import for any inference concerning parameter \u03b8 . \n SLP pairs . When the antecedent holds, x \u2019* and x \u201d* are said to have \u201cthe same likelihood function\u201d,\u00a0 i.e., f\u2019( x\u2019 ; \u03b8) = cf\u201d( x \u201d, \u03b8) for all \u03b8, c a positive constant. In such cases, we abbreviate by saying x \u2019* and x \u201d* are SLP pairs , and the asterisk * will be used to indicate this. \n So we can abbreviate the SLP as follows: \n SLP: for any two experiments, E\u2019 and E\u201d, if x \u2019* and x \u201d* are SLP pairs (from E\u2019 and E\u201d respectively) then \n \u00a0 Infr E\u2019 ( x \u2019*) equiv Infr E\u201d ( x \u201d*). \n -1- \n \n _________________________________________________________________ \n 2.1 SLP Violation with Binomial, Negative Binomial \n Example 1 . Binomial vs. Negative Binomial . Consider independent Bernoulli trials, with the probability of success at each trial an unknown constant \u03b8, but produced by different procedures, E\u2019, E\u201d.\u00a0 E\u2019 is Binomial with a pre-assigned number n of Bernoulli trials, say 20, and R, the number of successes observed. In E\u201d trials continue until a pre-assigned number r, say 6, of successes has occurred, with the number N trials recorded. The sampling distribution of R is Binomial: \n f(R; \u03b8) = ( n C r ) \u03b8 r (1\u2013 \u03b8) n-r \n while the sampling distribution of N is Negative Binomial. \n f(N; \u03b8) = ( n-1 C r-1 ) \u03b8 r (1\u2013 \u03b8) n-r \n If two outcomes from E\u2019 and E\u201d respectively, have the same number of successes and failures, r and n, then they have the \u201csame\u201d likelihood, in the sense that they are proportional to \u03b8 r (1\u2013 \u03b8) n-r . \n The two outcomes, x \u2019* and x \u201d* are SLP pairs. But the difference in the sampling distributions of the respective statistics, R and N, of E\u2019 and E\u201d respectively, entails a difference in p-values or confidence level assessments.\u00a0 Accordingly, their evidential appraisals differ for sampling distribution inference. Thus x \u2019* and x \u201d* are SLP pairs leading to an SLP violation. \n -2- \n __________________________________________________________________ \n An SLP violation with Binomial (E\u2019) and Negative Binomial (E\u201d):\u00a0\u00a0\u00a0\u00a0\u00a0 \n (E\u2019, r=6) and (E\u201d, n=20) have proportional likelihoods \nbut Infr E\u2019 ( x \u2019*= 6) is not equiv to Infr E\u201d ( x \u201d*=20). \n Loss of relevant information if the index is erased \n In making inferences about \u03b8 on the basis of data x in sampling theory , relevant information would be lost if the report removed the index from E and reported: \n Data x consisted of r successes in n Bernoulli trials, generated from either a Binomial experiment with n fixed at 20, or a negative binomial experiment with r fixed at 6\u2014erasing the index indicating the actual source of data. \n -3- \n \n __________________________________________________________________ \n 2.2 SLP violation with fixed normal testing and optional stopping: E\u2019, E\u201d \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \n Example 2 . Fixed vs. sequential sampling . Suppose X \u2019 and X \u201d are sets of independent observations from N(\u03bc,\u03c3 2 ), with \u03c3\u00a0known, and p-values are to be calculated for the null hypothesis \u03bc\u00a0= 0. In E\u2019 the sample size is fixed, whereas in E\u201d the sampling rule is to continue sampling until 1.96\u03c3/\u221an is attained or exceeded. Suppose E\u201d is first able to stop with n = 169 trials.\u00a0 Then x \u201d has a proportional likelihood to a result that could have occurred from E\u2019, where n was fixed in advance to be 169, and result x \u2019 is 1.96\u03c3/\u221an from 0.\u00a0 Although the corresponding p-values would be different, the two results would be inferentially equivalent according to the SLP. This application of the SLP to the case of optional stopping is often call this the Stopping Rule Principle SRP (Berger and Wolpert 1988). [i] \n SLP violation with Fixed Normal Testing and Optional Stopping: E\u2019, E\u201d \u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n (E\u2019, 1.96\u03c3/13) and (E\u201d, n = 169) have proportional likelihoods \nInfr E\u2019 (1.96\u03c3 /13) is not equiv to Infr E\u201d ( n = 169). \n -4- \n \n __________________________________________________________________ \n (a) Sufficient Statistic: Let data x = (x 1 ,x 2 ,\u2026,x n ) be a realization of random variable X , following a distribution f, a statistic T( x ) is a sufficient statistic if the following relation holds: \n f( x ; \u03b8) = f T ( t; \u03b8) f x|T ( x | t) \n where f x|T does not depend on the unknown parameter \u03b8. \n (b) Sufficiency Principle (general) : If random sample X , in experiment E, has probability density f( x; \u03b8), and the assumptions of the model are valid, and T is minimal sufficient for \u03b8, then if t( X \u2019) = t( X \u201d), then Infr E\u2019 ( x \u2019) = Infr E\u201d ( x \u201d). \n Since the sufficiency principle holds for different inference schools, any application must take into account the relevant method for inference under discussion (Cox and Mayo 2010). \n (c) Sufficiency Principle applied in sampling theory: If a random variable X , in experiment E, arises from f( x ;\u03b8), and the assumptions of the model are valid, then all the information about \u03b8 contained in the data may be obtained from considering its minimal sufficient statistic t and the sampling distribution f T (t;\u03b8) of experiment E. \n -5- \n \n __________________________________________________________________ \n Weak Conditionality Principle (WCP): If a mixture experiment is performed, with components E\u2019, E\u201d determined by a randomizer (independent of the parameter of interest), then once (E\u2019, x \u2019) is known, inference should be based on E\u2019 and its sampling distribution; not on the sampling distribution of the convex combination of E\u2019 and E\u201d. \n 4.1 Understanding the WCP \n The WCP includes a prescription and a proscription for the proper evidential interpretation of x \u2019, once it is known to have come from E\u2019: \n The evidential meaning of any outcome (E\u2019, x \u2019) of any experiment E having a mixture structure is the same as the evidential meaning of the corresponding outcome x \u2019 of the corresponding component experiment E\u2019, ignoring otherwise the over-all structure of the original experiment.\u201d (Birnbaum 1962, 279) \n -6- \n \n __________________________________________________________________ \n While the WCP seems obvious enough, it is actually rife with equivocal potential. To avoid this, we belabor here its three assertions. \n \n First , it applies once we know which component of the mixture has been observed, and what the outcome was (E j ,\u00a0 x j ). (Birnbaum considers mixtures with just two components). \n \n \n Second , there is the prescription about evidential equivalence. Once it is known E j has generated the data , given that our inference is about a parameter of E j , inferences are appropriately drawn in terms of the sampling distribution in E j \u2013the experiment known to have been performed. \n \n \n Third, there is the proscription: In the case of informative inferences about parameter of E j our inference should not be influenced by whether the decision to perform E j was determined by a coin flip or fixed all along. Misleading informative inferences result from averaging over the convex combination of E j and an experiment known not to have given rise to the data.\u00a0 The latter may be called the unconditional sampling distribution. \n \n -7- \n __________________________________________________________________ \n A second ambiguity. Casella and Berger (2002) write: \n The [weak] Conditionality principle simply says that if one of two experiments is randomly chosen and the chosen experiment is done, yielding data x , the information about q depends only on the experiment performed\u2026. The fact that this experiment was performed, rather than some other, has not increased, decreased, or changed knowledge of q . (emphasis added, 293) \n Casella and Berger\u2019s intended meaning is the correct claim: \n (i) Given it is known that measurement x \u2019 is observed as a result of using tool E\u2019, then it does not matter (and it need not be reported) whether or not E\u2019 was chosen by a random toss (that might have resulted in using tool E\u201d) or fixed all along. \n Compare this to a false and unintended reading: \n (ii) If some measurement x is observed, then it does not matter (and it need not be reported) if it came from a precise tool E\u2019 or imprecise tool E\u201d. \n Claim (i) by contrast, may be well be warranted, not on purely mathematical grounds, but as the most appropriate way to report the precision of the result attained, as when WCP applies. \n The linguistic similarity of (i) and (ii) may explain the equivocation that vitiates the Birnbaum argument. \n -8- \n __________________________________________________________________\u00a0 \n 4.3 Is WCP an Equivalence? (you may wish to compare this to my earlier treatment) \n \n A central question is whether WCP is a proper equivalence, holding in both directions (Evans, et.al..1986, Durbin 1970).\u00a0 Weighing against viewing it as an equivalence is this: it makes no sense to say one should use the unconditional rather than the conditional assessment (once it is known which component of a mixture was performed), and at the same time maintain the unconditional and conditional assessments are evidentially equivalent.\u00a0 WCP prescribes conditioning on the experiment known to have produced the data, and not the other way around . It is only because these do not yield equivalent appraisals that the WCP may serve to avoid counterintuitive assessments (e.g., that would otherwise be permitted from those famous weighing machines).\u00a0 It is their inequivalence, in short, that gives Cox\u2019s WCP its normative proscriptive force: \n WCP proscription: Once (E\u2019, x \u2019) is known, Infr E\u2019 ( x \u2019) should be computed using, not the unconditional sampling distribution over E\u2019 and E\u201d, but rather, the sampling distribution of E\u2019. \n -9- \n \n __________________________________________________________________ \n Yet there is an equivalence within the WCP , and so long as it is consistently interpreted, raises no problems. [ii] This turns out to be the linchpin of disentangling the Birnbaum argument. \n To hold WCP for a given context is to judge that the information that E\u2019 was determined by a flip is a redundancy, equivalent to conjoining a tautology to the outcome (E\u2019, x \u2019): \n \n Knowing that (E\u2019, x \u2019) occurred, \n Infr E\u2019 ( x \u2019) equiv [Infr E\u2019 ( x \u2019) and (Either E\u2019 was chosen by flipping, or E\u2019 was fixed) \n \n where it given that the flipping conjunct in no way alters the construal of (E\u2019, x \u2019).\u00a0 [iii] \n Viewing the WCP as endorsing a genuine \u201ctwo-way\u201d equivalence requires viewing any known experimental result as equivalent, evidentially, to its being a component of a corresponding mixture, even though it is known that in fact E was not chosen by a mixture. While this may seem unsettling, no untoward evidential interpretations result so long as the proscriptive part of the WCP remains, and is not contradicted (say by allowing the imaginary mixture to influence the interpretation of the known \u201ccomponent\u201d). \n -10- \n __________________________________________________________________ \n 5. Birnbaum\u2019s Argument \n SLP: for any two experiments, E\u2019 and E\u201d, if x \u2019* and x \u201d* are SLP pairs (from E\u2019 and E\u201d respectively) then Infr E\u2019 ( x \u2019*) equiv Infr E\u201d ( x \u201d*). \n Begin with any case where the antecedent of the SLP holds. The task is to show the two ought to be deemed evidentially equivalent. \n Premise 1: \n \n Suppose we have observed (E\u2019, x \u2019*) with an SLP pair (E\u201d, x \u201d*).\u00a0 Then view (E\u2019, x \u2019*) as having resulted from getting heads on the toss of a fair coin, where tails would have meant performing E\u201d (any other irrelevant randomizer would do).\u00a0 This is sometimes called the \u201cenlarged experiment\u201d. Now construct the Birnbaum test statistic T-B defined in terms of the enlarged experiment: \n T-B(E j , x j *) = (E\u2019, x \u2019*), if x \u2019= x \u2019* or\u00a0 j = 2 and x \u201d = x \u201d*. \n Else, report the outcome (E j , x j ). \n In words: in the case of a member of an SLP pair, statistic T-B has the effect of erasing the index j. Inference based on T-B is to be computed averaging over the performed and unperformed experiments E\u2019 and E\u201d. This is the unconditional formulation of the enlarged experiment. This gives premise one: \n -11- \n __________________________________________________________________ \n (1) For any (E\u2019, x \u2019*), the result of construing its evidential import in terms of the unconditional formation is that: \n Infr E-B ( x \u2019*) equiv Infr E-B ( x \u201d*) \n The likelihood functions of (E\u2019, x \u2019*) and (E\u201d, x \u201d*) are proportional for all q, being .5f( x \u2019*;q) and .5f( x \u201d*; q). \n However E\u2019 and E\u201d are different models of the experiment producing the two likelihoods, and the enlarged model associated with T-B is yet a third model of the experiment. The second premise now concerns the WCP: \n (2) Once it is known that E\u2019 produced the outcome x \u2019*, compute the inference just as if it were known all along that E \u2019 was going to be performed, i.e., one should use the conditional formulation, ignoring any mixture structure: \n Infr E-B ( x \u2019*) equiv Infr E\u2019 ( x \u2019*) \n More generally, once ( x j *)\u00a0 is known to have come from E j , j = 1 or 2, premise (2) is \n Infr E-B ( x j *) equiv Infr E\u2019 ( x j *) \n From premises (1) and (2) it is concluded, for any arbitrary SLP pair x \u2019*, x \u201d*, \n Infr E\u2019 ( x \u2019*) equiv Infr E\u201d ( x \u201d*) \n -12- \n \n __________________________________________________________________ \n The SLP is said to follow. This is an unsound argument. \n Consider the truth of the two premises of Birnbaum\u2019s argument. Premise one: (Infr E-B ( x \u2019*) equiv Infr E-B ( x \u201d*) is true provided that \n Infr E-B ( x \u2019*) is the inference from (E\u2019, x \u2019) averaging over the unconditional sampling distribution of statistic T-B. In effect it reports just the likelihood of x *, which enters inference in terms of the convex combination of E\u2019 and E\u201d. \n For premise two to be true \n (i.e.,\u00a0 Infr E-B ( x j *) equiv Infr E\u2019 ( x j *) for j= 1, 2) \n Infr E-B ( x j *) must refer the inference from (E j , x j* ) modeled in terms of the sampling distribution of E j alone. The experiment E-B on which inference is to be based has different meanings in each premise. The argument is invalid. \n -13- \n __________________________________________________________________ \n 5.2 Second formulation: allowing true \u201cif then\u201d premises \n We can formulate the argument so that both premises are true \u201cif then\u201d statements [iv] incorporating the stipulated sampling distributions: \n \n As before, suppose an arbitrary member of an SLP pair (E\u2019, E\u201d) is observed, e.g., \n (E\u2019, x \u2019*) is observed. The question is to its evidential import. \n (1) If Infr E-B ( x \u2019*) is computed unconditionally, averaging over the sampling distributions of T-B, then \n Infr E-B ( x \u2019*) equiv Infr E-B ( x \u201d*) \n (2) If Infr E-B (E j , x j *) is computed conditionally, using the sampling distribution of E j : \n Infr E-B ( x j *) equiv Infr E\u2019 ( x j *) for i= 1, 2. \n Construed as \u201cif then\u201d claims, the premises can both be true, but then we cannot validly infer the SLP: \n Infr E\u2019 ( x \u2019 *) equiv Infr E\u201d ( x \u201d *) \n We would need contradictory antecedents to hold. \n -14- \n __________________________________________________________________ \n The formal invalidity is proved by any SLP violation, since in that case, the premises are true and the conclusion is false. SLP violation pairs are readily available (e.g., Examples 1 and 2), and no contradiction results. In fact, we have demonstrated something stronger: whenever we deal with an SLP violation pair, the two \u201cif then\u201d premises, when true yield a false conclusion. \n REFERENCES: See Paper \n \n \n [i] Applying the stopping rule principle requires stipulating that the stopping rule was uninformative for the inference, as in the above example. \n [ii] Birnbaum himself is conflicted here. In his later, 1969 paper, Note 11, Birnbaum asserts, \u201cThe formulation of the conditionality concept as one of equivalence\u201d, as in [WCP] was proposed by him in (1962) as the natural explication of the concept, not withstanding the one-sided form to which applications of the concept had been restricted (substitution of simpler for less simple models of evidence). This proposal seems to have found general acceptance among those interested in the concept. \n \n \n [iii] \u00a0For that matter, as Birnbaum suggests (1969, 119), a \u201ctrivial but harmless\u201d augmentation to any experiment might be to toss a fair coin and report heads or tails (where this was irrelevant to the original model). Given (E\u2019, x \u2019), \n Infr E\u2019 ( x \u2019) equiv [Infr E\u2019 ( x \u2019) and either a coin was tossed or it was not]. \n He intends the move in applying the WCP is to be just as innocuous as the report of an irrelevant coin toss. \n \n \n [iv] \u00a0 I am deliberately avoiding the term \u201cconditional\u201d statement, since it is used with a very different sense throughout. \n #: This will give graduate students at my 28 Nov., 2012 presentation of this paper, as part of the (PH500) seminar, London School of Economics, a chance to submit something. Inquiries: error@vt.edu \n For some older examples of U-Phils, see an earlier post , and search this blog. \n -15- \n \n Filed under: Likelihood Principle , Statistics , U-Phil \n \n \n Please comment on the article here:  Error Statistics Philosophy \u00bb Statistics \n \n The post U-Phil: Blogging the Likelihood Principle: New Summary appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/_UuccjN0qQg/", "bloglinks": {}, "links": {"http://errorstatistics.com/": 16, "http://www.vt.edu/": 3, "http://errorstatistics.wordpress.com/": 2, "http://www.rmm-journal.de/": 1, "http://www.statsblogs.com/": 1, "http://www.statsblogs.com": 2, "http://errorstatistics.com": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Gianluca Baio's blog , and syndicated at StatsBlogs .)\n \n Francesco and Andrea have asked me to join them in doing a short course before the conference of the Italian Health Economics Association (AIES). The course is about Bayesian statistics and health economics and will be in Rome on November 14th.  I think we have quite a lot to cover and whether we'll actually manage to do everything depends on how people are familiar with Bayesian statistics. But potentially we can talk about quite a few interesting things, including how to do Bayesian statistics in health economics. I think I'll show at least some of the problems from the book, but there's no lab, which is a shame.  On second thoughts, however, if they had their computer to do the exercises, then we'd definitely not going to make it on time, so probably it's just as well...   \n \n \n Please comment on the article here:  Gianluca Baio's blog \n \n The post (B)AIES appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/REnL-A68ayA/", "bloglinks": {}, "links": {"http://www.statsblogs.com": 2, "http://gianlubaio.blogspot.com/": 2, "http://www.facebook.com/": 1, "http://www.statsblogs.com/": 1, "http://www.uniroma2.it/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Gianluca Baio's blog , and syndicated at StatsBlogs .)\n \n  One of my favourite Friends episodes is when Joey finally has a breakthrough and gets his first starring role in the show \" Mac & Cheese \" (in fact the robot was called C.H.E.E.S.E. $-$ \"Computerised Humanoid Electronically Enhanced Secret Enforcer\").  To me, and I know this is veeeeery mature of me, this has the same comic effect of when I first read a paper by David Cox and Deborah Mayo . These two are of course serious people, doing amazing work in their respective fields (of course, statistics for Sir David; and philosophy of science for Deborah Mayo). But, then again, as Sheldon Cooper, PhD would put it : \"what's life without whimsy?\"  Anyway, I've had a bit of Cox & Mayo this week; first, I've seen this post on Christian Robert's blog, in which he discusses some of Mayo's position on Bayesian statistics. Mayo works in the field of philosophy of science and is a proposer of the frequentist approach. In fact, as Christian also mentions, her position is quite critical of the Bayesian approach and I too have noticed (although I have to say I have not read as much of her work) that she has a sort of \"military resistance\" attitude to it.  Perhaps in philosophy of science there is a presumption that only the Bayesian approach has philosophically sound foundations; I can see that we Bayesians may sometimes see ourselves as the \"righteous ones\" (mainly because we are $-$ only kidding, of course), although I think this probably was a real issue quite a while back; certainly not any more, at least from where I stand.  If this is indeed the case, maybe her position is justifiable and she serves the purpose of keeping a balanced perspective in the field. In my very limited experience and interaction with that environment, I've been lucky enough to talk quite a few times with people like Hasok Chang (who at some point was my joint boss) and Donald Gillies . The impression I had was that there was no perception that from the philosophical point of view, \" most of Statistics is under threat of being overcome (or \u201cinundated\u201d) by the Bayesian perspective\" (as Christian put it). I really liked Christian's post and essentially agreed on all accounts.  The second part of this story is about this morning's lecture by David Cox (which was organised by Bianca and Rhian ). He talked about the principles of statistics, in what he said was a condensed version of a 5-hours lecture. Of course, it was interesting. He's a very good speaker and it's amazing to see how energetic he still is (he's 88 $-$ I was about to complain that my back is hurting today, but that put me in perspective!).  There were a few points I quite liked and a few more I quite didn't. First, I liked that he slipped in an example in which he implicitly said he's an Aston Villa fan; now I feel justified about putting a bit about Sampdoria in chapter 2 of the book $-$ I can always said I did it like Cox, which to a statistician is quite something...  Also, I liked the distinction he made between what he called \"phenomenological\" and \"substantive\" models. The first term indicates all those models that are general enough to be widely applicable (including, as he put it \"those strange models that people use to analyse survival data\"), but not directly related to the science underlying the problem at hand. Something like: you can \"always\" use regression analysis, and effectively the same formulation can apply to medicine, agriculture or social science. The maths behind the model is the same. The second term indicates models that are specifically built to describe a specific bit of science; they are thus very much specific , although of course you may not be able to apply them in many cases. A bit like decision models (as opposed to individual data models) in health economics. What I didn't quite like (but that was to be expected) was his take on Bayesian statistics, and specifically on the subjective/personalistic approach. I think I've heard him talk about it on another occasion, and that time it was even worse (from my Bayesian perspective), but the point is that he basically said that there's no room for such an approach in science, with which I respectfully (but totally) disagree. In fact, as Tom pointed out while we were walking back, at some point he was even a bit contradictory when he framed the significance of RCTs data in terms of the problem a doctor faces when deciding what's the best course of action for the next patient $-$ pretty much a personalistic decision problem! \n \n \n Please comment on the article here:  Gianluca Baio's blog \n \n The post Cox & Mayo appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/5xAVB3HccV8/", "bloglinks": {}, "links": {"http://xianblog.wordpress.com/": 1, "http://www.vt.edu/": 1, "http://www.tumblr.com/": 1, "http://www.ac.uk/": 5, "http://www.statsblogs.com/": 1, "http://www.statsblogs.com": 2, "http://www.imdb.com/": 1, "http://en.wikipedia.org/": 1, "http://gianlubaio.blogspot.com/": 2, "http://2.blogspot.com/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Statistical Modeling, Causal Inference, and Social Science , and syndicated at StatsBlogs .)\n \n I\u2019m sorry I don\u2019t have any new zombie papers in time for Halloween. Instead I\u2019d like to be a little monster by reproducing a mini-rant from this article on experimental reasoning in social science: \n I will restrict my discussion to social science examples. Social scientists are often tempted to illustrate their ideas with examples from medical research. When it comes to medicine, though, we are, with rare exceptions, at best ignorant laypersons (in my case, not even reaching that level), and it is my impression that by reaching for medical analogies we are implicitly trying to borrow some of the scientific and cultural authority of that field for our own purposes. Evidence-based medicine is the subject of a large literature of its own (see, for example, Lau, Ioannidis, and Schmid, 1998). \n\n \n \n Please comment on the article here:  Statistical Modeling, Causal Inference, and Social Science \n \n The post Social scientists who use medical analogies to explain causal inference are, I think, implicitly trying to borrow some of the scientific and cultural authority of that field for our own purposes appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/RgVfqsyic6s/", "bloglinks": {}, "links": {"http://www.statsblogs.com": 2, "http://www.columbia.edu/": 1, "http://www.statsblogs.com/": 1, "http://andrewgelman.com": 1, "http://andrewgelman.com/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Realizations in Biostatistics , and syndicated at StatsBlogs .)\n \n The fine folks over at Simply Statistics have a very good educational article about the difference between the probability of winning an election and vote share. This article stems from a controversial column over at Politico criticizing Nate Silver and his election forecasts. Twitter responses are even worse. Conservative filmmaker John Ziegler calls Nate Silver a \u201chyper-partisan fraud\u201d who is \u201cnot an expert on polls.\u201d   Glenn Thrush mentions a \u201cconservative 538:\u201d   And it\u2019s not hard to find other examples. I\u2019ve run into this reaction a bit, especially when it comes to politics. There are a large group of people, who will dismiss any evidence going against their beliefs. I guess the punditry wasn\u2019t so dismissive of Silver in 2010. At any rate, I give a recommendation I rarely give: read this Politico article and the comments (ignore the \u201cconservatives aren\u2019t bright\u201d nonsense, which is the same stuff coming from the left). And let\u2019s thank Nate Silver, RealClearPolitics, and all the honest pollsters who try to shine some data on this election.  \n      \n \n \n \n Please comment on the article here:  Realizations in Biostatistics \n \n The post Willful statistical illiteracy appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/GVqbvyqMyTU/", "bloglinks": {}, "links": {"http://simplystatistics.org/": 2, "http://www.politico.com/blog": 2, "http://realizationsinbiostatistics.blogspot.com/": 1, "http://www.statsblogs.com/": 1, "http://www.statsblogs.com": 2, "http://lh6.ggpht.com/": 2, "http://feedproxy.google.com/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at The DO Loop , and syndicated at StatsBlogs .)\n \n \nThe determinant of a matrix arises in many statistical computations, such as in estimating parameters that fit a distribution to multivariate data. For example, if you are using a log-likelihood function to fit a multivariate normal distribution, the formula for the log-likelihood involves the expression log(det(\u03a3)), where \u03a3 is the covariance matrix for the population. Similar formulas appear for log-likelihood estimation of spatial models.\n \nThe determinant of a matrix is a high-degree polynomial, so it can be huge for even moderate-sized matrices. For example, the determinant of a Vandermonde matrix with consecutive integer elements increases super-factorially with the dimension of the matrix! For a 30 x 30 integer Vandermonde matrix, the determinant is too large to represent as a standard double-precision floating-point number.\n \nBut here's an important point: in many statistical applications, you don't care about the determinant . All you need is the logarithm of the determinant. That is a much, much, smaller number!\n \nHere's a trick that you can use. To computing the logarithm of a determinant, do NOT try to compute the determinant itself. Instead, compute the log-determinant directly! For matrices with a large determinant, the computation of the log-determinant will usually succeed whereas the computation of the determinant might cause a numerical overflow error.\n \nHere's how you can compute the log-determinant in the important case of a positive definite matrix, such as for a covariance matrix.\n \nLet A be your matrix and let G = root(A) be the Cholesky root of the matrix A. Then the following equation is true: \n  log(det(A)) = 2*sum(log(vecdiag(G))) \n \nHere's a proof:\n \n A = G`*G, by definition of the Cholesky root\n log(det(A)) = log(det(G`*G)) = log(det(G`)*det(G)) = 2*log(det(G))\n Since G is triangular, det(G) = prod(vecdiag(G))\n Therefore log(det(G))=sum(log(vecdiag(G)))\n Consequently, log(det(A)) = 2*sum(log(vecdiag(G)))\n \n \nHere's how you can compute the log-determinant in the SAS/IML language:\n \n\n\n proc iml;\nstart logdet(A);\n G = root(A);\n return( 2*sum(log(vecdiag(G))) );\nfinish; \n\n\n\n\n Let's use this formula on a 50x50 symmetric positive definite matrix. The following statements create a symmetric matrix with random uniform variates on the off-diagonal and the value 50 along the diagonal. This matrix is diagonally dominant , and therefore positive definite:\n \n\n\n n = 50; \nr = j(n*(n+1)/2, 1);  /* allocate array for symmetric elements */\ncall randgen(r, \"uniform\"); /* fill r with U(0,1) */\nA = sqrvech(r);    /* A is symmetric */\ndiagIdx = do(1,n*n, n+1);\nA[diagIdx] = n;    /* set diagonal elements */ \n\n\n\n\n \nThe determinant of A is about 7.75 x 10 84 .\nTo test the log-determinant function, let's compute the log-determinant of A in three different ways: by calling the DET function, by computing eigenvalues, and by calling the logDet function:\n \n\n\n LogDet1 = log(det(A));   /* 1. DET function */\nLogDet2 = log(prod(eigval(A))); /* 2. product of eigenvalues */\nLogDet3 = logDet(A);   /* 3. direct computation of log(det(A)) */\n \nprint LogDet1 LogDet2 LogDet3; \n\n\n\n\n \n\n \nThe first two methods compute the determinant, and are therefore subject to overflow when the determinant is extremely large. The logDet function, however, does not ever compute the determinant. It computes the log-determinant directly.\n \nSweet! This TRICK is a real TREAT! tags: Matrix Computations , Numerical Analysis , Statistical Programming , Tips and Techniques \n   \n \n \n \n Please comment on the article here:  The DO Loop \n \n The post Compute the log-determinant of a matrix appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/i41duNikCW8/", "bloglinks": {}, "links": {"http://mathworld.wolfram.com/": 2, "http://www.statsblogs.com/": 1, "http://en.wikipedia.org/": 1, "http://www.statsblogs.com": 2, "http://feedproxy.google.com/": 1, "http://blogs.sas.com/": 5, "http://www.berkeley.edu/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Research tips , and syndicated at StatsBlogs .)\n \n Make is a marvellous tool used by programmers to build software, but it can be used for much more than that. I use make whenever I have a large project involving R files and LaTeX files, which means I use it for almost all of the papers I write, and almost of the consulting reports I produce. \n If you are using a Mac or Linux, you will already have make installed. If you are using Windows and have Rtools installed, then you will also have make . Otherwise, Windows users will need to install it. One implementation is in GnuWin . \n A typical project of mine will include several R files containing code that fit some models, and generate tables and graphs. I try to set things up so I can re-create all the results by simply running the R files. Then I will have a LaTeX file which contains the paper or report I am writing. The tables and graphs produced by R are pulled in to the LaTeX file. Consequently, all I need to do is run all the R files, and then process the tex file, and the paper/report is generated. \n Make relies on a Makefile to determine what it must do. Essentially, a Makefile specifies what files must be generated first, and how to generate them. So I need a Makefile that specifies that all the R files must be processed first, and then the LaTeX file. \n The beauty of a Makefile is that it will only process the files that have been updated. It is smart enough not to re-run code if it has already been run. So if nothing has changed, running make does nothing. If only the tex file changes, running make will re-compile the tex document. If the R code has changed, running make will re-run the R code to generate the new tables and graphs, and then re-compile the tex document. All I do is type make and it figures out what is required. \n A Makefile for LaTeX \n It is easy to tell if the latex document needs compiling \u2014 make simply has to check that the pdf version of the document is older than the tex version of the document. Here is a simple Makefile that will just handle a LaTeX document. \n\n  TEXFILE = paper\n$ ( TEXFILE ) .pdf: $ ( TEXFILE ) .tex\n\trubber --pdf $ ( TEXFILE )  \n\n The first line specifies the name of my file, in this case paper.tex . The second line specifies that the pdf file must be created from the tex file, and the last line explains how to do that. If you don\u2019t have rubber (which is a unix program for compiling a LaTeX document), you could use pdftexify (which is part of MikTeX for Windows). \n To use the above Makefile , copy the code into a plain text file called Makefile and store it in the same directory as your tex file. Change the first line so the name of your tex file (without the extension) is used. Then type make from a command prompt within the same directory as the tex file, and it should do whatever is necessary to convert your tex to pdf. \n Of course, you wouldn\u2019t normally bother with a Makefile if that is all it did. But throw in a whole lot of R files, and it becomes very worthwhile. \n A Makefile for R and LaTeX \n We need a way to allow make to be able to tell if an R file has been run. We could specify all the outputs of the R file, but that is messy. Instead, we will create empty files of the form file.Rdone whenever file.R is run. That way, make only has to check if file.Rdone is older than file.R . \n I also like to strip out all the white space from the pdf figures created in R before I put them in a LaTeX document. There is a nice command pdfcrop which does that. (You should already have it on a Mac or Linux, and also on Windows provided you are using MikTeX.) So I also want my Makefile to crop all images if they have not already been done. Once an image is cropped, an empty file of the form file.pdfcrop is created to indicate that file.pdf has already been cropped. \n OK, now we are ready for my marvellous Makefile . \n\n  # Usually, only these lines need changing \n TEXFILE = paper\n RDIR = . / figs\n FIGDIR = . / figs\n \n # list R files \nRFILES := $ ( wildcard $ ( RDIR ) /* .R ) \n # pdf figures created by R \nPDFFIGS := $ ( wildcard $ ( FIGDIR ) /* .pdf ) \n # Indicator files to show R file has run \nOUT_FILES:= $ ( RFILES:.R=.Rdone ) \n # Indicator files to show pdfcrop has run \nCROP_FILES:= $ ( PDFFIGS:.pdf=.pdfcrop ) \n \nall: $ ( TEXFILE ) .pdf $ ( OUT_FILES ) $ ( CROP_FILES ) \n \n # May need to add something here if some R files depend on others. \n \n # RUN EVERY R FILE \n$ ( RDIR ) /% .Rdone: $ ( RDIR ) /% .R $ ( RDIR ) / functions.R\n\tRscript $ < && touch $ @ \n \n # CROP EVERY PDF FIG FILE \n$ ( FIGDIR ) /% .pdfcrop: $ ( FIGDIR ) /% .pdf\n\tpdfcrop $ < $ < && touch $ @ \n \n # Compile main tex file and show errors \n$ ( TEXFILE ) .pdf: $ ( TEXFILE ) .tex $ ( OUT_FILES ) $ ( CROP_FILES ) \n\trubber --pdf $ ( TEXFILE ) \n\trubber-info $ ( TEXFILE ) \n \n # Run R files \nR: $ ( OUT_FILES ) \n \n # View main tex file \nview: $ ( TEXFILE ) .pdf\n\tevince $ ( TEXFILE ) .pdf & \n \n # Clean up stray files \nclean:\n\t rm -fv $ ( OUT_FILES ) \n\t rm -fv $ ( CROP_FILES ) \n\t rm -fv * .aux * .log * .toc * .blg * .bbl * .synctex.gz\n\t rm -fv * .out * .bcf * blx.bib * .run.xml\n\trubber --clean $ ( TEXFILE ) \n\t rm -fv $ ( TEXFILE ) .pdf\n \n.PHONY: all clean  \n\n Download the file here. For most projects I copy this file into the main directory of my project, then all I have to do is modify the first few lines. RDIR specifies where the R files are kept and FIGDIR specifies where the figures are kept. Normally I keep these together, but sometimes they might be in separate directories. \n Now make will do everything necessary \u2014 run the R files, crop the pdf graphics, and process the latex document. But it won\u2019t do any steps that don\u2019t need doing. \n make R will only process the R files. \n make view will run the pdf viewer, after updating the pdf file if necessary. \n make clean will delete all the files generated by latex or by make, so that the entire process must be run again at the next make command. \n Notice that my R files all depend on functions.R . This is a file that contains project-specific functions. If this file is updated, all the other R files will need updating also. \n For many projects, some R files will depend on some others having already run. For example, read.R may read in the data and reformat it for analysis, while plot.R might produce some graphs assuming that read.R has already run. To ensure make knows about this dependency, we need to add a line \n\n  $ ( RDIR ) / plot.Rdone: $ ( RDIR ) / plot.R $ ( RDIR ) / functions.R $ ( RDIR ) / read.R\n\tRscript $ < ; && touch $ @  \n\n This should be inserted where I have the comment # May need to add something here if some R files depend on others. \n This Makefile works on Linux. Mac and Windows users will need to replace evince by whatever pdf viewer they prefer. \n\n  \n  \n \n \n Please comment on the article here:  Research tips \n \n The post Makefiles for R/LaTeX projects appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/HQ55mIuDEAU/", "bloglinks": {}, "links": {"http://www.statsblogs.com": 2, "http://feedads.doubleclick.net/": 2, "http://www.gnu.org/": 1, "http://www.statsblogs.com/": 1, "http://robjhyndman.com/": 3, "http://feedproxy.google.com/": 1, "http://launchpad.net/": 1, "http://gnuwin32.sourceforge.net/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}, {"content": ["(This article was originally published at Normal Deviate , and syndicated at StatsBlogs .)\n \n The Future of Machine Learning (and the End of the World?) \n \nOn Thursday (Oct 25) we had an event called the ML Futuristic Panel Discussion . The panelists were Ziv Bar-Joseph , Steve Fienberg , Tom Mitchell Aarti Singh and Alex Smola . \n \nZiv is an expert on machine learning and systems biology. Steve is a colleague of mine in Statistics with a joint appointment in ML, Tom is the head and founder of the ML department, Aarti is an Assistant Professor in ML and Alex, who is well known as a pioneer in kernel methods, is joining us as a professor in ML in January. An august panel to say the least. \n \nThe challenge was to predict what the next important breakthroughs in ML would be. It was also a discussion of where the panelists thought ML should be going in the future. Based on my notoriously unreliable memory, here is my summary of the key points. \n \n 1. What The Panelists Said \n \nAarti: ML is good at important but mundane tasks (classification etc) but not at higher level tasks like thinking of new hypotheses. We need ML techniques that play a bigger role in the whole process of making scientific discoveries. The more machines can do, the more high level tasks humans can concentrate their efforts on. \n \nZiv: There is a gap between the advances in systems biology and its use on practical problems, especially medicine. Each person is a repository of an unimaginable amount of data. An unsolved problem in ML is how to use all the knowledge we have developed in systems biology and use it for personalized medicine. In a sense, this is the problem of bridging information at the cell level and information at the level of an individual (consisting of trillions of interacting cells). \n \nSteve: We should not forget the crucial role of intervention. Experiments involve manipulating variables. Passive ML methods are only part of the whole story. Statistics and ML methods help us learn, but then we have to decide what experiments to do, what interventions to make. Also, we have to decide what data to collect; not all data are useful. In other words, the future of ML has to still include human judgement. \n \nTom: He joked that his mother was not impressed with ML. After all, she saw Tom grow from an infant who knew nothing, to and adult who can do an amazing number of things. Tom says we need to learn how to \u201craise computers\u201d in analogy to raising children. We need machines that can learn how to learn. An example is the NELL project (Never Ending Language Learning) which Tom leads. This is a system which has been running since January 2010 and is learning how to read information from the web. See also here . Amazing stuff. \n \nAlex: More and more, computing is done on huge numbers of highly connected inexpensive processors. This raises many questions about how to design algorithms. There are interesting challenges for systems designers, ML people ad statisticians. For example: can you design an estimator that can easily be distributed with little loss of statistical efficiency and that is highly tolerant to failures of small numbers of processors? \n \n 2. The Future? \n \nI found the panel discussion very inspiring. All of the panelists had interesting things to say. There was much discussion after the presentations. Martin Azizyan asked (and I am paraphrasing), \u201cHave we really solved all the current ML problems?\u201d The panel agreed that, no, we have not. We need to keep working on current problems (even if they seem mundane compared to the futuristic things discussed by the panel). But we can also work on the next generation of problems at the same time. \n \nDiscussing future trends is important. But we have to remember that we are probably wrong about our predictions. Neils Bohr said \u201cPrediction is very difficult, especially about the future.\u201d And as Yogi Berra said, \u201cThe future ain\u2019t what it used to be. \u201d \n \nWhen I was a kid, it was routinely predicted that, by the year 2000, people would fly to work with jetpacks, we\u2019d have flying cars and we\u2019d harvest our food from the sea. No one really predicted the world wide web, laptops, cellphones, gene microarrays etc. \n \n 3. The Return of AI \n \nBut, I\u2019ll take my chances and make a prediction anyway. I think Tom is right: computers that learn in ways closer to the ways humans learn is the future. \n \nWhen I was in London in June, I had the pleasure to meet Shane Legg, from Deepmind Technologies . This is a startup that is trying to build a system that thinks. This was the original dream of AI. \n \nAs Shane explained to me, the has been huge progress in both neuroscience and ML and their goal is to bring these things together. I thought it sounded crazy until he told me the list of famous billionaires who have invested in the company. \n \nWhich raises an interested question. Suppose someone \u2014 Tom Mitchell, the people at Deepmind, or someone else \u2014 creates a truly intelligent system. Now they have a system as smart as a human. But all they have to do is put the system on a huge machine with more horsepower than a human brain. Suddenly, we are in the world of super-intelligent computers surpassing humans. \n \nPerhaps they\u2019ll be nice to us. Or, it could turn into Robopocalypse . If so, this could mean the end of the world as we know it. \n \nBy the way, Daniel Wilson, the author of Robopocalypse, was a student at CMU. I heard rumours that he kept a picture of me on his desk to intimidate himself to work hard. I don\u2019t think of myself as intimidating so maybe this isn\u2019t true. However, the book begins with a character named Professor Wasserman, a statistics professor, who unwittingly unleashes an intelligent program that leads to the Robopocalypse. \n \nSteve Speilberg is making a movie based on the book, to be released April 25 2104. So far, I have not had any calls from Speilberg. \n \nSo my prediction is this: someone other than me will be playing Professor Wasserman in the film adaptation of Robopocalypse. \n \nWhat are your predictions for the future of ML and Statistics? \n \n  \n \n \n Please comment on the article here:  Normal Deviate \n \n The post The Future of Machine Learning (and the End of the World?) appeared first on Statistics Blogs @ StatsBlogs.com ."], "link": "http://feedproxy.google.com/~r/statsblogs/~3/IU5yS2pPabo/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://normaldeviate.wordpress.com": 1, "http://www.statsblogs.com/": 1, "http://www.cmu.edu/": 5, "http://www.statsblogs.com": 2, "http://en.wikipedia.org/": 2, "http://deepmind.com/": 1, "http://rtw.cmu.edu/": 1, "http://normaldeviate.wordpress.com/": 1, "http://alex.smola.org/": 1}, "blogtitle": "Statistics Blogs @ StatsBlogs.com"}]