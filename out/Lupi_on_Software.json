[{"blogurl": "http://blog.lupi-software.com\n", "blogroll": [], "title": "Lupi on Software"}, {"content": ["Google used 16.000 cores and many engineering hours to power a deep learning algorithm that recognizes cats. \n Months later\u2026 \n  \n Here comes Heather Arthur \u00a0and makes a kittydar that works in Javascript. Check it out ! \n PS: Yes, Google Artificial Brain and kittydar are not comparable in any way, but who cares: we both know you\u2019re here for cute kittens . Right? \n Filed under: Everything else Tagged: Cat , computer vision , javascript"], "link": "http://blog.lupi-software.com/2012/09/29/kittydar/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 4, "http://feeds.wordpress.com/": 1, "http://harthur.github.com/": 2, "http://www.wired.com/": 1, "http://www.youtube.com/": 1, "https://github.com/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["I haven\u2019t written about it here yet. I changed job. \n Its the end of my first week at Google and the only thing I have to prove it is this lousy hat\u2026 \n  \n I am kidding, it\u2019s been awesome ! \n Filed under: Everything else Tagged: Google+"], "link": "http://blog.lupi-software.com/2012/08/10/i-am-a-noogler/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 2, "http://feeds.wordpress.com/": 1, "http://allupo.wordpress.com/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["Nice video about how Facebook works: \n \n Filed under: Developer Operations Tagged: devops , Facebook"], "link": "http://blog.lupi-software.com/2012/07/21/operations-at-facebook/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 3, "http://feeds.wordpress.com/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["Bruce Schneier ,\u00a0a well known expert on computer security, made an interesting note on a news about a new remote sensing technology: \n We\u2019re at a unique time in the history of surveillance: the cameras are everywhere, and we can still see them. Fifteen years ago, they weren\u2019t everywhere. Fifteen years from now, they\u2019ll be so small we won\u2019t be able to see them . Similarly, all the debates we\u2019ve had about national ID cards will become moot as soon as these surveillance technologies are able to recognize us without us even knowing it. \n David Brin , acclaimed sci-fi author and futurist, picks up with a theme long dear to him, sousveillance and the participatory panoptikon: \n If government will know everything about us, our sole option is to know everything about them. \n I believe we are at a similar turning point but on a much wider spectrum. Social media and individual profiling as a whole is at the same point and moving much faster: a few years ago, it was nowhere as extensive as it is now. A few years in the future, it will be so central to the products that we use that we won\u2019t notice it anymore and we won\u2019t be willing to let it go. \n Right now, for a brief time the issue is under the attention not just of the\u00a0 specialists but also of\u00a0 the general public . \n It\u2019s time to build technologies of freedom faster than they can build technologies of control. \n Filed under: Everything else Tagged: Sousveillance , Surveillance , Transparent Society"], "link": "http://blog.lupi-software.com/2012/07/19/a-turning-point-in-surveillance-and-sousveillance-lets-build-technologies-of-freedom-9/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 4, "http://feeds.wordpress.com/": 1, "http://www.schneier.com/blog": 1, "http://en.wikipedia.org/": 2, "http://firstmonday.org/": 1, "http://www.nytimes.com/": 1, "http://davidbrin.blogspot.ch/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["Apple Wants to Protect Your Identity \u2026 by Cloning You \n According to\u00a0 The Atlantic , Apple last February acquired several patent from Novell. Among them, one is quite interesting. Its aim is to protect privacy by creating a fog of fake identities, complete with real activity on the Internet. \n As the patent itself says, the objective is to \u201cmake any data collection about a principal less valuable and less reliable.\u201d \n It\u2019s quite an interesting acquisition. On one hand, Apple could use it to provide more privacy for its user. Apple\u2019s UDID \u2014 an unique device identifier of all iOS devices up to the current generation \u2014 has been abused by advertisers to track and profile user behavior across multiple applications, often violating user privacy. Apple vowed to solve this issue in the next release of iOS. \n It\u2019s not just about being nice to your users. Mobile is the future of consumer computing. Undermining the effectiveness of independent mobile advertising networks would also boost the struggling iAd, who underperformed since it was introduced last year. Facing an antitrust probe, Apple could circumvent it by requiring that all tracking on its iOS devices be approved by the users: they could get approval as part of the iOS shrink wrap license (they already do for the many tracking activities they already do), while independent networks would have to bug the user explicitly about it. It fits quite nicely with the\u00a0 Do Not Track initiatives of all major browsers (some more reluctantly than others), which is going to be a big theme once the new IE comes out and focus attention on it. \n Google and Facebook, along with a lot of smaller fishes, could be negatively affected if these kind of patents were to be implemented widely (as in all future iOS mobile devices). Their core revenue comes from how well they know their users. \n Filed under: Everything else Tagged: apple , patents"], "link": "http://blog.lupi-software.com/2012/06/24/apple-got-an-interesting-patent-from-novell/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 3, "http://feeds.wordpress.com/": 1, "http://www.theatlantic.com/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["Squirro is growing steady and well. We are looking for a talented Python developer to augment our team . \n I joined Squirro in mid January and it has been an exceptionally good experience so far. The company is great, the vision is spot on, the co-workers (many of them also co-founders of the company) are brilliant and charming, and Zurich is a wonderful city to live. \n \n \n Here\u2019s the job spec: \n We are looking for an experienced Python developer for full-time employment. \n You will be part of the core platform team building\u00a0 Squirro , the personal digital research assistant. Within days you will be taking over the responsibility for sizable portions of this new platform. You\u2019ll be part of a small developer team building a scalable real-time application. And that 100% in Python. \n Together we\u2019ll build an open platform based on REST web services. \n Requirements \n \n Extensive experience with Python \n Familiar with HTTP / REST, especially aspects such as performance and caching \n Familiar with asynchronous processing and queuing, for example using RabbitMQ \n Experience with testing and test-driven development \n Work permit in Switzerland , which is given for citizens of EU and EFTA member states \n \n Existing knowledge in any of the following is not required but helpful: \n \n Experience with search technologies such as Lucene, Solr, ElasticSearch \n Cloud deployments, especially with Amazon Web Services \n Configuration mangement with Puppet \n \n About the company \n Squirro is a Swiss startup. We are a team of passionate Internet geeks and entrepreneurs. We\u00a0 love Python \u00a0and are very open about\u00a0 how we develop . We mainly use\u00a0 Flask \u00a0and our own\u00a0 WsgiService \u00a0at the moment. We work with Git, believe in testing, continuous integration and continuous deployment. Our work is structured with Scrum. \n We are located in Zurich, Switzerland. The city has repeatedly been rated as\u00a0 one of the very best cities to live in . \n In the first phase we built the online note-taking tool Memonic.com available on the web as well as mobile and desktop devices. Now we are expanding the Memonic platform to become a real-time knowledge gathering tool. Call it the next level of note-taking. \n Get in Touch \n \n Patrice Neff \n Email: patrice /at/ memonic.com \n Skype: patriceneff  \n Phone: +41 44 586 98 98 \n \n \n Filed under: Everything else Tagged: job , Python , Switzerland , Zurich"], "link": "http://blog.lupi-software.com/2012/06/15/squirro-is-looking-for-a-talented-senior-python-developer/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 6, "https://en.wikipedia.org/": 1, "http://flask.pocoo.org/": 1, "http://www.flickr.com/": 1, "http://feeds.wordpress.com/": 1, "http://www.admin.ch/": 1, "http://pypi.python.org/": 1, "http://blog.memonic.com/": 2, "https://squirro.com/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["If you read my previous post about Particle Sketches and are literate in statistics, you may have already dismissed my ideas as flawed, convinced that they will shatter against the hard cliffs of classical statistics and its mightiest peak, the central limit theorem . \n It is not necessarily so. Don\u2019t get me wrong, I don\u2019t pretend to be able to circumvent it in any way or outsmart the math giants of the past. The naive way to put together count-min sketches and particle filters is bound to hit this wall, but if we are a little bit smart we can use the central limit theorem itself to guide us around these perilous shores. \n What is the problem? The core concept of count-min sketches is to use a set of hash functions to distribute hits on various counters, relying on the fact that good hash functions will have few collisions. The counter with the lowest number of hits will be an upper-bound approximation of the true frequency for a given item. \n Count-Mean-Min Sketches are a bit smarter and incorporate the number of expected collisions in the estimation itself. \n When we replace counters with statistical models, like particle filters, we are replacing integers with random variables. \n What\u2019s the deal with adding together independent random variables? Under very mild conditions, the result is the normal distribution (if the variance of the independent variables is finite) or one of the other well known and well studied distributions that you can find in literature (if the variance is infinite, such as in power-laws ). \n The key here is the word independent . That\u2019s surely the case if we choose a set of random hash functions in the same way we would do with normal count-min sketches . We\u00a0can be smarter than that. If our items are not opaque labels for the object they stand for, but instead have an internal structure, we can use that internal structure to drive the selection of which\u00a0 particle filter \u00a0to update. \n In simpler terms, we can use the attributes of our items to cluster them into specific, non-random categories. Let these attributes drive our hash functions: if they are effective properties around which we can cluster our items, we\u2019ll see non uniform distributions arise among various buckets. If not, we\u2019ll end up with very similar distributions along a single line. \n  \n Let\u2019s see a concrete example. A tweet is not just the text that the user sees, it includes a lot of metadata: the user location and timezone, the user bio, the number of followers the user has and the number of users he follows, how many times it has been retweeted, the kind of tweet (reply, retweet, plain text, with a link, with hashtags, with user mentions). \n We can maintain separate set of buckets for each of these dimensions, using hash functions that map closely related items in the domain to the same slots in the co-domain. \n If the processes underlying the phenomenon we are tracking do not change, we can expect these distributions not to change. In which case, we can discard these dimensions as not interesting. \n If the processes do evolve, we can use a key strength of particle filters to still keep the computing load manageable: we can dynamically reduce the number of particles used to track these uninteresting dimensions, allocating resources to more interesting ones. Over time, what is interesting and uninteresting may change and we\u2019ll just have to focus (i.e have more particles) where the real action is at the moment. That\u2019s one reason I like particle filters over other statistical methods.\u00a0The other reason is that they make it possible to estimate any kind of phenomenon, you don\u2019t have to commit to a specific statistical model up front. \n Filed under: Programming Tagged: particle sketches"], "link": "http://blog.lupi-software.com/2012/05/22/particle-sketches-2-working-around-the-central-limit-theorem/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 3, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 4, "http://allupo.wordpress.com/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["Come and meet our little beast at CloudForce in London \u00a0today. \n Squirro is\u00a0 the personal digital research app.\u00a0 Broader than feeds and more specific than search, Squirro scans multiple sources from Internet channels and social media, private databases and even internal systems such as Salesforce.com and SAP to find the most relevant information on your topic of interest, then updates it continuously and automatically. The result is a living collection of curated content you can save, synthesize and share with friends and colleagues in your own private workspace. \n Squirro gives you timely, relevant information to navigate fast changing business relationships. \n \n When I joined Nektoon in January, Squirro was still diagrams on a drawing board and some minimal proof of concept code. Now it\u2019s a powerful, responsive web application that runs on your phone, tablet and computer. It supports thousands of users and can track hundreds of thousand sources. \n It\u2019s amazing how far and how fast we progressed. A tribute to the talent of my\u00a0colleagues, I am humbled every day by how good you are, friends, and energized by how it fells to work with you . Great vibes! \n Filed under: Everything else Tagged: CloudForce , Salesforce.com , SAP , squirro"], "link": "http://blog.lupi-software.com/2012/05/22/were-unveiling-squirro-at-cloudforce/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 5, "http://feeds.wordpress.com/": 1, "http://www.squirro.com/": 1, "http://allupo.wordpress.com/": 1, "https://www.salesforce.com/": 1, "http://www.google.com/": 2}, "blogtitle": "Lupi on Software"}, {"content": ["I have been designing an interesting, novel (as far as I know) data structure. Its goal is to estimate a very large number Bayesian models in the smallest possible space and time. Sublinear complexity in space and time as well as online updates area key requirement. \n The key idea is to combine particle filters and sketches in a single powerful tool. That\u2019s why I call them Particle Sketches . \n I devised them as a tool to track the posting behavior of a very large set of Twitter users, in order to decide who to follow using Twitter streaming API and who to follow using the Rest API based on their expected tweeting frequency and distribution across the day and week. The data structure is however totally generic and suitable to all kind of large-scale data estimation problem: I can surely see applications in high-frequency trading and website traffic forecast. \n \n Let\u2019s look at the building blocks first. \n Particle Filters \u00a0\u2014otherwise known as Sequential Monte Carlo method\u2014 are model estimation techniques based on simulations. They are used to estimate Bayesian models in which the latent variables are connected in a Markov chain, typically when the state space of the latent variables is continuous and not sufficiently restricted to make exact interference tractable. \n In practice, they work by maintaining a set of differently-weighted samples \u2014called\u00a0 particles \u2014 that represent the expected distribution of the latent variable. You can then \u201cfilter\u201d, which in this context mean determine the distribution at a specific time, by estimating how the particle swarm should have evolved under the observed variable (e.g. time). \n On update, a new set of weighted particles is generated influenced by new observations. You can check Wikipedia\u2019s page on\u00a0 particle filters \u00a0for details or the multitude of pages about them available on the Internet. \n The key fact here is that particle filters are composed by a set of discrete, importance-weighted particles and their distribution is effectively the sum of these particles. \n We can thus split up a particle filter into multiple smaller filters by splitting up the particles into them. If we pick the particles at random, we should end up with essentially the same distribution among all of them. \n If we add noise to a few of these sub-filters, the cumulative distribution of the sum of all of them will still be fairly close to the original estimated distribution. It will be even more true if we can account for the noise and remove its effect. \n Count-Min (CM) Sketches \u00a0are probabilistic, memory efficient data structures that allow one to estimate frequency-related properties of a data set (estimate frequencies of particular items, find top-K frequent elements, performance range frequency queries \u2014i.e. find the sum of the frequencies of elements within a given range\u2014 such as estimate percentiles). \n They are conceptually quite simple, they maintain a two-dimensional array [ w, d ] of integer counters. A set of\u00a0 d\u00a0 random hash functions with co-domain between 0 and\u00a0 w\u00a0 is used to choose which counters to update whenever we a new sample arrives. \n Given an item, we can estimate its frequency by looking at all buckets in the two-dimensional array and choosing the one with the lowest value. \n CM sketches are extremely memory efficient: for example, 48 KB are sufficient to calculate the top-100 most frequent elements in a 40 MB dataset with 4% error. \n CM sketches tend to perform better with highly-skewed distributions, such as those generated by power laws or Zipf-like distributions. \n In case of low or moderately skewed data, a more careful estimation can be done by accounting for the noise due to hash collisions: for example, by estimating and removing the noise for each hash function (calculated as the average value of all remaining counters) and choosing the median instead of the minimum as the estimated frequency. This variation is called\u00a0 Count-Mean-Min Sketch . \n There are many other interesting properties of CM Sketches, such as various algebraic operations that they support or a way to measure how different two sketches are \u2014 calculating the cosine distance between them. \n For a good explanation of CM Sketches with sample code, check\u00a0 this nice post on highlyscalable . For deeper knowledge, check\u00a0 this site . \n Particle Sketches \u00a0replace the counters with particle filters. On each new observation, a set of filters is selected using hash functions and, for each of them, only a random subset of the particles is updated. \n To obtain a distribution for a given item, we put together all the particle filters that are associated with that item. For better performance, we can estimate and remove the expected noise due to collisions in a way analogous to Count-Mean-Min sketches. \n The nice thing is that many cool properties of CM Sketches are preserved. For example, by putting all particle filters together we have an estimation of the cumulative distribution of the whole population. Using techniques similar to those used to estimate the top-K elements in a set, we can calculate the cumulative particle filters of a sub-population and so on. \n It\u2019s still a work in progress, I am still exploring the characteristics of these models and the best way to merge together multiple particle filters discarding the peculiar noise generated by (pardon the repetition) structure of this data structure. \n Possible improvements : stacking together particle sketches at different resolutions, it should be possible to decompose these models in a way analogous to wavelet decomposition. It should make it possible to use even a more compact representation, thus dually to have \u2014if we use the same bits of information\u2014 a more faithful model with less noise. \n NEXT: Particle Sketches / 2 \u2014 working around the central limit theorem \n Filed under: Programming Tagged: cloud , data structures , particle sketches"], "link": "http://blog.lupi-software.com/2012/05/16/particle-sketches-a-way-to-model-a-very-large-number-of-bayesian-models-in-sub-linear-space-and-time/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 5, "http://feeds.wordpress.com/": 1, "http://highlyscalable.wordpress.com/": 1, "http://en.wikipedia.org/": 1, "http://allupo.wordpress.com/": 1, "https://sites.google.com/": 1}, "blogtitle": "Lupi on Software"}, {"content": ["Image via CrunchBase \n I decided at the end of last year that I was working in the wrong place. The company was nice, the people were great, but the development practices and project management felt on the wrong track. I quit. \n I looked all over Europe for a new interesting adventure. London, Berlin, Amsterdam \u2014 I was all over the place meeting new people, screening companies and interviewing with the few interesting choices. \n In the process, I clarified my goals. I want: \n \n to remain sane , so I need a company that understands and applies agile software development practices \n to learn cloud computing , so I need a company that has expertise in that area and has at least a successful deployment or product in the area of large scale computing \n to improve my data mining skills , so I need a company that tackles interesting problems \n to spend most of my time developing software , so I will avoid mixed half-management, half-development positions, particularly in large corporations. \n \n You can imagine how delighted I was when I found a company that meets these goals here in Zurich. I couldn\u2019t believe that to be possible. I thought that so improbable that I already gave up my rented apartment here and was about to move somewhere else. Now I am stuck in a rented room in a shared flat, while I search for a new place. \n \n The company I joined is Nektoon AG. A startup built by the core team of local.ch , a Switzerland whitepages site, that has already shipped a successful product, Memonic , a Evernote-like tool geared toward teams. Memonic predates Evernote, but when you have 100 times the money it\u2019s easy to beat the competition. \n Memonic is built entirely on Amazon Web Services , is written in my old friend Python and it\u2019s quite an interesting tool. You can find some details on its architecture on our Memonic blog . \n Squirro \n I, however, did not join Nektoon to work on Memonic. I am working on a shiny new product called Squirro . Memonic and Squirro attack the same problem, they help you pick valuable needles from the Internet haystack, but they do it in quite a different way. \n Memonic sits at your side while you browser and help you save interesting leads, shortlist relevant information and curate them for your peers. Searching for the right stuff, scoring sources and keeping up to date with new content is still a manual process. \n Squirro does that and more for you: give our little squirrel a topic and he will find the best acorn trees and harvest the tastier nuts for you. He will keep coming with better and better fruits, because it learns your taste over time. \n He can do much more, he\u2019s a sucker for serendipity: do you work at a Big Pharma company and have you got a meeting with Dr. Bob next Thursday about that new promising molecule for Alzheimer? Squirro will come up with relevant studies and papers that you haven\u2019t yet read, will tell about unexpected weather conditions/road blocks/strikes at the place where you\u2019re going to meet and will give you the latest spicy gossip about FCZ Zurich\u2026 because he knows Bob is a big fan of that soccer team. \n Search is the past, Google\u2019s business is telling you about what you are already thinking about. Social is the present, Facebook and Twitter tell you what others around you are thinking right now. Squirro is the future, we\u2019re in the business of telling you what you are not yet thinking about . \n Filed under: Everything else Tagged: squirro"], "link": "http://blog.lupi-software.com/2012/02/04/my-new-job/", "bloglinks": {}, "links": {"http://blog.lupi-software.com/": 2, "http://feeds.wordpress.com/": 1, "http://www.local.ch/": 1, "http://aws.amazon.com/": 1, "http://www.crunchbase.com/": 1, "http://www.squirro.com": 1, "http://blog.memonic.com": 1, "http://www.memonic.com": 1}, "blogtitle": "Lupi on Software"}]