[{"blogurl": "http://onbiostatistics.blogspot.com\n", "blogroll": [], "title": "On Biostatistics and Clinical Trials"}, {"content": ["Recently, I spent some time on exploring the feature of SAS ODS Graphics. ODS Graphics offers an easy way to generate high-quality statistical graphics without extensive programming using SAS/Graph. ODS Graphics has been included in almost all of SAS/Stat Procedure. \n \n \n \n In the discussions below, the example from SAS Proc Lifetest is used to illustrate the generation of Kaplan-Meier curve using ODS Graphics. \n \n \n \n On the basis of SAS Example 49.2 Enhanced Survival Plot and Multiple-Comparison Adjustments , we can run the following program. \n \n \n \n proc  format ; \n \n   value risk 1 = 'ALL'  2 = 'AML-Low Risk'  3 = 'AML-High Risk' ; \n \n   \n \n  data BMT; \n \n   input Group T Status @@; \n \n   format Group risk. ; \n \n   label T= 'Disease Free Time' ; \n \n   datalines ; \n \n  1 2081 0 1 1602 0 1 1496 0 1 1462 0 1 1433 0 \n \n  1 1377 0 1 1330 0 1 996 0 1 226 0 1 1199 0 \n \n  1 1111 0 1 530 0 1 1182 0 1 1167 0 1 418 1 \n \n  1 383 1 1 276 1 1 104 1 1 609 1 1 172 1 \n \n  1 487 1 1 662 1 1 194 1 1 230 1 1 526 1 \n \n  1 122 1 1 129 1 1  74 1 1 122 1 1  86 1 \n \n  1 466 1 1 192 1 1 109 1 1  55 1 1  1 1 \n \n  1 107 1 1 110 1 1 332 1 2 2569 0 2 2506 0 \n \n  2 2409 0 2 2218 0 2 1857 0 2 1829 0 2 1562 0 \n \n  2 1470 0 2 1363 0 2 1030 0 2 860 0 2 1258 0 \n \n  2 2246 0 2 1870 0 2 1799 0 2 1709 0 2 1674 0 \n \n  2 1568 0 2 1527 0 2 1324 0 2 957 0 2 932 0 \n \n  2 847 0 2 848 0 2 1850 0 2 1843 0 2 1535 0 \n \n  2 1447 0 2 1384 0 2 414 1 2 2204 1 2 1063 1 \n \n  2 481 1 2 105 1 2 641 1 2 390 1 2 288 1 \n \n  2 421 1 2  79 1 2 748 1 2 486 1 2  48 1 \n \n  2 272 1 2 1074 1 2 381 1 2  10 1 2  53 1 \n \n  2  80 1 2  35 1 2 248 1 2 704 1 2 211 1 \n \n  2 219 1 2 606 1 3 2640 0 3 2430 0 3 2252 0 \n \n  3 2140 0 3 2133 0 3 1238 0 3 1631 0 3 2024 0 \n \n  3 1345 0 3 1136 0 3 845 0 3 422 1 3 162 1 \n \n  3  84 1 3 100 1 3  2 1 3  47 1 3 242 1 \n \n  3 456 1 3 268 1 3 318 1 3  32 1 3 467 1 \n \n  3  47 1 3 390 1 3 183 1 3 105 1 3 115 1 \n \n  3 164 1 3  93 1 3 120 1 3  80 1 3 677 1 \n \n  3  64 1 3 168 1 3  74 1 3  16 1 3 157 1 \n \n  3 625 1 3  48 1 3 273 1 3  63 1 3  76 1 \n \n  3 113 1 3 363 1 \n \n  ; \n \n \n \n ods  rtf  file = \"c:\\temp\\test.doc\"  style =journal; \n \n ods  graphics  on ; \n \n ods  trace  on ; \n \n proc  lifetest  data =BMT plots =survival(atrisk= 0 to 2500 by 500 ); \n \n   ods  select SurvivalPlot; \n \n   time T * Status( 0 ); \n \n   strata Group / test =logrank adjust=sidak; \n \n   run ; \n \n ods  trace  off ; \n \n ods  graphics  off ; \n \n ods  rtf  close ; \n \n \n \n In the above statements, \n \n ods  rtf  file = \"c:\\temp\\test.doc\" specifies the ODS output as RTF file and the file location and file name. style =journal specifies the use of output in publication quality. \n \n \"ods graphics on\" to invoke the ODS Graphics \n \n \"ods trace on\" is not really necessary, but it is useful when we need to know the name/location of the SAS template. If we check the SAS Log Window, we will be able to see that the SAS template for the corresponding graph is Stat.Lifetest.Graphics.ProductLimitSurvival \n \n \n \n Output Added: \n \n ------------- \n \n Name:   SurvivalPlot \n \n Label:  Survival Curves \n \n Template:  Stat.Lifetest.Graphics.ProductLimitSurvival \n \n Path:   Lifetest.SurvivalPlot \n \n -------------  \n \n \n \n PROC LIFETEST is invoked to compute the product-limit estimate of the survivor function for each of three risk categories ( 1 = 'ALL'  2 = 'AML-Low Risk'  3 = 'AML-High Risk') . Using ODS Graphics, you can display the number of subjects at risk in the survival plot. The PLOTS= option requests that the survival curves be plotted, and the ATRISK= suboption specifies the time points at which the at-risk numbers are displayed. In the STRATA statement, the ADJUST=SIDAK option requests the id\u00e1k multiple-comparison adjustment, and by default, all paired comparisons are carried out. \n \n \n \n After the run, we will need to use the following statements to close ODS. \n \n ods  trace  off ; \n \n ods  graphics  off ; \n \n ods  rtf  close ; \n \n \n \n The above program will create a high quality figure like below: \n \n  \n \n \n \n \n \n Suppose we would like to change the title and labels of the figure above, we can use PROC Template. First thing we need to do is to identify which Template we need to modify. The SAS Log window from ods trace on statement indicates the following template. \n \n \n \n Stat.Lifetest.Graphics.ProductLimitSurvival \n \n \n \n To open this template, we would need to the pull down manu in SAS window, then choose View -> Results -> View -> Template \n \n From Template window, we will locate the following folder: \n \n SAShelp.tmplmst -> stat -> lifetest -> graphics \n \n \n \n We will then see the template called ProductLimitSurvival. Double click this file, we will be able to see the contents in this template file. \n \n \n \n \n \n \n \n \n \n The template file copied and pasted in the program window. We can then edit the template file to modify the title, labels, legends,\u2026 \n \n \n \n Suppose we run the following program to generate the Kaplan-Meier curve (upward curve instead of the original downward curve). The upward curve is sometimes easier to be understood. For example, it is useful in the time to event variable where the event is a better outcome (for example, time to recovery, time to release from the hospital, time to immune tolerance (in Hemophilia). By changing the layout of the figure, we would also need to modify the label. \n \n \n \n \n \n proc  lifetest  data =BMT plots =survival(f test); \n \n   ods  select failurePlot; \n \n    time T * Status( 0 ); \n \n   strata Group / test =logrank adjust=sidak; \n \n run ; \n \n \n \n plots =survival(f test) requests the failure Plot (upward) instead of the survival plot (downward). The \u2018test\u2019 indicates the display of the logrank test results. \n \n \n \n From the LOG window trigged by ODS Trace On statement, we will be able to know that the template is Stat.Lifetest.Graphics.ProductLimitFailure which is located in the same folder as Stat.Lifetest.Graphics.ProductLimitSurvival discussed before. \n \n \n \n Output Added: \n \n ------------- \n \n Name:   FailurePlot \n \n Label:  Failure Curves \n \n Template:  Stat.Lifetest.Graphics.ProductLimitFailure \n \n Path:   Lifetest.FailurePlot  \n \n \n \n \n \n \n \n We can copy and paste the template statements and run as part of the program. \n \n \n \n ods  rtf  file = \"c:\\temp\\test.doc\"  style =journal; \n \n ods  graphics  on ; \n \n ods  trace  on ; \n \n \n \n proc  template ;                 \n \n  define statgraph Stat.Lifetest.Graphics.ProductLimitFailure;     \n \n   dynamic NStrata xName maxTime plotAtRisk plotCensored plotCL plotHW   \n \n   plotEP labelCL labelHW labelEP yMin StratumID classAtRisk plotTest  \n \n   GroupName Transparency SecondTitle TestName pValue;      \n \n   BeginGraph ;                 \n \n   if (NSTRATA= 1 )               \n \n    if (EXISTS(STRATUMID))            \n \n    entrytitle \"Kaplan-Meier Curve\"  \" for \" STRATUMID;   /*Revised the Figure title*/ \n \n    else                  \n \n    entrytitle \"Kaplan-Meier Curve\" ;        \n \n    endif ;                  \n \n   if (PLOTATRISK)               \n \n    entrytitle \"with Number of Subjects at Risk\" / textattrs=    \n \n    GRAPHVALUETEXT;               \n \n    endif ;                 \n \n    layout overlay / xaxisopts=(label= \"Time to Recovery\" offsetmin= .05 linearopts /*xaxisopts=(label=\" \") to change the x-axis label*/ \n \n    =(viewmax=MAXTIME)) yaxisopts=(label= \"Probability of Achieving Tolerance\"    \n \n    shortlabel= \"Failure\" linearopts=(viewmin= 0 viewmax= 1 tickvaluelist \n \n    =( 0  .2  .4  .6  .8  1.0 )));            \n \n    if (PLOTHW= 1 AND PLOTEP= 0 )           \n \n     bandplot LimitUpper=eval ( 1 -HW_LCL) LimitLower=eval ( 1 -HW_UCL) \n \n     x=TIME / modelname= \"Failure\" fillattrs=GRAPHCONFIDENCE name=  \n \n     \"HW\" legendlabel=LABELHW;           \n \n     endif ;                \n \n    if (PLOTHW= 0 AND PLOTEP= 1 )           \n \n     bandplot LimitUpper=eval ( 1 -EP_LCL) LimitLower=eval ( 1 -EP_UCL) \n \n     x=TIME / modelname= \"Failure\" fillattrs=GRAPHCONFIDENCE name=  \n \n     \"EP\" legendlabel=LABELEP;           \n \n     endif ;                \n \n     if (PLOTHW= 1 AND PLOTEP= 1 )           \n \n     bandplot LimitUpper=eval ( 1 -HW_LCL) LimitLower=eval ( 1 -HW_UCL) \n \n     x=TIME / modelname= \"Failure\" fillattrs=GRAPHDATA1     \n \n     datatransparency= .55 name= \"HW\" legendlabel=LABELHW;    \n \n     bandplot LimitUpper=eval ( 1 -EP_LCL) LimitLower=eval ( 1 -EP_UCL) x= \n \n     TIME / modelname= \"Failure\" fillattrs=GRAPHDATA2     \n \n     datatransparency= .55 name= \"EP\" legendlabel=LABELEP;    \n \n     endif ;                \n \n    if (PLOTCL= 1 )               \n \n     if (PLOTHW= 1 OR PLOTEP= 1 )           \n \n     bandplot LimitUpper=eval ( 1 -SDF_LCL) LimitLower=eval ( 1 -SDF_UCL \n \n     ) x=TIME / modelname= \"Failure\" display=(outline) outlineattrs= \n \n     GRAPHPREDICTIONLIMITS name= \"CL\" legendlabel=LABELCL;     \n \n     else                 \n \n     bandplot LimitUpper=eval ( 1 -SDF_LCL) LimitLower=eval ( 1 -SDF_UCL \n \n     ) x=TIME / modelname= \"Failure\" fillattrs=GRAPHCONFIDENCE name= \n \n      \"CL\" legendlabel=LABELCL;           \n \n     endif ;                \n \n     endif ;                \n \n     stepplot y=eval ( 1 -SURVIVAL) x=TIME / name= \"Failure\" rolename=(  \n \n     _tip1=ATRISK _tip2=EVENT) tip=(y x Time _tip1 _tip2)    \n \n     legendlabel= \"Failure\" ;            \n \n    if (PLOTCENSORED)              \n \n     scatterplot y=eval ( 1 -CENSORED) x=TIME / markerattrs=(symbol=  \n \n     plus) name= \"Censored\" legendlabel= \"Censored\" ;      \n \n     endif ;                \n \n    if (PLOTCL= 1 OR PLOTHW= 1 OR PLOTEP= 1 )         \n \n     discretelegend \"Censored\"  \"CL\"  \"HW\"  \"EP\" / location=outside  \n \n     halign=center;              \n \n     else                 \n \n     if (PLOTCENSORED= 1 )            \n \n     discretelegend \"Censored\" / location=inside autoalign=(topleft \n \n     bottomright);              \n \n     endif ;                \n \n     endif ;                \n \n    if (PLOTATRISK= 1 )              \n \n     innermargin / align=bottom;          \n \n     blockplot x=TATRISK block=ATRISK / repeatedvalues=true display= \n \n      (values) valuehalign=start valuefitpolicy=truncate    \n \n      labelposition=left labelattrs=GRAPHVALUETEXT valueattrs=  \n \n      GRAPHDATATEXT (size= 7 pt) includemissingclass=false;    \n \n     endinnermargin ;              \n \n     endif ;                \n \n    endlayout ;                 \n \n    else                  \n \n    entrytitle \"Kaplan-Meier Curve\" ;       \n \n   if (EXISTS(SECONDTITLE))             \n \n    entrytitle SECONDTITLE / textattrs=GRAPHVALUETEXT;     \n \n    endif ;                 \n \n    layout overlay / xaxisopts=(label= \"Time to Recovery\" offsetmin= .05 linearopts \n \n    =(viewmax=MAXTIME)) yaxisopts=(label= \"Probability of Recovery\"   /*y-axis label*/   \n \n    shortlabel= \"Failure\" linearopts=(viewmin= 0 viewmax= 1 tickvaluelist \n \n    =( 0  .2  .4  .6  .8  1.0 )));            \n \n    if (PLOTHW= 1 )               \n \n     bandplot LimitUpper=eval ( 1 -HW_LCL) LimitLower=eval ( 1 -HW_UCL) \n \n     x=TIME / group=STRATUM index=STRATUMNUM modelname= \"Failure\"   \n \n     datatransparency=Transparency;          \n \n     endif ;                \n \n    if (PLOTEP= 1 )               \n \n     bandplot LimitUpper=eval ( 1 -EP_LCL) LimitLower=eval ( 1 -EP_UCL) \n \n     x=TIME / group=STRATUM index=STRATUMNUM modelname= \"Failure\"   \n \n     datatransparency=Transparency;          \n \n     endif ;                \n \n    if (PLOTCL= 1 )               \n \n     if (PLOTHW= 1 OR PLOTEP= 1 )           \n \n     bandplot LimitUpper=eval ( 1 -SDF_LCL) LimitLower=eval ( 1 -SDF_UCL \n \n     ) x=TIME / group=STRATUM index=STRATUMNUM modelname= \"Failure\"  \n \n     display=(outline);             \n \n     else                 \n \n     bandplot LimitUpper=eval ( 1 -SDF_UCL) LimitLower=eval ( 1 -SDF_LCL \n \n     ) x=TIME / group=STRATUM index=STRATUMNUM modelname= \"Failure\"  \n \n     datatransparency=Transparency;          \n \n     endif ;                 \n \n     endif ;                \n \n     stepplot y=eval ( 1 -SURVIVAL) x=TIME / group=STRATUM index=   \n \n     STRATUMNUM name= \"Failure\" rolename=(_tip1=ATRISK _tip2=EVENT)  \n \n     tip=(y x Time _tip1 _tip2);          \n \n    if (PLOTCENSORED)              \n \n     scatterplot y=eval ( 1 -CENSORED) x=TIME / group=STRATUM index=  \n \n     STRATUMNUM markerattrs=(symbol=plus);        \n \n     endif ;                \n \n    if (PLOTATRISK)              \n \n     innermargin / align=bottom;          \n \n     blockplot x=TATRISK block=ATRISK / class=CLASSATRISK    \n \n      repeatedvalues=true display=(label values) valuehalign=start \n \n      valuefitpolicy=truncate labelposition=left labelattrs=   \n \n      GRAPHVALUETEXT valueattrs=GRAPHDATATEXT (size= 7 pt)    \n \n      includemissingclass=false;          \n \n     endinnermargin ;               \n \n     endif ;                \n \n     DiscreteLegend  \"Failure\" / title= \"Risk Group\" location=inside autoalign=(bottomright); /*Revise the legend*/   \n \n    if (PLOTCENSORED)              \n \n     if (PLOTTEST)              \n \n     layout gridded / rows= 2 autoalign=(TOPLEFT BOTTOMRIGHT BOTTOM  \n \n     TOP) border=true BackgroundColor=GraphWalls:Color Opaque=true; \n \n     entry  \"+ Censored\" ;            \n \n     if (PVALUE < .0001 )            \n \n      entry TESTNAME \" p \" eval (PUT(PVALUE, PVALUE6.4 ));    \n \n     else                 \n \n      entry TESTNAME \" p=\" eval (PUT(PVALUE, PVALUE6.4 ));    \n \n     endif ;                \n \n     endlayout ;               \n \n     else                 \n \n     layout gridded / rows= 1 autoalign=(TOPLEFT BOTTOMRIGHT BOTTOM  \n \n     TOP) border=true BackgroundColor=GraphWalls:Color Opaque=true; \n \n     entry  \"+ Censored\" ;            \n \n     endlayout ;                \n \n     endif ;                \n \n     else                 \n \n     if (PLOTTEST)              \n \n     layout gridded / rows= 1 autoalign=(TOPLEFT BOTTOMRIGHT BOTTOM  \n \n     TOP) border=true BackgroundColor=GraphWalls:Color Opaque=true; \n \n     if (PVALUE < .0001 )            \n \n      entry TESTNAME \" p \" eval (PUT(PVALUE, PVALUE6.4 ));    \n \n     else                 \n \n      entry TESTNAME \" p=\" eval (PUT(PVALUE, PVALUE6.4 ));    \n \n     endif ;                \n \n     endlayout ;               \n \n     endif ;                \n \n     endif ;                 \n \n    endlayout ;                \n \n    endif ;                 \n \n   EndGraph ;                 \n \n  end ;                   \n \n run ;  \n \n \n \n proc  lifetest  data =BMT plots =survival(f test); \n \n   ods  select quartiles FailurePlot; \n \n   time T * Status( 0 ); \n \n   strata Group / test =logrank adjust=sidak; \n \n   run ; \n \n ods  trace  off ; \n \n ods  graphics  off ; \n \n ods  rtf  close ; \n \n \n \n At the end of the program, we use the following program to delete the user defined Template. \n \n proc template; \n \n  delete Stat.Lifetest.Graphics.ProductLimitFailure; \n \n run; \n \n \n \n \n \n \n \n Further reading: \n \n \n \n \n Resource for ODS Statistics Graphics \n \n \n Getting Started with ODS Statistical Graphics in SAS\u00ae 9.2 presents the fundamental information you need to get started with ODS Statistical Graphics in SAS 9.2. ( Revised 2009 ) \n \n \n An Overview of ODS Statistical Graphics in SAS 9.3 \n \n \n Summary of ODS Graphics Capabilities provides a concise overview of the capabilities of ODS Graphics. \n \n \n The Graph Template Language and the Statistical Graphics Procedures: \n \n \n An Example-Driven Introduction  \n \n \n Customizing ODS Statistical Graphs \n \n \n Customizing ODS Statistical Graphs Step-by-Step \n \n \n Modifying Graph Titles and Axis Labels \n \n \n Graph Templates"], "link": "http://onbiostatistics.blogspot.com/feeds/367526571962563012/comments/default", "bloglinks": {}, "links": {"http://www.blogger.com/": 1, "http://www2.sas.com/": 1, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1, "http://support.sas.com/": 13, "http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["Area Under Curve (AUC) has been frequently used as the endpoint measure in clinical trials. We use AUC commonly in clinical pharmacology - Area under the time concentration curve or in diagnostic research \u2013 Area Under the ROC curve. The use of AUC is much more broader than what we think. Many clinical endpoints can utilize the AUC as a measure for the aggregate effect over a period of time. Below are some of the examples that I have experienced where AUC is used in clinical trials not for the purpose of pharmacokinetics measure or ROC measure.  \n \n AUC Used in Pain Assessment  \n \n In the study of pain medications (usually acute pain medications), the pain intensity or pain relief in scales are measured at pre and serial time points post analgesic drug administration. The Summed Pain Intensity Difference (SPID) and total pain relief (TOTPAR) are usually calculated and used as the efficacy endpoints. TOTPAR is a time-weighted measure of AUC or total area under the pain relief curve and is a summary measure that integrates serial assessments of a subject\u2019s pain over the duration of the study. The area under the pain relief vs. time curve can be used to derive the proportion of patients experiencing typically 50% pain relief over a specified time frame. This can be calculated as the ratio of two AUCs: TOTPAR vs. maxTOTPAR (maximum potential value for TOTPAR) as illustrated in the \u201c Analysis of scale results \u2013 summary measures \u201d of pain. \n \n FEV1 AUC \n \n In Asthma and COPD studies, FEV1 can be measured at pre-dose and at several serial time points post the treatment. The AUC will then be calculated from the time-FEV1 curve. \n \n In  DULERA drug label , FEV1 AUC(0-12 hr) was mentioned as the efficacy measure: \n \n \n \n \u201c FEV1 AUC (0-12hr) was assessed as a co-primary efficacy endpoint to evaluate the contribution of the formoterol component to DULERA. Patients receiving DULERA 100 mcg/5 mcg had significantly higher increases from baseline at Week 12 in mean FEV1 AUC (0-12 hr) compared to mometasone furoate 100 mcg (the primary treatment comparison) and vs. placebo ......\" \n \n \n In a recent news release \u201c Results of Phase II Study of Boehringer Ingelheim's Investigational Bronchodilator for COPD Presented at 2012 ATS International Conference \u201d, FEV1 AUC was used to measure the treatment effect in COPD.  \n \n \n \n \u201cResults of the study found olodaterol 5 microgram QD provided significant improvement in lung function as measured by FEV1 AUC(0-12) versus twice-daily olodaterol 2 microgram, while twice-daily dosing of olodaterol 5 microgram had a better FEV1 AUC(0-12)  profile versus once-daily olodaterol 10 microgram\u201d \n \n  \n \n AUC in Type 1 Diabetes \n \n In type-1 diabetes research, the main purpose of the treatment is to preserve the beta-cell function. The assessment of beta-cell function is through the measurement of the C-peptide concentration after simulated Mixed Meal Tolerance Test (MMTT) - the gold standard measure of endogenous insulin secretion \n \n In the mixed-meal tolerance test (MMTT), commonly used in the U.S., a liquid meal (Sustacal/Boost) is ingested in the fasting state with timed measurements of C-peptide over the subsequent 2\u20134 h. The AUC is then calculated for the area under time-C-Peptide curve over 2 hour (AUC 0-2hr ) or 4 hours (AUC 0-4hr ) (see Greenbaum at al \u201c Mixed-Meal Tolerance Test Versus Glucagon Stimulation Test for the Assessment of \u03b2-Cell Function in Therapeutic Trials in Type 1 Diabetes \u201d). \n \n In type 1 diabetes research, a concept of mean AUC is also used. Mean AUC is calculated by the AUC divided by the time duration (i.e., AUC 0-2 hr / 120 minutes or AUC 0-4 hr / 240 minutes) \n \n AUCs to Assess the Responsiveness \n \n We recently published a paper \u201c Vigorimeter grip strength in CIDP: a responsive tool that rapidly measures the effect of IVIG \u2013 the ICE study \u201d where we used AUCs to compare the responsiveness of two difference measures. Since two different measures used different scales, we had to calculate the SRM (standardized response mean) before we calculated the AUCs for INCAT scale and for Grip Strength . The larger the AUC, the higher the responsiveness to the treatment. The results indicated that the Vigorimeter grip strength could be more sensitive measure comparing to INCAT scale to evaluate the treatment effect of IVIG in CIDP patients."], "link": "http://onbiostatistics.blogspot.com/feeds/6796520449650869161/comments/default", "bloglinks": {}, "links": {"http://painconsortium.nih.gov/": 1, "http://www.boomer.org/": 1, "http://www.plosone.org/": 1, "http://www.ac.uk/": 1, "http://care.diabetesjournals.org/": 1, "http://onlinelibrary.wiley.com/": 1, "http://www.premier-research.com/": 1, "http://www.prnewswire.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.fda.gov/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["For statisticians working in clinical trial field, the best challenge may not be in the statistical methodologies. The best challenge may be in communication with non-statisticians (such as physicians, clinical team members, corporate executives) about the statistical concepts and the statistical terminologies in plain languages.  \n \n \n \n The missing data is very common in clinical trials and the concept of the missing data is very easy to understand. However, the categories for missing data mechanisms (or taxonomy of missingness) are not so easy to understand. A formal taxonomy exists for classifying missing data mechanisms, including for longitudinal and event history data. The mechanisms can be classified as MCAR (missing completely at random), MAR (missing at random), and MNAR (missing not at random). Take a look at the definition of MCAR, MAR, and MNAR below, you will see that these definitions are not easy to be understood by non-statisticians. \n \n \n \n \n \n \n EMA  \n \n Academy (US) \n \n \n MCAR \n \n For the dependent variable (conditional on the covariates in the model), if the probability of an observation being missing does not depend on observed or unobserved measurements then the observation is Missing Completely At Random ( MCAR ). \n \n In the case of MCAR , the missing data are unrelated to the study variables: thus, the participants with completely observed data are in effect a random sample of all the participants assigned a particular intervention. With MCAR, the random assignment of treatments is assumed to be preserved, but that is usually an unrealistically strong assumption in practice. \n \n      \n \n \n MAR \n \n Conditional on the covariates in the model, if the probability of an observation being missing depends only on observed measurements then the observation is Missing At Random ( MAR ). \n \n In the case of MAR , whether or not data are missing may depend on the values of the observed study variables. However, after conditioning on this information, whether or not data are missing does not depend on the values of the missing data. \n \n \n MNAR \n \n When observations are neither MCAR nor MAR, they are classified as Missing Not At Random ( MNAR ), i.e. the probability of an observation being missing depends on unobserved measurements. In this scenario, the value of the unobserved responses depends on information not available for the analysis (i.e. not the values observed previously on the analysis variable or the covariates being used), and thus, future observations cannot be predicted without bias by the model. \n \n In the case of MNAR , whether or not data are missing depends on the values of the missing data. \n \n \n \n \n \n \n Thanks to Ziad Taib , the following example for three different missingness mechanisms were explained very well and were easy to be understood by the non-statisticians.  \n \n \n \n \n \n \n Suppose you are modelling weight (Y) as a function of sex (X). Some respondents wouldn't disclose their weight, so you are missing some values for Y. There are three possible mechanisms for the nondisclosure: \n \n  \n \n \n There may be no particular reason why some respondents told you their weights and others didn't. That is, the probability that Y is missing may has no relationship to X or Y. In this case our data is missing completely at random (MCAR)  \n \n \n One sex may be less likely to disclose its weight. That is, the probability that Y is missing depends only on the value of X. Such data are missing at random (MAR)  \n \n \n Heavy (or light) people may be less likely to disclose their weight. That is, the probability that Y is missing depends on the unobserved value of Y itself. Such data are not missing at random or missing not at random (MNAR)  \n \n \n \n \n \n  \n Understanding the concept of missing mechanism is one thing, fully understanding missing mechanism in practice is another story. The reason for missing data is often not collected or incompletely collected in the clinical trials. Patients may not tell the real reason for them to withdraw from the study (discontinue from the study earlier). Academy\u2019s suggestions below are reasonable, however, \u2018full and detailed documentation for each individual of the reasons for missing records or missing observations\u2019 is not the reality in the current clinical trial practice. \n \n \n \n \n \"Reasons for missing data must be documented as much as possible. This includes full and detailed documentation for each individual of the reasons for missing records or missing observations. Knowing the reason for missingness permits formulation of sensible assumptions about observations that are missing, including whether those observations are well defined. \n \n Missing data in clinical trials can seriously undermine the benefits provided by randomization into control and treatment groups. Two approaches to the problem are to reduce the frequency of missing data in the first place and to use appropriate statistical techniques that account for the missing data. The former approach is preferred, since the choice of statistical method requires unverifiable assumptions concerning the mechanism that causes the missing data, and so always involves some degree of subjectivity.\u201d"], "link": "http://onbiostatistics.blogspot.com/feeds/4517484525907619008/comments/default", "bloglinks": {}, "links": {"http://www.nap.edu/": 2, "http://feedads.doubleclick.net/": 2, "http://www.europa.eu/": 1, "http://www.chalmers.se/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["Subgroup analyses have been used in the clinical trials for many years and when the sample size is adequate, subgroup analyses can assess the qualitative consistency of treatment effect across different subgroups and can provide the information for identifying a sub-population that may have greater benefit from the treatment. Recently, the purpose of subgroup analyses has been expanded. As mentioned in the \u201c Concept paper on the need for a Guideline on the use of Subgroup Analyses in Randomised Controlled Trials \u201d, the subgroup analyses can be used to: \n \n \n \n -    Assess internal consistency, \n \n -    Try to rescue trials that \u2018fail\u2019 based on the full analysis set \n \n -    try to identify patient groups with the most favourable benefit-risk profile \n \n \n \n The subgroup analyses may: \n \n -    be pre-specified in the trial protocol, based on demographic, genomic or disease characteristics (e.g. sub-entities of a disease that are widely recognised within the medical community) \n \n -    materialise based on a need or desire to further explore study results. \n \n \n \n Sub-group analyses may be especially useful in personalized medicine. Through sub-group analyses, certain biomarkers/subgroups may be identified so that we can develop the tailored therapeutics. \n \n \n \n If the sub-group analyses are not pre-specified and are post-hoc after the data dredging / mining, the interpretation of the findings from the sub-group analyese needs to be cautioned. If we introduce the \u2018learning/confirming\u2019 concept, the post-hoc sub-group analyses is a learning process and the results of interest need to be confirmed in further prospectively designed trials. \n \n \n \n There are many examples of clinical trials where the statistically significant treatment effect in a specific sub-group identified from a study can not be subsequently verified / confirmed in prospectively designed trials. \n \n \n \n In PRASE ( THE P ROSPECTIVE  R ANDOMIZED A MLODIPINE S URVIVAL E VALUATION) study , the study was powered to detect the treatment difference  in death from any cause and hospitalization for major cardiovascular events between Amlodipine and Placebo in overall population. The study result was not statistically significant. Sub-group analyses were then performed to compare the treatment difference in the patients with ischemic heart disease and in the patients with nonischemic cardiomyopathy.The statistically significant difference between the Amlodipine and Placebo Groups was obtained among Patients with Nonischemic Dilated Cardiomyopathy. The author concluded \u201c Amlodipine did not increase cardiovascular morbidity or mortality in patients with severe heart failure. The possibility that amlodipine prolongs survival in patients with nonischemic dilated cardiomyopathy requires further study\u201d \n \n \n \n A subsequent PRAISE-2 trial was conducted to confirm the finding from the sub-group analyses of the PRAISE study. Unfortunately, the results are negative. The study results were not published in peer-reviewed paper (since it is negative), but was presented in the scientific meeting by American Heart Association \n \n \n \n \n \n \u201cThe Trial: PRAISE-2 \n Presenter: Milton Packer, Columbia University College of Physicians and Surgeons, New York, NY. \n The study : A randomized, double-blind, placebo-controlled trial of amlodipine in patients with nonischemic cardiomyopathy on maximal medical therapy. A total of 1652 patients were randomized to receive either amlodipine (initially 5 mg/d, then increased to 10 mg/d after 2 weeks) or placebo. The primary end point of the study was all-cause mortality. The study was powered at 90% to detect a 25% difference in mortality between the treatment arms. \n The results: No significant differences existed in all-cause mortality between the 2 arms (placebo, 31.7%; amlodipine, 33.7%; hazard ratio, 1.09; log-rank P =0.32). A pooled analysis of the PRAISE-1 and PRAISE-2 trials showed no significant affect of amlodipine on mortality (placebo, 34%; amlodipine, 33.4%; hazard ratio, 0.98; log-rank P =0.81). \n Summary: Despite the fact that in PRAISE-1 a survival benefit was noted with amlodipine in patients with nonischemic cardiomyopathy, no such difference was noted in PRAISE-2 or when PRAISE-1 and PRAISE-2 were combined. Long-term treatment with amlodipine does not seem to be of benefit in patients with severe, chronic heart failure. \u201c \n  \n \n In Sepsis indication, the Kybersept study compared the 28-day all cause mortality between High-Dose Antithrombin III and Placebo treatment groups. The results indicated \u201c High-dose antithrombin III therapy had no effect on 28-day all-cause mortality in adult patients with severe sepsis and septic shock when administered within 6 hours after the onset. High-dose antithrombin III was associated with an increased risk of hemorrhage when administered with heparin. There was some evidence to suggest a treatment benefit of antithrombin III in the subgroup of patients not receiving concomitant heparin.\u201d \n \n \n \n Subsequently, a paper based on the post-hoc sub-group analyses were published and concluded \u201c High-dose AT without concomitant heparin in septic patients with DIC may result in a significant mortality reduction. The adapted ISTHDIC score may identify patients with severe sepsis who potentially benefit from high dose AT treatment.\u201d \n \n \n \n Unfortunately, there was no formal randomized clinical trial to study this specific sub-group. Had a prospectively designed study been conducted to study the subjects without concomitant heparin, the results might not be significant as anticipated. \n \n \n \n If the results from the sub-group analyses are used for supporting the drug approval or claim in the product label, it is obvious that the multiplicity issue will arise. A good paper about this is \u201c A flexible strategy for testing subgroups and overall population \u201d by Alosh and Huque.  \n \n \n \n For statistical issues arising from the clinical trial practice, European Medicines Agency (EMA) seems to be always ahead of the US. For the sub-group analysis, EMA organized an expert workshop on subgroup analysis in November, 2011. Various topics related to sub-group analyses were discussed during the workshop. The presentation materials and the workshop summary can all be found at EMA\u2019s website. \n \n \n \n \n \n Programme - Expert workshop on subgroup analysis  \n \n \n \n \n \n Workshop report - Expert workshop on subgroup analysis \n \n \n \n \n \n Presentation - Subgroup analyses \u2013 scene setting from the EU regulators perspective (Hemmings) \n \n \n \n \n \n Presentation - Subgroup analysis: A view from an industry statistician (Keene) \n \n \n \n \n \n Presentation - Reliably basing conclusions on subgroups of clinical trials (Koch) \n \n \n \n \n \n Presentation - A Subgroup or a Subpopulation (Wang) \n \n \n \n \n \n Presentation - Confirmatory subgroup analyses: Case studies (Bretz)  \n \n \n \n \n \n Presentation - Efficacy Claims and Subset Analyses in Phase III (Carroll)  \n \n \n \n \n \n Presentation - Adaptive clinical trials with subgroup selection (Stallard) \n \n \n \n \n \n Presentation - Multiple testing methodology in the context of subgroup analysis (Dmitrienko)  \n \n \n \n \n \n Presentation - Use of Subgroups to \u201cRescue\u201d a Trial or Improve Benefit-Risk (King) \n \n \n \n \n \n Presentation - Subgroup analyses: from nasty business to stratified medicine \u2013 an HTA perspective (Lange) \n \n \n \n \n \n Presentation - Panitumumab: The KRAS Story (Fletcher) \n \n \n \n \n \n Presentation - Perspectives on analysing subgroup effects of clinical trials and their meta \u2010 analyses (Roes)"], "link": "http://onbiostatistics.blogspot.com/feeds/4531146860888784305/comments/default", "bloglinks": {}, "links": {"http://www.nejm.org/": 1, "http://feedads.doubleclick.net/": 2, "http://onlinelibrary.wiley.com/": 2, "http://www.europa.eu/": 17, "http://circ.ahajournals.org/": 1, "http://jama.jamanetwork.com/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["In the last issue, I stated that a more accurate definition of Screening Failures may be as following \u201c Potential subjects who were screened for the study participation, but were not enrolled (randomized or dosed) for the study\u201d \n \n Once we know the definition of the screening failures, the next question is about the data collection for screening subjects in clinical trials. How much data should we collect for screening failures? Should AE be collected and entered into the clinical database? for screening failures, should SAE be reported to regulatory agency and should SAE narratives be written? \n \n \n \nLet\u2019s take a look at a web posting of question and answers about the AE collection for screening failures below:  \n \n \n \n Question: \n I have a question related to the collecting and recording of screening failure adverse events during clinical trials. I work for a data management group of a medium sized pharma company. Currently we collect and database all AEs that occur to screening failures in our clinical trials. \n On further research I have not been able to find any regulation or guidance document that requires this. Can you tell me what the FDA position is on this? \n Should we \n 1) Record and database all AEs for screening failures? \n 2) Not record and database AEs for screening failures? \n Clinical Site Quality Control \n 3) Not record or database and put systems in place that can detect an unusually large number of AEs in a specific site for screening failures. \n \n Answer: \n I'm not sure what you mean by \"screening failure adverse events.\" Are you referring to an intercurrent illness or condition that occurs between the time that the subject was enrolled but before randomization that leads to the subject being considered a screening failure? If so, we do not consider these to be adverse events; because the subject has not yet received any study drug, it would not be \"associated\" with the use of the test article. Nevertheless, because the intercurrent illness or condition affects the subject's eligibility for the study, it should still be recorded by the study site and reported to the sponsor. The clinical investigator should also ensure that the subject receives appropriate medical care, either by providing it directly or by referring the subject back to the subject's primary care physician. \n  \n The answer above was not all accurate. During the screening period, subjects were exposed to the screening procedures which could cause adverse events; subjects could have emotional changes / nervousness / anxiety just because of the study participation; subjects could be asked to change their regular treatment/medication to meet the inclusion / exclusion criteria that in turn could cause side effects (such as withdrawal effect). Therefore, it is very possible for subjects to develop adverse events during the screening period. In other words, adverse events can be reported for screening failure subjects even though the subject has not yet received any study drug. \n \n Different situations for adverse event collections can be listed in the table blow: \n \n \n  \n All Subjects Screened \n \n \n Eligible for study participation \n \n Screening failures \n \n Adverse events during the screening period starting from ICF signing \n Non treatment-emergent adverse events \n \n Non treatment emergent adverse events \n \n Adverse events reported after the first dose of the study drug \n Treatment-emergent adverse events \n \n - \n \n Statistical analysis \n Non-treatment emergent and treatment emergent adverse events are summarized separately \n \n Not included in statistical analysis since screening failures are not included in safety population \n \n \n \n \n \nFrom the table above, we can see the followings: \n \n \n \n1. For subjects who are eventually randomized and receive the study medication, all adverse events need to be captured and recorded in the clinical database starting from the informed consent signing. By comparing the AE onset date/time with the first dose date/time, the AEs can be separated as non-treatment emergent AEs and treatment emergent AEs. \n \nFor this group of subjects, it is accurate to say that any AE occurred after the informed consent signing should be recorded. \n \n \n \n2. For screening failures, whether or not the AEs reported during the screening period should be recorded in the clinical database is up for debating. \n \n \n \n \nSome companies do not record any data into the clinical database for screening failures. All information about the screening failures are maintained in a screening log. \n \n \nSome companies record only the key information into the clinical database for screening failures. The key information may be demographics, reason for screening failures \n \n \nSome companies choose extreme conservative way and record all available information for screening failures in the clinical database. \n \n \n \n \n \nUnfortunately, there is no clear regulatory guidance on what information (especially adverse events) should be recorded into the clinical database for screening failures. The languages from In ICH E3 ( STRUCTURE AND CONTENT OF CLINICAL STUDY REPORTS) , seems to suggest that for screening failures, only information needed may be the reason for screening failures (and adverse event could be one of the reasons for screening failure). \n \n \n \nThe most extreme (or conservative) situation could be that in a study, the SAE narratives would be written for all subjects including the screening failures. In order to have sufficient information for SAE narrative writing for screening failures, all details about SAE and the ancillary information (physical example, medical history, vital signs, laboratory, \u2026) would need to be collected. A lot of time and efforts would be spent on the data collection, but the collected data would not be very useful or at least not relevant to the purpose of the study since in the end, the screening failures would be excluded from the safety population for the safety analysis. This practice of collecting almost every detail about the screening failures is not wrong, but is not an efficient way for conducting the clinical trials. \n \n \n \nNowadays, the industry trend is moving toward to being compliant with CDISC standards (SDTM, aDaM). There seems to be a lot of confusions about whether or not data for screening failures should be included in the database and if so, where to include. The following weblinks from CDISC Public Discussion Forum show the confusions. \n \n \n \n -    http://bbs.cdisc.org/bbs/forums/thread-view.asp?tid=3179&posts=1 \n \n -    http://bbs.cdisc.org/bbs/forums/thread-view.asp?tid=2287&posts=2 \n \n -    http://bbs.cdisc.org/bbs/forums/thread-view.asp?tid=2721&posts=2 \n \n -    http://bbs.cdisc.org/bbs/forums/thread-view.asp?tid=2257&posts=5 \n \n -    http://bbs.cdisc.org/bbs/forums/thread-view.asp?tid=3208&posts=2 \n \n \n \n To summarize, for screening failures, the best way for data collection may be to collect only the demographic information and the reason for screening failures in clinical database. The reason for screening failure should include a category of \u201cAE\u201d since subject can be screening failure due to AE (or precisely non-treatment emergent AE) during the screening period. The details about the AE / SAEs for screen failure subjects are not necessary to be entered into the clinical database."], "link": "http://onbiostatistics.blogspot.com/feeds/6989780562619939253/comments/default", "bloglinks": {}, "links": {"http://www.ich.org/": 1, "http://feedads.doubleclick.net/": 2, "http://bbs.cdisc.org/": 6, "http://firstclinical.com/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["In all clinical trials, the typical process starts with a screening period. The screening period starts with the signing of the informed consent. During the screening period, inclusion/exclusion criteria for the study participation will be checked / tested. Subjects who meet all inclusion criteria and do not meet any exclusion criterion will be eligible to be randomized (in randomized trial) or to be dosed (in non-randomized trial). Those who are not eligible for randomization or dosing will be considered as \u2018screening failures\u201d. \n \n \n \n It seems to be a straightforward concept. However, there could be confusions if there are subjects who are not randomized or dosed due to other reasons (for example consent withdrawal, family relocation, death during the screening period,\u2026). These situations may not be part of the inclusion/exclusion criteria, but still cause the subjects not to be randomized or dosed. \n \n \n \n What is the definition of \u201cscreening failures\u201d? Will screening failures only refer to subjects who do not meet the inclusion/exclusion criteria? \n \n \n \n In the most recent version of CDISC Clinical Research Glossary , the term S creening (of Subjects) is defined as \u201c A process of active consideration of potential subjects for enrollment in a trial\u2019 and the term Screen Failure is defined as \u201c Potential subject who did not meet one or more criteria required for participation in a trial .\u201d This definition of Screening Failure is accurate only if all other situations (such as consent withdrawal, lost to follow up,\u2026) are part of the inclusion/exclusion criteria. \n \n \n \n In ICH E3 ( S TRUCTURE AND C ONTENT OF C LINICAL S TUDY R EPORTS) , while no definition of Screening Failures are provided, it has the following statement and the example flow chart. \n \n \n \n \n \u201c\u2026 It may also be relevant to provide the number of patients screened for inclusion and a breakdown of the reasons for excluding patients during screening, if this could help clarify the appropriate patient population for eventual drug use.\u201d \n \n \n \n  \n \n \n \n \n The annex IV b above implied that there could be multiple reasons for screening failures and inclusion/exclusion criteria would just be one of these reasons. \n \n \n \n For example, in a clinical trial, we could have a case report form to ask the reasons for screening failures and we could have the following list of reasons: \n \n \n \n Primary reason for screening failure: \n \n \n Adverse Event \n \n \n Patient Non-compliance \n \n \n Consent Withdrawn \n \n \n Inclusion/exclusion criteria not met \n \n \n Lost to follow-up \n \n \n Death \n \n \n Other \n \n \n \n \n \n Therefore, a more accurate definition of Screening Failures may be as following \u201c Potential subjects who were screened for the study participation, but were not enrolled (randomized or dosed) for the study\u201d"], "link": "http://onbiostatistics.blogspot.com/feeds/1332103738065604543/comments/default", "bloglinks": {}, "links": {"http://www.appliedclinicaltrialsonline.com/": 1, "http://www.ich.org/": 1, "http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["Recently, I find out how convenient to include some summaries statistics in a statistical graph with a statement called INSET. An INSET statement places a box or table of summary statistics, called an inset, directly in a graph created with a CDFPLOT, HISTOGRAM, PPPLOT, PROBPLOT, or QQPLOT statement. INSET statement is available in many SAS procedures (Proc Univeriate, Proc Boxplot, Proc Lifereg,...). \n \n \n \nIf we run the following program in SAS, the INSET statement used in Proc Univariate will place a box on the left corner of the CDF graph to indicate the mean and standard deviation. \n \n \n \n \n \ndata Cord; \n \nlabel Strength=\"Breaking Strength (psi)\"; \n \ninput Strength @@; \n \ndatalines; \n \n6.94 6.97 7.11 6.95 7.12 6.70 7.13 7.34 6.90 6.83 \n \n7.06 6.89 7.28 6.93 7.05 7.00 7.04 7.21 7.08 7.01 \n \n7.05 7.11 7.03 6.98 7.04 7.08 6.87 6.81 7.11 6.74 \n \n6.95 7.05 6.98 6.94 7.06 7.12 7.19 7.12 7.01 6.84 \n \n6.91 6.89 7.23 6.98 6.93 6.83 6.99 7.00 6.97 7.01 \n \n; \n \nrun; \n \n \n \ntitle 'Cumulative Distribution Function of Breaking Strength'; \n \n \n \nproc univariate data=Cord noprint; \n \nhistogram strength /normal; \n \ncdf Strength / normal; \n \ninset normal(mu sigma); \n \nrun; \n \n \n \n The following are more examples of using INSET statements: \n \n \n \n Adding Insets with Descriptive Statistics \n Univariate Analysis and Normality Test Using SAS \n Positioning an Inset Using Compass Point Values \n INSET statement in Proc Lifereg \n INSET statement in Proc Boxplot \n INSET statement in Proc Probit \n Creating Statistical Graphics in SAS 9.2: What Every Statistical User Should Know where TEST=LOGRANK suboption instead of INSET statement is used  \n Displaying Regression Equations and Special Characters in Regression Fit Plots where INSET statement is used in PROC SGPLOT \n Now You Can Annotate Your Statistical Graphics Procedure Graph"], "link": "http://onbiostatistics.blogspot.com/feeds/4238238672420144458/comments/default", "bloglinks": {}, "links": {"http://www2.sas.com/": 3, "http://feedads.doubleclick.net/": 2, "http://www.indiana.edu/": 1, "http://support.sas.com/": 7}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["Now that we are in the internet era, the learning is not limited to be in the school. There are great resources on the web. The elite universities now post their video lectures for the public. \n \nOne great resource for mathematics/statistics and varriety of other topics is academicearth.org which features the lectures from Universities such as Harvard, MIT, Yale, Stanford,... For statistics , there are six classes listed. Unfortunately, there is no topic specifically to the biostatistics. Another resource is open course which currently listed 500 free online courses from top universities. \n \nFor biostatistics, while there is no video lectures, there are recorded lectures in mp3 format. For example, there are five biostatistics classes listed at education-portal.com . \n \n Introduction to Biostatistics \n Methods in Biostatistics I  \n Methods in Biostatistics II  \n Biostatistics Lecture Series  \n Biostatistics for Medical Product Regulation  \n \nFor topics in medical research (not necessarily clinical trials), there are more resources available. \n \n UCSF mini medical school for the public \n \n Pharmacy and Pharmacology IME Video Library\\ \n \n \n Video Sharing (GBS/CIDP) \n \n \n NPR Health Care News \n \n \n \n NPR Podcast \n People's Pharmacy \n iHealthTube \n \n Medlineplus surgery videos \n \n CME online education \n USMLE Podcast \n Microbiology Online Course"], "link": "http://onbiostatistics.blogspot.com/feeds/6966405983535239684/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.peoplespharmacy.com/": 1, "http://pathmicro.sc.edu/": 1, "http://www.podfeed.net/": 1, "http://www.peerviewpress.com/": 1, "http://www.npr.org/": 2, "http://ocw.jhsph.edu/": 5, "http://www.uctv.tv/": 1, "http://www.ihealthtube.com/": 1, "http://www.openculture.com/": 1, "http://videos.wisc.edu/": 1, "http://academicearth.org/": 1, "http://www.nih.gov/": 1, "http://vimeo.com/": 1, "http://www.academicearth.org/": 1, "http://education-portal.com/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["For clinical\ntrials with binary outcomes, the results can usually be presented as a 2x2\ncontingency table as below: \n \n \n \n \n \n \n \n \n \n \n \n Responder \n \n \n \n Non-responder \n \n \n \n Total \n \n \n \n \n \n Treatment\n 1 \n \n \n \n n11 \n \n \n \n n12 \n \n \n \n n1 \n \n \n \n \n \n Treatment\n 2 \n \n \n \n n21 \n \n \n \n n22 \n \n \n \n n2 \n \n \n \n \n \n \n We can\nthen calculate the proportion of responders for two treatment groups: \n \n \n \n  p1=n11/n1 \n \n \n \n  p2=n21/n2 \n \n \n \n We have\ntwo ways to compare two treatment groups: \n \n The\ndifference between two proportions: p1-p2 \n The ratio\nof two proportions: p1/p2 \n \n \n \n \n p1-p2 may\nbe called the absolute risk difference and p1/p2 is called relative risk (RR) \nor risk ratio.  \n \n \n \n The\nconfidence interval can be constructed for the difference between two\nproportions and for the relative risk. \n \n \n \n For the\ndifference between two proportions , t he\nasymptotic confidence interval is ca1culated using the following formula: \n \n \n \n         (p1-p2)\n+/- Z(alpha/2)*sqrt((p1 *(1-p1)/n1)+(p2*(1-p2)/n2)) \n \n \n \n Reference: Stokes, Davis, and Kock (2000) Categorical Data\nAnalysis using the SAS System, 2nd\nedition \n \n \n \n The\nnotations may be different in the reference book and in\nSAS manual , but the results should be the same. \n \n I had a posting a while ago about\n\u201c Confidence\nInterval for Difference in Two Proportions \u201d where I mentioned the corrections\nand the SAS codes. \n \n For relative\nrisk , the asymptotic confidence interval is calculated using the following\nformula: \n \n \n \n Exp(log(RR) +/- Z(alpha/2) * sqrt((1-p1)/(n1*p1) +\n(1-p2)/(n2*p2))) \n \n \n \n Reference: Agresti A (2007) An Introduction to Categorical\nData Analysis, 2nd edition, JohnWiley & Sons, Inc., \n \n \n \n The\nnotations may be different in the reference book and in\nSAS manual , but the results should be the same. \n \n \n \n The\nconfidence interval for relative risk can be obtained from SAS Proc Freq and\ncan also be manually calculated using the formula above and the formula from\nSAS manual. \n \n \n \n Suppose we\nhave study results as below: \n \n \n \n \n \n \n \n \n \n \n \n Success \n \n \n \n Non-success \n \n \n \n Total \n \n \n \n \n \n Trt1 \n \n \n \n 63 \n \n \n \n 3 \n \n \n \n 66 \n \n \n \n \n \n Trt2 \n \n \n \n 56 \n \n \n \n 13 \n \n \n \n 69 \n \n \n \n \n \n \n \n \n data example; \n \n  length trt $8; \n \n  input trt $ success $ count; \n \n  datalines; \n \n  trt1 \n yes 63 \n \n  trt1 \n no 3 \n \n  trt2  yes 56 \n \n  trt2 no 13 \n \n ; \n \n proc freq\ndata=example; \n \n  weight count; \n \n  tables trt*success/measures nopercent nocol; \n \n  title 'outputs from SAS Proc Freq'; \n \n run; \n \n \n \n \n \n data agresti; \n \n  n11=63; \n \n  n21=56; \n \n  n1=66; \n \n  n2=69; \n \n  p1=n11/n1; \n \n  p2=n21/n2; \n \n  rr = p1/p2; \n \n  v = (1-p1)/(n1*p1) + (1-p2)/(n2*p2); \n \n  upper = exp(log(rr) - probit(0.025)*sqrt(v)); \n \n  lower = exp(log(rr) + probit(0.025)*sqrt(v)); \n \n run; \n \n proc print\ndata=agresti; \n \n  title \"using the formula from Agresti's\nbook\" \n \n run; \n \n \n \n \n \n data sasmanual; \n \n  n11=63; \n \n  n21=56; \n \n  n1=66; \n \n  n2=69; \n \n  p1=n11/n1; \n \n  p2=n21/n2; \n \n  rr = p1/p2; \n \n  v = (1-p1)/n11 + (1-p2)/n21; \n \n  upper = rr * exp(-probit(0.025)*sqrt(v)); \n \n  lower = rr * exp(probit(0.025)*sqrt(v)); \n \n run; \n \n \n \n proc print\ndata=sasmanual; \n \n  title \"using the formula from SAS\nmanual\"; \n \n run; \n \n \n \n I recently read a paper by Fischer et al . The\nconfidence interval for relative risk was constructed using a method by\nKoopman. In Koopman\u2019s paper \u201c Confidence\nIntervals for the Ratio of Two Binomial Proportions\u201d, a Chi-square method was\nproposed and the method required using numerical procedure and the iterative computations.\nThere is no SAS program available for the calculation using Koopman's method. \n \n \n \n There are other approaches proposed for computing confidence\nintervals for the ratio of two proportions. However, the method\nfor calculating the asymptotic confidence interval adopted in SAS Proc Freq is\ncommonly used.  \n \n \n \n Further reading: \n \n Applied\nStatistics: Proportions, risk ratios and odds ratios \n An\nIntroduction to Categorical Data Analysis by Agresti \n Confidence\nlimits for the ratio of two rates based on likelihood scores: non-iterative\nmethod \n Discussions\nin Google forum about \u201c Confidence interval for ratio of independent\nbinomial proportions \u201d \n Fagerland\n\u201c Recommended\nconfidence intervals for two independent binomial proportions\u201d  \n Dann\nand Koch \u201c Review and evaluation of methods for computing confidence intervals for\nthe ratio of two proportions and considerations for non-inferiority clinical\ntrials\u201d  \n Test\nfor comparing two proportions"], "link": "http://onbiostatistics.blogspot.com/feeds/2825409406234972633/comments/default", "bloglinks": {}, "links": {"http://smm.sagepub.com/": 1, "http://www.cliffsnotes.com/": 1, "http://www.tuc.gr/": 1, "http://feedads.doubleclick.net/": 2, "https://groups.google.com/": 1, "http://www-users.ac.uk/": 1, "http://en.wikipedia.org/": 1, "http://support.sas.com/": 2, "http://onbiostatistics.blogspot.com/": 1, "http://www.ki.se/": 1, "http://cat.sagepub.com/": 1, "http://www.researchgate.net/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["The multiplicity issue has evolved in last several years and a lot of new procedures have been proposed mainly in handing the issues encountered in the clinical trial and the drug development area. \n \n \nEMA/CHMP recently released a \" Concept paper on the need for a guideline on multiplicity issues in clinical trials \" for seeking the public comments. In the introduction of this concept paper, it mentioned some new procedures \n \n \n \n\u201cThe guideline is not to give advice on technical questions related to a new methodology. However, the increasing complexity of hypothesis frameworks and methods used may result in new issues and pose questions on general principles that haven\u2019t been considered before. These include consistency problems, the construction of simultaneous confidence intervals and the usefulness of newly developed methods e.g. gatekeeping and fallback procedures as well as graphical solutions in the regulatory context.\u201d \n \nIt is necessary to differentiate the differences among three new procedures for multiplicity adjustment: \n \n Gatekeeping procedure \n Fixed sequence procedure \n Fallback procedure \n \n \n All these three procedures are mainly designed to deal with the issue with multiple endpoints (including the primary endpoints and the secondary endpoints). \n \n \n \n Gatekeeping Procedure is used in the situation when there are multiple endpoints and these multiple endpoints are grouped into different families. For example, a clinical trial will typically have one or more primary endpoints (family for primary endpoints) and have multiple secondary endpoints (family for secondary endpoints). If there are many secondary endpoints, the secondary endpoints can be further divided into multiple secondary different families. With gatekeeping procedure, the families are tested in a sequential manner and the tests for subsequent families will be performed only if the tests for the previous family is significant. In other words, the families of hypotheses examined earlier serve as gatekeepers. While the term \u2018gatekeeping procedure\u2019 may not used, this approach has been implemented in many clinical trials, especially in the regulatory setting. It is very typical that the secondary endpoints will only be tested only if the primary endpoint is tested significantly. In this way, the alpha-level for primary efficacy endpoints will be tested at alpha=0.05 level and not be compromised due to the consideration of the secondary endpoints. \n \n The website http://multxpert.com/wiki/Gatekeeping_Procedures maintained by Alex Dmitrienko et al contains a lot of useful information about the gatekeeping procedures. \n A slide presentation by Branching tests in clinical trials with multiple objectives is helpful in understanding the gatekeeping procedure. \n The gatekeeping strategy is used in NDA 22-554 GI Drugs Advisory Committee Meeting NDA 22-554 Xifaxan (Rifaximin) where the secondary endpoints were grouped as \u201cKey Secondary Endpoints\u201d and \u201cOther Secondary Endpoints\u201d. Key secondary endpoints are those designated as most clinically important with pre-specified order for their analysis. P-values and confidence intervals for all other analyses are presented with NO adjustment for multiplicity. Nominal p-values and confidence intervals are consequently exploratory and cannot be used as a basis for efficacy claims in the product label if approved. \n \n \n Fixed Sequence Procedure is a stepwise multiple testing procedure that is constructed using a pre-specified sequence of hypotheses. When there are multiple endpoints, these endpoints can be ordered according to their importance. All tests will be performed at the 0.05 level following the pre-specified order. Once one hypothesis is tested not significantly, all subsequent tests will not be performed. The advantage and disadvantage of this testing procedure are obvious: power will be maximized as long as previous hypotheses are rejected, but minimized if a previous hypothesis is not rejected. Another drawback for this procedure is that the ordering of multiple hypotheses based on the clinical importance is subjective in nature. \n \n \n \n \n Fixed Sequence Procedure could be used under the umbrella of gatekeeping procedure for one specific family. In previous example of Xifaxan NDA, the gatekeeping procedure is used in general with considering of both primary and secondary endpoints, however the fixed sequence procedure is used in testing the key secondary endpoints. \n \n \n While it is not explicitly stated, fixed sequence procedure is actually mentioned in the EMA\u2019s  \u201c Points to consider on multiplicity issues in clinical trials \u201d that is issued in 2002. In the case of \u201ctwo or more primary variables ranked according to clinical relevance, no formal adjustment is necessary. However, no confirmatory claims can be based on variables that have a rank lower than or equal to that variable whose null hypothesis was the first that could not be rejected.\u201d \n \n \nA   g \nood reference paper is \u201c A flexible fixed-sequence testing method for hierarchically ordered correlated multiple endpoints in clinical trials \u201d by Huque and Alosh \n \n \n \n \n \n \n The Fallback Procedure is concepturely similar to a fixed sequence test, in which hypotheses are tested in an a priori order at the full alpha level. The difference of the fallback procedure from the fixed sequence test is that the full alpha of 0.05 is split for endpoints in a pre-specified order (based on the clinical relevance) and the hypotheses in late order can still be tested (but with different alpha levels) if the previous hypothesis is not rejected. To explain how the fallback procedure differs from the fixed-sequence procedure, we can use an example from a paper \u201c the Fallback procedure for evaluating a single family of hypotheses \u201d by Wiens and Dmitrienko. There are five endpoints with actual p-values of 0.010, 0.060, 0.0002, 0.0004, and 0.0268. With the fixed-sequence procedure, the endpoints #3, #4, and #5 will never be tested since the endpoint #2 is not significant. However, with the fallback procedure, the endpoints #3, #4, and #5 can still be tested (just at different alpha levels). \n \n \n \n \nIn order \n \nWith Fixed-sequence procedure \n \nWith fallback procedure* \n \n \nEndpoint #1 \n \n0.010 comparing to alpha=0.05 \n \n0.010 comparing to alpha=0.04 \n \n \nEndpoint #2 \n \n0.06 comparing to alpha=0.05 \n \n0.060 comparing to alpha=0.04 + 0.005 and result is not significant \n \n \nEndpoint #3 \n \nNot tested due to the endpoint #2 is not significant \n \n0.0002 comparing to alpha=0.002 and result is significant \n \n \nEndpoint #4 \n \nNot tested \n \n0.0004 comparing to alpha=0.002 + 0.002 and result is significant \n \n \nEndpoint #5 \n \nNot tested \n \n0.0268 comparing to 0.002+0.002+0.001 and result is not significant \n \n \n \n* five endpoints are given weights for their importance and alpha levels are assigned as 0.04, 0.005, 0.002, 0.002, and 0.001 (corresponding to 0.80, 0.10, 0.04, 0.04, and 0.02 of total alpha of 0.05) \n \n \n \n \n \n Additional References: \n \n Free online conference on multiplicity issues in confirmatory clinical trials \n Tree-structured gatekeeping tests in clinical trials"], "link": "http://onbiostatistics.blogspot.com/feeds/2021668390492328377/comments/default", "bloglinks": {}, "links": {"http://sprmm.com/": 1, "http://biopharmnet.com/": 1, "http://feedads.doubleclick.net/": 2, "http://multxpert.com/": 1, "http://www.tandfonline.com/": 1, "http://www.europa.eu/": 2, "http://www.amstat.org/": 1, "http://www.fda.gov/": 1, "http://www.sciencedirect.com/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["Converting SAS data sets to Excel Book \n In clinical trials, the database may be stored in SAS data set format. Sometimes, there is a need to convert multiple SAS data sets into an excel book. The following program can be easily modified to serve this purpose. With the small macro using Proc Export, SAS data sets can be converted into Excel book with multiple tabs (each tab is corresponding to a SAS data set). \n \nlibname aa \"c:\\Data\\CRF Data\\Final Data\\\"; \n \n%macro export(dst=); \nPROC EXPORT DATA= aa.&dst \n \n   OUTFILE= \"c:\\Data\\CRF Data\\Final Data\\excel\\ExcelData.xls\" \n \n    LABEL DBMS=xls REPLACE; \n \n SHEET=\"&dst\"; \nRUN; \n%mend; \n \n%export(dst=IE); \n%export(dst=DM); \n%export(dst=MH); \n%export(dst=VS); \n%export(dst=PE); \n%export(dst=CLAB); \n%export(dst=PREG); \n%export(dst=DRUG); \n%export(dst=AE); \n%export(dst=CM); \n%export(dst=COM); \n \nIn the macro above, the keyword 'LABEL' is important. With 'LABEL', the SAS variable label will be used as the column header. Without 'LABEL', the SAS variable name will be used as the column header. The keyword 'LABEL' must be placed before the keyword 'DBMS' in order to be effective. \nDBMS=xls indicates that the output data file will be an excel book (no version number is needed). other options for DBMS are csv, dlm, tab, jmp. Check SAS manual for detail. \n \n \n Converting Excel Book to SAS data sets \nThe opposite way is to convert the Excel book (with multiple table) into different SAS data sets. The program below can be modified to fulfill this task. \n \nlibname aa \"c:\\temp\\\"; \n \n%macro import(dst=); \nPROC IMPORT DATAFILE= \"C:\\Dengc\\ExcelData.xls\" OUT= aa.&dst \n   DBMS=xls REPLACE; \n  SHEET=\"&dst\"; \n  GETNAMES=YES; \nRUN; \n%mend; \n \n%import(dst=primary); \n%import(dst=secondary); \n%import(dst= tertiary); \n \nIn the macro above, GETNAMES=YES indicates that the first row from excel spreadsheet will be used as the SAS variable name. \n \n \n Other References: \n \n How do I read/write Excel files in SAS? \n SAS Proc Import \n SAS Proc Export"], "link": "http://onbiostatistics.blogspot.com/feeds/248334842473552879/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.ucla.edu/": 1, "http://support.sas.com/": 2}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["For a non-inferiority trial, after the non-inferiority is shown, one will typically try to show the non-inferiority. People may argue that the multiplicity adjustment arise in this situation. This can be seen in a presentation of \" Branching tests in clinical trials with multiple objectives \" by Alex Dmitrienko and Brian Wiens. In their presentation, the multiplicity adjustment is considered for switching from non-inferiority test to superiority test as part of the gatekeeping methods. \n \nHowever, the regulatory guidelines clearly stated that no multiplicity adjustment is needed when interpreting a non-inferiority trial as a superiority trial. In EMA's guidance \" Point to consider on switching between superiority and non-inferiority \", the following statement is stated: \n \n \"if the 95% confidence interval for the treatment effect no only lies entirely above -delta but also above zero then there is evidence of superiority in terms of statistical significance at the 5% level (p<0.05). In this case it is acceptable to calculate the p-value associated with a test of superiority and to evaluate whether this is sufficiently small to reject convincingly the hypothesis of no difference. There is no multiplicity argument that affects this interpretation because, in statistical terms, it corresponds to a simple closed test procedure. Usually this demonstration of a benefit is sufficient on its own, provided the safety profiles of the new agent and the comparator are similar....\" \nIn FDA's guidance \" Non-inferiority clinical trials \", similar statements are included: \n \n\"In some cases, a study planned as an NI study may show superiority to the active control. ICH E-9 and FDA policy has been that such a superiority finding arising in an NI study can be interpreted without adjustment for multiplicity . Showing superiority to an active control is very persuasive with respect to the effectiveness of the test drug, because demonstrating superiority to an active drug is much more difficult than showing superiority to placebo. Similarly, a finding of less than superiority, but with a 95% CI upper bound for C-T considerably smaller than M2, is also statistically persuasive.\" \n The multiplicity adjustment is now everywhere. It is good to know that there is no need to do the multiplicity adjustment in the situation of interpreting a non-inferiority study as a superiority."], "link": "http://onbiostatistics.blogspot.com/feeds/7409604572372848799/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.amstat.org/": 1, "http://www.europa.eu/": 1, "http://www.fda.gov/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["To be a good statistician, mastering the programming (SAS, SPSS, R, ...) is very important. This is especially true for statisticians who are working in the pharmaceutical/biotech/CRO industry. While the other statistical software may be popular in other area, SAS is still dominant in the pharmaceutical/biotech/CRO industry. \n \nFortunately, there are a lot of resources for SAS programming. The following links are worth being bookmarked. \n \n \n SAS support from SAS.com \n Resources to help you learn and use SAS provided by UCLA Academic Technology Services \n SAS library provided by UCLA Academic Technology Services \n CTSpedia: a knowledge base for clinical and translational research \n CTSpedia: SAS codes for graphic presentation of the safety data \n Biopharmaceutical network: containing presentation slides and SAS codes \n http://www.sas-resources.com/"], "link": "http://onbiostatistics.blogspot.com/feeds/2652909207806456374/comments/default", "bloglinks": {}, "links": {"http://sasresources.com/": 1, "http://biopharmnet.com/": 1, "http://feedads.doubleclick.net/": 2, "https://www.ctspedia.org/": 2, "http://support.sas.com/": 1, "http://www.ucla.edu/": 2}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["When performing the data analysis, sometimes the data is skewed and not normal-distributed, and the data transformation is needed. We are very familiar with the typically data transformation approaches such as log transformation, square root transformation. As a special case of logarithm transformation, log(x+1) or log(1+x) can also be used.  The first time I had to use log(x+1) transformation is for a dose-response data set where the dose is in exponential scale with a control group dose concentration of zero. The data set is from a so-called Whole Effluent Toxicity Test. The Whole Effluent Toxicity test , one of the aquatic toxicological experiments, has been used by the US Environmental Protection Agency (USEPA) to identify effluents and receiving waters containing toxic materials, and to estimate the toxicity of waster water. In the Whole Effluent Toxicity testing, many different species and several endpoints are used to measure the aggregate toxic effect of an effluent. For many of these biological endpoints, toxicity is manifested as a reduction in the response relative to the control group. The whole Effluent toxicity testing is often designed as multi-concentrations, and includes a minimum of five concentrations of effluent and one control group. Therefore, from a dose-response analysis standpoint, the control group dose is considered as zero and the various concentrations are designed in exponential scale. Prior to the analysis, the log transformation for the dose, log(x), is usually applied. Since the control group dose is considered zero and log(x) does not exist, an easy solution is to use log(x+1). For the control group, the log(0+1) = 0, which seems to be a perfect approach in this case.  However, in clinical trials, I have seen many applications of the log-transformation, but not the log(x+1) transformation. From the FDA website, I could only find one study where the log(1+x) transformation was used. In advisory committee meeting document for AZ\u2019s Drug Esomeprazole , the statistical analysis for the primary endpoint was stated as:  \n \n\"The primary endpoint, change from baseline in signs and symptoms of GERD observed from video and cardiorespiratory monitoring, was analyzed by ANCOVA. Prior to the analysis, the number of events at baseline and final visit were normalized (to correspond to 8 hours observation time) then log-transformed via a log(1+x) transformation . The ANCOVA of change from baseline on the log-scale was adjusted for treatment and baseline. The least square means (lsmeans) for each treatment group were transformed and expressed as estimated percentage changes from baseline, and the lsmean for the esomeprazole treatment effect was transformed similarly, and expressed as a percentage difference from placebo, which was presented with the associated 2-sided 95% CI and p-value. \" \n \n \n \n \n \n Recently I read an article by Lachin et al \u201c Sample size requirements for studies of treatment effects on Beta-cell function in newly diagnosed type 1 diabetes \u201d, where various data transformation techniques were compared and the log(x+1) and sqrt(x) (square root of x) were suggested for the primary endpoint of C-peptide AUC mean. According to the paper \u201cMost C-peptide values will fall between 0 and 1 and the distribution is positively skewed. Thus, scale-contracting transformations were considered. However, the log transformation could introduce negative skewness because log(x) approaches negative infinity as the value x approaches zero. This can be corrected by using log(x+1)\" When discussing with my friend, Dr Song, from CDC, we came up with the following Q&A regarding the use of log(x+1) transformation:  Q: Is log(x+1) a fine approach for data transformation?  A: it\u2019s fine to use ln(x+1) as long as this transformation makes data normal and variance relatively constant.  Q: Since the reason for using log(x+1) transformation is to avoid the log(x) approaching negative infinity as the x approaches zero. Could we change the measurement unit from pmol/mL to pmol/dL (1 pmol/dL = 100 pmol/mL)? A: Log(100x) = log(100) + log(x). It only makes transformed value positive and it does not change the normality and variability. From statistical point of view, it is the same or equivalent to the transformation of log(x).  Q: With log(x), square root of x,\u2026transformation, we can essentially transfer the calculated values or estimates back to the originally scale. With log(x+1), do we have a problem to convert the calculated values or estimates back to the original scale?  A: According to the paper by Lachin, \u201cFor each transformation y=f(x), the mean values and confidence limits are presented using the inverse transformation applied to the mean of the transformed values, and the corresponding confidence limits. Thus, for an analysis using y=log(x), the inverse mean is the geometric mean exp(mean y). For an analysis using y=log(x+1), the inverse mean is the geometric-like mean exp(mean y) - 1. For an analysis using y=sqrt(x), the inverse mean is (mean y)**2.\u201d  Q: Whether or not one transformation approach is better than another depending on the range of the values?  A: log(x+1) transformation is often used for transforming data that are right-skewed, but also include zero values. The shape of the resulting distribution will depend on how big x is compared to the constant 1. Therefore the shape of the resulting distribution depends on the units in which x was measured. In the C-peptide AUC mean situation, all transformations are similar at the higher level of x (mean = 0.04 at month 24), but at the lower value level of x (mean = 0.01 at month 12), sqrt(x) is better than log(x+1) and log(x+1) is better than ln(x). This can be easily understood from the curvature of transformations shown in the following graph. \n \n  Some additional notes about the use of log(x+1) transformation: \n \n Any base for the logarithm can be used, but base 10 is often used because of interpretability \n In addition to log(x+1), log(2x+1) or log(x+3/8) transformation may also be used \n    Remember to re-inspect the data after transformation to confirm its suitability. This will also be true no matter which data transformation approach is used."], "link": "http://onbiostatistics.blogspot.com/feeds/896659067661882526/comments/default", "bloglinks": {}, "links": {"http://www.res.in/": 1, "http://feedads.doubleclick.net/": 2, "http://www.plosone.org/": 1, "http://water.epa.gov/": 1, "http://4.blogspot.com/": 2, "http://www.fda.gov/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["\u201cTo cope with situations\nwhere data collection is interrupted before the predetermined last evaluation\ntimepoint, one widely used single imputation method is Last Observation Carried\nForward (LOCF). This analysis imputes the last measured value of the endpoint\nto all subsequent, scheduled, but missing, evaluations. \u201c For a study with\nclinical outcomes measured at multiple timepoints (repeated measures), if the\nendpoint analysis approach is used for the primary efficacy variable, the most\nconvenient and easy-to-understand imputation method is LOCF. In endpoint\nanalysis, the change from the baseline to the last measurement (at a fixed\ntimepoint such as at one year, at two year) is the dependent variable. \n \n the LOCF is the easiest imputation approach for missing data to\nbe understood by the non-statisticians. However, the LOCF approach has been the\ntarget for criticisms from the statisticians for its lack of \n a sound statistical foundation and for its biases in either direction\n(i.e., it is not necessarily conservative). After the National Academies\npublished its draft report \" The\nprevention and treatment of missing data in clinical trials \u201d, using LOCF\napproach seemed to be out-dated and markedly out of step with modern statistical\nthinking. \n \n \n \n Is the LOCF dead? Can we\nstill use this approach in some situations in some clinical trials? \n \n \n \n In reviewing some of\nthe regulatory guidance, I believe that the LOCF is not totally dead. While we\nacknowledge that the LOCF is not a perfect approach, the LOCF approach should\nnot be totally abandoned. In some situations, the LOCF approach is commonly\nagreed to be a more conservative approach and may be appropriate to be used. \n \n \n \n In EMEA\u2019s guidance\n\" Guideline\non missing data in confirmatory clinical trials \", opinions and\nexamples about the use of LOCF was explained: \n \n \n \n \n Only under certain\nrestrictive assumptions does LOCF produce an unbiased estimate of the treatment\neffect. Moreover, in some situations, LOCF does not produce conservative\nestimates. However, this approach can still provide a conservative estimate of\nthe treatment effect in some circumstances . \n \n To give some particular\nexamples, if the patient\u2019s condition is expected to deteriorate over time (for\nexample in Alzheimer\u2019s disease) an LOCF analysis is very likely to give overly\noptimistic results for both treatment groups, and if the withdrawals on the\nactive group are earlier (e.g. because of adverse events) the treatment\ncomparison will clearly provide an inappropriate estimate of the treatment\neffect and may be biased in favour of the test product. Hence in this situation\nan LOCF analysis is not considered appropriate. Indeed in Alzheimer\u2019s disease,\nand other indications for diseases that deteriorate over time, finding a method\nthat gives an appropriate estimate of the treatment effect will usually be\ndifficult and multiple sensitivity analyses will frequently be required. \n \n However, in other\nclinical situations (e.g. depression), where the condition is expected to\nimprove spontaneously over time, LOCF (even though it has some sub-optimal\nstatistical properties) might be conservative in the situations where patients\nin the experimental group tend to withdraw earlier and more frequently.\nEstablishing a treatment effect based on a primary analysis which is clearly\nconservative represents compelling evidence of efficacy from a regulatory\nperspective. \n \n \n \n \n Some\nof the FDA\u2019s guidance gave clear instructions on the use of the LOCF approach\nin handling the missing data, which is more surprising to me. \n \n \n \n In FDA\u2019s guidance on \u201c  Diabetes Mellitus: DevelopingDrugs and Therapeutic Biologics for Treatment and Prevention \u201d, I am surprised\nto see that the LOCF approach is actually suggested even though the LOCF\napproach may not be a conservative approach as indicated in the statements\nbelow since the HbA1c is expected to increase if the experimental drug is\neffective. \n \n \n Although every reasonable attempt\nshould be made to obtain complete HbA1c data on all subjects, dropouts are\noften unavoidable in diabetes clinical trials. The resulting missing data\nproblems do not have a single general analytical solution. Statistical analysis\nusing last observation carried forward (LOCF) is easy to apply and transparent\nin the context of diabetes trials . Assuming an effective investigational\ntherapy, it is often the case that more placebo patients will drop out early\nbecause of a lack of efficacy, and as such, LOCF will tend to underestimate the\ntrue effect of the drug relative to placebo providing a conservative estimate\nof the drug\u2019s effect. The primary method the sponsor chooses for handling\nincomplete data should be robust to the expected missing data structure and the\ntime-course of HbA1c changes, and whose results can be supported by alternative\nanalyses. We also suggest that additional analyses be conducted in studies with\nmissing data from patients who receive rescue medication for lack of adequate\nglycemic control. These sensitivity analyses should take account of the effects\nof rescue medication on the outcome. \n \n \n \n \n In\nFDA\u2019s guidance \u201c Developing\nProducts for Weight Management \u201d, the LOCF is also suggested even though it\nalso says \u2018repeated measures analyses can be used to analyze longitudinal\nweight measurements but should estimate the treatment effect at the final time\npoint. \n \n \n \n \n The analysis of (percentage)\nweight change from baseline should use ANOVA or ANCOVA with baseline weight as\na covariate in the model. The analysis should be applied to the last\nobservation carried forward on treatment in the modified ITT population defined\nas subjects who received at least one dose of study drug and have at least one\npost-baseline assessment of body weight. Sensitivity analyses employing other\nimputation strategies should assess the effect of dropouts on the results. The\nimputation strategy should always be prespecified and should consider the\nexpected dropout patterns and the time-course of weight changes in the\ntreatment groups. No imputation strategy will work for all situations,\nparticularly when the dropout rate is high, so a primary study objective should\nbe to keep missing values to a minimum. Repeated measures analyses can be used\nto analyze longitudinal weight measurements but should estimate the treatment\neffect at the final time point. Statistical models should incorporate as\nfactors any variables used to stratify the randomization. As important as\nassessing statistical significance is estimating the size of the treatment\neffect. If statistical significance is achieved on the co-primary endpoints,\ntype 1 error should be controlled across all clinically relevant secondary\nefficacy endpoints intended for product labeling.  \n \n \n \n \n This guidance was criticized by academics for several issues including\nthe use of LOCF approach for the primary efficacy analyses. In comments submitted by UAB and Duke , there were the following statements: \n \n \n \n \n While we strongly\nagree with the use of ITT approaches, we believe that the use of last observation carried\nforward (LOCF) is markedly out of step with modern statistical thinking.\nThis perhaps reflects the fact that the 2004 FDA advisory meeting addressing\nthis topic did not include a statistician with clinical trial expertise. A review\nof the video tapes referred to above will show that several leading statisticians\nall eschewed LOCF and suggested alternatives. These alternatives are now\nwell established 2 and available in major statistical packages. We have a paper\nnearing completion that compares the performance of these various approaches in\nmultiple real obesity trials and will be glad to share a copy with FDA upon\nrequest. LOCF does not have a sound statistical foundation and can be biased in\neither direction (i.e., it is not necessarily conservative). Our own work suggests that\nmultiple imputation may be the best method for conducting ITT analyses in\nobesity trials and that standard mixed models also work quite well in reasonably sized\nstudies. \n \n \n \n However, An\nadvisory committee meet material for NDA 022580 for QNEXA in 2012 indicated\nthat the LOCF approach is used for the primary efficacy analyses for weight management product developments. \n\n \n \n \n When the outcome variable is\ndichotomous (success/failure, responder/non-responder,\u2026), the LOCF is more\nacceptable if any subject who withdraw from the study early is considered as treatment failure or\nnon-responder. This approach may also be called \u2018the treatment failure imputation\u2019,\nwhich is the most conservative approach. This approach is suggested in FDA  Draft Guidance on Tacrolimus . In a recent NDA submission ( 202-736/N0001\nSklice (Ivermectin), topical cream, 0.5%, augmented Treatment of head lice\ninfestations), this approach is also used in handling the missing data for\nprimary efficacy analysis.  \n \n \n \n In the end, there is no perfect imputation approach if\nthe missing data occurs too often. During the clinical trial from the study design to\nprotocol compliance, to the data collection, every effort should be made to\nminimize the missing data. I think that the statements about the handling of\nmissing data is pretty clear and reasonable in FDA\u2019s  Draft\nGuidance for Industry and Food and Drug Administration Staff - The Content of\nInvestigational Device Exemption (IDE) and Premarket Approval (PMA)\nApplications for Low Glucose Suspend (LGS) Device Systems \n \n  Handling of Missing Data \nStarting at the study design stage and throughout the clinical trial, every\neffort should be made to minimize patient withdrawals and lost to follow-ups.\nPremature discontinuation should be summarized by reason for discontinuation\nand treatment group. For an ITT population, an appropriate imputation method\nshould be specified to impute missing HbA1c and other primary endpoints in the\nprimary analysis. It is recommended that the Sponsor/Applicant plan a\nsensitivity analysis in the protocol to evaluate the impact of missing data\nusing different methods, which may include but is not limited to per protocol,\n Last Observation Carry Forward (LOCF)  , multiple imputation, all missing as\nfailures or success, worst case scenario, best case scenario, tipping point,\netc."], "link": "http://onbiostatistics.blogspot.com/feeds/8748446693857299146/comments/default", "bloglinks": {}, "links": {"http://www.nap.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://www.blogger.com/": 1, "http://www.europa.eu/": 1, "http://www.fda.gov/": 6}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["In Clinical Pharmacology, inferential statistics is performed to show the bioequivalence in terms of the Area Under the Curve (AUC) and the Maximum Concentration (Cmax) that are obtained from the time-concentration data. The typical clinical trial design is 2x2x2 crossover design contains two treatment sequences (Test followed by Reference vs. Reference followed by Test), two treatment periods (period 1 vs period 2), and two treatment groups (Test vs. Reference). According to FDA\u2019s guidance \u201c Statistical Approaches to Establishing Bioequivalence \u201d, the following assumptions can be made for the test of bioequivalence: 1. AUC and Cmax follow log-normal distribution 2. Bioequivalence is shown if the 90% confidence interval for the geometric least square mean ratio of Test/Reference is fall within 0.8 and 1.25  The statistical tests will follow so called two one-sided tests procedure (TOST) which can be implemented using the following cookbook SAS codes.  \n* Preparing the data and log-transfer the AUC and Cmax data; \n \ndata pkparm;  set pdkparm;  keep subno seqence treat period AUC CMAX;  lauc=log(auc);  lcmax=log(cmax); run; *** Fit the ANOVA model; ods output LSMeans=lsmean; ods output estimates=est; proc mixed data=pd;  class seqence period treat subno;  model LAUC or Cmax=seqence period treat;  random subno(seqence);  lsmeans treat/pdiff cl alpha=0.1;  estimate 'T/R' treat -1 1 / cl alpha=0.1;  * make 'LSMEANS' out=lsmean; *used in old SAS versions;  * make 'estimate' out=est; *used in old SAS versions; run; * Anti-log transformation to obtain the Geometric Means, data lsmean;   set lsmean;  gmean=exp(estimate); *Geometric means; run; proc print data=lsmean; run; * Anti-log transformation to obtain the ratio of Geometric Means (point estimate) and its 90% confidence interval (lower and upper bounds); data diffs;  set EST;  ratio=exp(est); ** Ratio of geometric mean;  lower=exp(lower); ** 90% CI lower bound;  upper=exp(upper); ** 90% CI upper bound; run; proc print data=diffs; run; \n Some Notes:  1. p-value for Treatment/Reference comparison can also be obtained from the model (in above EST data set). However, p-value is not the criteria for declaring the bioequivalence and must be interpreted appropriately. We could have a significant p-value (p<0.05) and still show bioequivalence as long as the 90% confidence interval of the geometric mean ratio is fall within 0.8 and 1.25 range. If we have a 90% confidence interval like [0.85, 0.95] or [1.05, 1.15], the bioequivalence will be shown even though the p-values are significant.  2. In SAS Proc Mixed model, the subject within sequence is coded as subno(seqence), not sequence(subno). However, if you use sequence(subno), the results will be the same. 3. For log transformation, it does not matter which base (base 10, 5, or e (natural log)) as long as the final results from the model are correctly an-log transferred back. 4. while we typically say \u2018Ratio of geometric mean\u2019, it is actually the \u2018ratio of geometric least square mean\u2019 from the model."], "link": "http://onbiostatistics.blogspot.com/feeds/4052096619978753778/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.springerlink.com/": 1, "http://www.fda.gov/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["In my previous article \" Should the design and conduct of clinical trials be simplified? \", I discussed several FDA guidance that suggested that in several areas, the dada collections may be reduced and the clinical trial monitoring may need to switch to the risk-based approach instead of the current frequent on-site visits and 100% source data verification.  \n Coincidently, yesterday, Sens. Richard Burr and Tom Coburn introduced a new plan to reform the FDA - The \" PATIENTS' FDA\" Act . The patient' FDA act (if approved) will force FDA to be further transparent and to be mindful in requesting too much data from the pharmaceutical companies. For last several years, after several high-profile drug withdrawals (Vioxx, Avandia for example), FDA has swung to another extreme and become very conservative, which subsequently made the clinical trials more difficult to execute and drug development more costly. Perhaps, it is really not the FDA's intension, however, many of its staff/reviewers become too conservative. Instead of working with the industry to bring the new medications to the patients with the reduced cost and within the reasonable timeframe, some reviewers request the sponsors to collect data with no real justification and ask the sponsors to implement something that may just be for reviewer's own interest or opinion.  \n  Forbes published a good article as a companion to this bill. Here are some of the paragraphs from this article.  \n  More accountability for meeting drug-review deadlines . The FDA has been increasingly failing to meet its PDUFA-mandated deadlines for giving companies approval decisions on new drug applications. The PATIENTS\u2019 FDA Act would require the FDA to \u201creport [to Congress] on a deeper level detail with respect to the performance goals agreed to in the prescription drug, generic drug, and biosimilar user fee agreements,\u201d and hold individual reviewers accountable for their speed in reviewing applications.  stop forcing companies to do unnecessary and expensive busywork  . The bill\u2019s summary notes that \u201csome FDA reviewers request reams of additional information about a drug or device that is beyond the scope of data needed to meet the FDA\u2019s approval standard.\u201d The FDA will be required, under the bill, to \u201cdocument the scientific and regulatory rationale\u201d for such decisions, and review within one year \u201cthe costs and adoption of the least burdensome approaches to regulation.\u201d The bill would also codify the FDA\u2019s \u201ccommitment to improve on patient risk-benefit considerations\u2026to ensure accountability for fulfilling\u2026the user fee agreements.\u201d     Take more advantage of clinical trials in other countries  . The bill would require FDA to work with \u201cother specific regulatory authorities of similar standing\u201d to encourage uniform standards for clinical trials. (The Geneva-based International Conference on Harmonisation of Technical Requirements for Registration of Pharmaceuticals for Human Use, or ICH , performs many of these functions.) FDA will also be instructed to help sponsors \u201cminimize the need for duplication of clinical studies, preclinical studies or non-clinical studies.\u201d"], "link": "http://onbiostatistics.blogspot.com/feeds/5067000256368585396/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.forbes.com/": 1, "http://onbiostatistics.blogspot.com/": 1, "http://burr.senate.gov/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["As mentioned in one of FDA\u2019s guidance, \u201cin the past two decades, the number and complexity of clinical trials have grown dramatically. These changes create new challenges in clinical trial oversight such as increased variability in investigator experience, ethical oversight, site infrastructure, treatment choices, standards of health care, and geographic dispersion.\u201d. According to an article by Mr Getz titled \u201c The Heavy Burden of Protocol Design More complex and demanding protocols are hurting clinical trial performance and success \u201d, companies sponsoring clinical research have openly acknowledged that protocol design negatively impacts clinical trial performance and may well be the single largest source of delays in getting studies completed. When designing a clinical trial, sponsors often try to include too many endpoints and too many measures hoping that all of these endpoints (if results are good) will contribute to the overall evidence of the clinical efficacy. The sponsors attempt to collect a lot of information that is not must-to-have, but nice-to-have (for future data dredging, marketing, publications,\u2026). We want to do it all in one clinical study. This obviously increases the complexity of the study protocol that subsequently increases the length of the clinical trial, the cost of the clinical trial, and the quality (more protocol incompliance) of clinical trial data.  In terms of the conduct of the clinical trials, emphasis on the compliance of good clinical practice has resulted in perceptions that the clinical trial data must be 100% monitored and source-verified, all data programming and analysis must be independently validated, over-reporting adverse events must be requirement of the GCP compliance\u2026 Over the last two years, FDA has issues several guidance in an attempt to change these perceptions.  In FDA\u2019s new draft guidance \" Determining the extent of safety data collection needed in late stage premarket and post-approval clinical investigations \", it states that its intention is \u201cto assist clinical trial sponsors in determining the amount and types of safety data to collect in late-stage premarket and post-market clinical investigations for drugs or biological products, based on existing information about a product\u2019s safety profile.\u201d This new guidance addresses the circumstances in which it may be acceptable to acquire a reduced amount of safety information during clinical trials. In some situations, excessive data collection may be unnecessary and not helpful. FDA conducted a webinar on this topic and the webinar is free to the general public at: https://collaboration.fda.gov/p42291115/ \n The presentation slides can be accessed at: http://www.fda.gov/downloads/Drugs/UCM297332.pdf \n In Guidance for Industry \u201cOversight of Clinical Investigations \u2014 A Risk-Based Approach to Monitoring \u201d, FDA encourages the sponsors to implement the alternative clinical monitoring instead of the only way of on-site monitoring. The webinar and presentations slides can be found at http://www.fda.gov/Training/GuidanceWebinars/ucm277044.htm \n  \u201cFor major efficacy trials, companies typically conduct on-site monitoring visits at approximately four- to eight-week intervals,8 at least partly because of the perception that the frequent on-site monitoring visit model, with 100% verification of all data, is FDA\u2019s preferred way for sponsors to meet their monitoring obligations. In contrast, academic coordinating centers, cooperative groups, and government organizations use on-site monitoring less extensively. For example, some government agencies and oncology cooperative groups typically visit sites only once every two or three years to qualify/certify clinical study sites to ensure they have the resources, training, and safeguards to conduct clinical trials. FDA also recognizes that data from critical outcome studies (e.g., many National Institutes of Health-sponsored trials, Medical Research Council-sponsored trials in the United Kingdom, International Study of Infarct Survival, and GISSI), which had no regular on-site monitoring and relied largely on centralized and other alternative monitoring methods, have been relied on by regulators and practitioners. These examples demonstrate that use of alternative monitoring approaches should be considered by all sponsors, including commercial sponsors when developing risk-based monitoring strategies and plans\u201d   Many sponsors may be reluctant to adopt this guidance and stick with the status quo approach of frequent on-site visits with 100% verification of all data. They may be worried that loosing the clinical monitoring could incur the incompliance.   \n \n In Guidance for Clinical Investigators, Sponsors, and IRBs Adverse Event Reporting to IRBs \u2014 Improving Human Subject Protection , FDA advised the sponsors to report to IRB (and FDA presumably) the AE only if it were unexpected, serious, and would have implications for the conduct of the study, not all unanticipated AEs. The sponsor should analyze the unanticipated AEs before reporting.   \u201cthe practice of local investigators reporting individual, unanalyzed events to IRBs, including reports of events from other study sites that the investigator receives from the sponsor of a multi-center study\u2014often with limited information and no explanation of how the event represents an unanticipated problem\u2014has led to the submission of large numbers of reports to IRBs that are uninformative. IRBs have expressed concern that the way in which investigators and sponsors of IND studies typically interpret the regulatory requirement to inform IRBs of all \"unanticipated problems\" does not yield information about adverse events that is useful to IRBs and thus hinders their ability to ensure the protection of human subjects.\u201d   There are many other areas in clinical trial practices that can and should be simplified. For example, some protocols instruct investigators to record and report all untoward events that occur during a study as AEs/SAEs, which could include common symptoms of the disease under study and/or other expected clinical outcomes that are not study endpoints. Over-reporting of AE/SAEs can incur additional burdens and can dilute or obscure signal identification. Another example is to spend too much efforts on the screening failure subjects. It is true that the recording of the adverse events starts once the informed consent form is signed. However it is unnecessary to write a full-blown SAE narrative for a screening failure subject that has nothing to do with the assessement of the safety of the experimental product."], "link": "http://onbiostatistics.blogspot.com/feeds/6994543402715218295/comments/default", "bloglinks": {}, "links": {"http://www.appliedclinicaltrialsonline.com/": 1, "https://collaboration.fda.gov/": 1, "http://feedads.doubleclick.net/": 2, "http://www.fda.gov/": 5}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["FDA has recently issued a series of draft guidance on \u201cproviding regulatory submission in electronic format\u201d. The most recent one is about \u201c providing regulatory submissions in electronic format \u2013 standardized study data \u201d. We have heard a lot of discussions about CDISC, SDTM, ADaM,\u2026While these data standards are not mandated yet, FDA is encouraging the submission of the electronic data in CDISC compliant format. The draft guidance about \u2018standardized study data\u2019 is another sign that the industry should move toward the compliance of the CDISC standards for the submission. \n In terms of what kinds of data standards to be used in electronic submissions, please see FDA\u2019s web pages on study data standards . CBER may have a little bit different requirements from other divisions. CBER has its own resource page for submission of data in CDISC format to CBER  While CDISC standards may be good for FDA reviewers and may accelerate the review process, it will indeed add burdens to the industry. For the original clinical datasets, there will be additional mapping to be implemented in order to prepare the datasets in SDTM compliant formats. SDTM format is not user-friendly and sometimes it is several steps to link back to the CRF/eCRFs.  Other readings: Providing Regulatory Submissions in Electronic Format \u2014 General Considerations Regulatory Submissions in Electronic Format for Biologic Products \n CDER Common Data Standards Issues Document issued last month (Version 1.1/December 2011) \n CBER Process for Planning and Accepting CDISC SDTM and ADaM Formatted Submissions in CBER \n Assessment of the Impact of the Electronic Submission and Review Environment on the Efficiency and Effectiveness of the Review of Human Drugs \u2013 Final Report \n Assessment of the Impact of the Electronic Submission and Review Environment on the Efficiency and Effectiveness of the Review of Human Drugs \u2013 Final Report"], "link": "http://onbiostatistics.blogspot.com/feeds/2618770326820622326/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.fda.gov/": 9}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["A colleague of mine asked me to explain the concept of \u201cfutility analysis\u201d using the plain languages. The question is triggered by the recent news such as \u201c Solanezumab, Gammagard Trials Survive Futility Analysis \u201d In an Alzheimer trial, it even says from the futility analysis, \u201c there is greater than a 20% statistical probability of success in achieving the primary outcome measure of cognitive function preservation.  \n  During a clinical trial, we can perform interim analysis (or DMC, DSMB review) for three different reasons:   Interim analysis for safety  \n  1)   with pre-specified stopping rule (for example stop the trial if we see # of cases of Serious Adverse Events)   2)   without pre-specified stopping rule (rely on DMC members to review the overall safety)  \n  Interim analysis for efficacy: To see if the new treatment is overwhelmingly better than control - then stop the trial for efficacy \n Interim analysis for futility: To see if the new treatment is unlikely to beat the control \u2013 then stop the trial for futility - this is called \u2018futility analysis\u2019.  \n \n  In situations 2 and 3, the criteria for stopping rule for efficacy could be different from the stopping rule for futility, but need to be pre-specified.  \n  An example for futility analysis: in the beginning of the trial, we assumed 65% successful rate for new treatment group and 50% successful rate for control group. We would like to establish the superiority. In the middle of the study, we did an interim analysis. The interim analysis showed 55% successful rate for new treatment group and 50% successful rate for control group. Based on the results from interim analysis, we can calculate the probability and conditional power: if we continue to finish the study, what is the probability of new treatment group better than control? If this probability is too small and meet the pre-specified criteria, we would stop the trial for futility. If this probability is reasonable, we can continue the trial as pre-planned or we can continue the trial with the sample size adjustment (typically increase due to the smaller effect size).  \n  In a paper by Miller et al \u201c Paclitaxel plus Bevacizumab versus Paclitaxel Alone for Metastatic Breast Cancer \u201d, one pre-planned interim analysis and two additional interim analyses were performed and three stopping rules (for safety, for efficacy, and for futility) were pre-specified and evaluated. It is reasonable to assume that none of these stopping rules was triggered since the study was not stopped.   \n  Futility analysis or stopping the trial for futility is not without controversy. An article by Schoenfeld and Meade discussed this issue. See \u201c Pro/con clinical debate: It is acceptable to stop large multicentre randomized controlled trials at interim analysis for futility \u201d.  \n Further readings:  Snapinn et al (2006)  Assessment of futility in clinical trials   \n  Lachin (2005)  A review of methods for futility stopping based on conditional power  (or free access from  here )"], "link": "http://onbiostatistics.blogspot.com/feeds/1447751794948800707/comments/default", "bloglinks": {}, "links": {"http://www.nejm.org/": 1, "http://feedads.doubleclick.net/": 2, "http://onlinelibrary.wiley.com/": 2, "http://www.nih.gov/": 1, "http://www.unimib.it/": 1, "http://www.alzforum.org/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["Here is a question posted on the web about the interpretation of odds ratios that are less than 1. \"I know that OR estimates= 1 mean that both groups/categories have the same odds. I also know that if OR estimates are greater than 1, e.g, 1.24 for Young vs. Old persons, then I can say: Young people have 24% increase in the odds of living in an apartment than older people. Or, I also know I can say, for example, for an OR of 0.322 Non-White vs. White, that the odds of Whites are 1/.322 = about 3 times higher than those of Non-Whites, to live in a house they own. Now, how would I say the odds are of a NON-White person in the example above, to live in a house they own? Is is 1-.322=.678 less likely, with respect to odds, to live in a house they own? Or, similarly, they have 67.8% lower odds to live in a house they own? \" If we have to say the odds for a Non-White person, we may say \"Non Whites have odds .322 times as great as those of Whites\". \n In an article \"When can odds ratio misled?\", Davies et al stated: \"the odds of an event is the number of those who experience the event divided by the number of those who do not. It is expressed as a number from zero (event will never happen) to infinity (event is certain to happen). Odds are fairly easy to visualise when they are greater than one, but are less easily grasped when the value is less than one. Thus odds of six (that is, six to one) mean that six people will experience the event for every one that does not (a risk of six out of seven or 86%). An odds of 0.2 however seems less intuitive: 0.2 people will experience the event for every one that does not. This translates to one event for every five non-events (a risk of one in six or 17%). \" Another webblog described the issue in interpreting the odds ratio that is less than one. \"When you are interpreting an odds ratio (or any ratio for that matter), it is often helpful to look at how much it deviates from 1. So, for example, an odds ratio of 0.75 means that in one group the outcome is 25% less likely. An odds ratio of 1.33 means that in one group the outcome is 33% more likely.\" In an article \" The odds ratio: calculation, usage, and interpretation\" in Biochemia Medica , the author clear suggest converting the odds ratio to be greater than 1 by arranging the higher odds of the evnet to avoid the difficulties in interpreting the odds ratio that is less than 1. \u201cAn OR of less than 1 means that the first group was less likely to experience the event. However, an OR value below 1.00 is not directly interpretable. The degree to which the first group is less likely to experience the event is not the OR result. It is important to put the group expected to have higher odds of the event in the first column. It is not valid to try to determine how much less the first group\u2019s odds of the event was than the second group\u2019s. When the odds of the first group experiencing the event is less than the odds of the second group, one must reverse the two columns so that the second group becomes the first and the first group becomes the second. Then it will be possible to interpret the difference because that reversal will calculate how many more times the second group experienced the event than the first. If we reverse the columns in the example above, the odds ratio is: (5/22)/(45/28) = (0.2273/1.607) = 0.14 and as can be seen, that does not tell us that the new drug group died 0.14 times less than the standard treatment group. In fact, this arrangement produces a result that can only be interpreted as \u201cthe odds of the first group experiencing the event is less than the odds of the second group experiencing the event\u201d. The degree to which the first group\u2019s odds are lower than that of the second group is not known.\u201d In practice, when dealing with the odds ratio less than 1, when possible, I almost always try to reverse the column or recode the response variable to get the odds ratio larger than 1 before I do an interpretation. It is easier for people (especially non-statisticians) to understand the odds ratio with the value greater than 1. \n In an example below, the treatment group is actually less effective in terms of the response. \n  \n Treatment Failure (0) Success (1) \n No (0) 21 30 \n Yes (1) 32 17 \n \n \nThe following SAS code can be easily used to calculate the odds ratio: \nData test; input Trt resp count; datalines; \n1 1 17 \n1 0 32 \n0 1 30 \n0 0 21 \n; \nproc logistic data=test descending; weight count; model resp=trt; \nrun; \n \nFrom the SAS outputs, we get the odds ratio of 0.372, which indicates that the treatment group has odds 0.372 times lower compared to the non-treatmetn group in terms of the success. The interpretation is somewhat difficult to understand. \n \nThe program can be easily revised to calculate the odds ratio of failure rate, which gives an odds ratio of 1/0.372 = 2.689. The odds ratio can be intepretated as \"the odds of achieve the success in non-treatment group is 2.689 times higher than that in treatment group\". \n \nproc logistic data=test; weight count; model resp=trt; \nrun; \n \nIn SAS PROC Logistic, with descending option, probability modeled is response=1 (success); without descending option, probability modeled is response=0 (failure);"], "link": "http://onbiostatistics.blogspot.com/feeds/5810886200698933518/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.nih.gov/": 1, "http://www.biochemia-medica.com/": 1, "http://www.childrensmercy.org/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["For bioequivalence studies, it is often for us to show the average bioequivalence by declaring the bioequivalence if the 90% confidence interval of the geometric least squares mean ratio is within 80-125%. The associated study design is typically 2x2x2 cross over design with reasonable sample size (for example, 12 subjects, 24 subjects,\u2026) if the within subject variable is not so big. This approach has been outlined in several FDA\u2019s guidelines: \n -    Statistical Approaches to Establishing Bioequivalence  -     Bioavailability and Bioequivalence Studies for Orally Administered Drug Products \u2014 General Considerations   -    Food-Effect Bioavailability and Fed Bioequivalence Studies  \n Recently, there are a lot of discussions about the bioequivalence studies for a product with high variability (high variable drugs). Highly Variable Drugs refer to the type of drugs with higher within subject variability and is Defined as one for which the root mean square error (RMSE) from the ANOVA bioequivalence analysis > 0.3 for either AUC or Cmax. \n For highly variable drugs, if we employ the common study design, the required sample size will be very large, which will cause the ethic concerns to implement such studies.  \n \n FDA had several advisory committee meeting in discussing this issue. The most recent meetings were in 2004 and 2009 . In 2009 meeting, the slide presentation by Dr Conner from FDA summarized the development in dealing with this issue and FDA\u2019s position (see slide presentation \u201c Bioequivalence Methods for Highly Variable Drugs and Drug Products \u201d) .  \n Among various approaches to address the bioequivalence issue for highly variable drugs, reference-scaled average BE approach has been suggested . This approach requires less subjects in the study, but with replicated treatment design such as three-period, reference- replicated, crossover design with sequences of TRR, RTR, & RRT or four-period design with sequences of TRTR and RTRT. The replicated crossover designs were also discussed in FDA guidance \u201cStatistical Approaches to Establish Bioequivalance\u201d, but was for dealing with the carryover effects. Here, the replicated crossover designs are for dealing with highly variable drugs.  \n The implementation of the reference-scaled average BE approaches have been detailed and discussed in FDA guidance (draft) many publications. The most relevant ones are:  -    FDA Guidance on Progesterone (2011) -    Sample Sizes for Designing Bioequivalence Studies for Highly Variable Drugs by Endrenyi and Tothfalusi (2012) -    Bioequivalence of Highly Variable Drugs: A Comparison of the Newly Proposed Regulatory Approaches by FDA and EMA by Karalis et al (2011) \n   \n  Other Readings: \n -    Bioequivalence Approaches for Highly Variable Drugs and Drug Products by Haidar et al -    Generic Drug Bioequivalence by Dr Aaron Sigler  -    An Example of How to Write the Statistical Section of a Bioequivalence Study Protocol for FDA Review  -    Global Bioequivalence Guidelines  -    Using SAS Proc Power to Perform Model-based Power Analysis for Clinical Pharmacology Studies"], "link": "http://onbiostatistics.blogspot.com/feeds/4874973758248500331/comments/default", "bloglinks": {}, "links": {"http://bebac.at/": 1, "http://www.pharmasug.org/": 1, "http://feedads.doubleclick.net/": 2, "http://biostats.bepress.com/": 1, "http://www.pharmagateway.net/": 1, "http://ejournals.ualberta.ca/": 1, "http://www.pharmacybenefitsacademy.com/": 1, "http://www.springerlink.com/": 1, "http://www.fda.gov/": 8}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["In clinical trials, death event may occur. According to CDISC CDASH standards , death should not be recorded as an adverse event (AE) or serious AE, but should be recorded as the outcome of the event. The condition that resulted in the death should be recorded as the AE/SAE. \n \nIn the case of multiple AE/SAEs contributing to the fatal (death) outcome, there seem to be two different ways in recording the AE outcome. There is no clear regulatory guideline in detail about this situation. The most closely related guideline may be from the ICH E2B where it states: \n \n B.2.i.8 Outcome of reaction/event at the time of last observation \n recovered/resolved \n recovering/resolving \n not recovered/not resolved \n recovered/resolved with sequelae \n fatal \n unknown \n \n User Guidance: \nIn case of irreversible congenital anomalies the choice, not recovered/not resolved should be used. \nFatal should be used when death is possibly related to the reaction/event. Considering the difficulty of deciding between \"reaction/event caused death\" and \"reaction/event contributed significantly to death\", both were grouped in a single category. Where the death is unrelated, according to both the reporter and the sender, to the reaction/event, death should not be selected here, but should be reported only under section B.1.9. In practice, one way is to record multiple SAEs and record 'fatal' for each of these SAEs. The drawback of this approach is to have multiple SAEs with 'fatal' outcome for the same subject while subject can only die once. \n \nAnother way is to identify one SAE as the principal cause of the death. In this case, only will one SAE have the outcome recorded as Fatal. The subject can only die once so it makes sense to record \u2018fatal\u2019 as the outcome for the principal event The question is that what the appropriate outcome should be for other SAEs that may also contribute to the death event. If there is an 'Ongoing' option in the list of AE outcomes, the appropriate choice may be 'ongoing' indicating that the SAE is ongoing during the time of death. If there is no choice of 'ongoing' in the AE outcome list (as specified in E2B above), the most appropriate choice seems to be \"not recovered/not resolved\" indicating that the SAE is still not resolved during the time of death. For AE stop time, the principal SAE with fatal outcome should be the time of death. For other SAEs contributing to death, the stop time may be appropriately recorded as 'ongoing' instead of recording the death time as the SAE stop time."], "link": "http://onbiostatistics.blogspot.com/feeds/5155468489336929571/comments/default", "bloglinks": {}, "links": {"http://www.ich.org/": 1, "http://feedads.doubleclick.net/": 2, "http://www.cdisc.org/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["Professionals who are new to the clinical trial field are often confused with the concept of ' Serious Adverse Events (SAEs)' and ' Severe Adverse Events\". Severity is not synonymous with seriousness. SAE is based on patient/event outcome or action criteria usually associated with events that pose a threat to a patient's life or functioning. Seriousness (not severity) serves as a guide for defining regulatory reporting obligations. In other words, the SAEs need to be filfill additional reporting process (reported to corporate global drug safety group or pharmacovigilence group, regulatory authorities, EC/IRBs). Severe AE is one class of AEs with severity (old term intensity) classified as 'severe'. Severe AE is one of the AE classifications \u2013 AE severity (other classifications are relationships/causality). \n \n  The FDA defines a serious adverse event (SAE) as one when the patient outcome is one of the following: Death \n Life-threatening \n Hospitalization (initial or prolonged) \n Disability - significant, persistent, or permanent change, impairment, damage or disruption in the patient's body function/structure, physical activities or quality of life. \n Congenital anomaly \n Requires intervention to prevent permanent impairment or damage \n On the other hand, Severity of an AE is a point on an arbitrary scale of intensity of the adverse event in question. The terms \"severe\" and \"serious\" when applied to adverse events are technically very different. They are easily confused but can not be used interchangeably, require care in usage.  A headache is severe, if it causes intense pain. There are scales like \"visual analog scale\" that help us assess the severity. On the other hand, a headache is not usually serious (but may be in case of subarachnoid haemorrhage, subdural bleed, even a migraine may temporally fit criteria), unless it also satisfies the criteria for seriousness listed above. Similarly, a severe rash is not likely to be an SAE. However, mild chest pain may result in a day\u2019s hospitalization and thus is an SAE.  Classifications of the AE sevirity often include the following:  Mild: Awareness of signs or symptoms, but easily tolerated and are of minor irritant type causing no loss of time from normal activities. Symptoms do not require therapy or a medical evaluation; signs and symptoms are transient. \n Moderate: Events introduce a low level of inconvenience or concern to the participant and may interfere with daily activities, but are usually improved by simple therapeutic measures; moderate experiences may cause some interference with functioning \n Severe: Events interrupt the participant\u2019s normal daily activities and generally require systemic drug therapy or other treatment; they are usually incapacitating \n The guidelines for AE severity assessment is based on: \n Common Terminology Criteria for Adverse Events (CTCAE) CTC criteria, 4.03) \n It is also available as iPhone App. Here is the link . \n Guidance for Industry: Toxicity Grading Scale for Healthy Adult and Adolescent Volunteers Enrolled in Preventive Vaccine Clinical Trials \n Other sources about this topic may be useful: Wikipedia \n HHS The Office for Human Research Protections (OHRP): Guidance on Reviewing and Reporting Unanticipated Problems Involving Risks to Subjects or Others and Adverse Events \n NIH National Institute of Aging: Adverse Event and Serious Adverse Event Guidelines"], "link": "http://onbiostatistics.blogspot.com/feeds/5931129634929154664/comments/default", "bloglinks": {}, "links": {"http://www.hhs.gov/": 1, "http://feedads.doubleclick.net/": 2, "http://evs.nih.gov/": 1, "http://itunes.apple.com/": 1, "http://www.nih.gov/": 1, "http://en.wikipedia.org/": 1, "http://www.fda.gov/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}, {"content": ["In clinical trials, the serious adverse event reporting is critical to the safety assessment and to fulfill the regulatory requirements. The criteria for defining an SAE have been documented in many regulatory guidelines. However, in clinical trial implementation, the confusion could arise whether or not an event should be reported as an SAE or outcome of an SAE. Misinterpretation of the regulatory guidelines could cause in the inappropriate reporting of SAEs. \n \nAcording to ICH E2A \u201c CLINICAL SAFETY DATA MANAGEMENT: DEFINITIONS AND STANDARDS FOR EXPEDITED REPORTING \u201d \n \n     A serious adverse event (experience) or reaction is any untoward medical occurrence that at any dose:       * results in death, \n       * is life-threatening, \n        NOTE: The term \"life-threatening\" in the definition of \"serious\" refers to an event in which the patient was at \n        risk of death at the time of the event; it does not refer to an event which hypothetically might have caused death \n        if it were more severe. \n       * requires inpatient hospitalisation or prolongation of existing hospitalisation, \n       * results in persistent or significant disability/incapacity, or \n       * is a congenital anomaly/birth defect. \n FDA website has provided a little bit more detail descriptions on SAE \n  \"What is a Serious Adverse Event? \n \nAn adverse event is any undesirable experience associated with the use of a medical product in a patient. The event is serious and should be reported to FDA when the patient outcome is: \nDeath \n \nReport if you suspect that the death was an outcome of the adverse event, and include the date if known. \nLife-threatening \n \nReport if suspected that the patient was at substantial risk of dying at the time of the adverse event, or use or continued use of the device or other medical product might have resulted in the death of the patient. \nHospitalization (initial or prolonged) \n \nReport if admission to the hospital or prolongation of hospitalization was a result of the adverse event. \n \nEmergency room visits that do not result in admission to the hospital should be evaluated for one of the other serious outcomes (e.g., life-threatening; required intervention to prevent permanent impairment or damage; other serious medically important event). \nDisability or Permanent Damage \n \nReport if the adverse event resulted in a substantial disruption of a person's ability to conduct normal life functions, i.e., the adverse event resulted in a significant, persistent or permanent change, impairment, damage or disruption in the patient's body function/structure, physical activities and/or quality of life. \nCongenital Anomaly/Birth Defect \n \nReport if you suspect that exposure to a medical product prior to conception or during pregnancy may have resulted in an adverse outcome in the child. \nRequired Intervention to Prevent Permanent Impairment or Damage (Devices) \n \nReport if you believe that medical or surgical intervention was necessary to preclude permanent impairment of a body function, or prevent permanent damage to a body structure, either situation suspected to be due to the use of a medical product. \nOther Serious (Important Medical Events) \n \nReport when the event does not fit the other outcomes, but the event may jeopardize the patient and may require medical or surgical intervention (treatment) to prevent one of the other outcomes. Examples include allergic brochospasm (a serious problem with breathing) requiring treatment in an emergency room, serious blood dyscrasias (blood disorders) or seizures/convulsions that do not result in hospitalization. The development of drug dependence or drug abuse would also be examples of important medical events.\" \n \nThe standard coding dictionary for adverse events is MedDRA (Medical Dictionary for Regulatory Activities). The guidance document MedDRA\u00ae TERM SELECTION: POINTS TO CONSIDER gives clear explanation how death and other patient outcomes should be handled. \n \n 3.2 \u2013 Death and Other Patient Outcomes \n \nDeath, disability, and hospitalization are considered outcomes in the context of safety reporting and not usually considered ARs/AEs. Outcomes are typically recorded in a separate manner (data field) from AR/AE information. A term for the outcome should be selected if it is the only information reported or provides significant clinical information. \n \n(For reports of suicide and self-harm, see Section 3.3). \n \n3.2.1 Death with ARs/AEs \n \nDeath is an outcome and not usually considered an AR/AE. If ARs/AEs are reported along with death, select terms for the ARs/AEs. Record the fatal outcome in an appropriate data field. \n \n \n3.2.4 Other patient outcomes (non-fatal) \n \nHospitalization, disability and other patient outcomes are not generally considered ARs/AEs. \n There are many other examples in terms of recording the outcome instead of AE/SAE. Adverse events represent the untoward medical event, not the intervention to treat that event. For example, if a subject has appendectomy, the AE is appendicitis not the surgical procedure; if a subject has an limb amputation, the AE is the cause for amputation (perhaps, the worsening of the ischemia in the peripheral artery) and limb amputation should be reported as the outcome of the AE/SAE; If a patient is hospitalized due to congestive heart failure, congestive heart failure should be reported SAE and hospitalization should be reported as an outcome for congestive heart failure.  We should also be aware that not every hospitalization will have an associated SAE to be reported. Any AE leading to hospitalization or prolongation of hospitalization meets ONE of the followings should not be reported as SAE. A hospitalization admission is pre-planned (ie, elective or scheduled surgery arranged prior to the start of the study). European Commission\u2019s guidelines on medical devices \u201c CLINICAL INVESTIGATIONS: SERIOUS ADVERSE EVENT REPORTING \u201c indicated that a planned hospitalization for pre-existing condition, or a procedure required by the Clinical Investigation Plan, without a serious deterioration in health, is not considered to be a serious adverse event. \n A hospitalization admission is clearly not associated with an AE (eg, social hospitalization for purposes of respite care). If a patient wants to be stay in the hospital during the drug treatment because of the fear that something bad could happen, this should not be reported as SAE just because of the hospital stay if nothing else happens \n According to these definitions, the events with outcome of death, hospitalization, disability or permanent damage, congenital anomaly/birth defect, \u2026 should be reported as SAE while death, hospitalization, disability or permanent damage, congenital anomaly/birth defect\u2026should be reported as the outcome of the corresponding SAE. To be crystal clear, the Death, Hospitalization should not be reported as SAE and the causes leading to the death and hospitalization should be reported as SAE."], "link": "http://onbiostatistics.blogspot.com/feeds/4772674262919648550/comments/default", "bloglinks": {}, "links": {"http://www.ich.org/": 1, "http://feedads.doubleclick.net/": 2, "http://ec.europa.eu/": 1, "https://meddramsso.com/": 1, "http://www.fda.gov/": 1}, "blogtitle": "On Biostatistics and Clinical Trials"}]