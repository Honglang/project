[{"blogurl": "http://terrytao.wordpress.com\n", "blogroll": [], "title": "What's new"}, {"content": ["If is a connected topological manifold , and is a point in , the (topological) fundamental group of at is traditionally defined as the space of equivalence classes of loops starting and ending at , with two loops considered equivalent if they are homotopic to each other. (One can of course define the fundamental group for more general classes of topological spaces, such as locally path connected spaces, but we will stick with topological manifolds in order to avoid pathologies.) As the name suggests, it is one of the most basic topological invariants of a manifold, which among other things can be used to classify the covering spaces of that manifold. Indeed, given any such covering , the fundamental group acts (on the right) by monodromy on the fibre , and conversely given any discrete set with a right action of , one can find a covering space with that monodromy action (this can be done by \u201ctensoring\u201d the universal cover with the given action, as illustrated below the fold). In more category-theoretic terms: monodromy produces an equivalence of categories between the category of covers of , and the category of discrete -sets.\n \n \nOne of the basic tools used to compute fundamental groups is van Kampen\u2019s theorem :\n \n Theorem 1 (van Kampen\u2019s theorem) Let be connected open sets covering a connected topological manifold with also connected, and let be an element of . Then is isomorphic to the amalgamated free product . \n \n \nSince the topological fundamental group is customarily defined using loops, it is not surprising that many proofs of van Kampen\u2019s theorem (e.g. the one in Hatcher\u2019s text ) proceed by an analysis of the loops in , carefully deforming them into combinations of loops in or in and using the combinatorial description of the amalgamated free product (which was discussed in this previous blog post ). But I recently learned (thanks to the responses to this recent MathOverflow question of mine ) that by using the above-mentioned equivalence of categories, one can convert statements about fundamental groups to statements about coverings. In particular, van Kampen\u2019s theorem turns out to be equivalent to a basic statement about how to glue a cover of and a cover of together to give a cover of , and the amalgamated free product emerges through its categorical definition as a coproduct , rather than through its combinatorial description. One advantage of this alternate proof is that it can be extended to other contexts (such as the \u00e9tale fundamental groups of varieties or schemes) in which the concept of a path or loop is no longer useful, but for which the notion of a covering is still important. I am thus recording (mostly for my own benefit) the covering-based proof of van Kampen\u2019s theorem in the topological setting below the fold.\n \n \n \n \n \n \u2014 1. Proof of van Kampen theorem \u2014 \n \nThe proof of van Kampen\u2019s theorem boils down (after using the above-mentioned equivalence of categories between covers of a manifold , and sets with an action of the fundamental group) to the following fact about covers:\n \n Proposition 2 (Gluing of covers)  Let be connected open sets covering a connected topological manifold with also connected, and let be an element of . If and are covering maps which become isomorphic upon restricting the base to , then there is a covering map which becomes isomorphic to on restricting the base to , and isomorphic to on restricting the base to (and with all four isomorphisms forming a commuting square). \n \n \nThis proposition is easily verified by gluing together and as topological spaces along the indicated isomorphism between and , and checking that the resulting space is still a covering space.\n \n \nNow we can prove van Kampen\u2019s theorem. Suppose that one has group homomorphisms , to a target group which form a commuting square with the canonical homomorphisms from to and . It will suffice to show that there is a unique homomorphism such that factors as the composition of with the canonical homomorphism from to for .\n \n \nWe first demonstrate existence. Let be a universal cover for , thus is simply connected, and if we pick a base point , then every other point in that fibre there is a unique element for which , where is the (right) action of by monodromy on . This gives a left action of on by deck transformations for each , which maps to for any : \n \n \nThe fibres of the universal cover are copies of . We can now form a new cover whose fibres are copies of , by first forming the Cartesian product (which still covers ) and then quotienting out by the equivalence for all , , . This is a (possibly disconnected) covering space for , whose fibre above can be identified with by identifying with the equivalence class . The monodromy (right) action of on this fibre is then identified with the right action of on induced by . Furthermore, the group acts on on the left by deck transformations, with each mapping to .\n \n \nSimilarly, we can form a covering of whose monodromy action of on the fibre can be identified with the right action of on induced by . When both of these covering spaces are restricted to , the monodromy action of on the fibre above are then isomorphic to each other, and thus the two restrictions are isomorphic to each other too. By Proposition 2 , we can then glue these two covers together to obtain a cover which is isomorphic to or upon restricting the base to or respectively, with all four isomorphisms forming a commuting square; in particular, the fibre is still identified with . The monodromy right action of on then restricts to the previously described action of on and of on . Also, because acted on the left by deck transformations on both and , in a manner which can be seen to be compatible with the isomorphism on restriction to , continues to act by deck transformations on the gluid cover . As monodromy actions commute with deck transformations, we conclude that the right action of an element of on is given by right multiplication by an element of . It is then routine to verify that is a homomorphism with the required properties.\n \n \nNow we prove uniqueness. It suffices to show that is generated as a group by the images of and . Suppose this were not the case, so that the images of and generate a proper subgroup of . Let be a universal cover of , so that the fibre may be identified with (after fixing a reference point in that fibre as before). Let be the restriction of to . The monodromy (right) action of on the fibre above is induced by the canonical homomorphism from to . In particular, is preserved by this action, and so one can find a proper subcover of whose fibre corresponds to . Similarly, we can find a proper subcover of the restriction of to with the same fibre. By Proposition 2 , we may glue these two covers together to obtain a proper subcover of . But such a proper subcover cannot exist because is connected, and the claim follows.\n \n Remark 1 The arguments above relied heavily on the universal cover and its attendant deck transformations and monodromy actions, mostly in order to give a concrete equivalence between coverings of and discrete sets with actions of the fundamental group . It is also possible to proceed without constructing this cover, working instead with a directed family of Galois covers as a substitute for the universal cover to obtain this equivalence of categories ; this is the approach in Grothendieck\u2019s Galois theory , which among other things can be used to construct the \u00e9tale fundamental group , as this is a context in which universal covers need not exist, but one still has plenty of Galois covers. See for instance Szamuely\u2019s text for more details. \n Filed under: expository , math.AT , math.GN Tagged: covering space , fundamental group , Grothendieck's Galois theory , van Kampen's theorem"], "link": "http://terrytao.wordpress.com/2012/10/28/van-kampens-theorem-via-covering-spaces/", "bloglinks": {}, "links": {"http://www.ams.org/": 1, "http://feeds.wordpress.com/": 1, "http://www.cornell.edu/": 1, "http://mathoverflow.net/": 1, "http://en.wikipedia.org/": 13, "http://terrytao.wordpress.com/": 10}, "blogtitle": "What's new"}, {"content": ["Two weeks ago I was at Oberwolfach, for the Arbeitsgemeinschaft in Ergodic Theory and Combinatorial Number Theory that I was one of the organisers for. At this workshop, I learned the details of a very nice recent convergence result of Miguel Walsh (who, incidentally, is an informal grandstudent of mine, as his advisor, Roman Sasyk , was my informal student), which considerably strengthens and generalises a number of previous convergence results in ergodic theory (including one of my own ), with a remarkably simple proof. Walsh\u2019s argument is phrased in a finitary language (somewhat similar, in fact, to the approach used in my paper mentioned previously), and (among other things) relies on the concept of metastability of sequences, a variant of the notion of convergence which is useful in situations in which one does not expect a uniform convergence rate; see this previous blog post for some discussion of metastability. When interpreted in a finitary setting, this concept requires a fair amount of \u201cepsilon management\u201d to manipulate; also, Walsh\u2019s argument uses some other epsilon-intensive finitary arguments, such as a decomposition lemma of Gowers based on the Hahn-Banach theorem. As such, I was tempted to try to rewrite Walsh\u2019s argument in the language of nonstandard analysis to see the extent to which these sorts of issues could be managed. As it turns out, the argument gets cleaned up rather nicely, with the notion of metastability being replaced with the simpler notion of external Cauchy convergence (which we will define below the fold).\n \n \nLet\u2019s first state Walsh\u2019s theorem. This theorem is a norm convergence theorem in ergodic theory, and can be viewed as a substantial generalisation of one of the most fundamental theorems of this type, namely the mean ergodic theorem :\n \n Theorem 1 (Mean ergodic theorem) Let be a measure-preserving system (a probability space with an invertible measure-preserving transformation ). Then for any , the averages converge in norm as , where . \n \n \nIn this post, all functions in and similar spaces will be taken to be real instead of complex-valued for simplicity, though the extension to the complex setting is routine.\n \n \nActually, we have a precise description of the limit of these averages, namely the orthogonal projection of to the -invariant factors. (See for instance my lecture notes on this theorem .) While this theorem ostensibly involves measure theory, it can be abstracted to the more general setting of unitary operators on a Hilbert space:\n \n Theorem 2 (von Neumann mean ergodic theorem)  Let be a Hilbert space, and let be a unitary operator on . Then for any , the averages converge strongly in as . \n \n \nAgain, see my lecture notes (or just about any text in ergodic theory) for a proof.\n \n \nNow we turn to Walsh\u2019s theorem.\n \n Theorem 3 (Walsh\u2019s convergence theorem)  Let be a measure space with a measure-preserving action of a nilpotent group . Let be polynomial sequences in (i.e. each takes the form for some and polynomials ). Then for any , the averages converge in norm as , where . \n \n \nIt turns out that this theorem can also be abstracted to some extent, although due to the multiplication in the summand , one cannot work purely with Hilbert spaces as in the von Neumann mean ergodic theorem, but must also work with something like the Banach algebra . There are a number of ways to formulate this abstraction (which will be of some minor convenience to us, as it will allow us to reduce the need to invoke the nonstandard measure theory of Loeb, discussed for instance in this blog post ); we will use the notion of a (real) commutative probability space , which for us will be a commutative unital algebra over the reals together with a linear functional which maps to and obeys the non-negativity axiom for all . The key example to keep in mind here is of essentially bounded real-valued measurable functions with the supremum norm, and with the trace . We will also assume in our definition of commutative probability spaces that all elements of are bounded in the sense that the spectral radius is finite. (In the concrete case of , the spectral radius is just the norm.)\n \n \nGiven a commutative probability space, we can form an inner product on it by the formula \n \n This is a positive semi-definite form, and gives a (possibly degenerate) inner product structure on . We could complete this structure into a Hilbert space (after quotienting out the elements of zero norm), but we will not do so here, instead just viewing as providing a semi-metric on . For future reference we record the inequalities\n \n \n \n for any , which we will use in the sequel without further comment; see e.g. these previous blog notes for proofs. (Actually, for the purposes of proving Theorem 3 , one can specialise to the case (and ultraproducts thereof), in which case these inequalities are just the triangle and H\u00f6lder inequalities.) \n \nThe abstract version of Theorem 3 is then\n \n Theorem 4 (Walsh\u2019s theorem, abstract version)  Let be a commutative probability space, and let be a nilpotent group acting on by isomorphisms (preserving the algebra, conjugation, and trace structure, and thus also preserving the spectral radius and norm). Let be polynomial sequences. Then for any , the averages form a Cauchy sequence in (semi-)norm as . \n \n \nIt is easy to see that this theorem generalises Theorem 3 . Conversely, one can use the commutative Gelfand-Naimark theorem to deduce Theorem 4 from Theorem 3 , although we will not need this implication. Note how we are abandoning all attempts to discern what the limit of the sequence actually is, instead contenting ourselves with demonstrating that it is merely a Cauchy sequence. With this phrasing, it is tempting to ask whether there is any analogue of Walsh\u2019s theorem for noncommutative probability spaces, but unfortunately the answer to that question is negative for all but the simplest of averages, as was worked out in this paper of Austin, Eisner, and myself .\n \n \nOur proof of Theorem 4 will proceed as follows. Firstly, in order to avoid the epsilon management alluded to earlier, we will take an ultraproduct to rephrase the theorem in the language of nonstandard analysis; for reasons that will be clearer later, we will also convert the convergence problem to a problem of obtaining metastability (external Cauchy convergence). Then, we observe that (the nonstandard counterpart of) the expression can be viewed as the inner product of (say) with a certain type of expression, which we call a dual function . By performing an orthogonal projection to the span of the dual functions, we can split into the sum of an expression orthogonal to all dual functions (the \u201cpseudorandom\u201d component), and a function that can be well approximated by finite linear combinations of dual functions (the \u201cstructured\u201d component). The contribution of the pseudorandom component is asymptotically negligible, so we can reduce to consideration of the structured component. But by a little bit of rearrangement, this can be viewed as an average of expressions similar to the initial average , except with the polynomials replaced by a \u201clower complexity\u201d set of such polynomials, which can be greater in number, but which have slightly lower degrees in some sense. One can iterate this (using \u201cPET induction\u201d) until all the polynomials become trivial, at which point the claim follows.\n \n \n \n \n \n \u2014 1. Nonstandard analysis and metastability \u2014 \n \nWe will assume some familiarity with nonstandard analysis , as covered for instance in these previous blog posts .\n \n \nAs is common practice in nonstandard analysis, we will need to select a non-principal ultrafilter . Using this ultrafilter, we can now form the ultraproduct of any sequence of (standard) spaces, defined as the space of all ultralimits of sequences defined for sufficiently close to , with two sequences considered to have the same ultralimit iff they agree sufficiently close to . Any operation or relation on the standard spaces can then be defined on the nonstandard space in a natural fashion. For instance, given a sequence of standard functions , one can form their ultralimit from the nonstandard space to the nonstandard space by the formula \n \n \nAs usual, we call a nonstandard real bounded if we have for some standard , and infinitesimal if we have for every standard , and in the latter case we also write . Every bounded nonstandard real is infinitesimally close to a unique standard real, called the standard part of .\n \n \nWe will need the following fundamental properties about nonstandard analysis:\n \n \n ( Transfer / Los\u2019s theorem) If for each , is a sequence of mathematical objects, spaces, or functions, with ultralimit or ultraproduct , then for any first-order predicate involving mathematical objects of the appropriate type, the claim is true if and only if . \n ( Overspill ) If an internal set (an ultraproduct of standard sets, also known as a nonstandard set ) of nonstandard numbers contains all unbounded natural numbers, then there exists a standard natural number such that contains all nonstandard numbers larger than . \n ( Loeb measure , hyperfinite case) If is a non-empty nonstandard finite set (i.e. the ultraproduct of standard finite sets, also known as a hyperfinite set ), and the Loeb -algebra is defined as the -algebra generated by the internal subsets of , then there exists a unique countably additive probability measure on the Loeb -algebra, called Loeb measure , such that for any internal subset of , one has . See e.g. this previous blog post for the details of the construction.\n \n \n \nTo motivate the discussion that follows, let us recall some equivalent formulations of a Cauchy sequence in a pseudometric space (i.e. a generalisation of a metric space in which some distances are allowed to vanish).\n \n Proposition 5  Let be a sequence in a pseudometric space (not necessarily complete). Let be the nonstandard extension of , taking values in the nonstandard metric space . Then the following are equivalent: \n \n (standard Cauchy sequence) For every standard , there exists a standard such that for all standard . \n (nonstandard Cauchy sequence) For every nonstandard , there exists a nonstandard such that for all nonstandard . \n (standard metastability) For every standard function and standard , there exists a standard such that for all standard . \n (nonstandard metastability) For every nonstandard function and nonstandard , there exists a nonstandard such that for all nonstandard . \n (asymptotic stability) One has for all unbounded .\n \n \n \n \n \n Proof: The equivalence of 1 and 2 follows from the transfer principle (or Los\u2019s theorem), as does the equivalence of 3 and 4. The implication of 3 from 1 is also clear. Finally, suppose that 1 failed, then there is an such that for every standard we can find a larger number such that . Setting , we see that 3 fails also.\n \n \nIf 1 holds, then from transfer we see that for any unbounded , one has for every standard , giving 5. Conversely, if 1 fails, then letting be as before, we see from transfer that for every nonstandard , contradicting 5. \n \nNow we consider more general sequences, in which the above notions of convergence begin to diverge:\n \n Definition 6 Let be a nonstandard pseudometric space (i.e. the ultraproduct of standard pseudometric spaces ; in particular, takes values in rather than ), and let be an nonstandard sequence (or internal sequence ) in , that is to say a nonstandard map (or internal map ) from to (and thus an ultralimit of maps from to ). \n \n We say that the sequence is internally Cauchy if for every nonstandard , there exists a nonstandard such that for all nonstandard . \n We say that the sequence is externally Cauchy or metastable if for every standard , there exists a standard such that for all standard . \n We say that the sequence is asymptotically stable if whenever are unbounded.\n \n \n \n \n \nThese three notions are now distinct, even for a simple nonstandard metric space such as the ultrapower of the unit interval with the usual metric, as the following examples demonstrate:\n \n \n If is an unbounded natural number, then the nonstandard sequence is internally Cauchy, but not externally Cauchy or asymptotically stable. \n If is an unbounded natural number, then the nonstandard sequence is internally and externally Cauchy, but not asymptotically stable. \n If is an unbounded natural number, then the nonstandard sequence is externally Cauchy, but not internally Cauchy or asymptotically stable. \n If is an unbounded natural number, then the nonstandard sequence is asymptotically stable and externally Cauchy, but not internally Cauchy. \n Any monotone bounded nonstandard sequence of nonstandard reals is automatically both externally Cauchy and internally Cauchy, but is not necessarily asymptotically stable, as the example above shows. \n The property of being externally Cauchy is only dependent on an initial segment of the sequence: if is externally Cauchy, and one modifies arbitrarily for and some fixed unbounded , then the modified sequence will still be externally Cauchy. The same claim is certainly not true for the notions of internally Cauchy or asymptotically stable, as can be seen by considering examples such as , and . \n The property of being externally Cauchy is closed under (external) uniform limits; if is a nonstandard sequence such that for every standard one can find an externally Cauchy sequence with for all , then is itself externally Cauchy. The same claim holds as well for asymptotically stability, but not for the internal Cauchy property (unless one allows to be nonstandard).\n \n \n \nOne can equate these three nonstandard notions of convergence with standard notions as follows:\n \n Proposition 7 Let be a nonstandard pseudometric space (the ultraproduct of standard pseudometric spaces ), and let the nonstandard sequence be the ultralimit of standard sequences . \n \n The nonstandard sequence is internally Cauchy if and only if the standard sequences are Cauchy for all sufficiently close to . \n The nonstandard sequence is externally Cauchy if and only if for every standard and standard , there exists a standard such that for all and all sufficiently close to . \n The nonstandard sequence is asymptotically stable if and only if for every standard , there exists a standard such that one has for all standard and all sufficiently close to . \n The nonstandard sequence is externally Cauchy if and only if there exists an unbounded such that is asymptotically stable up to , in the sense that for all unbounded .\n \n \n \n \n \nInformally: internally Cauchy sequences are ultralimits of sequences that are Cauchy; externally Cauchy sequences are ultralimits of sequences that are uniformly metastable for an asymptotically infinite period of time; and asymptotically stable sequences are ultralimits of sequences that converge at a uniform rate for an asymptotically infinite period of time.\n \n \n Proof: The claim 1 follows directly from the transfer principle. Claim 2 follows from the equivalences of parts 1 and 3 of Proposition 5 applied to the standard portion of the sequence (replacing the nonstandard metric by its standard part). Finally, we verify claim 3. If is asymptotically stable, then for every standard , we have for all unbounded , and so by the overspill principle , there is a standard such that for all , which by transfer gives the \u201conly if\u201d portion of Claim 3. Reversing these steps gives the \u201cif\u201d direction.\n \n \nTo show Claim 4, observe that if is externally Cauchy, then for every standard , one has for all sufficiently large standard , and thus by overspill there is an unbounded such that for all unbounded . By overspill (or countable saturation) one can find an unbounded such that for every standard , giving the \u201conly if\u201d direction. The \u201cif\u201d implication follows by reversing the steps. \n \nFrom these equivalences one sees that asymptotic stability implies externally Cauchy, but as the above counterexamples show, there are no other implications between the three concepts.\n \n \nOf the three notions of convergence for nonstandard sequences, we will focus almost exclusively on the notion of external Cauchy convergence, which at the finitary level corresponds to uniform metastability bounds (as opposed to qualitative convergence, or convergence at a uniform rate). In particular, we will deduce Walsh\u2019s theorem from the following nonstandard version:\n \n Theorem 8 (Walsh\u2019s theorem, nonstandard version)  Let be a nonstandard commutative probability space (i.e. the ultraproduct of standard commutative probability spaces), and let be a nilpotent nonstandard group acting on by isomorphisms. Let be polynomial nonstandard functions (i.e. each takes the form for some standard , some and some standard polynomials ). Then for any elements which are bounded (in the sense that are bounded), the averages form an externally Cauchy sequence with respect to the (non-standard) pseudometric. \n \n \nFrom Proposition 5 , Theorem 8 implies Theorem 4 and thus Theorem 3 . But it is actually somewhat stronger, in that it gives a uniform metastability on the averages occuring in those latter two theorems. (This uniform metastability was already derived in Walsh\u2019s original paper, and I did something similar in the special case of linear commuting averages.) This uniformity ultimately comes from the fact that in the above theorem, the polynomial sequences are allowed to have nonstandard coefficients, rather than just standard ones (and the space is a general nonstandard space, rather than an ultrapower).\n \n \nFrom the definition of external Cauchy convergence, it is clear that if , are two externally Cauchy convergent sequences of (nonstandard) reals, then their sum is also externally Cauchy convergent, and more generally any (standard) finite linear combination (with standard real coefficients) of externally Cauchy convergent sequences of nonstandard reals is also externally Cauchy convergent. A key property for us is that external Cauchy convergence is also preserved by hyperfinite averages involving a nonstandardly finite number of sequences:\n \n Proposition 9 (Metastable dominated convergence theorem)  Let be a non-empty nonstandard finite set (i.e. the ultraproduct of standard finite sets), and let be an internal family of internal sequences of bounded elements of a nonstandard normed vector space. If the sequences are externally Cauchy convergent for each , then the (nonstandardly) averaged sequence is also externally Cauchy convergent. \n \n \nThis is an infinitary version of the finitary metastable dominated convergence theorem that first appeared in this paper of mine , which roughly speaking claims that the average of uniformly metastable bounded sequences is again metastable. The proof was infinitary (deducing it from the Lebesgue dominated convergence theorem), and we will take a similar approach here. The argument was eventually finitised (and strengthened) in this paper of Avigad, Dean, and Rute , but the finitary argument is surprisingly non-trivial.\n \n \n Proof: As each is individually bounded (i.e. smaller in norm than any unbounded natural number), and depends internally on , we see from overspill that there is a uniform bound for some standard natural number .\n \n \nFor each standard and standard natural number , let denote the subset of given by the formula \n \n These sets are not internal subsets of , but are instead -internal (i.e. countable intersections of internal sets). In particular, they are still Loeb measurable subsets of and thus have a well-defined Loeb measure . \n \nBy hypothesis, we see that for any fixed , the increase to in the sense that and . By monotone convergence, we conclude that there exists a standard such that . We then have for that \n \n \n \n \n As can be arbitrarily small, this gives the external Cauchy convergence of as desired. \n Remark 1 This proof is significantly shorter than the finitary proof of Avigad, Dean, and Rute, but the complexity has been concealed in the construction of Loeb measure and the monotone convergence theorem. This is typically how nonstandard analysis arguments work; they are unable to magically make the \u201chard\u201d component of an argument disappear entirely, but they are often able to efficiently conceal such components in fundamental building blocks which are of independent interest, and which can be usefully applied as a black box to a wide spectrum of problems. (In contrast, a hard argument in a finitary argument often needs to be reworked each time one wishes to apply it to a new problem.) \n \n \u2014 2. A simple case: the von Neumann ergodic theorem \u2014 \n \nBefore we prove Theorem 8 , let us first warm up by establishing an easy case, namely the nonstandard version of the von Neumann ergodic theorem (Theorem 2 ):\n \n Theorem 10 (Nonstandard von Neumann mean ergodic theorem)  Let be a nonstandard inner product space (i.e. the ultraproduct of standard inner product spaces), and let be a be a nonstandard unitary operator on . Then for any bounded , the averages are externally Cauchy in . \n \n \nWe first observe that if one takes the bounded elements of and forms the Hilbert space completion using the standard norm \n \n one obtains a Hilbert space . Thanks to the bound\n \n for all bounded in , we see that these ergodic averages can be defined in , and so it suffices to show that the averages are externally Cauchy in for all . \n \nLet us first investigate a condition that would force to be asymptotifcally stable in . We expand out \n \n \n (where all expressions have been extended to nonstandard values of or in the usual fashion, and operations are extended from the bounded elements of to by continuity). With a little bit of rearrangement, this expression can be rewritten as , where the dual function for any is defined by the formula\n \n (The terminology of dual functions originates from this paper of Ben Green and myself .) Thus, if we let be the linear span in of all functions of the form with unbounded and , and is orthogonal to , then vanishes in for any unbounded . This makes asymptotically stable, and thus externally Cauchy, in norm. \n \nIn view of this fact, the existence of an orthogonal projection to the closure of , and the linearity of in , it suffices to show that for any in the closure of in , the expression is eventually Cauchy in .\n \n Remark 2 This step used the existence of orthogonal projections from a Hilbert space to a closed subspace, and is closely related to an analogous use of such projections in the textbook proof of Theorem 2 (see e.g. the proof of Theorem 2 in these blog notes ). In the finitary argument of Walsh, one uses instead a decomposition established by Gowers using the Hahn-Banach theorem as a substitute for orthogonal projections. See also the \u201cHilbert space finite convergence principle\u201d from this blog post for a closely related link between orthogonal projections and quantitative decompositions. \n \n \nHaving eliminated the \u201cpseudorandom case\u201d when is orthogonal to , we have now reduced to the \u201cstructured case\u201d when lies in the closure of . By linearity and an approximation argument, we may reduce to the case when is just a projection of a single dual function for some and unbounded , and by a further density and approximation argument we can assume that is a bounded element of , rather than a general element of . (Incidentally, the non-standard analysis formalism is painlessly skipping over a certain amount of epsilon management here which is much more visible in the finitary version of the argument.) Thus, our task is now to show that the expression is externally Cauchy in .\n \n \nWe expand \n \n Fixing , we now restrict to the regime , thus for all standard . Then as well. We can then shift the range by by making a substitution . If we then return to the range , this creates an error of norm , and so\n \n in for all . In particular, is asymptotically stable up to , and thus externally Cauchy as required. \n \u2014 3. Descent \u2014 \n \nTheorem 8 is proven by an induction on the \u201ccomplexity\u201d of the . Fix and the action of the nilpotent nonstandard group . Given any finite tuple of internal functions from to (not necessarily polynomials), let us say that is good if the conclusion of Theorem 8 holds, thus the averages form an external Cauchy sequence in for all bounded . Trivially, any permutation of a good tuple is good, and any tuple that consists entirely of copies of the constant function mapping to the group identity of is good. Furthermore, if is a finite tuple, and is obtained from by removing duplicate functions (e.g. converting into ) and also removing all copies of , then is good if and only if is good.\n \n \nIf the tuple is non-empty (i.e. ), then for any standard integer , we define the -reduction of to be the tuple consisting of the functions , together with the function \n \n and the functions\n \n for . (We will see why these particular functions arise in the argument shortly.) The key step in proving Theorem 8 is then the following result, reminiscent of the van der Corput lemma in ergodic theory (see e.g. this blog post ). \n Proposition 11 (Descent)  Let be a nonstandard commutative probability space, and let be a nonstandard group acting on by isomorphisms. Let be a non-empty finite tuple of internal functions from to . If is good for every nonstandard integer , then is good. \n \n \nOnce one has this proposition, Theorem 8 will be an immediate consequence of the following combinatorial claim (and the remarks made at the beginning of this section).\n \n Proposition 12 (PET induction)  Let be a nilpotent nonstandard group. Then there exists a well-ordered set and a way to assign to each finite tuple of polynomial nonstandard functions from to a nilpotent nonstandard group a tuple which is a permutation of after all duplicates and copies of have been removed, and a weight in , with the following property: if is non-empty, and is an nonstandard integer, then there is a permutation of which has a strictly smaller weight than that of . \n \n \nIndeed, once one has this proposition and Proposition 11 , Theorem 8 follows by strong induction on the weight .\n \n \nWe prove Proposition 11 in this section, and Proposition 12 in the next section.\n \n \nThe proof of Proposition 11 closely mimics the proof of Theorem 10 . Fix , , the tuple , and bounded elements , and assume that is good for all standard integers . We consider the averages \n \n and our task is to show that the form an external Cauchy sequence in .\n \n \nWe fix bounded elements , and largely work with manipulation of . The map is then an operator from to . We have the easily verified bound \n \n Because of this, the linear operator can be uniquely continuously extended to a linear operator from to , where is defined as the Hilbert space completion of the bounded elements of under the norm\n \n In particular quotients out all the elements of infinitesimal norm. In this Hilbertian formalism, the problem can now be viewed as one of establishing a weighted variant of Theorem 10 . \n \nLet us first investigate a condition that would force to be be asymptotically stable in . We expand out \n \n \n (where all expressions have been extended to nonstandard values of or in the usual fashion, and operations are extended from the bounded elements of to by continuity). With a little bit of rearrangement, this expression can be rewritten as , where the dual function for any is defined by the formula\n \n Thus, if we let be the linear span in of all functions of the form with unbounded and , and is orthogonal to , then vanishes in for any unbounded . This makes asymptotically stable, and thus externally Cauchy, in norm. \n \nIn view of this fact, the existence of an orthogonal projection to the closure of , and the linearity of in , it suffices to show that for any in the closure of in , the expression is eventually Cauchy in .\n \n \nHaving eliminated the \u201cpseudorandom case\u201d when is orthogonal to , we have now reduced to the \u201cstructured case\u201d when lies in the closure of . By linearity and an approximation argument, we may reduce to the case when is just a projection of a single dual function for some and unbounded , and by a further density and approximation argument we can assume that is a bounded element of , rather than a general element of .\n \n \nInspecting the definition (1) of , we see that we need to understand the shifts for . It is here that we perform the \u201cvan der Corput\u201d or \u201cWeyl differencing\u201d calculation that is pervasive in multiple recurrence theory. Namely, we expand \n \n \n Fixing , we now restrict to the regime , thus for all standard . Then as well. We can then shift the range by by making a substitution . If we then return to the range , this creates an error of norm , and so\n \n \n \n (with both sides being interpreted in ). (In the language of Walsh\u2019s paper, this identity asserts that dual functions are reducible .) Substituting this into (1) , and recalling the definition of the tuples , we thus obtain the \u201cWeyl differencing identity\u201d \n \n \n \n in whenever . In particular, since the property of being externally Cauchy is unaffected by truncation to for any unbounded (and in particular to an unbounded ), we see that the left-hand side of (2) is externally Cauchy in if and only if the right-hand side is. But by the induction hypothesis, each of the sequences is externally Cauchy in , and from Proposition 9 we see that is externally Cauchy in , and Proposition 11 follows. \n \u2014 4. PET induction \u2014 \n \nNow we prove Proposition 12 , which will follow the general PET induction method first introduced by Bergelson . We prove the claim first for abelian groups (where there is an obvious notion of the \u201cdegree\u201d of a polynomial sequence), and indicate at the end of the section how to modify the argument to handle nilpotent groups.\n \n \nHenceforth the nonstandard abelian group is fixed. In the abelian case, we can take to be the well-ordered set of tuples of standard non-negative integers with only finitely many of the non-zero, with the reverse lexicographical ordering, thus if there exists such that and for all .\n \n \nIt is easy to see that any polynomial nonstandard function can be uniquely expressed in the discrete Taylor expansion form \n \n for some finite number of group elements with non-trivial (or with if is trivial). We call the degree of ; in the case that is the nonstandard integers with the additive group operation, this corresponds to the usual notion of the degree of a polynomial. \n \nWe observe the ultratriangle inequality \n \n with the inequality being equality if have different degree; we also have the symmetry property \n \n Also, we observe the key fact that if is a non-trivial polynomial sequence, then for any , the derivative defined by is a polynomial sequence of strictly smaller degree.\n \n \nWe can now place an ultrametric on , with the distance between two polynomials defined as \n \n with the convention that . One easily verifies that the ultrametric axioms are obeyed.\n \n Example 1  If , and we consider the four polynomials , , , , then is separated from by a distance of , is separated from by a distance of , and are separated from each other by a distance of . \n \n \nNow let be a finite tuple of polynomials in for some . Selecting a reference polynomial (not necessarily in the tuple), we say that two polynomials are equivalent relative to if . From the ultrametric property we see that this is an equivalence relation, and each equivalence class is a constant distance from . We can then define the weight function of the tuple relative to to equal , where is the number of equivalence classes that have distance exactly from .\n \n Example 2 Let be as in Example 1 . Relative to , are all equivalent and at distance from , so the weight function here is . Relative instead to , none of the are equivalent, and at are distances from , so the weight function here is . For the tuple , the weight function relative to any one of these three polynomials is . \n \n \nNow let be a non-empty tuple of nonstandard polynomials. We form by removing all duplicates and copies of from (starting from the left and moving right), and if does not already have maximal degree amongst all the , permute the tuple in some arbitrary fashion to make this the case. We define the weight of to be the weight of the augmented tuple relative to the final element of the tuple: \n \n \nSuppose , thus there are equivalence classes intersecting that are at a distance exactly from . We set .\n \n \nNow let be a nonstandard integer, and consider the -reduction \n \n where . We first consider the weight of the augmented tuple \n \n relative to . Observe that for any , one has\n \n \n \n \n thus, relative to , and are in the same equivalence class. As such, we see that the weight of the tuple (6) relative to is equal to , thus there are still equivalence classes intersecting the tuple (6) that are distance exactly from . We remark for future reference that the abelian nature of was not directly used in the above calculation. \n \nNow let be the element of the tuple (6) which has the minimal distance to , and has maximal degree. The two requirements are compatible, as any element of the tuple has degree less than that of (which has the maximal degree by construction) necessarily has the maximal distance to . The weight of (6) relative to is then strictly smaller than the weight of (6) relative to , because the weight function at is decreased by one, while the weight function at all values strictly greater than are unchanged. (The weight function at values less than can increase dramatically, but with the lexicographical ordering this does not change the validity of the previous assertion.) Because of this, if we then permute to place at the end, then we see that (note that removing duplicates and copies of from only serves to decrease the weight vector, not to increase it), and the claim follows.\n \n Example 3 Suppose we start with the tuple , whose weight vector is . Performing an -reduction, we obtain\n \n with a weight vector now reduced to . Note that already has maximal degree and has minimal distance to , so no additional permutation is needed at this stage. Performing another -reduction, we obtain\n \n but now we need to permute to move (which has maximal degree and minimal distance to ) to the end, giving\n \n with a weight vector now of . Performing another -reduction, we obtain\n \n which after eliminating duplicates and moving (which has maximal degree and minimal distance to ) to the end, gives\n \n with a weight vector of . Another -reduction then gives\n \n \n (note the elimination of all quadratic terms) which after eliminating duplicates becomes\n \n with a weight vector of . Performing yet another -reduction gives\n \n with a weight vector of . Continuing this process, we will see that the linear terms will eventually all be eliminated, leaving only the constant terms, which can then be eliminated one at a time using further reduction until only the empty tuple remains. See also Walsh\u2019s paper for several further examples of this reduction process, as well as some commentary on how the process can be speeded up somewhat if one observes that one can eliminate not only duplicate polynomials, but also polynomials which differ from an existing polynomial by a constant. \n \n \nFinally, we address the case of a nilpotent group , which will be a modification of the previous argument. The main issue is how to define degree properly. If is a polynomial nonstandard sequence, then by many applications of (discrete analogues of) Baker-Campbell-Hausdorff formula, we can (as before) place uniquely in the Taylor expansion form \n \n for some (standard) finite number of group elements of ; see e.g. Exercise 11 of this previous blog post . In the abelian case, we used the largest for which was non-trivial as the degree of . This turns out to not be a good choice in the nilpotent case, because the crucial ultratriangle property (3) does not hold for this concept of degree. For instance, if is a two-step nilpotent group, and are non-commuting elements of , then the sequences would ostensibly have degree with this definition, but the product \n \n where is the commutator of and , would then have degree , thus contradicting (3) . (The symmetry property (4) can also be shown to break down.) \n \nFortunately, the theory of polynomial sequences in nilpotent groups has been understood since the work of Leibman . The trick is not to view the coefficients appearing above as roaming unrestrictedly in the whole -step nilpotent group , but to restrict some or all of these coefficients to subgroups in the lower central series , defined by setting and for all . Given natural numbers , we then say that a sequence has filtered degree at most if, when using the Taylor expansion (7) , we have whenever . Thus, for instance, if , and , then the sequence has filtered degree at most . A fundamental result of Leibman (proven for instance in this previous post ) asserts that if the sequence is superadditive in the sense that whenever , then the collection of polynomial sequences of filtered degree at most form a group. A related fact is that if a sequence has filtered degree at most for some superadditive , then any derivative of has filtered degree at most (which is still superadditive).\n \n \nIf we let be the set of all superadditive degree sequences, we can order such sequences lexicographically by declaring if there is an with and for all . This makes a well-ordered set, and then we can define the filtered degree of a polynomial sequence to be the minimal in for which has filtered degree at at most . Thus, for instance, in an -step nilpotent group, a sequence with non-trivial would have filtered degree . From Leibman\u2019s results we then have the key properties (3) , (4) , and also that any derivative of has strictly smaller filtered degree.\n \n \nUnfortunately, as filtered degrees are not numbers, we cannot define an ultrametric taking values in in the using the formula (5) , but this is not a real difficulty; we simply declare an \u201cultrametric\u201d taking values in instead of , by declaring if are distinct, and otherwise. If we view as being smaller than any element of , we see that the ultrametric axioms are still obeyed, and one can still run the argument more or less exactly as given above; we leave the details to the interested reader.\n \n Filed under: expository , math.CA , math.DS , math.LO , math.OA Tagged: ergodic theory , metastability , Miguel Walsh , nonstandard analysis , norm convergence"], "link": "http://terrytao.wordpress.com/2012/10/25/walshs-ergodic-theorem-metastability-and-external-cauchy-convergence/", "bloglinks": {}, "links": {"http://www.ams.org/": 3, "http://cms.uba.ar/": 1, "http://www.ens-lyon.fr/": 1, "http://www.mfo.de/": 1, "http://arxiv.org/": 5, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 17, "http://terrytao.wordpress.com/": 72}, "blogtitle": "What's new"}, {"content": ["Garth Gaudry, who made many contributions to harmonic analysis and to Australian mathematics, and was also both my undergradaute and masters advisor as well as the head of school during one of my first academic jobs, died yesterday \u00a0after a long battle with cancer, aged 71. \n Garth worked on the interface between real-variable harmonic analysis and abstract harmonic analysis (which, despite their names, are actually two distinct fields, though certainly related to each other). \u00a0He was one of the first to realise the central importance of Littlewood-Paley theory as a general foundation for both abstract and real-variable harmonic analysis, writing an influential text with Robert Edwards on the topic. \u00a0He also made contributions to Clifford analysis , which was also the topic of my masters thesis . \n But, amongst Australian mathematicians at least, Garth will be remembered for his tireless service to the field, most notably for his pivotal role in founding the Australian Mathematical Sciences Institute (AMSI) and then serving as AMSI\u2019s first director, and then in directing the International Centre of Excellence for Education in Mathematics (ICE-EM), the educational arm of AMSI which, among other things, developed a full suite of maths textbooks and related educational materials covering Years 5-10 (which I reviewed here \u00a0back in 2008). \n I knew Garth ever since I was an undergraduate at Flinders University . \u00a0 He was head of school then (a position roughly equivalent to department chair in the US), but still was able to spare an hour a week to meet with me to discuss real analysis, as I worked my way through Rudin\u2019s \u201cReal and complex analysis\u201d and then Stein\u2019s \u201cSingular integrals\u201d, and then eventually completed a masters thesis under his supervision on Clifford-valued singular integrals. \u00a0When Princeton accepted my application for graduate study, he convinced me to take the opportunity without hesitation. \u00a0Without Garth, I certainly wouldn\u2019t be where I am at today, and I will always be very grateful for his advisorship. \u00a0He was a good person, and he will be missed very much by me and by many others. \n Filed under: obituary"], "link": "http://terrytao.wordpress.com/2012/10/18/garth-gaudry/", "bloglinks": {}, "links": {"http://www.ams.org/": 1, "http://feeds.wordpress.com/": 1, "http://austmaths.wordpress.com/": 1, "http://www.ucla.edu/": 1, "http://terrytao.wordpress.com/": 1, "http://en.wikipedia.org/": 2, "http://www.org.au/": 4, "http://www.edu.au/": 1}, "blogtitle": "What's new"}, {"content": ["One of the basic general problems in analytic number theory is to understand as much as possible the fluctuations of the M\u00f6bius function , defined as when is the product of distinct primes, and zero otherwise. For instance, as takes values in , we have the trivial bound \n \n and the seemingly slight improvement \n \n is already equivalent to the prime number theorem , as observed by Landau (see e.g. this previous blog post for a proof), while the much stronger (and still open) improvement\n \n is equivalent to the notorious Riemann hypothesis. \n \nThere is a general M\u00f6bius pseudorandomness heuristic that suggests that the sign pattern behaves so randomly (or pseudorandomly) that one should expect a substantial amount of cancellation in sums that involve the sign fluctuation of the M\u00f6bius function in a nontrivial fashion, with the amount of cancellation present comparable to the amount that an analogous random sum would provide; cf. the probabilistic heuristic discussed in this recent blog post . There are a number of ways to make this heuristic precise. One of these is the following old conjecture of Chowla :\n \n Conjecture 1 (Chowla\u2019s conjecture) For any fixed integer and exponents , with at least one of the odd (so as not to completely destroy the sign cancellation), we have\n \n \n \n \nNote that as for any , we can reduce to the case when the take values in here. When only one of the are odd, this is essentially the prime number theorem in arithmetic progressions (after some elementary sieving), but with two or more of the are odd, the problem becomes completely open. For instance, the estimate \n \n is morally very close to the conjectured asymptotic\n \n for the von Mangoldt function , where is the twin prime constant ; this asymptotic in turn implies the twin prime conjecture. (To formally deduce estimates for von Mangoldt from estimates for M\u00f6bius, though, typically requires some better control on the error terms than , in particular gains of some power of are usually needed. See this previous blog post for more discussion.) \n Remark 1 The Chowla conjecture resembles an assertion that, for chosen randomly and uniformly from to , the random variables become asymptotically independent of each other (in the probabilistic sense) as . However, this is not quite accurate, because some moments (namely those with all exponents even) have the \u201cwrong\u201d asymptotic value, leading to some unwanted correlation between the two variables. For instance, the events and have a strong correlation with each other, basically because they are both strongly correlated with the event of divisible by . A more accurate interpretation of the Chowla conjecture is that the random variables are asymptotically conditionally independent of each other, after conditioning on the zero pattern ; thus, it is the sign of the M\u00f6bius function that fluctuates like random noise, rather than the zero pattern. \n \n \nA more recent formulation of the M\u00f6bius randomness heuristic is the following conjecture of Sarnak. Given a bounded sequence , define the topological entropy of the sequence to be the least exponent with the property that for any fixed , and for going to infinity the set of can be covered by balls of radius . (If arises from a minimal topological dynamical system by , the above notion is equivalent to the usual notion of the topological entropy of a dynamical system.) For instance, if the sequence is a bit sequence (i.e. it takes values in ), then there are only -bit patterns that can appear as blocks of consecutive bits in this sequence. As a special case, a Turing machine with bounded memory that had access to a random number generator at the rate of one random bit produced every units of time, but otherwise evolved deterministically, would have an output sequence that had a topological entropy of at most . A bounded sequence is said to be deterministic if its topological entropy is zero. A typical example is a polynomial sequence such as for some fixed ; the -blocks of such polynomials sequence have covering numbers that only grow polynomially in , rather than exponentially, thus yielding the zero entropy. Unipotent flows are another good source of deterministic sequences.\n \n Conjecture 2 (Sarnak\u2019s conjecture) Let be a deterministic bounded sequence. Then\n \n \n \n \nThis conjecture in general is still quite far from being solved. However, special cases are known:\n \n \n For constant sequences, this is essentially the prime number theorem (1) . \n For periodic sequences, this is essentially the prime number theorem in arithmetic progressions. \n For quasiperiodic sequences such as for some continuous , this follows from the work of Davenport. \n For nilsequences, this is a result of Ben Green and myself . \n For horocycle flows, this is a result of Bourgain, Sarnak, and Ziegler . \n For rank one sequences such as the Thue-Morse sequence , this is a result of Bourgain . (See also the related result of Green establishing asymptotic orthogonality of the M\u00f6bius function to bounded depth circuits, although such functions are not necessarily deterministic in nature.) \n For the Rudin-Shapiro sequence , I sketched out an argument at this MathOverflow post . \n The M\u00f6bius function is known to itself be non-deterministic, because its square (i.e. the indicator of the square-free functions) is known to be non-deterministic (indeed, its topological entropy is ). (The corresponding question for the Liouville function , however, remains open, as the square has zero entropy.) \n In the converse direction, it is easy to construct sequences of arbitrarily small positive entropy that correlate with the M\u00f6bius function (a rather silly example is for some fixed large (squarefree) , which has topological entropy at most but clearly correlates with ).\n \n \n \nSee this survey of Sarnak for further discussion of this and related topics.\n \n \nIn this post I wanted to give a very nice argument of Sarnak that links the above two conjectures:\n \n Proposition 3 The Chowla conjecture implies the Sarnak conjecture. \n \n \nThe argument does not use any number-theoretic properties of the M\u00f6bius function; one could replace in both conjectures by any other function from the natural numbers to and obtain the same implication. The argument consists of the following ingredients:\n \n \n To show that , it suffices to show that the expectation of the random variable , where is drawn uniformly at random from to , can be made arbitrary small by making large (and even larger). \n By the union bound and the zero topological entropy of , it suffices to show that for any bounded deterministic coefficients , the random variable concentrates with exponentially high probability. \n Finally, this exponentially high concentration can be achieved by the moment method, using a slight variant of the moment method proof of the large deviation estimates such as the Chernoff inequality or Hoeffding inequality (as discussed in this blog post ).\n \n \n \nAs is often the case, though, while the \u201ctop-down\u201d order of steps presented above is perhaps the clearest way to think conceptually about the argument, in order to present the argument formally it is more convenient to present the arguments in the reverse (or \u201cbottom-up\u201d) order. This is the approach taken below the fold.\n \n \n \n \n \n \u2014 1. Proof of proposition \u2014 \n \nWe first establish step 3 of the above outline.\n \n Proposition 4 Assume the Chowla conjecture. Then for any , any , and any coefficients bounded in magnitude by , we have \n \n where is an absolute constant and goes to zero as for fixed (uniformly in the choice of coefficients ), and is drawn uniformly at random from to . \n \n \n Proof: We use the moment method. Let be a large even integer to be optimised in later. By Chebyshev\u2019s inequality, we can bound the left-hand side of (2) by \n \n expanding out the power and using the triangle inequality, we can bound this by \n \n The summand here is always bounded by . By Chowla\u2019s conjecture, the summand can in fact be bounded by unless none of the indices occur an odd number of times, so that the indices are in fact grouped into at most classes. So we are now facing the enumerative combinatorics problem of counting how many tuples are of this form. We can estimate this count as follows. Reading the from left to right, there are two cases: each is either \u201cfresh\u201d (in that is not equal to any of the ), or a \u201crepeat\u201d. In the former case, there are at most choices for ; in the latter case, there are at most . Also, at most of the cases are fresh, so once it is decided which indices are fresh or repeats, there are at most remaining choices to be made (assuming that ). Thus the total count is bounded by . Using this bound, we can thus upper bound (3) by\n \n If we then optimise in by choosing to be the largest even integer less than (say), we obtain the claim. \n \nNow let be a deterministic sequence, which we can normalise to be bounded to be real-valued and in magnitude by . Let and , and let be rounded to the nearest multiple of that is also bounded in magnitude by ., then from the zero-entropy hypothesis, there are at most different values for the tuple , as ranges over the natural numbers. Let be the set of all such tuples. From the previous proposition and the union bound, we have \n \n \n and hence\n \n \n The right-hand side can be simplified to\n \n if is sufficiently large depending on . This in turn leads to the bound\n \n since the expression inside the integrand is bounded in magnitude by (this is Step 2 of the sketch). But by approximate translation invariance we have\n \n for each , hence\n \n This gives (for a suitable choice of )\n \n and thus\n \n letting go to zero sufficiently slowly in , we obtain the Sarnak conjecture. \n Remark 2 The final part of this argument (Step 1 of the sketch) is very lossy: control of short-range correlations (such as ) can be easily averaged out to yield control of long-range correlations (such as those ), but it is very difficult to reverse the process and deduce short-range control from long-range control. Indeed, conjectures such as the Chowla conjecture, which control -point correlations of an arithmetic function such as are generally considered to be significantly more difficult than conjectures such as the Sarnak conjecture, which involve only simple correlations of that function. (In particular, one can plan to tackle the Sarnak conjecture by using Cauchy-Schwarz type methods to eliminate the role of arithmetic functions , as was discussed for instance in this blog post , but there is no obvious way to usefully eliminate arithmetic from the Chowla conjeture.) \n Filed under: expository , math.NT , math.PR Tagged: Chowla conjecture , Mobius function , Peter Sarnak , pseudorandomness , Sarnak conjecture , topological entropy"], "link": "http://terrytao.wordpress.com/2012/10/14/the-chowla-conjecture-and-the-sarnak-conjecture/", "bloglinks": {}, "links": {"http://www.ams.org/": 1, "http://feeds.wordpress.com/": 1, "http://mathworld.wolfram.com/": 1, "http://arxiv.org/": 3, "http://mathoverflow.net/": 1, "http://publications.ias.edu/": 1, "http://en.wikipedia.org/": 9, "http://terrytao.wordpress.com/": 18}, "blogtitle": "What's new"}, {"content": ["One of the basic problems in the field of operator algebras is to develop a functional calculus for either a single operator , or a collection of operators. These operators could in principle act on any function space, but typically one either considers complex matrices (which act on a complex finite dimensional space), or operators (either bounded or unbounded) on a complex Hilbert space. (One can of course also obtain analogous results for real operators, but we will work throughout with complex operators in this post.)\n \n \nRoughly speaking, a functional calculus is a way to assign an operator or to any function in a suitable function space, which is linear over the complex numbers, preserve the scalars (i.e. when ), and should be either an exact or approximate homomorphism in the sense that \n \n should hold either exactly or approximately. In the case when the are self-adjoint operators acting on a Hilbert space (or Hermitian matrices), one often also desires the identity \n \n to also hold either exactly or approximately. (Note that one cannot reasonably expect (1) and (2) to hold exactly for all if the and their adjoints do not commute with each other, so in those cases one has to be willing to allow some error terms in the above wish list of properties of the calculus.) Ideally, one should also be able to relate the operator norm of or with something like the uniform norm on . In principle, the existence of a good functional calculus allows one to manipulate operators as if they were scalars (or at least approximately as if they were scalars), which is very helpful for a number of applications, such as partial differential equations, spectral theory, noncommutative probability, and semiclassical mechanics. A functional calculus for multiple operators can be particularly valuable as it allows one to treat as being exact or approximate scalars simultaneously . For instance, if one is trying to solve a linear differential equation that can (formally at least) be expressed in the form \n \n for some data , unknown function , some differential operators , and some nice function , then if one\u2019s functional calculus is good enough (and is suitably \u201celliptic\u201d in the sense that it does not vanish or otherwise degenerate too often), one should be able to solve this equation either exactly or approximately by the formula\n \n which is of course how one would solve this equation if one pretended that the operators were in fact scalars. Formalising this calculus rigorously leads to the theory of pseudodifferential operators , which allows one to (approximately) solve or at least simplify a much wider range of differential equations than one what can achieve with more elementary algebraic transformations (e.g. integrating factors, change of variables, variation of parameters, etc.). In quantum mechanics, a functional calculus that allows one to treat operators as if they were approximately scalar can be used to rigorously justify the correspondence principle in physics, namely that the predictions of quantum mechanics approximate that of classical mechanics in the semiclassical limit . \n \nThere is no universal functional calculus that works in all situations; the strongest functional calculi, which are close to being an exact *-homomorphisms on very large class of functions, tend to only work for under very restrictive hypotheses on or (in particular, when , one needs the to commute either exactly, or very close to exactly), while there are weaker functional calculi which have fewer nice properties and only work for a very small class of functions, but can be applied to quite general operators or . In some cases the functional calculus is only formal, in the sense that or has to be interpreted as an infinite formal series that does not converge in a traditional sense. Also, when one wishes to select a functional calculus on non-commuting operators , there is a certain amount of non-uniqueness: one generally has a number of slightly different functional calculi to choose from, which generally have the same properties but differ in some minor technical details (particularly with regards to the behaviour of \u201clower order\u201d components of the calculus). This is a similar to how one has a variety of slightly different coordinate systems available to parameterise a Riemannian manifold or Lie group. This is on contrast to the case when the underlying operator is (essentially) normal (so that commutes with ); in this special case (which includes the important subcases when is unitary or (essentially) self-adjoint), spectral theory gives us a canonical and very powerful functional calculus which can be used without further modification in applications.\n \n \nDespite this lack of uniqueness, there is one standard choice for a functional calculus available for general operators , namely the Weyl functional calculus ; it is analogous in some ways to normal coordinates for Riemannian manifolds, or exponential coordinates of the first kind for Lie \tgroups, in that it treats lower order terms in a reasonably nice fashion. (But it is important to keep in mind that, like its analogues in Riemannian geometry or Lie theory, there will be some instances in which the Weyl calculus is not the optimal calculus to use for the application at hand.)\n \n \nI decided to write some notes on the Weyl functional calculus (also known as Weyl quantisation ), and to sketch the applications of this calculus both to the theory of pseudodifferential operators. They are mostly for my own benefit (so that I won\u2019t have to redo these particular calculations again), but perhaps they will also be of interest to some readers here. (Of course, this material is also covered in many other places. e.g. Folland\u2019s \u201c harmonic analysis in phase space \u201c.)\n \n \n \n \n \n \u2014 1. Weyl quantisation of polynomials \u2014 \n \nThe simplest class of functions to which one can set up a functional calculus are the polynomials, as this does not require any analytic tools to define. To begin with we will ignore the conjugation structure, thus we will not attempt to implement (2) .\n \n \nIn order to be able to freely compose all the operators under consideration, we will assume that there is some dense space of test functions (or perhaps Schwartz functions) which is preserved by all of the , so that any composition of finitely many of the will be densely defined. (Alternatively, one could proceed at a purely formal level for this discussion, working in an abstract algebra generated by the .)\n \n \nIn the case of a single operator , the polynomial calculus is obvious: given any polynomial \n \n with complex coefficients , one can define to be the operator\n \n In other words, the functional calculus is the linear map that takes each monomial to the operator . This calculus is of course an exact homomorphism, as it is linear and obeys (1) exactly. \n \nNow we consider the situation with multiple operators. For simplicity, let us just consider the case of two operators . We then consider a polynomial \n \n with complex coefficients , and ask how to define the operator . \n \nThe most obvious way to define is by direct substitution, which I will call the Kohn-Nirenberg calculus : \n \n (Depending on the interpretation of the operators , this calculus might also be referred to as the Wick-ordered calculus or normal-ordered calculus .) This is certainly a well-defined calculus, but when and do not commute, the calculus has a bias in that it always places to the left of ; in particular, if one interchanges the roles of and then one obtains a different calculus, which one might call the anti-Kohn-Nirenberg calculus:\n \n Intermediate, and more symmetric, between these two calculi, is the Weyl calculus \n \n where the Weyl ordering of the monomial is defined to be the average of all the ways to multiply copies of and copies of together:\n \n where range over all tuples which contain copies of and copies of , thus for instance\n \n \n \n \n and so forth. \n Remark 1 Strictly speaking, the use of the terminology here is an abuse of notation, because it suggests a functional relationship between and which need not be the case. In particular, if the monomials and are equal as operators, this does not necessarily imply that the Weyl-ordered monomials and are equal. One could fix this notation by working first with formal symbols generating a free commutative algebra, and writing in place of , so that the Weyl map becomes a not-quite-homomorphism from the commutative algebra generated by and to the non-commutative algebra generated by and . We have chosen however to not be quite so formal, and allow some abuse of notation to simplify the exposition. \n \n \nWhen and commute, of course, all these calculi coincide with each other; it is only in the non-commuting case that some distinctions between the calculi emerge. The Weyl calculus may seem complicated, but it has somewhat cleaner formulae with regard to products. For instance, the Weyl calculus works perfectly with respect to powers of affine combinations of , where are scalars: \n \n (By this equation, we mean that if is the polynomial , then is the operator .) Thus for instance \n \n \n \n In contrast, the Kohn-Nirenberg calculus (or the anti-Kohn-Nirenberg calculus) distorts this ordering, for instance we have\n \n Indeed, by comparing coefficients in (3) and using linearity we see that the identity (3) (for arbitrary ) in fact uniquely defines the Weyl calculus. One consequence of this is that the Weyl calculus is not only symmetric with respect to interchange of the underlying operators , but in fact respects all linear changes of variable: if are scalar affine combinations of , is a polynomial of two variables, and is the polynomial , then we have that\n \n Indeed, from (3) we see that this identity holds for the mixed monomials , and then by linearity it is true for all polynomials. \n \nOne can also extend the Weyl calculus to formal (i.e. not necessarily convergent) infinite series \n \n by declaring to be the formal series of operators\n \n In doing so, the identity (3) can be expressed in a very convenient form \n \n where we view the exponential function here as the formal infinite series \n \n \n Remark 2 The fact that Weyl calculus preserves the exponential map is the reason why we view this calculus as being analogous to normal coordinates in Riemannian geometry, as well as exponential coordinates (of the first kind) on Lie groups, discussed for instance in this previous blog post . In contrast, the Kohn-Nirenberg quantisation gives\n \n which is analogous to exponential coordinates of the second kind on Lie groups. \n \n \nNow we study the extent to which the homomorphism property (1) holds in the Weyl calculus, i.e. we study the discrepancy between and . When and commute, it is easy to see that (1) holds exactly. In general, these two expressions can be quite different; but when and almost commute, so that the commutator is non-zero but small, we expect (1) to be approximately true. Motivated by quantum-mechanical examples, we will study a model case when and obey the Heisenberg commutator relationship \n \n where is a small positive real number. (In some literature, the sign convention here is reversed.) In this case, we see that any two ways to multiply copies of and copies of together will differ by polynomials of degree strictly less than . Among other things, this shows that any polynomial of and can be rewritten into a Weyl-ordered form, by first ordering the top degree terms (at the cost of introducing some messy lower degree terms) and then recursively working on the lower order terms. The same procedure also implies that this Weyl-ordered form is unique. For instance, \n \n This already implies that for any two polynomials , there must be a unique \u201cproduct\u201d polynomial with the property that \n \n In the classical limit , this operation is simply pointwise product:\n \n Now we work out what the product operation (known as the Moyal product ) is for non-zero . To avoid some rather messy combinatorics, it turns out to be cleanest to proceed using the formal exponential function (5) applied to various combinations of and . (This is an instance of the method of generating functions in action.) \n \nWe first observe from (6) that we have the general commutation relationship \n \n where is the anti-symmetric form \n \n This is already the first hint of the correspondence between quantum mechanics (whose dynamics is based on the commutator ) and classical mechanics (whose dynamics is based on the Poisson bracket ). From this and the Baker-Campbell-Hausdorff formula , we conclude (formally, at least) that \n \n \n \n One can make this identity rigorous (at the level of formal power series) as follows. We first observe that the Hadamard lemma \n \n holds at the level of formal power series, where is the commutator operation . This can be seen by first establishing the formal differentiation identity\n \n (which comes from the formal identity and the product rule) and then using this to solve for as a formal power series to conclude that\n \n and then setting . Using this lemma together with (8) , we see in particular that \n \n \n \nNow consider the expression \n \n as a formal power series in . Then is the identity, and the formal derivative can be expressed as\n \n \n \n \n \n \n which after using (10) simplifies to\n \n which on solving the formal series leads to\n \n which gives (9) after setting . \n \nCombining (9) , (5) and (7) (as well as the uniqueness of the Weyl ordering), we conclude that \n \n \n where both sides are viewed as formal power series in indeterminates . Comparing coefficients, we see that\n \n \n with the convention that vanishes for , and similarly for . We write\n \n where is the tensor product of copies of . Since\n \n and similarly\n \n where denotes the gradient with respect to the variables, we conclude that\n \n \n This formula is valid for all monomials , and so by bilinearity we conclude the explicit formula \n \n for the Moyal product, thus\n \n \n The expression is also known as the Poisson bracket of and , and is denoted , thus\n \n \nNote that is symmetric for even and anti-symmetric for odd . Subtracting, we conclude that \n \n thus \n \n Thus we see that the normalised commutator of the Moyal product converges in the semiclassical limit ; this is one manifestation of the correspondence principle. \n \nThe above discussion was for two operators obeying the Heisenberg commutation relation (6) , but one can check that a similar calculus can also be developed for any tuple of operators, and if they obey a Heisenberg-like commutation law \n \n for some anti-symmetric form , then the Moyal product formula (11) holds; the arguments are basically identical to the two variable case (except for some notational complications) and are left as an exercise to the reader. \n \nOnce one has set up a calculus for complex polynomials of some non-self-adjoint operators , one can then extend it to polynomials of the original arguments and their conjugates (or equivalently, to polynomials of the real and imaginary parts of the ) by using the calculus for . This allows one to define operators when is a polynomial of the real and imaginary parts of , basically by substituting and for and respectively. One advantage of the Weyl calculus, not shared in general by the Kohn-Nirenberg calculus, is that the identity (2) holds exactly; this is basically a special case of the affine invariance properties of the Weyl calculus mentioned earlier. In a similar vein, the Weyl calculus ensures that \n \n for any complex polynomial , where is the complex polynomial whose coefficients are the complex conjugates of the coefficients of . In particular, if are self-adjoint and has real coefficients, then is again self-adjoint. \n \u2014 2. Weyl quantisation of smooth symbols \u2014 \n \nThe above algebraic formalism allowed one to define the Weyl quantisation of any -tuple of operators (preserving some dense space of test functions) as long as was a polynomial; one could also handle the case when was a formal power series, provided that one was willing to make a formal power series also. But of course we would like to expand the calculus beyond the spaces of polynomials and formal power series.\n \n \nOne obvious way is to work with convergent power series instead of formal power series, but this approach, by definition, only applies for real analytic functions . Also, being real analytic (which, by the root test, can be viewed as an exponential decay hypothesis on the coefficients ) may not be enough in some cases to ensure convergence of the formal power series ; one may need faster decay on the (e.g. decay like or something similar) before one can justify convergence in a suitable sense.\n \n \nTo get beyond the analytic category, it turns out that one can use Fourier-analytic methods, using Fourier expansions as a substitute for power series expansions. The key point, of course, is that Fourier analysis works well for spaces of functions that are significantly rougher than real analytic.\n \n \nTo illustrate the method, let us again work with just two operators and . We will assume that and are (essentially) self-adjoint, which by spectral theory suggests that one should be able to define for (sufficiently nice) real-valued functions . (The distinction between real and complex functions was not apparent at the polynomial level, since any polynomial on a real vector space can be automatically complexified uniquely to a polynomial on the associated complex vector space.) The starting point is the Fourier inversion formula \n \n where is the Fourier transform, which in our choice of normalisations becomes\n \n From the inversion formula, it is natural to expect that the Weyl quantisation of should be given by the formula\n \n which in view of (5) leads us to \n \n which we will take as a definition of for general functions . \n \nOf course, to make this definition rigorous, one has to specify some regularity hypotheses on , and check that the integral actually converges in some suitable sense; also, it would be reassuring to verify that the definition is compatible with the polynomial calculus given earlier. At present, and are completely abstract operators, which makes this task quite difficult; so we will now work with a much more concrete situation, namely that of the one-dimensional position operator and momentum operator , defined on test functions in by the formulae \n \n Note that this obeys the Heisenberg relation\n \n and thus by (9) one has (formally, at least)\n \n for any . On the other hand, we have\n \n while from solving the transport equation we see that\n \n for suitable test functions , and thus\n \n Again working formally, we conclude that\n \n which we expand as\n \n \n Making the change of variables , we rewrite this as\n \n \n Next, from the distributional Fourier inversion formula\n \n where is the Dirac delta, we can rewrite this as\n \n which on performing the integral simplifies to\n \n or after the change of variables , \n \n We remark that the Kohn-Nirenberg calculus comes out almost identically, except that the argument is replaced with . Similarly for the anti-Kohn-Nirenberg calculus (in which is replaced by ). The Weyl calculus treats the input variable and the output variable on equal footing, whereas the Kohn-Nirenberg calculus prefers one to the other. In particular, we have the convenient formal identity \n \n so that formally self-adjoint when is real-valued. \n \nWe will take the formula (14) as our definition of the Weyl quantisation , at least when is a Schwarz function and obeys the weak regularity bounds \n \n for all , all , some constant , and some constants depending on . (This is weaker than the symbol bounds that usually arise in the theory of pseudodifferential operators, but will suffice for the discussion here.)\n \n Exercise 1 Show that for and as above, the integrand is absolutely integrable in , and that the integrand is absolutely integrable in , making well-defined. Show in fact that is a Schwartz function. \n \n Exercise 2 Show that the above definition agrees with the previous definitions of and . (One approach is by direct computation; another is to try to make the above formal calculations fully rigorous.) \n \n Exercise 3 (Translation invariance)  If obeys the weak regularity bound (16) and for some , show that\n \n where is the translation operator\n \n and is the modulation operator\n \n One can in fact obtain similar symmetries for the entire Weil representation of the (affine) metaplectic group , but we will not do so here. \n \n \nOne can get some intuition as to what the operators do by testing them on gaussian wave packets \n \n It turns out that the Taylor series of around is a good approximation for the action of on such a packet: \n Lemma 1  For any , one has \n \n \n \n as , where denotes a quantity with norm of . \n \n \nThus we have the asymptotic series \n \n \n \n \n \n with each term of order being of norm . (Note that this does not imply that the series is convergent in , because we have no control of the dependence on of the implied constant in the notation.) \n \n Proof: (Sketch) By using the symmetries in Exercise 3 , we may normalise . If is a polynomial of degree at most , then the the two sides of (17) agree exactly, with no need for the error term . By subtracting off the order Taylor expansion, we thus see that it suffices to show that \n \n whenever vanishes to order at least at the origin. But this can be verified from (14) and a routine stationary phase computation. \n \nThe same computation allows one to replace the norm by any other of the Frechet norms in the Schwartz space, as long as are assumed to be of polynomial size in , i.e. .\n \n \nFrom (14) , we see that any quantisation is formally an integral operator \n \n with distributional kernel \n \n Applying the Fourier inversion formula, we thus see (formally, at least) that any integral operator (18) is also a quantisation where is uniquely determined by the formula \n \n In particular, given two quantisations and , their composition is formally an integral operator with distributional kernel\n \n \n and hence is formally a quantisation , where\n \n \n Thus, for instance,\n \n \n This integral will, in general, not be absolutely integrable. However, the phase is only stationary at . By using a suitable smooth dyadic decomposition and many integrations by parts, we can then show that this integral can be given a convergent interpretation if obey the weak regularity bound (16) . Similarly for other evaluations , as well as higher derivatives of evaluated at arbitrary points . Indeed, with a certain amount of technical effort (which we omit here) we see that is well-defined and obeys (16) , and we can rigorously establish the composition law \n \n \n \nIf and are polynomials, we see (using the fact that any operator has a unique symbol ) that the product given here coincides exactly with the Moyal product (11) . As a consequence, by testing on gaussian wave packets using Lemma 1 , one can show that this composition law has an asymptotic series given by the Moyal product, thus for any , one has \n \n where the error obeys the bound (16) (with the constant independent of and , and the constant depending on but uniform in ). We will not give full details here, but roughly speaking, bound to high order at some point , one can subtract off a Taylor polynomial from both and to reduce to the case when at least one of vanishes to high order at , at which point Lemma 1 shows that almost annihilates any gaussian wave packet supported near in phase space, and thus must have very small symbol at ; a similar (but more complicated) argument also allows one to control derivatives of the symbol of . In a similar vein, the commutator of two operators with symbols obeying (16) can be described by a symbol with asymptotic series (12) ; we omit the details. These facts form the basis for the pseudodifferential calculus , which we will not develop further here; see e.g. Taylor\u2019s book or Folland\u2019s book for more details. \n \nWe can also relate the Moyal product to the Wigner transform \n \n Comparing this with (19) , we see that can also be interpreted as the symbol of the rank one operator\n \n where . Given a quantised operator , we (formally, at least) have the relatino\n \n and hence (by (15) and (20) ) we see (again formally, at least) that\n \n so that the action of pseudodifferential operators such as can be described through the Wigner transform by some applications of the Moyal product. \n \nContinuing this line of thought, suppose that is a time-dependent function obeying Schr\u00f6dinger\u2019s equation \n \n for some real-valued symbol (representing the Hamiltonian of the system). Passing from the wave function to the density matrix , we observe that Schr\u00f6dinger\u2019s equation implies Heisenberg\u2019s equation \n \n and hence (by formal applicaton of the Moyal product)\n \n Applying (12) , we see that the Wigner transform of obeys the perturbed Hamiltonian flow equation \n \n This should be compared with the evolution of a classical density function in phase space with respect to transport along Hamilton\u2019s equation of motion \n \n which is given by the transport equation \n \n Thus we see in the semiclassical limit , the equation of motion for the Wigner transform (21) under the Schr\u00f6dinger flow converges formally to the equation of motion for a classical density transported under Hamiltonian flow. This is one of the basic manifestations of the correspondence principle relating quantum and classical mechanics, and suggests that the Wigner transform of a wave function is a quantum analogue of a density function in phase space. (But the analogy is not perfect; in particular, the Wigner transform is almost never a non-negative function. See Folland\u2019s book for more discussion.) Making all of these formal calculations and heuristics rigorous is a non-trivial task, requiring the development of the theory of Fourier integral operators , and will not be discussed here. \n Filed under: expository , math.CA , math.OA , math.QA Tagged: Moyal product , semiclassical analysis , Weyl calculus"], "link": "http://terrytao.wordpress.com/2012/10/07/some-notes-on-weyl-quantisation/", "bloglinks": {}, "links": {"http://www.ams.org/": 4, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 21, "http://www.encyclopediaofmath.org/": 1, "http://terrytao.wordpress.com/": 52}, "blogtitle": "What's new"}, {"content": ["Way back in 2007, I wrote a blog post giving Einstein\u2019s derivation of his famous equation for the rest energy of a body with mass . (Throughout this post, mass is used to refer to the invariant mass (also known as rest mass ) of an object.) This derivation used a number of physical assumptions, including the following: \n \n The two postulates of special relativity : firstly, that the laws of physics are the same in every inertial reference frame, and secondly that the speed of light in vacuum is equal in every such inertial frame. \n Planck\u2019s law and de Broglie\u2019s law for photons, relating the frequency, energy, and momentum of such photons together. \n The law of conservation of energy , and the law of conservation of momentum , as well as the additivity of these quantities (i.e. the energy of a system is the sum of the energy of its components, and similarly for momentum). \n The Newtonian approximations , to energy and momentum at low velocities. \n \n The argument was one-dimensional in nature, in the sense that only one of the three spatial dimensions was actually used in the proof. \n As was pointed out in comments in the previous post by Laurens Gunnarsen, this derivation has the curious feature of needing some laws from quantum mechanics (specifically, the Planck and de Broglie laws) in order to derive an equation in special relativity (which does not ostensibly require quantum mechanics). One can then ask whether one can give a derivation that does not require such laws. As pointed out in previous comments, one can use the representation theory of the Lorentz group to give a nice derivation that avoids any quantum mechanics, but it now needs at least two spatial dimensions instead of just one. I decided to work out this derivation in a way that does not explicitly use representation theory (although it is certainly lurking beneath the surface). The concept of momentum is only barely used in this derivation, and the main ingredients are now reduced to the following: \n \n The two postulates of special relativity; \n The law of conservation of energy (and the additivity of energy); \n The Newtonian approximation at low velocities. \n \n The argument (which uses a little bit of calculus, but is otherwise elementary) is given below the fold. Whereas Einstein\u2019s original argument considers a mass emitting two photons in several different reference frames, the argument here considers a large mass breaking up into two equal smaller masses. Viewing this situation in different reference frames gives a functional equation for the relationship between energy, mass, and velocity, which can then be solved using some calculus, using the Newtonian approximation as a boundary condition, to give the famous formula. \n Disclaimer: As with the previous post, the arguments here are physical arguments rather than purely mathematical ones, and thus do not really qualify as a rigorous mathematical argument, due to the implicit use of a number of physical and metaphysical hypotheses beyond the ones explicitly listed above. (But it would be difficult to say anything non-tautological at all about the physical world if one could rely solely on rigorous mathematical reasoning.) \n \n \u2014 1. The main argument \u2014 \n We will assume that the total energy of a moving body depends only on the mass of that body, and the velocity of that body: \n \n (This is actually a non-trivial assumption; it excludes the possibility that the energy might also be depenent on other features of the body, such as spin or charge.) At present, this functional relationship is arbitrary. However, we can use some physical arguments to constrain this relationship. We first use the following argument of Galileo. Consider two bodies side by side, traveling at the same velocity , with the first body of mass and the second of mass . Then, the first body has energy and the second has energy , so the combined system of two bodies has total energy . On the other hand, if we imagine connecting the two bodies by an infinitesimally thin thread, we can view the system as a single body of mass traveling at the same velocity . This leads us to the relationship \n \n for any , which (under reasonable hypotheses of continuity) implies a linear relationship between energy and mass, thus \n \n for some function depending only on the velocity . \n We still have to determine this unknown functional relationship . We assume rotational symmetry of the laws of physics (which one can view as a special case of the first postulate of special relativity): if two bodies of equal mass move at the same speed, but at different directions, the energies should be the same. In other words, should be spherically symmetric, so by abuse of notation we write \n \n \n \n \n Now consider a body of mass at rest at the origin(in some reference frame ), which somehow disintegrates (at time , for simplicity) into two smaller bodies of equal mass , one moving in the positive direction at some velocity , and the other moving in the negative direction at the opposite velocity (note that this situation is consistent with the law of conservation of momentum). (If one prefers, one could also view the time-reversed situation, in which two masses of equal and opposite velocity collide to form a large stationary mass; the analysis of this situation is basically identical to the one given here.) In Newtonian mechanics, we have conservation and additivity of mass, so that must equal ; but we will not assume conservation and additivity of mass here (and in fact at least one of these laws must break down in special relativity, at least if one insists on using an invariant notion of mass). Instead, we can link , , and to each other by the law of conservation of energy. Before the disintegration, the body has total energy , while after the disintegration the system has total energy (using the spherically symmetric nature (1) ) of , and so \n \n \n \n \n Now we view the same system relative to another reference frame , which relative to is moving at a velocity in the direction for some , while keeping the and coordinates unchanged. The spacetime coordinates of are then related to those of by the usual Lorentz transformations \n \n \n \n \n which can be deduced from the postulates of special relativity by a standard derivation that we will not give here (it is sketched in the previous blog post ). The pre-disintegration body is moving along the worldline in the reference frame, and is thus moving along the line in the reference frame; in particular, it has velocity in this frame and thus has energy in this frame. \n Now consider the first post-disintegration body . It is moving along the worldline in the reference frame, and thus along the line in the reference frame; in particular, the speed of in this frame is (the well known velocity addition (or subtraction) formula ), and so the energy of this body is . Similarly, has energy . Equating energies, we are thus led to \n \n We can eliminate using (2) , to obtain a functional equation for : \n \n \n \n This equation should hold for all (physically attainable) velocities . To solve this equation, it is convenient to work with the change of variables \n \n the hyperbolic angles are known as the rapidities associated to and respectively. The point of using this change of variables is that the hyperbolic tangent addition formula yields \n \n Thus if we make the change of variables \n \n then (3) simplifies to \n \n It is tempting to plug in some special values into this equation, such as , but this only gives a trivial equation. However, if we first differentiate twice in to obtain \n \n and then set , we obtain the non-trivial equation \n \n This is a differential equation in , and can be solved as \n \n for some unknowns , where is the square root of . From (1) , should have vanishing derivative at the origin, and so , and so we have \n \n \n \n This is significant progress in constraining the behaviour of , but there are still two unknown parameters . To proceed further, it becomes necessary to utilise a second dimension. Namely, we repeat the previous arguments, but with now moving at velocity instead of . The Lorentz transformations are now \n \n \n \n \n The pre-disintegration body is moving along the worldline in the reference frame, and is thus moving along the line in the reference frame; in particular, it has velocity in this frame and thus has energy in this frame. \n Now consider the first post-disintegration body . It is moving along the worldline in the reference frame, and thus along the line in the reference frame; in particular, the speed of in this frame is , and so the energy of this body is . Similarly for . Equating energies, we are thus led to \n \n We can eliminate using (2) , to obtain a functional equation for : \n \n This equation should hold for all (physically attainable) velocities . To solve this equation, we work with infinitesimal and perform a Taylor expansion. From the symmetry (2) , should be flat at the origin, and so (assuming sufficient smoothness for ) we have \n \n while from the Taylor approximation \n \n we have \n \n Inserting these expansions and extracting the coefficient, we obtain the differential equation \n \n which we can rewrite as \n \n for some constant . We can integrate this as \n \n and thus \n \n for some parameters . In rapidity coordinates , this becomes \n \n Comparing this with (4) (e.g. by performing a Taylor expansion to fourth order around ) we see that , thus \n \n or equivalently \n \n Thus we have \n \n For infinitesimal velocities , we may Taylor expand \n \n and so the kinetic energy of a slowly moving mass is . Comparing this with the Newtonian approximation of we conclude that , and thus \n \n \n \n In particular, setting we see that the rest energy of a body of mass is , as required. \n Remark 1 The above derivation did not explicitly use the law of conservation of momentum (other than to observe that the scenario of one mass at rest splitting into two smaller masses moving in equal and opposite directions was compatible with this law). Actually, if one defines the momentum of a body of mass and velocity by the formula \n \n and the momentum of a system as the sum of the momenta of its components, one can use (5) and the Lorentz transformations to (after some algebra) express the total momentum of a system as a linear combination of the total energy of that system viewed in a couple reference frames (or, if one prefers, as the derivatives of the total energy with respect to infinitesimal reference frame changes), and as a consequence one can actually derive the law of conservation of momentum from the law of conservation of energy, together with special relativity. (Actually, this can also be done in Galilean relativity as well, using the classical formula ; we leave this as an exercise to the reader.) Indeed, in special relativity it is natural to unify energy and momentum together as a single quantity known as the four-momentum . \n Remark 2 The above arguments ultimately rely on the fact that the Lorentz group has an essentially unique linear action on when the spatial dimension is at least two. For , the group becomes abelian, and there is a multiplicity of such actions (parameterised by the different possibilities for the quantity appearing in (4) ), and one could a priori have a number of different laws relating energy and momentum with mass and velocity that are consistent with special relativity and the conservation laws. Indeed, for any choice of , one could postulate the laws \n \n and \n \n for the energy and momentum of a body of mass moving at rapidity (i.e. at velocity ). One can verify that such laws are consistent with the laws of conservation of mass and energy, with the postulates of special relativity, and with the Newtonian approximation, as long as one is only in one spatial dimension; one needs to use at least one other dimension to be able to reduce to the case. Thus we see that higher-dimensional relativity is more rigid than one-dimensional relativity. In the case of Einstein\u2019s original argument, the quantum mechanical properties of photons are used instead to show that in the lightspeed limit , which gives the reduction to . \n Filed under: expository , math.MP , math.RT Tagged: Albert Einstein , mass-energy equivalence , special relativity"], "link": "http://terrytao.wordpress.com/2012/10/02/einsteins-derivation-of-emc2-revisited/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://terrytao.wordpress.com/": 17, "http://en.wikipedia.org/": 9}, "blogtitle": "What's new"}, {"content": ["One of the first non-trivial theorems one encounters in classical algebraic geometry is B\u00e9zout\u2019s theorem , which we will phrase as follows:\n \n Theorem 1 (B\u00e9zout\u2019s theorem) Let be a field, and let be non-zero polynomials in two variables with no common factor. Then the two curves and have no common components, and intersect in at most points. \n \n \nThis theorem can be proven by a number of means, for instance by using the classical tool of resultants . It has many strengthenings, generalisations, and variants; see for instance this previous blog post on B\u00e9zout\u2019s inequality . B\u00e9zout\u2019s theorem asserts a fundamental algebraic dichotomy, of importance in combinatorial incidence geometry: any two algebraic curves either share a common component, or else have a bounded finite intersection; there is no intermediate case in which the intersection is unbounded in cardinality, but falls short of a common component. This dichotomy is closely related to the integrality gap in algebraic dimension: an algebraic set can have an integer dimension such as or , but cannot attain any intermediate dimensions such as . This stands in marked contrast to sets of analytic, combinatorial, or probabilistic origin, whose \u201cdimension\u201d is typically not necessarily constrained to be an integer.\n \n \nB\u00e9zout\u2019s inequality tells us, roughly speaking, that the intersection of a curve of degree and a curve of degree forms a set of at most points. One can consider the converse question: given a set of points in the plane , can one find two curves of degrees with and no common components, whose intersection contains these points?\n \n \nA model example that supports the possibility of such a converse is a grid that is a Cartesian product of two finite subsets of with . In this case, one can take one curve to be the union of vertical lines, and the other curve to be the union of horizontal lines, to obtain the required decomposition. Thus, if the proposed converse to B\u00e9zout\u2019s inequality held, it would assert that any set of points was essentially behaving like a \u201cnonlinear grid\u201d of size .\n \n \nUnfortunately, the naive converse to B\u00e9zout\u2019s theorem is false. A counterexample can be given by considering a set of points for some large perfect square , where is a by grid of the form described above, and consists of points on an line (e.g. a or grid). Each of the two component sets can be written as the intersection between two curves whose degrees multiply up to ; in the case of , we can take the two families of parallel lines (viewed as reducible curves of degree ) as the curves, and in the case of , one can take as one curve, and the graph of a degree polynomial on vanishing on for the second curve. But, if is large enough, one cannot cover by the intersection of a single pair of curves with no common components whose degrees multiply up to . Indeed, if this were the case, then without loss of generality we may assume that , so that . By B\u00e9zout\u2019s theorem, either contains , or intersects in at most points. Thus, in order for to capture all of , it must contain , which forces to not contain . But has to intersect in points, so by B\u00e9zout\u2019s theorem again we have , thus . But then (by more applications of B\u00e9zout\u2019s theorem) can only capture of the points of , a contradiction.\n \n \nBut the above counterexample suggests that even if an arbitrary set of (or ) points cannot be covered by the single intersection of a pair of curves with degree multiplying up to , one may be able to cover such a set by a small number of such intersections. The purpose of this post is to record the simple observation that this is, indeed, the case:\n \n Theorem 2 (Partial converse to B\u00e9zout\u2019s theorem) Let be a field, and let be a set of points in for some . Then one can find and pairs of coprime non-zero polynomials for such that \n \n and \n \n  \n \n \nInformally, every finite set in the plane is (a dense subset of) the union of logarithmically many nonlinear grids. The presence of the logarithm is necessary, as can be seen by modifying the example to be the union of logarithmically many Cartesian products of distinct dimensions, rather than just a pair of such products.\n \n \nUnfortunately I do not know of any application of this converse, but I thought it was cute anyways. The proof is given below the fold.\n \n \n \n \n \n \u2014 1. Proof of theorem \u2014 \n \nThe first step is to cover by a single, reasonably low-degree curve, which is a trick which also underlies the polynomial method in incidence combinatorics. Let be the first natural number such that , thus . The space of all polynomials in two variables of degree at most is , while the requirement that such a polynomial vanishes at every point of comprises linear constraints (not necessarily independent). By linear algebra, we conclude that there exists a non-zero polynomial of degree at most which vanishes at every point of .\n \n \nLet denote the distinct irreducible factors of , and the degree of each , thus \n \n and\n \n \nIn particular, we can partition into , where \n \n for each , with the being disjoint. \n \nFix an . By applying a generic linear change of variables, we may assume that the coefficient of is non-zero. (Geometrically, we are changing coordinates so that the point at infinity on the -axis does not lie on the zero locus of .) Observethat for any , the space of polynomials in the ring of degree at most is at least , since the monomials for and are linearly independent. Setting to be the first natural number with , we conclude that we can find a polynomial that is not a multiple of , which vanishes on , and has degree \n \n \n \n \nBy construction, we have \n \n This is getting close to what we want, except that we have far too many pairs ; the quantity could be potentially as large as , whereas we would like a logarithmic number instead. However, we can resolve this problem simply by dyadic pigeonholing on the quotient appearing in (3) . Namely, for every integer (say), let be the set of indices such that\n \n then the partition . For each , we then set\n \n and\n \n Observe that are coprime polynomials with\n \n thus giving (1) (after some relabeling). Also, for each we have\n \n and\n \n \n and thus\n \n \n Summing in , we conclude that\n \n \n \n \n \n and (2) follows. \n Remark 1 The above argument implicitly relied on some estimates on the Hilbert polynomial of the ring . These Hilbert polynomials become more complicated in higher dimensions (especially given that the intersection of multiple hypersurfaces need not be a complete intersection in higher dimensions), and so I do not see immediately how to extend the above result to higher dimensions. \n Filed under: expository , math.AG , math.CO Tagged: algebraic curves , Bezout's theorem , polynomials"], "link": "http://terrytao.wordpress.com/2012/09/25/a-partial-converse-to-bezouts-theorem/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 4, "http://terrytao.wordpress.com/": 10}, "blogtitle": "What's new"}, {"content": ["There has been a lot of recent interest in the abc conjecture , since the release a few weeks ago of the last of a series of papers by Shinichi Mochizuki which, as one of its major applications, claims to establish this conjecture. It\u2019s still far too early to judge whether this proof is likely to be correct or not (the entire argument encompasses several hundred pages of argument, mostly in the area of anabelian geometry, which very few mathematicians are expert in, to the extent that we still do not even have a full outline of the proof strategy yet), and I don\u2019t have anything substantial to add to the existing discussion around that conjecture. (But, for those that are interested, the Polymath wiki page on the ABC conjecture has collected most of the links to that discussion, and to various background materials.)\n \n \nIn the meantime, though, I thought I might give the standard probabilistic heuristic argument that explains why we expect the ABC conjecture to be true. The underlying heuristic is a common one, used throughout number theory, and it can be summarised as follows:\n \n Heuristic 1 (Probabilistic heuristic)  Even though number theory is a deterministic subject (one does not need to roll any dice to factorise a number, or figure out if a number is prime), one expects to get a good asymptotic prediction for the answers to many number-theoretic questions by pretending that various number-theoretic assertions (e.g. that a given number is prime) are probabilistic events (with a probability that can vary between and ) rather than deterministic events (that are either always true or always false). Furthermore: \n \n (Basic heuristic) If two or more of these heuristically probabilistic events have no obvious reason to be strongly correlated to each other, then we should expect them to behave as if they were (jointly) independent. \n (Advanced heuristic) If two or more of these heuristically probabilistic events have some obvious correlation between them, but no further correlations are suspected, then we should expect them to behave as if they were conditionally independent, relative to whatever data is causing the correlation.\n \n \n \n \n \nThis is, of course, an extremely vague and completely non-rigorous heuristic, requiring (among other things) a subjective and ad hoc determination of what an \u201cobvious reason\u201d is, but in practice it tends to give remarkably plausible predictions, some fraction of which can in fact be backed up by rigorous argument (although in many cases, the actual argument has almost nothing in common with the probabilistic heuristic). A famous special case of this heuristic is the Cram\u00e9r random model for the primes , but this is not the only such instance for that heuristic.\n \n \nTo give the most precise predictions, one should use the advanced heuristic in Heuristic 1 , but this can be somewhat complicated to execute, and so we shall focus instead on the predictions given by the basic heuristic (thus ignoring the presence of some number-theoretic correlations), which tends to give predictions that are quantitatively inaccurate but still reasonably good at the qualitative level.\n \n \nHere is a basic \u201ccorollary\u201d of Heuristic 1 :\n \n Heuristic 2 (Heuristic Borel-Cantelli) Suppose one has a sequence of number-theoretic statements, which we heuristically interpet as probabilistic events with probabilities . Suppose also that we know of no obvious reason for these events to have much of a correlation with each other. Then: \n \n If , we expect only finitely many of the statements to be true. (And if is much smaller than , we in fact expect none of the to be true.) \n If , we expect infinitely many of the statements to be true.\n \n \n \n \n \nThis heuristic is motivated both by the Borel-Cantelli lemma , and by the standard probabilistic computation that if one is given jointly independent, and genuinely probabilistic, events with , then one almost surely has an infinite number of the occuring.\n \n \nBefore we get to the ABC conjecture, let us give two simpler (and well known) demonstrations of these heuristics in action:\n \n Example 1 (Twin prime conjecture) One can heuristically justify the twin prime conjecture as follows. Using the prime number theorem , one can heuristically assign a probability of to the event that any given large integer is prime. In particular, the probability that is prime will then be . Making the assumption that there are no strong correlations between these events, we are led to the prediction that the probability that and are simultaneously prime is . Since , the Borel-Cantelli heuristic then predicts that there should be infinitely many twin primes. \n \nNote that the above argument is a bit too naive, because there are some non-trivial correlations between the primality of and the primality of . Most obviously, if is prime, this greatly increases the probability that is odd, which implies that is odd, which then elevates the probability that is prime. A bit more subtly, if is prime, then is likely to avoid the residue class , which means that avoids the residue class , which ends up decreasing the probability that is prime. However, there is a standard way to correct for these local correlations; see for instance in this previous blog post . As it turns out, these local correlations ultimately alter the prediction for the asymptotic density of twin primes by a constant factor (the twin prime constant ), but do not affect the qualitative prediction of there being infinitely many twin primes. \n \n \n Example 2 (Fermat\u2019s last theorem) Let us now heuristically count the number of solutions to for various and natural numbers (which we can reduce to be coprime if desired). We recast this (in the spirit of the ABC conjecture) as , where are powers. The number of powers up to any given number is about , so heuristically any given natural number has a probability about of being an power. If we make the naive assumption that (in the coprime case at least) there is no strong correlation between the events that is an power, is an power, and being an power, then for typical , the probability that are all simultaneously powers would then be . For fixed , the total number of solutions to the Fermat equation would then be predicted to be\n \n (Strictly speaking, we need to restrict to the coprime case, but given that a positive density of pairs of integers are coprime, it should not affect the qualitative conclusion significantly if we now omit this restriction.) It might not be immediately obvious as to whether this sum converges or diverges, but (as is often the case with these sorts of unsigned sums) one can clarify the situation by dyadic decomposition. Suppose for instance that we consider the portion of the sum where lies between and . Then this portion of the sum can be controlled by\n \n which simplifies to\n \n Summing in , one thus expects infinitely many solutions for , only finitely many solutions for (indeed, a refinement of this argument shows that one expects only finitely many solutions even if one considers all at once), and a borderline prediction of there being a barely infinite number of solutions when . Here is of course a place where a naive application of the probabilistic heuristic breaks down; there is enough arithmetic structure in the equation that the naive probabilistic prediction ends up being an inaccurate model. Indeed, while this heuristic suggests that a typical homogeneous cubic should have a logarithmic number of integer solutions of a given height , it turns out that some homogeneous cubics (namely, those associated to elliptic curves of positive rank) end up with the bulk of these solutions, while other homogeneous cubics (including those associated to elliptic curves of zero rank, including the Fermat curve ) only get finitely many solutions. The reasons for this are subtle, but certainly the high degree of arithmetic structure present in an elliptic curve (starting with the elliptic curve group law which allows one to generate new solutions from old ones, and which also can be used to exclude solutions to via the method of descent) is a major contributing factor. \n \n \nBelow the fold, we apply similar heuristics to suggest the truth of the ABC conjecture.\n \n \n \n \n \n \u2014 1. The ABC conjecture \u2014 \n \nThe ABC conjecture asserts that for every , one has \n \n whenever are coprime natural numbers, and is the product of all the primes dividing . Since are coprime, , and an equivalent formulation of the conjecture is as follows: if are real numbers with , then for sufficiently large , there does not exist any solution to the equation with (say), coprime, and\n \n \nIndeed, one can deduce the former version of the ABC conjecture from the latter by using a finite mesh of triples with spacing equal to small multiple of ; we leave the details to the interested reader.\n \n \nWe can now try to randomly find counterexamples to the abc conjecture as follows: \n \n Pick a large (say, a power of two). \n Pick coprime squarefree numbers , , . \n Pick numbers with with comparable to . \n Check if .\n \n \n \nSteps 1, 2, and 4 are easy to apply probabilistic heuristics to. For Step 3, we need the following lemma, reminiscent of the classical divisor bound :\n \n Lemma 3 Let be a square-free integer. Then there are at most integers less than with radical . \n \n \n Proof: Factorise . We are trying to establish the bound , where is the number of numbers less than which are divisible by but by no other primes, that is to say they lie in the set . To do this, it is convenient to work with Dirichlet series, and specifically with the sum \n \n for some parameter to be optimised in later. On the one hand, this expression is at least . On the other hand, we have the factorisation\n \n We crudely bound\n \n for some depending only on , using the trivial bound and the geometric series formula, leading to\n \n On the other hand, since\n \n we see from the prime theorem (which gives a lower bound on the sum of the logarithms of the first primes, which in turn lower bounds ) that\n \n and so\n \n for any fixed ; as can be arbitrarily small, the claim follows. \n \nNow we can run the probabilistic heuristics. After selecting as a large power of two in Step 1, we have choices for in Step 2. By the above lemma, for each , there are choices for that are of magnitude , and assuming (in the case when are coprime) that there is no prior correlation between and , which we think of as being randomly distributed amongst the numbers of size , the probability that should be of order . This leads to a total probability of ; since , this sum is convergent (for ranging over powers of ), so we expect only finitely many counterexamples, thus supporting the ABC conjecture.\n \n Remark 1 In the case of , the naive heuristic prediction was incorrect, basically because of the algebraic structure in the Fermat curve (which, among other things, is an elliptic curve and thus enjoys a group law). In the case of the general equation, though, no analogous algebraic structure appears to be present. So there is no obvious correlation here. (Of course, this does not rule out the possibility of a much less obvious correlation; this is one of the reasons why the above arguments are only heuristic, and fall well short of a rigorous proof of anything.) \n Filed under: math.NT , math.PR Tagged: abc conjecture , probabilistic heuristic"], "link": "http://terrytao.wordpress.com/2012/09/18/the-probabilistic-heuristic-justification-of-the-abc-conjecture/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.ac.jp/": 1, "http://michaelnielsen.org/": 2, "http://mathworld.wolfram.com/": 1, "http://en.wikipedia.org/": 4, "http://terrytao.wordpress.com/": 8}, "blogtitle": "What's new"}, {"content": ["Much as group theory is the study of groups, or graph theory is the study of graphs, model theory is the study of models (also known as structures ) of some language (which, in this post, will always be a single-sorted, first-order language). A structure is a set , equipped with one or more operations, constants, and relations. This is of course an extremely general type of mathematical object, but (quite remarkably) one can still say a substantial number of interesting things about very broad classes of structures.\n \n \nWe will observe the common abuse of notation of using the set as a metonym for the entire structure, much as we usually refer to a group simply as , a vector space simply as , and so forth. Following another common bending of the rules, we also allow some operations on structures (such as the multiplicative inverse operation on a group or field) to only be partially defined, and we allow use of the usual simplifying conventions for mathematical formulas (e.g. writing instead of or , in cases where associativity is known). We will also deviate slightly from the usual practice in logic by emphasising individual structures, rather than the theory of general classes of structures; for instance, we will talk about the theory of a single field such as or , rather than the theory of all fields of a certain type (e.g. real closed fields or algebraically closed fields).\n \n \nOnce one has a structure , one can introduce the notion of a definable subset of , or more generally of a Cartesian power of , defined as a set of the form \n \n for some formula in the language with free variables and any number of constants from (that is, is a well-formed formula built up from a finite number of constants in , the relations and operations on , logical connectives such as , , , and the quantifiers ). Thus, for instance, in the theory of the arithmetic of the natural numbers , the set of primes is a definable set, since we have \n \n In the theory of the field of reals , the unit circle is an example of a definable set,\n \n but so is the the complement of the circle,\n \n and the interval :\n \n Due to the unlimited use of constants, any finite subset of a power of any structure is, by our conventions, definable in that structure. (One can of course also consider definability without parameters (also known as -definability), in which arbitrary constants are not permitted, but we will not do so here.) \n \nWe can isolate some special subclasses of definable sets: \n \n An atomic definable set is a set of the form (1) in which is an atomic formula (i.e. it does not contain any logical connectives or quantifiers). \n A quantifier-free definable set is a set of the form (1) in which is quantifier-free (i.e. it can contain logical connectives, but does not contain the quantifiers ).\n \n \n Example 1 In the theory of a field such as , an atomic definable set is the same thing as an affine algebraic set (also known as an affine algebraic variety , with the understanding that varieties are not necessarily assumed to be irreducible), and a quantifier-free definable set is known as a constructible set ; thus we see that algebraic geometry can be viewed in some sense as a special case of model theory. (Conversely, it can in fact be quite profitable to think of model theory as an abstraction of algebraic geometry; for instance, the concepts of Morley rank and Morley degree in model theory (discussed in this previous blog post ) directly generalises the concepts of dimension and degree in algebraic geometry.) Over , the interval is a definable set, but not a quantifier-free definable set (and certainly not an atomic definable set); and similarly for the primes over . \n \n \nA quantifier-free definable set in is nothing more than a finite boolean combination of atomic definable sets; in other words, the class of quantifier-free definable sets over is the smallest class that contains the atomic definable sets and is closed under boolean operations such as complementation and union (which generate all the other boolean operations). Similarly, the class of definable sets over is the smallest class that contains the quantifier-free definable sets, and is also closed under the operation of projection from to for every natural number , where is the map .\n \n \nSome structures have the property of enjoying quantifier elimination , which means that every definable set is in fact a quantifier-free definable set, or equivalently that the projection of a quantifier-free definable set is again quantifier-free. For instance, an algebraically closed field (with the field operations) has quantifier elimination (i.e. the projection of a constructible set is again constructible); this fact can be proven by the classical tool of resultants , and among other things can be used to give a proof of Hilbert\u2019s nullstellensatz . (Note though that projection does not necessary preserve the property of being atomic; for instance, the projection of the atomic set is the non-atomic, but still quantifier-free definable, set .) In the converse direction, it is not difficult to use the nullstellensatz to deduce quantifier elimination. For theory of the real field , which is not algebraically closed, one does not have quantifier elimination, as one can see from the example of the unit circle (which is a quantifier-free definable set) projecting down to the interval (which is definable, but not quantifer-free definable). However, if one adds the additional operation of order to the reals, giving it the language of an ordered field rather than just a field, then quantifier elimination is recovered (the class of quantifier-free definable sets now enlarges to match the class of definable sets, which in this case is also the class of semi-algebraic sets ); this is the famous Tarski-Seidenberg theorem .\n \n \nOn the other hand, many important structures do not have quantifier elimination; typically, the projection of a quantifier-free definable set is not, in general, quantifier-free definable. This failure of the projection property also shows up in many contexts outside of model theory; for instance, Lebesgue famously made the error of thinking that the projection of a Borel measurable set remained Borel measurable (it is merely an analytic set instead). Turing\u2019s halting theorem can be viewed as an assertion that the projection of a decidable set (also known as a computable or recursive set) is not necessarily decidable (it is merely semi-decidable (or recursively enumerable ) instead). The notorious P=NP problem can also be essentially viewed in this spirit; roughly speaking (and glossing over the placement of some quantifiers), it asks whether the projection of a polynomial-time decidable set is again polynomial-time decidable. And so forth. (See this blog post of Dick Lipton for further discussion of the subtleties of projections.)\n \n \nNow we consider the status of quantifier elimination for the theory of a finite field . If interpreted naively, quantifier elimination is trivial for a finite field , since every subset of is finite and thus quantifier-free definable. However, we can recover an interesting question in one of two (essentially equivalent) ways. One is to work in the asymptotic regime in which the field is large, but the length of the formulae used to construct one\u2019s definable sets stays bounded uniformly in the size of (where we view any constant in as contributing a unit amount to the length of a formula, no matter how large is). A simple counting argument then shows that only a small number of subsets of become definable in the asymptotic limit , since the number of definable sets clearly grows at most polynomially in for any fixed bound on the formula length, while the number of all subsets of grows exponentially in .\n \n \nAnother way to proceed is to work not with a single finite field , or even with a sequence of finite fields, but with the ultraproduct of a sequence of finite fields, and to study the properties of definable sets over this ultraproduct. (We will be using the notation of ultraproducts and nonstandard analysis from this previous blog post .) This approach is equivalent to the more finitary approach mentioned in the previous paragraph, at least if one does not care to track of the exact bounds on the length of the formulae involved. Indeed, thanks to Los\u2019s theorem , a definable subset of is nothing more than the ultraproduct of definable subsets of for all sufficiently close to , with the length of the formulae used to define uniformly bounded in . In the language of nonstandard analysis, one can view as a nonstandard finite field.\n \n \nThe ultraproduct of finite fields is an important example of a pseudo-finite field \u2013 a field that obeys all the sentences in the languages of fields that finite fields do, but is not necessarily itself a finite field. The model theory of pseudo-finite fields was first studied systematically by Ax (in the same paper where the Ax-Grothendieck theorem , discussed previously on this blog , was established), with important further contributions by Kiefe , by Fried-Sacerdote , by two papers of Chatzidakis-van den Dries-Macintyre , and many other authors.\n \n \nAs mentioned before, quantifier elimination trivially holds for finite fields. But for infinite pseudo-finite fields, such as the ultraproduct of finite fields with going to infinity, quantifier elimination fails. For instance, in a finite field , the set of quadratic residues is a definable set, with a bounded formula length, and so in the ultraproduct , the set of nonstandard quadratic residues is also a definable set. However, in one dimension, we see from the factor theorem that the only atomic definable sets are either finite or the whole field , and so the only constructible sets (i.e. the only quantifier-free definable sets) are either finite or cofinite in . Since the quadratic residues have asymptotic density in a large finite field, they cannot form a quantifier-free definable set, despite being definable.\n \n \nNevertheless, there is a very nice almost quantifier elimination result for these fields, in characteristic zero at least, which we phrase here as follows:\n \n Theorem 1 (Almost quantifier elimination)  Let be a nonstandard finite field of characteristic zero, and let be a definable set over . Then is the union of finitely many sets of the form \n \n where is an atomic definable subset of (i.e. the -points of an algebraic variety defined over in ) and is a polynomial. \n \n \nResults of this type were first obtained essentially due to Catarina Kiefe , although the formulation here is closer to that of Cherlin-van den Dries-Macintyre .\n \n \nInformally, this theorem says that while we cannot quite eliminate all quantifiers from a definable set over a nonstandard finite field, we can eliminate all but one existential quantifier. Note that negation has also been eliminated in this theorem; for instance, the definable set uses a negation, but can also be described using a single existential quantifier as .) I believe that there are more complicated analogues of this result in positive characteristic, but I have not studied this case in detail (Kiefe\u2019s result does not assume characteristic zero, but her conclusion is slightly different from the one given here). In the one-dimensional case , the only varieties are the affine line and finite sets, and we can simplify the above statement, namely that any definable subset of takes the form for some polynomial (i.e. definable sets in are nothing more than the projections of the -points of a plane curve).\n \n \nThere is an equivalent formulation of this theorem for standard finite fields, namely that if is a finite field and is definable using a formula of length at most , then can be expressed in the form (2) with the degree of bounded by some quantity depending on and , assuming that the characteristic of is sufficiently large depending on .\n \n \nThe theorem gives quite a satisfactory description of definable sets in either standard or nonstandard finite fields (at least if one does not care about effective bounds in some of the constants, and if one is willing to exclude the small characteristic case); for instance, in conjunction with the Lang-Weil bound discussed in this recent blog post , it shows that any non-empty definable subset of a nonstandard finite field has a nonstandard cardinality of for some positive standard rational and integer . Equivalently, any non-empty definable subset of for some standard finite field using a formula of length at most has a standard cardinality of for some positive rational of height and some natural number between and . (For instance, in the example of the quadratic residues given above, is equal to and equal to .) There is a more precise statement to this effect, namely that the Poincar\u00e9 series of a definable set is rational; see Kiefe\u2019s paper for details.\n \n \nBelow the fold I give a proof of Theorem 1 , which relies primarily on the Lang-Weil bound mentioned above.\n \n \n \n \n \n \u2014 1. Proof of theorem \u2014 \n \nFix a nonstandard finite field of characteristic zero. Let us temporarily define a basic set to be a set (2) for some atomic definable set and some polynomial , and a good set to be a finite union of good sets in some vector space . Our objective is thus to show that all definable sets are good.\n \n \nClearly, every atomic definable set is already a basic set, and is thus certainly good. Given the relationship between atomic definable sets and all definable sets, it thus suffices to show thaat the class of good sets are closed under boolean operations (e.g. union, intersection, negation) and projection. Closure under unions is automatic by definition; the interesting and difficult closure properties are projection and negation. (Closure under intersection is, strictly speaking, a consequence of closure under unions and negations, thanks to de Morgan\u2019s laws ; but it turns out to be convenient to obtain closure under intersection first as a stepping stone to closure under negation.)\n \n \nWe first observe that we may place good sets in a suitable \u201cnormal form\u201d. Let be the algebraic closure of . This is an algebraically closed field that has a nonstandard Frobenius endomorphism , defined as the ultralimit of the Frobenius maps on each of the finite fields . This is a nonstandard automorphism of whose fixed points are precisely . An atomic definable set over is the same thing as the -points of an variety over which is invariant with resspect to the Frobenius automorphism (this can be established for instance by inspecting a reduced Grobner basis for the associated ideal). Such a variety can be decomposed over into absolutely irreducible components. Some of these components may remain Frobenius-invariant and will thus continue to be defined over . If a component is not Frobenius invariant, we may intersect it with all of its Frobenius conjugates without losing any of its -points; by the Noetherian property , this will reduce that component to an absolutely irreducible variety defined over . Thus, every variety defined over can be replaced with a union of finitely many absolutely irreducible varieties defined over , without losing or gaining any -points. From this, we see that any good set can be placed in a form in which all of the varieties involved are absolutely irreducible.\n \n \nNow consider a basic set of the form \n \n with absolutely irreducible. We consider the constant term of . If this term vanishes identically on , then this basic set is all of . Otherwise, observe that we may factorise\n \n for some polynomial with , and then\n \n \nFrom this, we see that any good set can be expressed as the union of the -points of a variety (i.e. an atomic definable set) and finitely many basic sets which have unit constant term in the sense that they have the form \n \n with absolutely irreducible and . \n \nThis leads us to the following useful lemma:\n \n Lemma 2 (Hypersurface removal)  Let be a good set in , and let be a polynomial. Then is also good. \n \n \n Proof: By replacing with its norm over the polynomials over , we may assume that takes coefficients in . By the preceding discussion, it suffices to establish this in the case when is either the -points an irreducible variety , or a basic set on such a variety with unit constant term. In the former case, we simply note that \n \n In the latter case, we observe that\n \n whenever has unit constant term. \n \nThis allows us to generalise to be constructible rather than algebraic:\n \n Corollary 3  Any set of the form , where is a quantifier-free definable set (i.e. the -points of a constructible set), and is a polynomial from to , is good. \n \n \n Proof: Again, we may assume takes coefficients in . A constructible set can be expressed as the union of finitely many quasiprojective varieties (i.e. the set-theoretic difference of two affine varieties). A quasiprojective variety can be written as the finite union of sets of the form for various polynomials , and the claim then follows from the preceding lemma. \n \nNow we can handle images of constructible sets with -dimensional fibres:\n \n Proposition 4  Let be constructible sets defined over with irreducible Zariski closures, and let be a regular map , also defined over , with the property that all fibres , are zero-dimensional. Then is good. (In particular, the set of -points of any constructible set is a good set.) \n \n \n Proof: By restricting if necessary, we may assume that is dominant . The field of -valued rational functions on can then be viewed as a field extension of (using the pullback map by ); as the fibres are zero-dimensional, this extension has transcendence degree zero, i.e. it is algebraic. By the primitive element theorem (and it is here that we are using our hypothesis that has characteristic zero, to ensure that the field extension is separable), is generated by and a rational function on , which by clearing denominators one can assume to be integral with respect to the polynomial ring , thus \n \n for some polynomial with coefficients in with monic as a polynomial over . Each of the coordinate functions on the affine variety can then be viewed (again after clearing denominators) as a polynomial (over ) in , divided by some non-trivial polynomial , thus\n \n for all and all in , and some polynomials with coefficients in . From this, we see that for any with , has an -point if and only if has a root in . Thus we have\n \n Using Corollary 3 and an induction hypothesis on the dimension of to deal with the residual term , we obtain the claim. \n \nNext, we record an important property of infinite pseudofinite fields, known as the pseudo algebraically closed (PAC) axiom :\n \n Lemma 5 (PAC axiom)  Let be an absolutely irreducible variety defined over . Then is non-empty. Furthermore, for any Zariski-dense constructible subset of , is non-empty also. \n \n \n Proof: We write as an ultralimit of varieties defined over . As is absolutely irreducible, we have absolutely irreducible with dimension for close to (see Lemma 3 and Lemma 8 of this previous blog post ), and so by the Lang-Weil bound (see this previous blog post ) we have \n \n Since goes to infinity as , we conclude that is non-empty for sufficiently close to , and so is non-empty also by Los\u2019s theorem. The same argument works for , using the Schwarz-Zippel bound from the previous post to handle the set removed from to form . \n \nWe can now establish closure with respect to projection:\n \n Corollary 6  If is good, then is also good. \n \n \n Proof: It suffices to establish that \n \n is a good set whenever is a constructible subset of . Let be the projection map from to , so our task is to show that is good. By decomposing , we may assume that has irreducible closure, and that all the fibres of in have the same dimension, which is either , , or . If the fibres have dimension , then by the PAC axiom , and the claim follows from Corollary 3 . If the fibres have dimension , the claim follows instead from Proposition 4 . So the only remaining case is when the fibres are one-dimensional curves. \n \nWe can work generically in , since the contribution of any subvariety of of strictly smaller dimension can be handled by an induction hypothesis. Generically, the fibre over a point is then given by a curve for some polynomial defined over . If is an -point of , then by the PAC axiom, this curve contains an -point in the plane if and only if , viewed as a polynomial of two variables, contains an absolutely irreducible factor defined over . If one fixes the degree of this factor, this is a definable constraint on coefficients, and by Proposition 4 , the set of for which this constraint is obeyed is a good set. Letting vary from to and taking unions, we obtain the claim. \n \nFrom closure under projection, we can deduce closure under intersection:\n \n Corollary 7  The intersection of two good sets in is again a good sets in . \n \n \n Proof: It suffices to show that the intersection of two basic sets \n \n and\n \n in is good. But observe that can be obtained from the atomic definable (and hence basic) set by applying two projections, so the claim follows from two applications of Corollary 6 . \n The only remaining task is to show closure under negation. To get a hint of how to proceed on this, consider again the example of the quadratic residues in : \n \n One can almost describe the complement of this set by selecting a non-quadratic residue of and using the set \n \n though this is not quite the complement of and one has to delete the origin, for instance by using Lemma 2 . Applying the algorithm implicit in the proof of that lemma, one ends up with\n \n as an explicit representation of the non-quadratic residues as a basic set. \n \nThe selection of an arbitrary non-quadratic residue may seem like a rather ad hoc step to take in the above example, which does not initially bode well for generalisation to other definable sets. However, one can make the argument a bit more \u201ccanonical\u201d by a bit of Galois theory. Note that while an arbitrary element of need not necessarily have a square root in , it always has a square root in the (unique) quadratic extension of . Of course, will also be a square root. Letting be the Galois conjugation of over , we have \n \n and so is equal to either or . In the former case, is a quadratic residue; otherwise, it is a non-quadratic residue. (Let us ignore the situation when , as Lemma 2 and Corollary 7 give us enough tools to deal with what is going on exceptional sets of lower-dimensional varieties.) Thus, outside of the point , we can express as the set of for which one has for the two square roots , and the complement as the set of for which (extending from to in some arbitrary manner if desired). The descriptions of and its (near-)complement in (3) , (4) can be viewed as (shadows of) \u201cnorms\u201d of the above assertions , in some sense. This suggests that more generally, given a basic set on a variety , one can use Galois theory to naturally partition (outside of some exceptional set of smaller dimension) into disjoint components, one of which is , and all of which can be described as unions of basic sets, which would lead to the desired closure property with respect to negation. (Not coincidentally, a very similar trick was used in Bombieri\u2019s version of Stepanov\u2019s proof of the Hasse-Weil bound discussed in this previous post , in which an upper bound on for curves was converted into a matching lower bound.) \n Proposition 8 The complement of a good set in is again a good set. \n \n \n Proof: In view of Corollary 7 and de Morgan\u2019s laws, it suffices to show that the complement of a basic set in is a good set, whenever is absolutely irreducible and defined over . From Proposition 4 , we already know that is a good set, so it suffices to show that the set \n \n is good. By an induction on the dimension of , we may freely delete any lower-dimensional component of , thus it will suffice to show that\n \n is good for some generic subset of . \n \nBy an induction on the degree of , and Corollary 7 , we may assume that is absolutely irreducible on . In particular, and have no common factor (here we are again implicitly using the characteristic zero hypothesis), and so for generic , and have no common zero (otherwise their resultant would vanish generically, leading to a common factor). If we write \n \n for some polynomials on with generically non-zero on , we thus see that for generic , has distinct zeroes in . Indeed, recalling that any finite field has a unique extension of degree , has a unique extension of degree , and all the zeroes of lie in this extension. \n \nIt is a standard fact from Galois theory that is a Galois extension of , with the Galois group isomorphic to and generated by the Frobenius endomorphism ; taking ultralimits, we see that is also a Galois extension of , with Galois group again isomorphic to and generated by the ultralimit of the standard Frobenius endomorphisms . For any generic , the distinct zeroes of are acted upon by the Frobenius map. Thus, there exists a permutation such that for all .\n \n \nObserve that for generic the statement holds precisely when has no fixed points. Thus, it suffices to show that for each permutation , the set of generic for which one can enumerate the zeroes of as with the property that for all , is a good set. But by viewing as a -dimensional linear vector space over and expanding in terms of some arbitrarily chosen basis (cf. the factor in the quadratic residue discussion), and noting that the Frobenius map is a linear transformation of this vector space, we see that this set is of the form covered by Proposition 4 , and the claim follows. (One could also avoid the use of an arbitrarily chosen basis here by working with a more abstract version of Proposition 4 , if desired.) \n Remark 1 A closer inspection of the above arguments show that there were really only two properties of the nonstandard finite field that were really used (besides the fact that was a characteristic zero field). The first was the PAC axiom (Lemma 5 ). The second was the properties of the Galois group , namely that this group is isomorphic to the profinite integers (or equivalently, that has precisely one field extension of each degree). It turns out that these properties in fact completely characterise infinite pseudo-finite fields, which is the main result of the paper of Ax cited previously. \n Filed under: expository , math.AG , math.LO , math.NT Tagged: definability , finite fields , Galois theory , Lang-Weil bound , model theory , nonstandard analysis , quantifiers , ultraproducts"], "link": "http://terrytao.wordpress.com/2012/09/12/definable-subsets-over-nonstandard-finite-fields-and-almost-quantifier-elimination/", "bloglinks": {}, "links": {"http://rjlipton.wordpress.com/": 1, "http://www.ams.org/": 9, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 32, "http://terrytao.wordpress.com/": 40}, "blogtitle": "What's new"}, {"content": ["[ Note: the idea for this post originated before the recent preprint of Mochizuki on the abc conjecture was released, and is not intended as a commentary on that work, which offers a much more non-trivial perspective on scheme theory. -T. ]\n \n \nIn classical algebraic geometry, the central object of study is an algebraic variety over a field (and the theory works best when this field is algebraically closed). One can talk about either affine or projective varieties; for sake of discussion, let us restrict attention to affine varieties. Such varieties can be viewed in at least four different ways:\n \n \n (Algebraic geometry) One can view a variety through the set of points (over ) in that variety. \n (Commutative algebra) One can view a variety through the field of rational functions on that variety, or the subring of polynomial functions in that field. \n (Dual algebraic geometry) One can view a variety through a collection of polynomials that cut out that variety. \n (Dual commutative algebra) One can view a variety through the ideal of polynomials that vanish on that variety.\n \n \n \nFor instance, the unit circle over the reals can be thought of in each of these four different ways:\n \n \n (Algebraic geometry) The set of points . \n (Commutative algebra) The quotient of the polynomial ring by the ideal generated by (or equivalently, the algebra generated by subject to the constraint ), or the fraction field of that quotient. \n (Dual algebraic geometry) The polynomial . \n (Dual commutative algebra) The ideal generated by .\n \n \n \nThe four viewpoints are almost equivalent to each other (particularly if the underlying field is algebraically closed), as there are obvious ways to pass from one viewpoint to another. For instance, starting with the set of points on a variety, one can form the space of rational functions on that variety, or the ideal of polynomials that vanish on that variety. Given a set of polynomials, one can cut out their zero locus, or form the ideal that they generate. Given an ideal in a polynomial ring, one can quotient out the ring by the ideal and then form the fraction field. Finally, given the ring of polynomials on a variety, one can form its spectrum (the space of prime ideals in the ring) to recover the set of points on that variety (together with the Zariski topology on that variety).\n \n \nBecause of the connections between these viewpoints, there are extensive \u201cdictionaries\u201d (most notably the ideal-variety dictionary ) that convert basic concepts in one of these four perspectives into any of the other three. For instance, passing from a variety to a subvariety shrinks the set of points and the function field, but enlarges the set of polynomials needed to cut out the variety, as well as the associated ideal. Taking the intersection or union of two varieties corresponds to adding or multiplying together the two ideals respectively. The dimension of an (irreducible) algebraic variety can be defined as the transcendence degree of the function field, the maximal length of chains of subvarieties, or the Krull dimension of the ring of polynomials. And so on and so forth. Thanks to these dictionaries, it is now commonplace to think of commutative algebras geometrically, or conversely to approach algebraic geometry from the perspective of abstract algebra. There are however some very well known defects to these dictionaries, at least when viewed in the classical setting of algebraic varieties. The main one is that two different ideals (or two inequivalent sets of polynomials) can cut out the same set of points, particularly if the underlying field is not algebraically closed. For instance, if the underlying field is the real line , then the polynomial equations and cut out the same set of points, namely the empty set, but the ideal generated by in is certainly different from the ideal generated by . This particular example does not work in an algebraically closed field such as , but in that case the polynomial equations and also cut out the same set of points (namely the origin), but again and generate different ideals in . Thanks to Hilbert\u2019s nullstellensatz , we can get around this problem (in the case when is algebraically closed) by always passing from an ideal to its radical , but this causes many aspects of the theory of algebraic varieties to become more complicated when the varieties involved develop singularities or multiplicities, as can already be seen with the simple example of Bezout\u2019s theorem .\n \n \nNowadays, the standard way to deal with these issues is to replace the notion of an algebraic variety with the more general notion of a scheme . Roughly speaking, the way schemes are defined is to focus on the commutative algebra perspective as the primary one, and to allow the base field to be not algebraically closed, or even to just be a commutative ring instead of a field. (One could even consider non-commutative rings, leading to non-commutative geometry , but we will not discuss this extension of scheme theory further here.) Once one generalises to these more abstract rings, the notion of a rational function becomes more complicated (one has to work locally instead of globally, cutting out the points where the function becomes singular), but as a first approximation one can think of a scheme as basically being the same concept as a commutative ring. (In actuality, due to the need to localise, a scheme is defined as a sheaf of rings rather than a single ring, but these technicalities will not be important for the purposes of this discussion.) All the other concepts from algebraic geometry that might previously have been defined using one of the other three perspectives, are then redefined in terms of this ring (or sheaf of rings) in order to generalise them to schemes.\n \n \nThus, for instance, in scheme theory the rings and describe different schemes; from the classical perspective, they cut out the same locus, namely the point , but the former scheme makes this point \u201cfatter\u201d than the latter scheme, giving it a degree (or multiplicity) of rather than .\n \n \nBecause of this, it seems that the link between the commutative algebra perspective and the algebraic geometry perspective is still not quite perfect in scheme theory, unless one is willing to start \u201cfattening\u201d various varieties to correctly model multiplicity or singularity. But \u2013 and this is the trivial remark I wanted to make in this blog post \u2013 one can recover a tight connection between the two perspectives as long as one allows the freedom to arbitrarily extend the underlying base ring.\n \n \nHere\u2019s what I mean by this. Consider classical algebraic geometry over some commutative ring (not necessarily a field). Any set of polynomials in indeterminate variables with coefficients in determines, on the one hand, an ideal \n \n \n in , and also cuts out a zero locus\n \n since each of the polynomials clearly make sense as maps from to . Of course, one can also write in terms of :\n \n Thus the ideal uniquely determines the zero locus , and we will emphasise this by writing as . As the previous counterexamples illustrate, the converse is not true. However, whenever we have any extension of the ring (i.e. a commutative ring that contains as a subring), then we can also view the polynomials as maps from to , and so one can also define the zero locus for all the extensions:\n \n \n As before, is determined by the ideal :\n \n \n \nThe trivial remark is then that while a single zero locus is insufficient to recover , the collection of zero loci for all extensions of (or more precisely, the assignment map , known as the functor of points of ) is sufficient to recover , as long as at least one zero locus, say , is non-empty. Indeed, suppose we have two ideals of that cut out the same non-empty zero locus for all extensions of , thus \n \n for all extensions of . We apply this with the extension of given by . Note that the embedding of in is injective, since otherwise would cut out the empty set as the zero locus over , and so is indeed an extension of . Tautologically, the point lies in , and thus necessarily lies in as well. Unpacking what this means, we conclude that whenever , that is to say that . By a symmetric argument, we also have , and thus as claimed. (As pointed out in comments, this fact (and its proof) is essentially a special case of the Yoneda lemma . The connection is tighter if one allows to be any ring with a (not necessarily injective) map from into it, rather than an extension of , in which case one can also drop the hypothesis that is non-empty for at least one . For instance, for every extension of the integers, but if one also allows quotients such as or instead, then and are no longer necessarily equal.) \n \nThus, as long as one thinks of a variety or scheme as cutting out points not just in the original base ring or field, but in all extensions of that base ring or field, one recovers an exact correspondence between the algebraic geometry perspective and the commutative algebra perspective. This is similar to the classical algebraic geometry position of viewing an algebraic variety as being defined simultaneously over all fields that contain the coefficients of the defining polynomials, but the crucial difference between scheme theory and classical algebraic geometry is that one also allows definition over commutative rings, and not just fields. In particular, one needs to allow extensions to rings that may contain nilpotent elements, otherwise one cannot distinguish an ideal from its radical.\n \n \nThere are of course many ways to extend a field into a ring, but as an analyst, one way to do so that appeals particularly to me is to introduce an epsilon parameter and work modulo errors of . To formalise this algebraically, let\u2019s say for sake of concreteness that the base field is the real line . Consider the ring of real-valued quantities that depend on a parameter (i.e. functions from to ), which are locally bounded in the sense that is bounded whenever is bounded. (One can, if one wishes, impose some further continuity or smoothness hypotheses on how depends on , but this turns out not to be relevant for the following discussion. Algebraists often prefer to use the ring of Puiseux series here in place of , and a nonstandard analyst might instead use the hyperreals , but again this will not make too much difference for our purposes.) Inside this commutative ring, we can form the ideal of quantities that are of size as , i.e. there exists a quantity independent of such that for all sufficiently small . This can easily be seen to indeed be an ideal in . We then form the quotient ring . Note that is equivalent to the assertion that , so we are encoding the analyst\u2019s notion of \u201cequal up to errors of \u201d into algebraic terms.\n \n \nClearly, is a commutative ring extending . Hence, any algebraic variety \n \n defined over the reals (so the polynomials have coefficients in ), also is defined over :\n \n \n In language that more closely resembles analysis, we have\n \n \n Thus we see that is in some sense an \u201c -thickening\u201d of , and is thus one way to give rigorous meaning to the intuition that schemes can \u201cthicken\u201d varieties. For instance, the scheme associated to the ideal , when interpreted over , becomes an neighbourhood of the origin\n \n but the scheme associated to the smaller ideal , when interpreted over , becomes an -neighbourhood of the origin, thus being a much \u201cfatter\u201d point:\n \n \n \nOnce one introduces the analyst\u2019s epsilon, one can see quite clearly that is coming from a larger scheme than , with fewer polynomials vanishing on it; in particular, the polynomial vanishes to order on but does not vanish to order on .\n \n \nBy working with this analyst\u2019s extension of , one can already get a reasonably good first approximation of what schemes over look like, which I found particularly helpful for getting some intuition on these objects. However, since this is only one extension of , and not a \u201cuniversal\u201d such extension, it cannot quite distinguish any two schemes from each other, although it does a better job of this than classical algebraic geometry. For instance, consider the scheme cut out by the polynomials in two dimensions. Over , this becomes \n \n \n Note that the polynomial vanishes to order on this locus, but fails to lie in the ideal . Equivalently, we have , despite and being distinct ideals. Basically, the analogue of the nullstellensatz for does not completely remove the need for performing a closure operation on the ideal ; it is less severe than taking the radical, but is instead more like taking a \u201cconvex hull\u201d in that one needs to be able to \u201cinterpolate\u201d between two polynomials in the ideal (such as and to arrive at intermediate polynomials (such as ) that one then places in the ideal. \n \nOne can also view ideals (and hence, schemes), from a model-theoretic perspective. Let be an ideal of a polynomial ring generated by some polynomials . Then, clearly, if is another polynomial in the ideal , then we can use the axioms of commutative algebra (which are basically the axioms of high school algebra) to obtain the syntactic deduction \n \n (since is just a sum of multiples of ). In particular, we have the semantic deduction \n \n for any assignment of indeterminates in (or in any extension of ). If we restrict to lie in only, then (even if is an algebraically closed field), the converse of the above statement is false; there can exist polynomials outside of for which (1) holds for all assignments in . For instance, we have\n \n for all in an algebraically closed field, despite not lying in the ideal . Of course, the nullstellensatz again explains what is going on here; (1) holds whenever lies in the radical of , which can be larger than itself. But if one allows the indeterminates to take values in arbitrary extensions of , then the truth of the converse is restored, thus giving a \u201ccompleteness theorem\u201d relating the syntactic deductions of commutative algebra to the semantic interpretations of such algebras over the extensions . For instance, since\n \n we no longer have a counterexample to the converse coming from and once we work in instead of . On the other hand, we still have\n \n so the extension is not powerful enough to detect that does not actually lie in ; a larger ring (which is less easy to assign an analytic interpretation to) is needed to achieve this. \n Filed under: expository , math.AC , math.AG Tagged: algebraic varieties , commutative rings , Hilbert's nullstellensatz , nullstellensatz , schemes"], "link": "http://terrytao.wordpress.com/2012/09/05/a-trivial-remark-about-schemes/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://terrytao.wordpress.com/": 10, "http://en.wikipedia.org/": 15, "http://www.ac.jp/": 1}, "blogtitle": "What's new"}]