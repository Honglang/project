[{"blogurl": "http://matlabdatamining.blogspot.com\n", "blogroll": [], "title": "Data Mining in MATLAB"}, {"content": ["Perhaps the most fundamental statistical summary beyond simple counting or totaling is the mean. The mean reduces a collection of numbers to a single value, and is one of a number of measures of location . The mean is by far the most commonly used and widely understood way of averaging data, but it is not the only one, nor is it always the \"best\" one. In terms of popularity, the median is a distant second, but it offers a mixture of behaviors which make it an appealing alternative in many circumstances. One important property of the median is that it is not affected- at all- by extreme values. No matter how we change any observation, its value will have zero effect on the sample median unless it changes from being above the median to below it, or vice versa. If the maximum value in a data set is multiplied by a million, the median will not change. This behavior is usually characterized as a benefit of the median, as it means that noisy or extreme values will not tug at the median. This is in stark contrast to the sample mean, which blows around in the wind of any change to the data set. Indeed, this is one of the nice things about working with the median: It is highly resistant to noise. Also typically cast as a benefit is the median's resistance to being tugged to extreme values by long-tailed distributions. Whether this is truly a strength or a weakness, though, I will leave to the analyst to decide, since capturing the nature of a long-tail may be important for some problems. Another important quality of the sample median is that, for odd numbers of observations, it is always a value from the original data set. This is also true for many data sets with even numbers of observations. In some situations, this is desirable. Data summaries being shown to non-technical people often provide more aesthetic appeal if there are no \"odd\" values. Consider a retail store in which all products are prices with .99 at the end, yet the reported mean ends in .73- this is normally not an issue with the median. Replacing missing values with means very likely introduces new values to the data set, and this is especially awkward when dealing with variables which present limited distributions, such as counts. As an example, consider a database with missing values for \"number of children\", which (one would expect!) would always be an integer. Substituting the mean for missings may result in observations sporting \"1.2\" children. This is not a problem with the median. Note that this behavior has a darker side for signal- and image-processing problems: data treated with a windowed median tend to form stepped plateaus, rather than smooth curves. Such artifacts can be distracting or worse. On the downside, the median is not always the most statistically efficient summary. This is a technical issue which means simply that the median may \"wander\" from the \"true\" value (the population parameter) more than other summaries when finite data are available. For instance, when the data are known to be drawn from a true normal distribution, the sample mean is known to \"wander\" the least from the true value. I'm sparing the reader the technical details here, but suffice it to say that, though the mean or median might be closer to the \"true\" value in any particular situation, the mean is likely to be closer most of the time. Statistical efficiency may be measured, and it is worth noting that different summaries achieve different relative efficiencies, depending on the size of the data sample, and the shape of the population distribution. Hopefully, this short note has whet your appetite for the median and other alternatives to the mean. I encourage the reader to explore yet other alternatives, such as trimmed means, of which the mean and median are special cases."], "link": "http://matlabdatamining.blogspot.com/feeds/3495347452275115253/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Overview Linear discriminant analysis (LDA) is one of the oldest mechanical classification systems, dating back to statistical pioneer Ronald Fisher, whose original 1936 paper on the subject, The Use of Multiple Measurements in Taxonomic Problems , can be found online (for example, here ). The basic idea of LDA is simple: for each class to be identified, calculate a (different) linear function of the attributes. The class function yielding the highest score represents the predicted class. There are many linear classification models, and they differ largely in how the coefficients are established. One nice quality of LDA is that, unlike some of the alternatives, it does not require multiple passes over the data for optimization. Also, it naturally handles problems with more than two classes and it can provide probability estimates for each of the candidate classes. Some analysts attempt to interpret the signs and magnitudes of the coefficients of the linear scores, but this can be tricky, especially when the number of classes is greater than 2. LDA bears some resemblance to principal components analysis (PCA), in that a number of linear functions are produced (using all raw variables), which are intended, in some sense, to provide data reduction through rearrangement of information. (See the Feb-26-2010 posting to this log, Principal Components Analysis .) Note, though, some important differences: First, the objective of LDA is to maximize class discrimination, whereas the objective of PCA is to squeeze variance into as few components as possible. Second, LDA produces exactly as many linear functions as there are classes, whereas PCA produces as many linear functions as there are original variables. Last, principal components are always orthogonal to each other (\"uncorrelated\"), while that is not generally true for LDA's linear scores. An Implementation I have made available on MATLAB Central , a routine, aptly named LDA which performs all the necessary calculations. I'd like to thank Deniz Sevi\u015f, whose prompting got me to finally write this code (with her) and whose collaboration is very much appreciated. Note that the LDA function assumes that the data its being fed is complete (no missing values) and performs no attribute selection. Also, it requires only base MATLAB (no toolboxes needed). Use of LDA is straightforward: the programmer supplies the input and target variables and, optionally, prior probabilities. The function returns the fitted linear discriminant coefficients. help LDA provides a good example:  % Generate example data: 2 groups, of 10 and 15, respectively X = [randn(10,2); randn(15,2) + 1.5]; Y = [zeros(10,1); ones(15,1)];  % Calculate linear discriminant coefficients W = LDA(X,Y); This example randomly generates an artificial data set of two classes (labeled 0 and 1) and two input variables. The LDA function fits linear discriminants to the data, and stores the result in W . So, what is in W ? Let's take a look: >> W W =  -1.1997 0.2182 0.6110 -2.0697 0.4660 1.4718 The first row contains the coefficients for the linear score associated with the first class (this routine orders the linear functions the same way as unique() ). In this model, -1.1997 is the constant and 0.2182 and 0.6110 are the coefficients for the input variables for the first class (class 0). Coefficients for the second class's linear function are in the second row. Calculating the linear scores is easy:  % Calulcate linear scores for training data L = [ones(25,1) X] * W'; Each column represents the output of the linear score for one class. In this case, the first column is class 0, and the second column is class 1. For any given observation, the higher the linear score, the more likely that class. Note that LDA's linear scores are not probabilities, and may even assume negative values. Here are the values from my run: >> L L =  -1.9072 -3.8060  1.0547 3.2517 -1.2493 -2.0547 -1.0502 -1.7608 -0.6935 -0.8692 -1.6103 -2.9808 -1.3702 -2.4545 -0.2148 0.2825  0.4419 1.6717  0.2704 1.3067  1.0694 3.2670 -0.0207 0.7529 -0.2608 0.0601  1.2369 3.6135 -0.8951 -1.4542  0.2073 1.1687  0.0551 0.8204  0.1729 1.1654  0.2993 1.4344 -0.6562 -0.8028  0.2195 1.2068 -0.3070 0.0598  0.1944 1.2628  0.5354 2.0689  0.0795 1.0976 To obtain estimated probabilities, simply run the linear scores through the softmax transform (exponentiate everything, and normalize so that they sum to 1.0):  % Calculate class probabilities P = exp(L) ./ repmat(sum(exp(L),2),[1 2]); As we see, most of the first 10 cases exhibit higher probabilities for class 0 (the first column) than for class 1 (the second column) and the reverse is true for the last 15 cases: >> P P =  0.8697 0.1303  0.1000 0.9000  0.6911 0.3089  0.6705 0.3295  0.5438 0.4562  0.7975 0.2025  0.7473 0.2527  0.3782 0.6218  0.2262 0.7738  0.2619 0.7381  0.1000 0.9000  0.3157 0.6843  0.4205 0.5795  0.0850 0.9150  0.6363 0.3637  0.2766 0.7234  0.3175 0.6825  0.2704 0.7296  0.2432 0.7568  0.5366 0.4634  0.2714 0.7286  0.4093 0.5907  0.2557 0.7443  0.1775 0.8225  0.2654 0.7346 This model is not perfect, and would really need to be tested more rigorously (via holdout testing, k-fold cross validation, etc.) to determine how well it approximates the data. I will not demonstrate its use here, but the LDA routine offers a facility for modifying the prior probabilities. Briefly, the function assumes that the true distribution of classes is whatever it observes in the training data. Analysts, however, may wish to adjust this distribution for several reasons, and the third, optional, parameter allows this. Note that the LDA routine presented here always performs the adjustment for prior probabilities: Some statistical software drops the adjustment for prior probabilities altogether if the user specifies that classes are equally likely, and will produce different results than LDA . Closing Thoughts Though it employs a fairly simple model structure, LDA has held up reasonably well, sometimes still besting more complex algorithms. When its assumptions are met, the literature records it doing better than logistic regression. It is very fast to execute and fitted models are extremely portable- even a spreadsheet will support linear models (...or, one supposes, paper and pencil!) LDA is at least worth trying at the beginning of a project, if for no other reason than to establish a lower bound on acceptable performance. See Also Feb-16-2010 posting, Single Neuron Training: The Delta Rule Mar-15-2009 posting, Logistic Regression"], "link": "http://matlabdatamining.blogspot.com/feeds/2782936907946760214/comments/default", "bloglinks": {}, "links": {"http://www.mathworks.com/": 2, "http://digital.edu.au/": 1, "http://matlabdatamining.blogspot.com/": 3}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction In response to my Nov-10-2006 posting, Introduction To Entropy , an anonymous reader asked: Can we use entropy for distinguishing random signals and deterministic signal? Lets say i generate two signals in matlab. First signal using sin function and second using randn function. Can we use entropy to distinguish between these two signal? The short answer is: Yes, we can use entropy for this purpose, although even simpler summary statistics would reveal that the normally distributed randn data included values outside of -1..+1, while the sin data did not. In this article, I will be using my own entropy calculating routines, which can be found on MATLAB Central: Entropy , JointEntropy , ConditionalEntropy and MutualInformation . A Slightly Harder Problem To illustrate this application of entropy, I propose a slightly different problem, in which the sine data and the random data share the same distribution. To achieve this, the \"random\" data will be a random sample from the sine function: >> X = [1:1000]'; >> Sine = sin(0.05 * X); >> RandomData = sin(2 * pi * rand(size(X))); As a quick check on the distributions, we will examine their respective histograms: >> figure >> subplot(2,1,1), hist(Sine), xlabel('Sine Value'), ylabel('Frequency'), grid on >> subplot(2,1,2), hist(RandomData), xlabel('RandomData Value'), ylabel('Frequency'), grid on   Click image to enlarge. More or less, they appear to match. A First Look, Using Entropy At this point, the reader may be tempted to calculate the entropies of the two distributions, and compare them. Since their distributions (as per the histograms) are similar, we should expect their entropies to also be similar. To date, this Web log has only dealt with discrete entropy , yet our data is continuous. While there is a continuous entropy , we will stick with the simpler (in my opinion) discrete entropy for now. This requires that the real-valued numbers of our data be converted to symbols. We will accomplish this via quantization (\"binning\") to 10 levels: >> Sine10 = ceil(10 * (Sine + 1) / 2); >> RandomData10 = ceil(10 * (RandomData + 1) / 2); If the MATLAB Statistics Toolbox is installed, one can check the resulting frequencies thus (I apologize for Blogger's butchering of the text formatting): >> tabulate(Sine10) Value Count Percent  1  205  20.50%  2  91  9.10%  3  75  7.50%  4  66  6.60%  5  60  6.00%  6  66  6.60%  7  66  6.60%  8  75  7.50%  9  91  9.10%  10  205  20.50% >> tabulate(RandomData10) Value Count Percent  1  197  19.70%  2  99  9.90%  3  84  8.40%  4  68  6.80%  5  66  6.60%  6  55  5.50%  7  68  6.80%  8  67  6.70%  9  82  8.20%  10  214  21.40% It should be noted that other procedures could have been used for the signal-to-symbol conversion. For example, bin frequencies could have been made equal. The above method was selected because it is simple and requires no Toolbox functions. Also, other numbers of bins could have been utilized. Now that the data is represented by symbols, we may check the earlier assertion regarding similar distributions yielding similar entropies (measured in bits per observation): >> Entropy(Sine10) ans =  3.1473 >> Entropy(RandomData10) ans =  3.1418 As these are sample statistics, we would not expect them to match exactly, but these are very close. Another Perspective One important aspect of the structure of a sine curve is that it varies over time (or whatever the domain is). This means that any given sine value is typically very similar to those on either side. With this in mind, we will investigate the conditional entropy of each of these two signals versus themselves, lagged by one observation: >> ConditionalEntropy(Sine10(2:end),Sine10(1:end-1)) ans =  0.6631 >> ConditionalEntropy(RandomData10(2:end),RandomData10(1:end-1)) ans =  3.0519 Ah! Notice that the entropy of the sine data, given knowledge of its immediate predecessor is much lower than the entropy of the random data, given its immediate predecessor. These data are indeed demonstrably different insofar as they behave over time, despite sharing the same distribution. An astute reader may at this point notice that the conditional entropy of the random data, given 1 lagged value, is less than the entropy of the raw random data. This is an artifact of the finite number of samples and the quantization process. Given more observations and a finer quantization, this discrepancy between sample statistics and population statistics will shrink. Entropy could have been applied to this problem other ways, too. For instance, one might calculate entropy for short time windows. I would point out that other, more traditional procedures might be used instead, such as calculating the auto-correlation for lag 1. It is worth seeing how entropy adds to the analyst's toolbox, though. Further Reading See also the Apr-01-2009 posting, Introduction to Conditional Entropy . Print: The Mathematical Theory of Communication by Claude Shannon (ISBN 0-252-72548-4) Elements of Information Theory by Cover and Thomas (ISBN 0-471-06259)"], "link": "http://matlabdatamining.blogspot.com/feeds/7265113213949480442/comments/default", "bloglinks": {}, "links": {"http://www.mathworks.com/": 4, "http://2.blogspot.com/": 1, "http://matlabdatamining.blogspot.com/": 2}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Context The last posting to this Web log, Principal Components Analysis (Feb-26-2010), gave an overview of principal component analysis (PCA), and how to effect it within MATLAB. This article will cover three uses of PCA: 1. pre-processing for empirical modeling, 2. data compression and 3. noise suppression. To serve the widest possible audience, this article will conduct PCA using only base MATLAB functions, but realize that users with the Statistics Toolbox have, as mentioned in the last posting, the option of using tools like princomp and zscore . We will continue to use the very small data set used in the last article: >> A = [269.8 38.9 50.5 272.4 39.5 50.0 270.0 38.9 50.5 272.0 39.3 50.2 269.8 38.9 50.5 269.8 38.9 50.5 268.2 38.6 50.2 268.2 38.6 50.8 267.0 38.2 51.1 267.8 38.4 51.0 273.6 39.6 50.0 271.2 39.1 50.4 269.8 38.9 50.5 270.0 38.9 50.5 270.0 38.9 50.5 ]; We calculate the sample parameters, and standardize the data table: >> [n m] = size(A) n =  15 m =  3 >> AMean = mean(A) AMean = 269.9733 38.9067 50.4800 >> AStd = std(A) AStd =  1.7854 0.3751 0.3144 >> B = (A - repmat(AMean,[n 1])) ./ repmat(AStd,[n 1]) B =  -0.0971 -0.0178 0.0636  1.3591 1.5820 -1.5266  0.0149 -0.0178 0.0636  1.1351 1.0487 -0.8905 -0.0971 -0.0178 0.0636 -0.0971 -0.0178 0.0636 -0.9932 -0.8177 -0.8905 -0.9932 -0.8177 1.0178 -1.6653 -1.8842 1.9719 -1.2173 -1.3509 1.6539  2.0312 1.8486 -1.5266  0.6870 0.5155 -0.2544 -0.0971 -0.0178 0.0636  0.0149 -0.0178 0.0636  0.0149 -0.0178 0.0636 Now that the data is centered with mean 0.0 and standard deviation 1.0, we perform the eigenanalysis of the sample covariances to determine the coefficient matrix which generates the principal components: >> [V D] = eig(cov(B)) V =  0.6505 0.4874 -0.5825 -0.7507 0.2963 -0.5904 -0.1152 0.8213 0.5587 D =  0.0066   0   0   0 0.1809   0   0   0 2.8125 Recall that the MATLAB eig function orders information for the principal components from last to first when reading the columns from left to right. The matrix V contains the linear coefficients for the principal components. The diagonal of matrix D contains the variances for the principal components. So far, we have accomplished the principal components analysis itself. To put the PCA to use, we will want to know what proportion each principal component represents of total variance. We can do this by extracting and normalizing the diagonal of matrix D (we use flipud because the principal components are in \"reverse\" order): >> cumsum(flipud(diag(D))) / sum(diag(D)) ans =  0.9375  0.9978  1.0000 We interpret the above column of numbers to mean that the first principal component contains 93.75% of the total variance of the original data, the first two principal components together contain 99.78% and of course all principal components taken together have all of the variance (exactly as much as in the original standardized data). Last, to calculate the principal components themselves, simply multiply the standardized data by the coefficient matrix: >> PC = B * V PC =  -0.0571 -0.0003 0.1026 -0.1277 -0.1226 -2.5786  0.0157 0.0543 0.0373  0.0536 0.1326 -1.7779 -0.0571 -0.0003 0.1026 -0.0571 -0.0003 0.1026  0.0704 -1.4579 0.5637 -0.1495 0.1095 1.6299  0.1041 0.2496 3.1841  0.0319 0.3647 2.4306  0.1093 0.2840 -3.1275  0.0892 0.2787 -0.8467 -0.0571 -0.0003 0.1026  0.0157 0.0543 0.0373  0.0157 0.0543 0.0373 To verify the condensing of the variance, calculate the sample variances: >> var(PC) ans =  0.0066 0.1809 2.8125 Again, note that the first principal component appears in the last column when using MATLAB's eig function, and columns to the left have less and less variance until the last principal component, stored in the first column. Application: Pre-processing Data for Empirical Modeling This application of PCA is simple: calculate the principal components and choose from them rather than the original data to construct the empirical model (regression, neural network, etc.). The (hoped for) advantage of doing this is that since PCA squeezes information into a subset of the new variables, less of them will be necessary to construct the model. In fact, it would not be unreasonable to simply step through the first so many principal components to build the model: First, use just the first principal component, then try the first and second, then the first, second and third, etc. A nice side benefit is that all the principal components are uncorrelated with each other. As was mentioned in the last article, this may or may not work well, for several reasons: PCA may not be able to squeeze the variance much if the original variables are already highly uncorrelated with one another. Also, statistical variance may not be the same thing as \"information\" for the purposes of model building. Last, even if this process works, one is left with the reality that PCA needs all of the original variables to calculate the principal components, even if only a subset of them are used. Regardless, this is a data processing technique which can yield benefit, so it is worth trying. Application: Data Compression PCA offers a mechanism for performing lossy data compression. When data compression is \"lossy\", it may not return exactly the original data. The trade-off is that much greater compression can be achieved than with \"lossless\" data compression (compression in which the original data is returned exactly). In many cases, such as audio (MP3) and images (JPEG), some loss in fidelity is acceptable and greater compression is very much desired. All compression schemes rely on the discovery of regularities within the data. In the case of PCA, the regularity is a linear relationship among the variables. To the extent that PCA finds this relationship, the data may be compressed. The idea is to discard the last principal components (those exhibiting the least variance). In MATLAB, this means simply dropping the columns representing the unwanted principal components. In this case, we will retain only the first principal component: >> VReduced = V(:,3) VReduced =  -0.5825 -0.5904  0.5587 >> PCReduced = B * VReduced PCReduced =  0.1026 -2.5786  0.0373 -1.7779  0.1026  0.1026  0.5637  1.6299  3.1841  2.4306 -3.1275 -0.8467  0.1026  0.0373  0.0373 Decompression is accomplished by inverting the process, which we can do by transposing the coefficient vector and multiplying: >> PCReduced * VReduced' ans =  -0.0598 -0.0606 0.0573  1.5020 1.5224 -1.4406 -0.0217 -0.0220 0.0209  1.0356 1.0497 -0.9933 -0.0598 -0.0606 0.0573 -0.0598 -0.0606 0.0573 -0.3284 -0.3328 0.3150 -0.9494 -0.9623 0.9106 -1.8547 -1.8799 1.7789 -1.4158 -1.4351 1.3580  1.8217 1.8465 -1.7473  0.4932 0.4999 -0.4730 -0.0598 -0.0606 0.0573 -0.0217 -0.0220 0.0209 -0.0217 -0.0220 0.0209 The result is not exactly the same as the original standardized data, but it is pretty close. We \"un-standardize\" by reversing the original standardization step: >> Z = ((PCReduced * VReduced') .* repmat(AStd,[n 1])) + repmat(AMean,[n 1]) Z = 269.8667 38.8840 50.4980 272.6550 39.4777 50.0270 269.9345 38.8984 50.4866 271.8223 39.3004 50.1677 269.8667 38.8840 50.4980 269.8667 38.8840 50.4980 269.3870 38.7818 50.5790 268.2783 38.5457 50.7663 266.6619 38.2016 51.0393 267.4455 38.3684 50.9070 273.2259 39.5992 49.9306 270.8539 39.0942 50.3313 269.8667 38.8840 50.4980 269.9345 38.8984 50.4866 269.9345 38.8984 50.4866 Again, the result is pretty similar to the original, but not exactly: about 94% of the variance has been preserved, and we have compressed the data to 33% of its original size. The trade-off here is between compression (count of principal components retained) and compression fidelity (the variance preserved). In a typical application, there will be more variables and the variance compression is normally not quite as dramatic as in our illustration. This means that there will be more data compression \"levels\", represented by the number of principal components retained. Application: Noise Suppression Extending the data compression application, we may use PCA for noise suppression. The basic idea is that the variance captured by the least important principal components is noise which should be rejected. Assuming that the variables bear a linear relationship, they will lie in a line (plane, hyperplane) and noise items will lift them away from the line. Dropping the last principal components means flattening the data in a geometric sense and (hopefully) eliminating some of the noise. This process is much like the data compression process described in the last section, except: 1. discarded components have their coefficients set to zero instead of being deleted outright and 2. the PCA coefficient matrix and its inverse are multiplied together to allow a single processing step which (again, hopefully) reduces noise in the data. As before, we calculate the PCA coefficients: >> [V D] = eig(cov(B)) V =  0.6505 0.4874 -0.5825 -0.7507 0.2963 -0.5904 -0.1152 0.8213 0.5587 D =  0.0066   0   0   0 0.1809   0   0   0 2.8125 Deciding to eliminate the last principal component, we set its coefficients to zero: >> VDenoise = V; VDenoise(:,1) = 0 VDenoise =   0 0.4874 -0.5825   0 0.2963 -0.5904   0 0.8213 0.5587 This matrix will project the standardized data into a flat surface- in this case a plane, since we have retained 2 dimensions. Not wanting to bother with two steps, we multiply this matrix by its inverse, which in this case is easily obtained by taking the transpose: >> VDenoise = VDenoise * VDenoise' VDenoise =  0.5769 0.4883 0.0749  0.4883 0.4364 -0.0865  0.0749 -0.0865 0.9867 This magical matrix will, in a single matrix multiplication, denoise the standardized data: >> B * VDenoise ans =  -0.0599 -0.0607 0.0570  1.4422 1.4861 -1.5414  0.0047 -0.0060 0.0654  1.1002 1.0890 -0.8844 -0.0599 -0.0607 0.0570 -0.0599 -0.0607 0.0570 -1.0390 -0.7648 -0.8824 -0.8960 -0.9299 1.0005 -1.7330 -1.8060 1.9839 -1.2380 -1.3270 1.6575  1.9601 1.9307 -1.5141  0.6290 0.5825 -0.2442 -0.0599 -0.0607 0.0570  0.0047 -0.0060 0.0654  0.0047 -0.0060 0.0654 Naturally, we still need to multiply back the standard deviation and add back the mean to get to the original scale: >> Z = ((B * VDenoise) .* repmat(AStd,[n 1])) + repmat(AMean,[n 1]) Z = 269.8664 38.8839 50.4979 272.5483 39.4640 49.9954 269.9817 38.9044 50.5006 271.9377 39.3151 50.2019 269.8664 38.8839 50.4979 269.8664 38.8839 50.4979 268.1183 38.6198 50.2025 268.3736 38.5579 50.7946 266.8791 38.2293 51.1038 267.7630 38.4090 51.0012 273.4731 39.6308 50.0040 271.0964 39.1251 50.4032 269.8664 38.8839 50.4979 269.9817 38.9044 50.5006 269.9817 38.9044 50.5006 The degree of noise reduction is controlled by the number of principal components retained: the less principal components retained, the greater the noise reduction. Obviously, like all such schemes, this process has limitations and the big assumption here is that the original variables are linearly related so that noise stands out as a departure from this linearity. Final Thoughts PCA is a powerful tool, and is quickly computed on current computers, even on fairly large data. While there are limits to what it can do, it is a handy tool which is inexpensive in terms of compute time. Further Reading As a general reference on PCA see: Multivariate Statistical Methods: A Primer , by Manly (ISBN: 0-412-28620-3) Note: The first edition is adequate for understanding and coding PCA, and is at present much cheaper than the second or third editions. The noise suppression application is described in the article, Vectors help make sense of multiple signals , by Sullivan, Personal Engineering and Instrumentation News (Dec-1997), in which it is referred to as subspace projection . See also the Dec-11-2010 posting, Linear Discriminant Analysis (LDA) ."], "link": "http://matlabdatamining.blogspot.com/feeds/7895316337508116644/comments/default", "bloglinks": {}, "links": {"http://matlabdatamining.blogspot.com/": 2}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction Real-world data sets usually exhibit relationships among their variables. These relationships are often linear, or at least approximately so, making them amenable to common analysis techniques. One such technique is principal component analysis (\"PCA\"), which rotates the original data to new coordinates, making the data as \"flat\" as possible. Given a table of two or more variables, PCA generates a new table with the same number of variables, called the principal components . Each principal component is a linear transformation of the entire original data set. The coefficients of the principal components are calculated so that the first principal component contains the maximum variance (which we may tentatively think of as the \"maximum information\"). The second principal component is calculated to have the second most variance, and, importantly, is uncorrelated (in a linear sense) with the first principal component. Further principal components, if there are any, exhibit decreasing variance and are uncorrelated with all other principal components. PCA is completely reversible (the original data may be recovered exactly from the principal components), making it a versatile tool, useful for data reduction, noise rejection, visualization and data compression among other things. This article walks through the specific mechanics of calculating the principal components of a data set in MATLAB, using either the MATLAB Statistics Toolbox, or just the base MATLAB product. Performing Principal Components Analysis Performing PCA will be illustrated using the following data set, which consists of 3 measurements taken of a particular subject over time: >> A = [269.8 38.9 50.5 272.4 39.5 50.0 270.0 38.9 50.5 272.0 39.3 50.2 269.8 38.9 50.5 269.8 38.9 50.5 268.2 38.6 50.2 268.2 38.6 50.8 267.0 38.2 51.1 267.8 38.4 51.0 273.6 39.6 50.0 271.2 39.1 50.4 269.8 38.9 50.5 270.0 38.9 50.5 270.0 38.9 50.5 ]; We determine the size of this data set thus: >> [n m] = size(A) n =  15 m =  3 To summarize the data, we calculate the sample mean vector and the sample standard deviation vector: >> AMean = mean(A) AMean = 269.9733 38.9067 50.4800 >> AStd = std(A) AStd =  1.7854 0.3751 0.3144 Most often, the first step in PCA is to standardize the data. Here, \"standardization\" means subtracting the sample mean from each observation, then dividing by the sample standard deviation. This centers and scales the data. Sometimes there are good reasons for modifying or not performing this step, but I will recommend that you standardize unless you have a good reason not to. This is easy to perform, as follows: >> B = (A - repmat(AMean,[n 1])) ./ repmat(AStd,[n 1]) B =  -0.0971 -0.0178 0.0636  1.3591 1.5820 -1.5266  0.0149 -0.0178 0.0636  1.1351 1.0487 -0.8905 -0.0971 -0.0178 0.0636 -0.0971 -0.0178 0.0636 -0.9932 -0.8177 -0.8905 -0.9932 -0.8177 1.0178 -1.6653 -1.8842 1.9719 -1.2173 -1.3509 1.6539  2.0312 1.8486 -1.5266  0.6870 0.5155 -0.2544 -0.0971 -0.0178 0.0636  0.0149 -0.0178 0.0636  0.0149 -0.0178 0.0636 This calculation can also be carried out using the zscore function from the Statistics Toolbox: >> B = zscore(A) B =  -0.0971 -0.0178 0.0636  1.3591 1.5820 -1.5266  0.0149 -0.0178 0.0636  1.1351 1.0487 -0.8905 -0.0971 -0.0178 0.0636 -0.0971 -0.0178 0.0636 -0.9932 -0.8177 -0.8905 -0.9932 -0.8177 1.0178 -1.6653 -1.8842 1.9719 -1.2173 -1.3509 1.6539  2.0312 1.8486 -1.5266  0.6870 0.5155 -0.2544 -0.0971 -0.0178 0.0636  0.0149 -0.0178 0.0636  0.0149 -0.0178 0.0636 Calculating the coefficients of the principal components and their respective variances is done by finding the eigenfunctions of the sample covariance matrix: >> [V D] = eig(cov(B)) V =  0.6505 0.4874 -0.5825 -0.7507 0.2963 -0.5904 -0.1152 0.8213 0.5587 D =  0.0066   0   0   0 0.1809   0   0   0 2.8125 The matrix V contains the coefficients for the principal components. The diagonal elements of D store the variance of the respective principal components. We can extract the diagonal like this: >> diag(D) ans =  0.0066  0.1809  2.8125 The coefficients and respective variances of the principal components could also be found using the princomp function from the Statistics Toolbox: >> [COEFF SCORE LATENT] = princomp(B) COEFF =  0.5825 -0.4874 0.6505  0.5904 -0.2963 -0.7507 -0.5587 -0.8213 -0.1152 SCORE =  -0.1026 0.0003 -0.0571  2.5786 0.1226 -0.1277 -0.0373 -0.0543 0.0157  1.7779 -0.1326 0.0536 -0.1026 0.0003 -0.0571 -0.1026 0.0003 -0.0571 -0.5637 1.4579 0.0704 -1.6299 -0.1095 -0.1495 -3.1841 -0.2496 0.1041 -2.4306 -0.3647 0.0319  3.1275 -0.2840 0.1093  0.8467 -0.2787 0.0892 -0.1026 0.0003 -0.0571 -0.0373 -0.0543 0.0157 -0.0373 -0.0543 0.0157 LATENT =  2.8125  0.1809  0.0066 Note three important things about the above: 1. The order of the principal components from princomp is opposite of that from eig(cov(B)) . princomp orders the principal components so that the first one appears in column 1, whereas eig(cov(B)) stores it in the last column. 2. Some of the coefficients from each method have the opposite sign. This is fine: There is no \"natural\" orientation for principal components, so you can expect different software to produce different mixes of signs. 3. SCORE contains the actual principal components, as calculated by princomp . To calculate the principal components without princomp , simply multiply the standardized data by the principal component coefficients: >> B * COEFF ans =  -0.1026 0.0003 -0.0571  2.5786 0.1226 -0.1277 -0.0373 -0.0543 0.0157  1.7779 -0.1326 0.0536 -0.1026 0.0003 -0.0571 -0.1026 0.0003 -0.0571 -0.5637 1.4579 0.0704 -1.6299 -0.1095 -0.1495 -3.1841 -0.2496 0.1041 -2.4306 -0.3647 0.0319  3.1275 -0.2840 0.1093  0.8467 -0.2787 0.0892 -0.1026 0.0003 -0.0571 -0.0373 -0.0543 0.0157 -0.0373 -0.0543 0.0157 To reverse this transformation, simply multiply by the transpose of the coefficent matrix: >> (B * COEFF) * COEFF' ans =  -0.0971 -0.0178 0.0636  1.3591 1.5820 -1.5266  0.0149 -0.0178 0.0636  1.1351 1.0487 -0.8905 -0.0971 -0.0178 0.0636 -0.0971 -0.0178 0.0636 -0.9932 -0.8177 -0.8905 -0.9932 -0.8177 1.0178 -1.6653 -1.8842 1.9719 -1.2173 -1.3509 1.6539  2.0312 1.8486 -1.5266  0.6870 0.5155 -0.2544 -0.0971 -0.0178 0.0636  0.0149 -0.0178 0.0636  0.0149 -0.0178 0.0636 Finally, to get back to the original data, multiply each observation by the sample standard deviation vector and add the mean vector: >> ((B * COEFF) * COEFF') .* repmat(AStd,[n 1]) + repmat(AMean,[n 1]) ans = 269.8000 38.9000 50.5000 272.4000 39.5000 50.0000 270.0000 38.9000 50.5000 272.0000 39.3000 50.2000 269.8000 38.9000 50.5000 269.8000 38.9000 50.5000 268.2000 38.6000 50.2000 268.2000 38.6000 50.8000 267.0000 38.2000 51.1000 267.8000 38.4000 51.0000 273.6000 39.6000 50.0000 271.2000 39.1000 50.4000 269.8000 38.9000 50.5000 270.0000 38.9000 50.5000 270.0000 38.9000 50.5000 This completes the round trip from the original data to the principal components and back to the original data. In some applications, the principal components are modified before the return trip. Let's consider what we've gained by making the trip to the principal component coordinate system. First, more variance has indeed been squeezed in the first principal component, which we can see by taking the sample variance of principal components: >> var(SCORE) ans =  2.8125 0.1809 0.0066 The cumulative variance contained in the first so many principal components can be easily calculated thus: >> cumsum(var(SCORE)) / sum(var(SCORE)) ans =  0.9375 0.9978 1.0000 Interestingly in this case, the first principal component contains nearly 94% of the variance of the original table. A lossy data compression scheme which discarded the second and third principal components would compress 3 variables into 1, while losing only 6% of the variance. The other important thing to note about the principal components is that they are completely uncorrelated (as measured by the usual Pearson correlation), which we can test by calculating their correlation matrix: >> corrcoef(SCORE) ans =  1.0000 -0.0000 0.0000 -0.0000 1.0000 -0.0000  0.0000 -0.0000 1.0000 Discussion PCA \"squeezes\" as much information (as measured by variance) as possible into the first principal components. In some cases the number of principal components needed to store the vast majority of variance is shockingly small: a tremendous feat of data manipulation. This transformation can be performed quickly on contemporary hardware and is invertible, permitting any number of useful applications. For the most part, PCA really is as wonderful as it seems. There are a few caveats, however: 1. PCA doesn't always work well, in terms of compressing the variance. Sometimes variables just aren't related in a way which is easily exploited by PCA. This means that all or nearly all of the principal components will be needed to capture the multivariate variance in the data, making the use of PCA moot. 2. Variance may not be what we want condensed into a few variables. For example, if we are using PCA to reduce data for predictive model construction, then it is not necessarily the case that the first principal components yield a better model than the last principal components (though it often works out more or less that way). 3. PCA is built from components, such as the sample covariance, which are not statistically robust. This means that PCA may be thrown off by outliers and other data pathologies. How seriously this affects the result is specific to the data and application. 4. Though PCA can cram much of the variance in a data set into fewer variables, it still requires all of the variables to generate the principal components of future observations. Note that this is true, regardless of how many principal components are retained for the application. PCA is not a subset selection procedure, and this may have important logistical implications.  Further Reading See also the Feb-28-2010 posting, Putting PCA to Work and the Dec-11-2010 posting, Linear Discriminant Analysis (LDA) . Multivariate Statistical Methods: A Primer , by Manly (ISBN: 0-412-28620-3) Note: The first edition is adequate for understanding and coding PCA, and is at present much cheaper than the second or third editions."], "link": "http://matlabdatamining.blogspot.com/feeds/2511612350739234558/comments/default", "bloglinks": {}, "links": {"http://matlabdatamining.blogspot.com/": 2}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["I have recently put together a routine, DeltaRule, to train a single artificial neuron using the delta rule. DeltaRule can be found at MATLAB Central . This posting will not go into much detail, but this type of model is something like a logistic regression, where a linear model is calculated on the input variables, then passed through a squashing function (in this case the logistic curve). Such models are most often used to model binary outcomes, hence the dependent variable is normally composed of the values 0 and 1. Single neurons with linear functions (with squashing functions or not) are only capable of separating classes that may be divided by a line (plane, hyperplane), yet they are often useful, either by themselves or in building more complex models. Use help DeltaRule for syntax and a simple example of its use. Anyway, I thought readers might find this routine useful. It trains quickly and the code is straightforward (I think), making modification easy. Please write to let me know if you do anything interesting with it. If you are already familiar with simple neural models like this one, here are the technical details: Learning rule: incremental delta rule Learning rate: constant Transfer function: logistic Exemplar presentation order: random, by training epoch See also the Mar-15-2009 posting, Logistic Regression and the Dec-11-2010 posting, Linear Discriminant Analysis (LDA) ."], "link": "http://matlabdatamining.blogspot.com/feeds/7194781382987334661/comments/default", "bloglinks": {}, "links": {"http://www.mathworks.com/": 2, "http://matlabdatamining.blogspot.com/": 2}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Every month, TIOBE Software publishes its TIOBE Programming Community Index , a measure of programming language popularity based on a variety of sources. The current list, TIOBE Programming Community Index for July 2009 lists MATLAB as entering the Top 20 list (for the first time, I believe). While no such ordering is likely to be perfect, TIOBE seems to be one of the more comprehensive efforts for this sort of thing. I encourage readers to visit the TIOBE site because it is interesting to know what other tools are available, and what new languages are emerging. For interested parties, the July, 2009 Top 20 are, in descending order of popularity: 1. Java 2. C 3. C++ 4. PHP 5. (Visual) Basic 6. C# 7. Python 8. Perl 9. JavaScript 10. Ruby 11. Delphi 12. PL/SQL 13. SAS 14. RPG (OS/400) 15. Pascal 16. ABAP 17. Lisp/Scheme 18. D 19. Lua 20. MATLAB"], "link": "http://matlabdatamining.blogspot.com/feeds/8140144097709846915/comments/default", "bloglinks": {}, "links": {"http://www.tiobe.com/": 1}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction One nice feature of MATLAB is its provision of handy functions which are not part of the programming language proper. An excellent example of this is its support for images. The base MATLAB product provides routines for the loading from disk, manipulation, display and storing to disk of raster images. While it's true that one can find code libraries to perform these functions for other programming languages, like C++, the MATLAB model offers several advantages, not the least of which is standardization. If I write image-handling code in MATLAB, I know that every other MATLAB user on Earth can run my code without modification or the need for extra header files, libraries, etc. This article will serve as a brief introduction to the use of image data within MATLAB. Image Data Images are nearly always stored in digital computers in one of two forms: vector or raster . Vector images store images as line drawings (dots, line segments, polygons) defined by the spatial coordinates of their end points or vertices, and are most often used these days in artistic settings. A raster image is simply a 2-dimensional array of colored pixels (represented by numbers). This article will concentrate on the much more common raster form. Raster images, being arrays of numbers, are a natural fit for MATLAB, and indeed MATLAB is a convenient tool for applications such as image processing. Raster images always have 2 spatial dimensions (horizontal and vertical), and 1 or more color planes. Typically, grayscale images are stored as a 2-dimensional array, representing 1 color plane with values of 0.0 indicating black, 1.0 indicating white and intermediate values indicating various shades of gray. Color images are similar to grayscale images, but are most often stored as a 3-dimensional array, which is really a stack of three 2-dimensional color planes: one for each primary color: red, green and blue (\"RGB\"). As with grayscale images, values in the RGB color planes represent brightness of each color. Note that when all three color values are the same, the resulting color is a shade of gray. For the reader's knowledge, there are also index images which will not be covered here, but which are full of index numbers (integers) which do not represent colors directly, but instead indicate locations in a palette. Also, brightness values are often stored in files as integers, such as 0 - 255 instead of 0.0 to 1.0. Loading Images from Disk In MATLAB, images are read from disk using the imread function. Using imread is easy. The basic parameters are the location of the image file and the file format: >> A = imread('c:\\QRNG.png','PNG'); >> whos Name  Size     Bytes Class Attributes A   942 x 1680 x 3   4747680 uint8  This image is 942 pixels vertical by 1680 pixels horizontal, with 3 color planes (red, green and blue). Note that image data has been store in MATLAB as unsigned 8-bit integers (uint8). Since I often make multiple calculations on images, I typically convert the data type to double-precision real (double) and scale to 0.0 - 1.0 (though this will slow calculation): >> B = double(A) / 255;  Displaying Images Showing images on the screen is most easily accomplish using the image function: image(A) Grayscale images will display using a default palette, which can be changed via the colormap command: >>colormap gray Images will be fit to the screen, which may distort their aspect ratio. This can be fixed using: >>axis equal ...meaning that pixels will use equal scales horizontally and vertically. Manipulating Images As arrays, images can be modified using all the fun things we usually do to arrays in MATLAB (subsetting, math operations, etc.). I will mention one other useful base MATLAB tool for image processing: the rgb2hsv function, which converts an RGB image to an HSV one. HSV is a different colorspace (way of representing colors). HSV arrays are similar to RGB arrays, except their 3 color planes are hue , saturation and value (in that order). It is often convenient to work on the value (\"brightness\") plane, to isolate changes in light/dark from changes in the color. To get back to the land of RGB, use the function hsv2rgb . Saving Images to Disk Images can be saved to disk using the imwrite command. This is essentially the inverse of the imread command: imwrite(A,'New Image.bmp','BMP') ...with the parameters indicating the array to be saved as an image file, the file location and image file format, in that order. Note that MATLAB understands images as both 0 - 255 uint8 s and 0.0 - 1.0 double s, so there is no need to reverse this transformation before image storage. Conclusion Working on images in MATLAB is very convenient, especially when compared to more general-purpose languages. I urge the reader to check the help facility for the functions mentioned here to learn of further functionality. Further Reading For more information on image processing, I recommend either of the following books: Digital Image Processing (3rd Edition) by Gonzalez and Woods (ISBN-13: 978-0131687288) Algorithms for Image Processing and Computer Vision by J. R. Parker (ISBN-13: 978-0471140566)"], "link": "http://matlabdatamining.blogspot.com/feeds/5974504500467806389/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction In one of the earliest posts to this log, Introduction To Entropy (Nov-10-2006), I described the entropy of discrete variables. Finally, in this posting, I have gotten around to continuing this line of inquiry and will explain conditional entropy . Quick Review: Entropy Recall that entropy is a measure of uncertainty about the state of a variable. In the case of a variable which can take on only two values (male/female, profit/loss, heads/tails, alive/dead, etc.), entropy assumes its maximum value, 1 bit, when the probability of the outcomes is equal. A fair coin toss is a high-entropy event: Beforehand, we have no idea about what will happen. As the probability distribution moves away from a 50/50 split, uncertainty about the outcome decreases since there is less uncertainty as to the outcome. Recall, too, that entropy decreases regardless of which outcome class becomes the more probable: an unfair coin toss, with a heads / tails probability distribution of 0.10 / 0.90 has exactly the same entropy as another unfair coin with a distribution of 0.90 / 0.10. It is the distribution of probabilities, not their order which matters when calculating entropy. Extending these ideas to variables with more than 2 possible values, we note that generally, distributions which are evenly spread among the possible values exhibit higher entropy, and those which are more concentrated in a subset of the possible values exhibit lower entropy. Conditional Entropy Entropy, in itself, is a useful measure of the mixture of values in variables, but we are often interested in characterizing variables after they have been conditioned (\"modeled\"). Conditional entropy does exactly that, by measuring the entropy remaining in a variable, after it has been conditioned by another variable. I find it helpful to think of conditional entropy as \"residual entropy\". Happily for you, I have already assembled a MATLAB function, ConditionalEntropy , to calculate this measure. As an example, think of sporting events involving pairs of competitors, such as soccer (\"football\" if you live outside of North America). Knowing nothing about a particular pair of teams (and assuming no ties), the best probability we may assess that any specific team, say, the Black Eagles (Go, Be\u015fikta\u015f!), will win is: p(the Black Eagles win) = 0.5 The entropy of this variable is 1 bit- the maximum uncertainty possible for a two-category variable. We will attempt to lower this uncertainty (hence, lowering its conditional entropy). In some competitive team sports, it has been demonstrated statistically that home teams have an advantage over away teams. Among the most popular sports, the home advantage appears to be largest for soccer. One estimate shows that (excluding ties) home teams have historically won 69% of the time, and away teams 31% of the time. Conditioning the outcome on home vs. away, we may provide the follwing improved probability estimates: p(the Black Eagles win, given that they are the home team) = 0.69 p(the Black Eagles win, given that they are the away team) = 0.31 Ah, Now there is somewhat less uncertainty! The entropy for probability 0.69 is 0.8932 bits. The entropy for probability 0.31 (being the same distance from 0.5 as 0.69) is also 0.8932 bits. Conditional entropy is calculated as the weighted average of the entropies of the various possible conditions (in this case home or away). Assuming that it is equally likely that the Black Eagles play home or away, the conditional entropy of them winning is 0.8932 bits = (0.5 * 0.8932 + 0.5 * 0.8932). As entropy has gone down with our simple model, from 1 bit to 0.8932 bits, we learn that knowing whether the Black Eagles are playing at home or away provides information and reduces uncertainty. Other variables might be used to condition the outcome of a match, such as number of player injuries, outcome of recent games, etc. We can compare these candidate predictors using conditional entropy. The lower the conditional entropy, the lower the remaining uncertainty. It is even possible to assess a combination of predictors by treating each combination of univariate conditions as a separate condition (\"symbol\", in information theoretic parlance), thus: p(the Black Eagles win, given: that they are the home team and there are no injuries) = ... that they are the away team and there are no injuries) = ... that they are the home team and there is 1 injury) = ... that they are the away team and there is 1 injury) = ... etc. My routine, ConditionalEntropy can accommodate multiple conditioning variables. Conclusion The models being developed here are tables which simplistically cross all values of all input variables, but conditional entropy can also, for instance, be used to evaluate candidate splits in decision tree induction, or to assess class separation in discriminant analysis. Note that, as the entropies calculated in this article are based on sample probabilities, they suffer from the same limitations as all sample statistics (as opposed to population statistics). A sample entropy calculated from a small number observations likely will not agree exactly with the population entropy. Further Reading See also my Sep-12-2010 posting, Reader Question: Putting Entropy to Work . Print: The Mathematical Theory of Communication by Claude Shannon (ISBN 0-252-72548-4) Elements of Information Theory by Cover and Thomas (ISBN 0-471-06259)"], "link": "http://matlabdatamining.blogspot.com/feeds/9159087591578206888/comments/default", "bloglinks": {}, "links": {"http://www.mathworks.com/": 1, "http://matlabdatamining.blogspot.com/": 2}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["The L-1 regression routine, L1LinearRegression , originally mentioned in the Oct-23-2007 posting, L-1 Linear Regression , has been updated. The old version produced correct results, but the new one is more efficient. Thanks to reader Andreas Steimer for contacting me about this routine."], "link": "http://matlabdatamining.blogspot.com/feeds/127823086475110219/comments/default", "bloglinks": {}, "links": {"http://dwinnell.com/": 1, "http://matlabdatamining.blogspot.com/": 1}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["I'd like to alert readers to Rexer Analytics' 2009 Data Miner Survey. I urge you to participate by visiting the on-line survey at: Rexer Analytics' 2009 Data Miner Survey The Access Code is: TW4D2 Your entry is confidential and will help all of us better understand what is happening in the field of data mining. Thanks!"], "link": "http://matlabdatamining.blogspot.com/feeds/3159867815577325720/comments/default", "bloglinks": {}, "links": {"http://www.RexerAnalytics.com/": 1}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["This is just a short note to let everyone know that I have been working (finally) to restore the broken links on this Web log. I believe that the source code links have all now been fixed. This log has passed the 100,000 unique visitor mark without fanfare because I was too busy at the time to notice. I am thankful for those who continue to write in response to this Web log, and am glad to learn how many people this helps. The pace of posting has picked up, and I expect this to continue this year."], "link": "http://matlabdatamining.blogspot.com/feeds/2869445531316489428/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction Often, the analyst is required to construct a model which estimates probabilities. This is common in many fields: medical diagnosis (probability of recovery, relapse, etc.), credit scoring (probability of a loan being repaid), sports (probability of a team beating a competitor- wait... maybe that belongs in the \"investment\" category?). Many people are familiar with linear regression - why not just use that? There are several good reasons not to do this, but probably the most obvious is that linear models will always fall below 0.0 and poke out above 1.0, yielding answers which do not make sense as probabilities. Many different classification models have been devised which estimate the probability of class membership, such as linear and quadratic discriminant analysis, neural networks and tree induction. The technique covered in this article is logistic regression - one of the simplest modeling procedures. Logistic Regression Logistic regression is a member of the family of methods called generalized linear models (\"GLM\"). Such models include a linear part followed by some \"link function\". If you are familiar with neural networks, think of \"transfer functions\" or \"squashing functions\". So, the linear function of the predictor variables is calculated, and the result of this calculation is run through the link function. In the case of logistic regression, the linear result is run through a logistic function (see figure 1), which runs from 0.0 (at negative infinity), rises monotonically to 1.0 (at positive infinity). Along the way, it is 0.5 when the input value is exactly zero. Among other desirable properties, note that this logistic function only returns values between 0.0 and 1.0. Other GLMs operate similarly, but employ different link functions- some of which are also bound by 0.0 - 1.0, and some of which are not.  Figure 1: The Most Interesting Part of the Logistic Function (Click figure to enlarge) While calculating the optimal coefficients of a least-squares linear regression has a direct, closed-form solution, this is not the case for logistic regression. Instead, some iterative fitting procedure is needed, in which successive \"guesses\" at the right coefficients are incrementally improved. Again, if you are familiar with neural networks, this is much like the various training rules used with the simplest \"single neuron\" models. Hopefully, you are lucky enough to have a routine handy to perform this process for you, such as glmfit , from the Statistics Toolbox. glmfit The glmfit function is easy to apply. The syntax for logistic regression is: B = glmfit(X, [Y N], 'binomial', 'link', 'logit'); B will contain the discovered coefficients for the linear portion of the logistic regression (the link function has no coefficients). X contains the pedictor data, with examples in rows, variables in columns. Y contains the target variable, usually a 0 or a 1 representing the outcome. Last, the variable N contains the count of events for each row of the example data- most often, this will be a columns of 1s, the same size as Y . The count parameter, N , will be set to values greater than 1 for grouped data. As an example, think of medical cases summarized by country: each country will have averaged input values, an outcome which is a rate (between 0.0 and 1.0), and the count of cases from that country. In the event that the counts are greater than one, then the target variable represents the count of target class observations. Here is a very small example: >> X = [0.0 0.1 0.7 1.0 1.1 1.3 1.4 1.7 2.1 2.2]'; >> Y = [0 0 1 0 0 0 1 1 1 1]'; >> B = glmfit(X, [Y ones(10,1)], 'binomial', 'link', 'logit') B =  -3.4932  2.9402 The first element of B is the constant term, and the second element is the coefficient for the lone input variable. We apply the linear part of this logistic regression thus: >> Z = B(1) + X * (B(2)) Z =  -3.4932 -3.1992 -1.4350 -0.5530 -0.2589  0.3291  0.6231  1.5052  2.6813  2.9753 To finish, we apply the logistic function to the output of the linear part: >> Z = Logistic(B(1) + X * (B(2))) Z =  0.0295  0.0392  0.1923  0.3652  0.4356  0.5815  0.6509  0.8183  0.9359  0.9514 Despite the simplicity of the logistic function, I built it into a small function, Logistic , so that I wouldn't have to repeatedly write out the formula: % Logistic: calculates the logistic function of the input % by Will Dwinnell % % Last modified: Sep-02-2006 function Output = Logistic(Input) Output = 1 ./ (1 + exp(-Input)); % EOF  Conclusion Though it is structurally very simple, logistic regression still finds wide use today in many fields. It is quick to fit, easy to implement the discovered model and quick to recall. Frequently, it yields better performance than competing, more complex techniques. I recently built a logistic regression model which beat out a neural network, decision trees and two types of discriminant analysis. If nothing else, it is worth fitting a simple model such as logistic regression early in a modeling project, just to establish a performance benchmark for the project. Logistic regression is closely related to another GLM procedure, probit regression , which differs only in its link function (specified in glmfit by replacing 'logit' with 'probit'). I believe that probit regression has been losing popularity since its results are typically very similar to those from logistic regression, but the formula for the logistic link function is simpler than that of the probit link function.  References Generalized Linear Models , by McCullagh and Nelder (ISBN-13: 978-0412317606) See Also The Apr-21-2007 posting, Linear Regression in MATLAB , the Feb-16-2010 posting, Single Neuron Training: The Delta Rule and the Dec-11-2010 posting, Linear Discriminant Analysis (LDA) ."], "link": "http://matlabdatamining.blogspot.com/feeds/8522370383437188088/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://matlabdatamining.blogspot.com/": 3}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["MATLAB is on the move. Release 2009a brings a number of changes. The function of the random number generators had already begun to change in the base product as of the last release, if you hadn't noticed, and several functions (min, max, sum and prod, as well as several of the FFT functions) are now multi-threaded. This release also witnesses several changes to the analytical toolboxes. Among others... In the Statistics Toolbox... A Na\u00efve Bayes modeling tool has been added. This is a completely different way of modeling than the other technique in the Statistics Toolbox. Obviously, the more diverse to set of modeling tools, the better. Data table joining has been enhanced several ways, including the ability to use multiple keys and different types of joins (inner, outer, etc.). A number of changes to the tree induction facility ( classregtree ), including a fix to the quirky splitmin parameter. Now the programmer can specify the minimum number of cases per leaf node, which seems like a better way to control decision tree growth. There are also new options for model ensembles and performance curve summaries.  In the Curve Fitting Toolbox... Yipee! There are now functions for surface fitting (functions fit to 2 inputs, instead of just 1). Both interactive and programmatic fitting is available. In the Parallel Computing Toolbox... The maximum number of local workers has been increased from 4 to 8."], "link": "http://matlabdatamining.blogspot.com/feeds/6336291720407435833/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction This article introduces the percentile bootstrap , the simplest of the bootstrap methods. The bootstrap family of techniques are used to establish confidence intervals and calculate hypothesis tests for statistical measures. Problem Statement and Conventional Solution One is often required to summarize a set of data, such as the following: X =  2  10  10  5  8  1  4  9  8  10 The most commonly used summary is the mean, in MATLAB calculated thus:  >> mean(X) ans =  6.7000  Summaries, however, discard a great deal of information. In any situation, it is helpful to know the quality of our summary. In the case above, we may wonder how far our sample mean is likely to be the true population mean (the mean of all numbers drawn from the theoretical statistical population). Our sample mean, after all, was calculated from only 10 observations. We may establish some idea of how far off the sample mean may be from the population mean by calculating the standard error of the sample mean , which is the standard deviation divided by the square root of the sample size. In MATLAB: >> StandardError = std(X) / sqrt(length(X)) StandardError =  1.0858 Note that there are fancier versions of this calculation, for example for cases in which the population size is finite, or when the standard error of a proportion is being calculated. The standard error gives us an estimate of how far away our sample error might be form the true population mean, and acts like a z-score : the population mean is within 2 times the standard error from the sample mean about 95% of the time. In the case of our little data set, this would be from 4.5285 (= 6.7000 - 2 * 1.0858) to 8.8715 (= 6.7000 + 2 * 1.0858). Note that as the number of observations grows, the bottom part of the standard error fraction becomes larger and the standard error decreases. This seems natural enough: with more data, our confidence in our statistic increases. Complications of the Problem Statement So far, so good: We may have had to look up the standard error formula in a book, but we have established some sort of parameters as to the certainty of our summary. What if we didn't have such a reference, though? The median for example, has no such simple formula to establish its certainty. (Actually, I believe there is a formula for the median, but that it is a real bear!) Anyway, there certainly are other measures which we may calculate (even ones which we invent on the spot), for which there are no handy standard error formulas. What to do? An Alternative Solution: The Bootstrap Just as we are about to throw up our hands and consider another career, the bootstrap appears. The basic method of the bootstrap is simple: Draw many samples with replacement from the original sample (\"replicates\"), and tabulate the summary statistic when calculated on each those replicate samples. The distribution of those replicated summaries is intended to mimic the distribution being parameterized by the standard error of the mean. Above, I mentioned that the population mean would be found inside the band from the sample mean minus two times the standard error to the sample mean plus two times the standard error about 95% of the time. The equivalent area in our bootstrap process would be between the 2.5 and 97.5 percentiles of our replicate summaries. We use 2.5 and 97.5 because that leaves a total of 5% outside of the range, half on each end of the spectrum. An example using the median will illustrate this process. For reference, let's calculate the sample median first: >> median(X) ans =  8 Drawing a single sample with replacement can be done in MATLAB by indexing using random integers: RandomSampleWithReplacement =  5  8  1  1  10  10  9  9  5  1 This is our first bootstrap replicate. Now, we calculate our summary on this replicate: >> median(RandomSampleWithReplacement) ans =  6.5000 To discern the distribution, though, will require many more replicates. Since the computer is doing all of the work, I generally like to run at least 2000 replicates to give the bootstrap distribution a chance to take shape: rand('twister',1242) % Seed the random number generator for repeatability T = NaN(2000,1); % Allocate space for the replicated summaries for i = 1:2000 % The machine's doing the work, so why not? RandomSampleWithReplacement = X(ceil(length(X) * rand(length(X),1))); % Draw a sample with replacement T(i) = median(RandomSampleWithReplacement); % Calculate the replicated summary end (I apologize if the code is a bit cramped, but I have not been able to figure out how to insert tabs or indentation in this edit window.) Now, estimating where the \"real\" median (the population median) is likely to be is a simple matter of checking percentiles in our replicated summaries. I have the Statistic Toolbox, so I will cheat by using a function from there: >> prctile(T,[2.5 97.5]) ans =  3.5000 10.0000 So, our population median is likely to lie between 3.5 and 10. That is a pretty wide range, but this is the consequence of having so little data. Wrap-Up The fundamental trade-off of the bootstrap is that one forsakes pat statistical formulas in favor of strenuous computation. In summary: Good: -The bootstrap solves many problems not amenable to conventional methods. -Even in cases where conventional solutions exist, the bootstrap requires no memory or selection of correct formulas for given situations. Bad: -The bootstrap requires considerable numerical computation. Of course, in an era of cheap and powerful computing machinery, this is much less of an issue. Still, if there are many of these to perform... -The bootstrap presented in this article, the bootstrap percentile, is known to deviate from theoretically correct answers, though generally in a small way. There are more sophisticated bootstrap procedures which address some of these concerns, though. This process, owing to its very general nature, can be applied to tasks much more complex than estimating the uncertainty of statistical summaries, such as hypothesis testing and predictive model performance evaluation. Further Reading An Introduction to the Bootstrap by Efron and Tibshirani (ISBN 0-412-04231-2) This is the seminal work in this field. It covers a lot of ground, but is a bit mathematical."], "link": "http://matlabdatamining.blogspot.com/feeds/9050869399005827929/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction Recently, I needed to assemble a report which is to be generated quarterly and distributed to those unfortunate enough not to have MATLAB. Currently, such reports are distributed as Excel workbooks. Nearly everyone who produces such reports where I work does so by generating tables of numbers and cutting-and-pasting them into Excel (yuck!). As I have a number of these reports to produce, I was motivated to construct a more automatic solution. Happily, MATLAB can easily get data from or send data to Excel documents. The mechanics of this are not difficult, but I thought that readers might not be aware that this facility exists and just how easy this is to accomplish. There is just one catch: For users without Excel to act as a COM server (such as UNIX users), 'basic' mode is required, and functionality will be limited: See help xlsread for details. Getting Data From Excel in MATLAB MATLAB's function for extracting data from Excel documents is xlsread . Using it is as simple as this: [NumericData TextData] = xlsread(FileName,SheetName,CellRange) ...where NumericData and TextData contain the numeric and text data read from the workbook, respectively; and FileName , SheetName and CellRange are the names of the Excel document, sheet name and cell range from which to read. Often, I find myself needing to read data from growing ranges of cells within Excel spreadsheets. Think of daily rainfall data stored in a single column within a spreadsheet, which periodically has data appended to it. To load such data, simply set the range of cells to be much larger than the existing range: xlsread will ignore the extra empty spreadsheet cells. Getting Data Into Excel in MATLAB Writing data to Excel documents is also quite simple. Just use xlswrite : xlswrite(FileName,DataArray,SheetName,CellRange) ...where FileName , SheetName and CellRange are the names of the Excel document, sheet name and cell range to which to write, and DataArray contains the data to be written. Final note Refer to the help facility for diagnostic and other capabilities of both of these functions. See also wk1read and wk1write to handle the old Lotus 1-2-3 .WK1 format."], "link": "http://matlabdatamining.blogspot.com/feeds/7855423740766639207/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction In my last posting, Parallel Programming: A First Look (Nov-16-2008), I introduced the subject of parallel programming in MATLAB. In that case, I briefly described my experiences with the MATLAB Parallel Computing Toolbox , from the MathWorks. Since then, I have been made aware of another parallel programming product for MATLAB, the Jacket Engine for MATLAB , from AccelerEyes. Jacket differs from the Parallel Computing Toolbox in that Jacket off-loads work to the computer's GPU (graphics processing unit), whereas the Parallel Computer Toolbox distributes work over multiple cores or processors. Each solution has its merits, and it would be worth the time of MATLAB programmers interested in accelerating computation to investigate the nuances of each. Some History Having observed the computer hardware industry for several decades now, I have witnessed the arrival and departure of any number of special-purpose add-in cards which have been used to speed up math for things like neural networks, etc. For my part, I have resisted the urge to employ such hardware assistance for several reasons: First, special hardware nearly always requires special software. Accommodating the new hardware environment with custom software means an added learning curve for the program and drastically reduced code portability. Second, there is the cost of the hardware itself, which was often considerable. Third, there was the fundamental fact that general-purpose computing hardware was inexorably propelled forward by a very large market demand. Within 2 or 3 years, even the coolest turbo board would be outclassed by new PCs, which didn't involve either of the two issues mentioned above. Two significant items have emerged in today's computer hardware environment: multi-core processors and high-power graphics processors. Even low-end PCs today sport central processors featuring at least two cores, which you may think of more-or-less as \"2 (or more) computers on a single chip\". As chip complexity has continued to grow, chip makers like Intel and AMD have fit multiple \"cores\" on single chips. It is tempting to think that this would yield a direct benefit to the user, but the reality is more subtle. Most software was written to run on single-core computers, and is not equipped to take advantage of the extra computing power of today's multi-core computers. This is where the Parallel Computer Toolbox steps in, by providing programmers a way to distribute the execution of their programs over several cores or processors, resulting in a substantially improved performance. Similarly, the graphics subsystem in desktop PCs has also evolved to a very sophisticated state. At the dawn of the IBM PC (around 1980), graphics display cards with few exceptions basically converted the contents of a section of memory into a display signal usable by a computer monitor. Graphics cards did little more. Over time, though, greater processing functionality was added to the graphics cards culminating in compute engines which would rival supercomputer-class machines of only a few years ago. This evolution has been fueled by the inclusion of many processing units (today, some cards contain hundreds of these units). Originally designed to perform specific graphics functions, many of these units are not small, somewhat general-purpose computers and they can be programmed to do things having nothing to do with the image shown on the computer's monitor. Tapping into this power requires some sort of programming interface, though, which is where Jacket comes in. Caveats Here is a simple assessment of the pros and cons of these two methods of achieving parallel computing on the desktop: Multi-Core: Good: The required hardware is cheap. If you program in MATLAB, you probably have at least 2 cores at your disposal already, if not more. Bad: Most systems top out 4 cores, limiting the potential speed-up with this method (although doubling or quadrupling performance isn't bad). GPU: Good: The number of processing units which can be harnessed by this method is quite large. Some of the fancier graphics cards have over 200 such units. Bad: The required hardware may be a bit pricey, although the price/performance is probably still very attractive. Most GPUs will only perform single-precision floating point math. Newer GPUs, though, will perform double-precision floating-point math. Moving data from the main computer to the graphics card and back takes time, eating into the potential gain. Conclusion My use of the Parallel Computing Toolbox has been limited to certain, very specific tasks, and I have not used Jacket at all. The use of ubiquitous multi-core computers and widely-available GPUs avoids most of the problems I described regarding special-purpose hardware. It will be very interesting to see how these technologies fit into the technological landscape over the next few years, and I am eager to learn of readers' experiences with them."], "link": "http://matlabdatamining.blogspot.com/feeds/2630474888203746005/comments/default", "bloglinks": {}, "links": {"http://www.mathworks.com/": 1, "http://www.accelereyes.com/": 1, "http://matlabdatamining.blogspot.com/": 1}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction Recently, I have been experimenting with the MATLAB Parallel Computing Toolbox, which permits MATLAB programmers to spread work over multiple cores, processors or computers. My primary interest is in leveraging my quad-core desktop PC to accelerate the compute-intensive programs I use for data mining. The Parallel Computing Toolbox is a MATLAB add-on package from the Mathworks which provides a number of parallel programming mechanisms. The one I have spent the most time with is parallel looping, which is accomplished via the parfor command. The basic idea is to have separate iterations of a for-loop be executed on separate cores or processors. The required change to conventional code is tiny. For example, this conventional loop: >> for i = 1:10, disp(int2str(i)), end 1 2 3 4 5 6 7 8 9 10 ...becomes this parallel loop: >> matlabpool open 4, parfor i = 1:10, disp(int2str(i)), end, matlabpool close Starting matlabpool using the parallel configuration 'local'. Waiting for parallel job to start... Connected to a matlabpool session with 4 labs. Sending a stop signal to all the labs... Waiting for parallel job to finish... 4 3 2 1 6 5 9 8 10 7 Performing parallel job cleanup... Done. Notice three important differences: First, the command \"for\" becomes \"parfor\"- easy, right? Second, there is some stuff before and after the loop regarding the matlabpool. These commands, respectively, start up and shut down the parallel programming capability. They do not need to bracket every parfor-loop: you can start the matlabpool at the beginning of a program, use any number of parfor-loops and shut down the matlabpool at the end of the program. Third, notice that the loop iterations did not execute in order. In many situations, this will not matter. In some, it will. This is one of the quirks of programming for a parallel processor. Being aware of this is the programmer's responsibility. Welcome to the future of computing! Experiences My experiences programming with the Parallel Computing Toolbox have been mixed. The good news is that, just using the parallel looping functionality, I have seen code which runs as much as 3 times as fast on my quad-core computer. My tests have involved large numbers of regressions or clusterings (k-means): tasks typical of a data mining project, especially where parameter sweeps or bootstrapping are involved. The bad news is that I have not always seen such dramatic improvement, and in fact I sometimes see minor slow-downs. As far as I can tell, there is a limit to the amount of data I can be juggling at any one time, and going beyond that (remember that each core will need space for its own share of the problem) exceeds my system's available RAM, consequently slowing parallel processing as cores fight for memory. For reference, my current system is thus: Manufacturer: Velocity Micro Model: Vector Z35 CPU: Intel Q6600, 2.4GHz (4 cores) RAM: 4GB OS: Windows XP (32-bit) At present, Windows only shows about 3.24GB of that physical RAM. My strong suspicion is that moving to a 64-bit environment (there are 64-bit versions of both Windows XP and Window Vista, as well as Linux) would permit access to more physical RAM and allow acceleration of parallel code which deals with larger data. In the meantime, though, at least some of my code is running 3 times as fast as it was, which would require the equivalent of a single core processor running at about 7.2GHz! See also: Parallel Programming: Another Look (Feb-12-2009)"], "link": "http://matlabdatamining.blogspot.com/feeds/813800158255402219/comments/default", "bloglinks": {}, "links": {"http://matlabdatamining.blogspot.com/": 1}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Readers of this Web log may be interested in, An Introduction to Combinatorics , an article on the perms , randperm and nchoosek functions which I authored as a guest of Blinkdagger . Blinkdagger covers MATLAB programming, among other things, and I suggest you have a look."], "link": "http://matlabdatamining.blogspot.com/feeds/7083325794516924755/comments/default", "bloglinks": {}, "links": {"http://www.blinkdagger.com/": 2}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Grids are used for a variety of purposes in data analysis, such as division of physical areas into equal-sized units, or for data visualization. Some clustering techniques, such as Kohonen's Self-Organizing Map use grids to organize their internal structure. Square Grids By far, the most commonly-employed grids are square grids. Square grids are convenient in that every cell is the same shape with the same orientation, and boundaries between rows or columns are straight lines. Indexing square grids is easy: (x, y), and extension to more dimensions is straightforward: (x, y, z), etc. Generating square grids in MATLAB is a breeze: >> [X Y] = meshgrid(0:8) X =  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8  0  1  2  3  4  5  6  7  8 Y =  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  X and Y now contain the coordinates for the centers of the square cells, which can be plotted in MATLAB thus (click the figure to enlarge): >> figure, voronoi(X(:),Y(:)), axis square   meshgrid -generated grids need not have the same axes, nor equal spacing. See 'help meshgrid' for more information. The linspace and logspace MATLAB routines are handy as meshgrid arguments, as well. Hexagonal Grids Despite their advantages, square grids do have one basic failing: their representations of circles and other non-rectangular forms are awkward. With a square grid, cells surrounding a central cell have mixed distances. Repeated single-unit \"hops\" from a central cell (such as activation in a cellular automata) result in square or diamond patterns, not circles. Hexagonal grids, one alternative to square grids, are much cleaner in their approximation of circular regions. All six immediate neighbors of any hexagonal cell are the same distance away. Repeated single-unit hops from a given hexagonal cell maintain a relatively \"round\" form (at least a better one than those provided by square grids). Generating hexagonal grids is a bit trickier than generating square grids, but with a little geometry it can be done (as always, click the figure to enlarge): % Generate hexagonal grid Rad3Over2 = sqrt(3) / 2; [X Y] = meshgrid(0:1:41); n = size(X,1); X = Rad3Over2 * X; Y = Y + repmat([0 0.5],[n,n/2]); % Plot the hexagonal mesh, including cell borders [XV YV] = voronoi(X(:),Y(:)); plot(XV,YV,'b-') axis equal, axis([10 20 10 20]), zoom on  Shifting the resulting grid coordinates is accomplished through addition. Scaling is accomplised by multiplication. Note that individual hexagons produced by the code above are oriented with their tops and bottoms flat. Rotating the cells so that the left and right sides are flat is a simple as reversing the rolls of the x and y coordinates in the code."], "link": "http://matlabdatamining.blogspot.com/feeds/62780719772357442/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["Introduction In this author's opinion, validating the performance of predictive models is the single most important step, if one can be chosen, in the process of data mining. One important mechanism for testing models is resampling , the subject of this article. No MATLAB this time, just technique. When selecting a validation technique, it is vital to keep in mind the purpose of such validation: to estimate the level of performance we may expect from models generated by our modeling process, when such models are run on future cases. Note an important subtlety here: We are not so much interested in testing the performance of individual models as we are in testing the model-generating process (feature selection process, complexity selection process, etc., as well as the actual modeling algorithm). Apparent Performance: Warning! The most obvious testing method is to simply execute the model on the very same data upon which it was built. The result is known as the apparent performance. The apparent performance is known to be statistically biased in an optimistic way. This is like giving out the answers to the test before administering the test! At the extreme, a model could simply memorize the development observations and regurgitate them during testing. Assuming no mutually contradictory cases, such a system would deliver perfect validation performance! Certainly this is not what we are interested in. The whole point in making a predictive model is so that said model may be used on future cases. What is desired is generalization to new cases, not simple memorization of historical ones. Ultimately, there is no way to know precisely how optimistic apparent performance estimates are, rendering such performance measures largely useless. Despite its hazards, calculation of the apparent performance is used as the final assessment of models with shocking frequency in industry. Do not become one of its victims. Holdout Testing Given the dangers of apparent performance measures, one might logically reason that a model could be built using all presently available data, and tested at some future point in time, after further observations had been collected. This idea makes perfect sense, but involves potentially considerable delay. Rather than wait for new data, holdout testing splits the data randomly into two sets: training (also called \"in-sample\") and testing (also called \"out-of-sample\"). This is the simplest form of resampling. Incidentally, it is not uncommon to stratify the assignment to training and testing groups, based on variables believed to be significant, including the dependent variables. The idea here is simple: fit the model using the training data, and test it on the testing data. No \"cheating\" takes place since the test data is not used during model construction. Holdout testing provides an unbiased measure of performance, provided (and this caveat is rather important) that the test data is used only once to test the model. If the test data is used more than once to test the data, then all bets are off regarding the unbiased nature of the performance measure. Surprisingly many modelers in industry violate this \"use once\" rule (Shame on you, industry, shame!). In the event that another set of data is needed to make adjustments to the model (to experiment with different numbers of predictors, for instance), a third randomly assigned data set, the validation set (also called the \"tuning set\") should be employed. This simple test process works well in many instances in practice. Its biggest drawback is that it trades off training accuracy for testing accuracy. Typically, the data miner is faced with finite supply of data. Every observation which is moved to the testing set is no longer available for training. As indicated above, our primary interest is in evaluation of the model-generating process. Once we know what to expect from models that come from our process, we may apply our modeling process to the entire data set (without regard to train/test designations) to construct the final model.  k -Fold Cross Validation Smaller data sets force an uncomfortable choice on the modeler using holdout testing: either short-change model construction or short-change testing. One solution is to use k -fold cross-validation (sometimes referred to as simply \"cross-validation\"). k -fold cross-validation builds on the idea of holdout testing in a clever way by rotating data through the process. Data is again divided randomly into groups, but now k equal-sized groups are used. As with holdout testing, stratification is sometimes used to force the folds to be statistically similar. The train-test process is repeated k times, each time leaving a different segment of the data out, as the test set. A common choice for k is 10, resulting in 10-fold cross-validation. In 10-fold cross-validation, the observations are randomly assigned to 10 groups. Ten separate models are built and tested on distinct data segments. The resulting 10 performance measures are unbiased since none of them was built with test data that was used during training. The single, final performance measurement is taken as the mean of these 10 performance measures. The magic of this process is that during each fold, 90% of the data is available for training, yet the final performance metric is based on 100% of the data! When k is equal to the number of observations, this process goes by the special name leave-one-out . While this may be tempting, there are good reasons for choosing k in the range of 5 to 10. The good news with k -fold cross-validation is that reliable, unbiased testing may be performed on smaller data sets than would be possible with simple train-and-test holdout testing. The only really bad news is that this process obviously requires much more computational effort than holdout testing. As with holdout testing, once the modeling process has been evaluated, it may run over the entire data set to produce the final model. Closing Thoughts Other resampling techniques are available, such as the bootstrap. Holdout testing and k -fold cross validation are real workhorses, though, and should cover many machine learning and data mining situations. Few other segments of the empirical modeling pipeline are as critical as model testing- perhaps only problem definition and the collection of appropriate data are as important. Assuming that these other two have been performed properly, model validation is the acid test of model performance: pay it the attention it deserves. Further Reading I strongly recommend the book \"Computer Systems That Learn\", by Weiss and Kulikowski (ISBN: 1-55860-065-5) for a quite readable introduction to this subject. Also very worthy of consideration is chapter 5 of \"Data Mining: Practical Machine Leearning Tools and Techniques\", by Witten and Frank (ISBN: 1-55860-552-5). The Usenet comp.ai.neural-nets FAQ Part 1 contains solid material on this subject as well. See, especially, the section titled \"What are the population, sample, training set, design set, validation set, and test set?\""], "link": "http://matlabdatamining.blogspot.com/feeds/3377202918417506140/comments/default", "bloglinks": {}, "links": {"http://www.faqs.org/": 1}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["At 9:32AM local time today, this Web log received its 50,000th visitor, which I consider a significant milestone. Visitation continues to trend upward, with this month (not yet complete) already exhibiting the highest number of visitors yet. Recently, I was also made aware that this log appears very near the top of some Google search results (which is only one way to measure success). As an example, a posting here is the 2nd item returned when searching for Mahalanobis distance . I humbly interpret these events as evidence of the helpfulness of this log to readers. I'd like to say \"Te\u015fekk\u00fcrler\" to Deniz, who recently reminded me of this, among so many other things."], "link": "http://matlabdatamining.blogspot.com/feeds/6830398748077471857/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["I am occasionally asked for introductory MATLAB materials. The only posts I've written here which I'd consider \"introductory\" are somewhat specialized: Basic Summary Statistics in MATLAB (Apr-13-2007) Getting Data Into MATLAB Using textread (Apr-08-2007) Statistical Data Management in MATLAB (Mar-26-2008) Much broader tutorials can easily be found on-line using any search engine. Searching AllTheWeb for MATLAB introduction ... yields a number of likely prospects, including a nice clearinghouse of such information hosted by the MathWorks: MATLAB Tutorials A number of very well-written introductions to MATLAB have been written, especially by university professors and graduate students. Try searching things like MATLAB tutorial . As always, I suggest including PDF or PPT to improve the quality of discovered documents."], "link": "http://matlabdatamining.blogspot.com/feeds/3504046726534949504/comments/default", "bloglinks": {}, "links": {"http://www.mathworks.com/": 1, "http://www.alltheweb.com": 1, "http://matlabdatamining.blogspot.com/": 3}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["In the Apr-08-2007 posting, Getting Data Into MATLAB Using textread , basic use of the textread function was explained, and I alluded to code which I used to load the variables names. The name-handling code was not included in that post, and reader Andy asked about it. The code in question appears below, and an explanation follows (apologies for the Web formatting). % Specify filename InFilename = 'C:\\Data\\LA47INTIME.tab'; % Import data from disk A = textread(InFilename,'','headerlines',1, ... 'delimiter','\\t','emptyvalue',NaN,'whitespace','.'); % Establish number of observations ('n') and variables ('m') [n m] = size(A); % Note: Load the headers separately because some software % writes out stupid periods for missing values!!! % Import headers from disk FileID = fopen(InFilename);   % Open data file VarLabel = textscan(FileID,'%s',m); % Read column labels VarLabel = VarLabel{1};    % Extract cell array fclose(FileID);      % Close data file % Assign variable names for i = 1:m % Loop over all variables  % Shave off leading and trailing double-quotes  VarLabel{i} = VarLabel{i}(2:end-1);   % Assign index to variable name  eval([VarLabel{i} ' = ' int2str(i) ';']); end After the user specifies the data file to be loaded, data is stored in array 'A', whose size is stored in 'n' and 'm'. Next, the file is re-opened to read in the variable names. Variable names are stored two ways: the actual text names are stored in a cell array, 'VarLabel', and 1 new MATLAB variable is created as an index for each column. To illustrate, consider a file containing 4 columns of data, \"Name\", \"Sex\", \"Age\" and \"Height\". The variable 'VarLabel' would contain those 4 names as entries. Assuming that one stores lists of columns as vectors of column indices, then labeling is easy: VarLabel(3) is \"Age\". This is especially useful when generating series of graphs which need appropriate labels. Also, four new variables will be created, which index the array 'A'. They are 'Name' (which has a value of 1), 'Sex' (value: 2), 'Age' (3) and 'Height' (4). They make indexing into the main data array easy. The column of ages is: A(:,Age) I had begun bundling this code as a function, but could not figure out how to assign the variable indices outside of the scope of the function. It is a short piece of code, and readers will likely want to customize some details, anyway. Hopefully, you find this helpful."], "link": "http://matlabdatamining.blogspot.com/feeds/3304194762747566122/comments/default", "bloglinks": {}, "links": {"http://matlabdatamining.blogspot.com/": 1}, "blogtitle": "Data Mining in MATLAB"}, {"content": ["In a surprising range of circumstances, it is necessary to calculate the area or volume of a region. When the region is a simple shape, such as a rectangle or triangle, and its exact dimensions are known, this is easily accomplished through standard geometric formulas. Often in practice, however, the region's shape is irregular and perhaps of very high dimension. Some regions used in financial net present value calculations, for instance, may lie in spaces defined by hundreds of dimensions! One method for estimating the areas of such regions is Monte Carlo integration . This is a conceptually simple, but very effective solution for some difficult problems. The basic idea is to sample over a region of known area, checking whether sampled points are within the region of interest or not. The proportion of sampled points found to lie within the region of interest multiplied by the (already known) area of the sampled region is an approximation of the area occupied by the region of interest. A related technique, Quasi-Monte Carlo integration , utilizes quasirandom (\"low discrepancy\") numbers in place of random ones. An Example To illustrate Monte Carlo integration, consider a problem with a known analytical solution: calculating the area of a circle. Suppose our circle lies within the unit square, ranging from 0.0 to 1.0 on both dimensions. This circle's center is at (0.5,0.5) and has a radius of 0.5. By the well-known formula, area of a circle = pi * radius squared , this circle has an area of: >> pi * (0.5 ^ 2) ans =  0.7854 Using MATLAB to approximate this measurement using Monte Carlo, we have: >> n = 1e5; % Set number of samples to draw >> rand('twister',9949902); % Initialize pseudorandom number generator >> A = rand(n,2); % Draw indicated number of 2-dimensional coordinates in unit square >> AA = sqrt(sum((A' - 0.5) .^ 2))' >> mean(AA) % Estimate area of the circle via the Monte Carlo method ans =  0.7874 So, after one hundred thousand samples we are off by a small amount. Note that in this case, the area of the total sampled region is 1.0, so there's no need to divide. Let's see how well the Quasi-Monte Carlo technique performs: >> n = 1e5; % Set number of samples to draw >> HSet = haltonset(2); % Set up the Halton quasirandom process for 2-dimensional samples >> B = net(HSet,n); % Draw indicated number of samples using quasirandom numbers >> BB = sqrt(sum((B' - 0.5) .^ 2))' >> mean(BB) % Estimate area of the circle via the quasi-Monte Carlo method ans =  0.7853 In this instance (and this will be true for many problems), quasirandom numbers have converged faster. See also the Mar-19-2008 posting, Quasi-Random Numbers . As mentioned in that post, coordinates of a regular grid could be substituted for the random or quasirandom numbers, but this requires knowing in advance how many samples are to be drawn, and does not allow arbitrary amounts of further sampling to be used to improve the approximation. Naturally, one would never bother with all of this to calculate the area of a circle, given the availability of convenient formula, but Monte Carlo can be used for regions of arbitrary, possibly highly irregular- even disjoint- regions. As with the bootstrap, simulated annealing and genetic algorithms, this method is made possible by the fact that modern computers provide tremendous amounts of rapid, accurate computation at extremely low cost. Further Reading For a good overview of the Monte Carlo method, see: Monte Carlo Method"], "link": "http://matlabdatamining.blogspot.com/feeds/3877763860703483699/comments/default", "bloglinks": {}, "links": {"http://www.riskglossary.com/": 1, "http://matlabdatamining.blogspot.com/": 1}, "blogtitle": "Data Mining in MATLAB"}]