[{"blogurl": "http://highlyscalable.wordpress.com\n", "blogroll": [], "title": "Highly Scalable Blog"}, {"content": ["Scalability is one of the main drivers of the NoSQL movement. As such, it encompasses distributed system coordination, failover, resource management and many other capabilities. It sounds like a big umbrella, and it is. Although it can hardly be said that NoSQL movement brought fundamentally new techniques into distributed data processing, it triggered an avalanche of practical studies and real-life trials of different combinations of protocols and algorithms. These developments gradually highlight a system of relevant database building blocks with proven practical efficiency. In this article I\u2019m trying to provide more or less systematic description of techniques related to distributed operations in NoSQL databases. \n In the rest of this article we study a number of distributed activities like replication of failure detection that could happen in a database. These activities, highlighted in bold below, are grouped into three major sections: \n \n Data Consistency. Historically, NoSQL paid a lot of attention to tradeoffs between consistency, fault-tolerance and performance to serve geographically distributed systems, low-latency or highly available applications. Fundamentally, these tradeoffs spin around data consistency, so this section is devoted data replication and data repair . \n Data Placement. A database should accommodate itself to different data distributions, cluster topologies and hardware configurations. In this section we discuss how to distribute or rebalance data in such a way that failures are handled rapidly, persistence guarantees are maintained, queries are efficient, and system resource like RAM or disk space are used evenly throughout the cluster. \n System Coordination. Coordination techniques like leader election are used in many databases to implements fault-tolerance and strong data consistency. However, even decentralized databases typically track their global state, detect failures and topology changes . This section describes several important techniques that are used to keep the system in a coherent state. \n \n Data Consistency \n It is well known and fairly obvious that in geographically distributed systems or other environments with probable network partitions or delays it is not generally possible to maintain high availability without sacrificing consistency because isolated parts of the database have to operate independently in case of network partition. This fact is often referred to as the CAP theorem. However, consistency is a very expensive thing in distributed systems, so it can be traded not only to availability. It is often involved into multiple tradeoffs. To study these tradeoffs, we first note that consistency issues in distributed systems are induced by the replication and the spatial separation of coupled data, so we have to start with goals and desired properties of the replication: \n \n Availability. Isolated parts of the database can serve read/write requests in case of network partition. \n Read/Write latency. Read/Write requests are processes with a minimal latency. \n Read/Write scalability. Read/Write load can be balanced across multiple nodes. \n Fault-tolerance. Ability to serve read/write requests does not depend on availability of any particular node. \n Data persistence. Node failures within certain limits do not cause data loss. \n Consistency. Consistency is a much more complicated property than the previous ones, so we have to discuss different options in detail. It beyond this article to go deeply into theoretical consistency and concurrency models, so we use a very lean framework of simple properties.\n \n Read-Write consistency. From the read-write perspective, the basic goal of a database is to minimize a replica convergence time (how long does it take to propagate an update to all replicas) and guarantee eventual consistency. Besides these weak guarantees, one can be interested in stronger consistency properties:\n \n Read-after-write consistency. The effect of a write operation on data item X, will always be seen by a successive read operation on X. \n Read-after-read consistency. If some client reads the value of a data item X, any successive read operation on X will always return that same or a more recent value. \n \n \n \n \n Write-Write consistency. Write-write conflicts appear in case of database partition, so a database should either handle these conflicts somehow or guarantee that concurrent writes will not be processed by different partitions. From this perspective, a database can offer different consistency models:\n \n Atomic Writes. If a database provides an API where a write request can only be an independent atomic assignment of a value, one possible way to avoid write-write conflicts is to pick the \u201cmost recent\u201d version of each entity. This guarantees that all nodes will end up with the same version of data irrespectively to the order of updates which can be affected by network failures and delays. Data version can be specified by a timestamps or application-specific metric. This approach is used for example in Cassandra. \n Atomic Read-modify-write. Applications often do a read-modify-write sequence instead of independent atomic writes. If two clients read the same version of data, modify it and write back concurrently, the latest update will silently override the first one in the atomic writes model. This behavior can be semantically inappropriate (for example, if both clients add a value to a list). A database can offer at least two solutions:\n \n Conflict prevention. Read-modify-write can be thought as a particular case of transaction, so distributed locking or consensus protocols like PAXOS [20, 21] are both a solution. \u00a0This is a generic technique that can support both atomic read-modify-write semantics and arbitrary isolated transactions. An alternative approach is to prevent distributed concurrent writes entirely and route all writes of a particular data item to a single node (global master or shard master). \u00a0To prevent conflicts, a database must sacrifice availability in case of network partitioning and stop all but one partition. This approach is used in many systems with strong consistency guarantees (e.g. most RDBMSs, HBase, MongoDB). \n Conflict detection. A database track concurrent conflicting updates and either rollback one of the conflicting updates or preserve both versions for resolving on the client side. Concurrent updates are typically tracked by using vector clocks [19] (which can be though as a generalization of the optimistic locking) or by preserving an entire version history. This approach is used in systems like Riak, Voldemort, CouchDB. \n \n \n \n \n \n \n \n Now let\u2019s take a closer look at commonly used replication techniques and classify them in accordance with the described properties. The first figure below depicts logical relationships between different techniques and their coordinates in the system of the consistency-scalability-availability-latency tradeoffs. The second figure illustrates each technique in detail. \n  \n  Replication factor 4. It is assumed that read/write coordinator can be either an external client or a proxy node within a database. \n Let\u2019s go through all these techniques moving from weak to strong consistency guarantees: \n \n (A, Anti-Entropy) Weakest consistency guarantees are provided by the following strategy. Writer updates any arbitrary selected replica. Reader reads any replica and sees the old data until a new version is not propagated via background anti-entropy protocol (more on anti-entropy protocols in the next section). The main properties of this approach are:\n \n High propagation latency makes it quite impractical for data synchronization, so it is typically used only as an auxiliary background process that detects and repairs unplanned inconsistencies. However, databases like Cassandra use anti-entropy as a primary way to propagate information about database topology and other metadata. \n Consistency guarantees are poor: write-write conflicts and read-write discrepancies are very probable even in absence of failures. \n Superior availability and robustness against network partitions. This schema provides good performance because individual updates are replaced by asynchronous batch processing. \n Persistence guarantees are weak because new data are initially stored on a single replica. \n \n \n (B) An obvious improvement of the previous schema is to send an update to all (available) replicas asynchronously as soon as the update request hits any replica. It can be considered as a kind of targeted anti-entropy.\n \n In comparison with pure anti-entropy, this greatly improves consistency with a relatively small performance penalty. However, formal consistency and persistence guarantees remain the same. \n If some replica is temporary unavailable due to network failures or node failure/replacement, updates should be eventually delivered to it by the anti-entropy process. \n \n \n (C) In the previous schema, failures can be handled better using the hinted handoff technique [8]. Updates that are intended for unavailable nodes are recorded on the coordinator or any other node with a hint that they should be delivered to a certain node as soon as it will become available. This improves persistence guarantees and replica convergence time. \n (D, Read One Write One) Since the carrier of hinted handoffs can fail before deferred updates were propagated, it makes sense to enforce consistency by so-called read repairs. Each read (or randomly selected reads) triggers an asynchronous process that requests a digest (a kind of signature/hash) of the requested data from all replicas and reconciles inconsistencies if detected. We use term ReadOne-WriteOne for combination of techniques A, B, C and D \u2013 they all do not provide strict consistency guarantees, but are efficient enough to be used in practice as an self-contained approach. \n (E, Read Quorum Write Quorum) The strategies above are heuristic enhancements that decrease replicas convergence time. To provide guarantees beyond eventual consistency, one has to sacrifice availability and guarantee an overlap between read and write sets. A common generalization is to write synchronously W replicas instead of one and touch R replicas during reading.\n \n First, this allows one to manage persistence guarantees setting W>1. \n Second, this improves consistency for R+W>N because synchronously written set will overlap with the set that is contacted during reading (in the figure above W=2, R=3, N=4), so reader will touch at least one fresh replica and select it as a result. This guarantees consistency if read and write requests are issued sequentially (e.g. by one client, read-your-writes consistency), but do not guarantee global read-after-read consistency. Consider an example in the figure below to see why reads can be inconsistent. In this example R=2, W=2, N=3. However, writing of two replicas is not transactional, so clients can fetch both old and new values while writing is not completed: \n \n \n \n  \n \n \n Different values of R and W allows to trade write latency and persistence to read latency and vice versa. \n Concurrent writers can write to disjoint quorums if W<=N/2. Setting W>N/2 guarantees immediate conflict detection in Atomic Read-modify-write with rollbacks model. \n Strictly speaking, this schema is not tolerant to network partitions, although it tolerates failures of separate nodes. In practice, heuristics like sloppy quorum [8] can be used to sacrifice consistency provided by a standard quorum schema in favor of availability in certain scenarios. \n \n (F, Read All Write Quorum) The problem with read-after-read consistency can be alleviated by contacting all replicas during reading (reader can fetch data or check digests). This ensures that a new version of data becomes visible to the readers as soon as it appears on at least one node. Network partitions of course can lead to violation of this guarantee. \n (G, Master-Slave) The techniques above are often used to provide either Atomic Writes or Read-modify-write with Conflict Detection consistency levels. To achieve a Conflict Prevention level, one has to use a kind of centralization or locking. A simplest strategy is to use master-slave asynchronous replication. All writes for a particular data item are routed to a central node that executes write operations sequentially. This makes master a bottleneck, so it becomes crucial to partition data into independent shards to be scalable. \n (H, Transactional Read Quorum Write Quorum and Read One Write All) Quorum approach can also be reinforced by transactional techniques to prevent write-write conflicts. A well-known approach is to use two-phase commit protocol. However, two-phase commit is not perfectly reliable because coordinator failures can cause resource blocking. PAXOS commit protocol [20, 21] is a more reliable alterative, but with a price or performance penalty. A small step forward and we end up with the Read One Write All approach where writes update all replicas in a transactional fashion. This approach provides strong fault-tolerant consistency but with a price of performance and availability. \n \n It is worth noting that the analysis above highlights a number of tradeoffs: \n \n Consistency-availability tradeoff . This strict tradeoff is formalized by the CAP theorem. In case of network partition, a database should either stop all partitions except one or accept the possibility of data conflicts. \n Consistency-scalability tradeoff . One can see that even read-write consistency guarantees impose serious limitations on a replica set scalability, and write-write conflicts can be handled in a relatively scalable fashion only in the Atomic Writes model. The Atomic Read-modify-write model introduces short casual dependencies between data and this immediately requires global locking to prevent conflicts. This shows that even a slight spatial or casual dependency between data entries or operations could kill scalability , so separation of data into independent shards and careful data modeling is extremely important for scalability. \n Consistency-latency tradeoff . As it was shown above, there exists a tendency to Read-All and Write-All techniques when strong consistency or persistence guarantees are provides by a database. These guarantees are clearly in inverse proportion to requests latency. Quorum techniques are a middle ground. \n Failover-consistency/scalability/latency tradeoff . It is interesting that contention between failover and consistency/scalability/latency is not really severe. Failures of up to N/2 nodes can often be tolerated with reasonable performance/consistency penalty. However, this tradeoff is visible, for example, in the difference between 2-phase commit and PAXOS protocols. Another example of this tradeoff is ability to lift certain consistency guarantees like read-your-writes using sticky sessions which complicate failover [22]. \n \n Anti-Entropy Protocols, Gossips \n Let us start our study with the following problem statement: \n There is a set of nodes and each data item is replicated to a subset of nodes. Each node serves update requests even if there is no network connection to other nodes. Each node periodically synchronizes its state with other nodes is such a way that if no updates take place for a long time, all replicas will gradually become consistent. How this synchronization should be organized \u2013 when synchronization is triggered, how a peer to synchronize with is chosen, what is the data exchange protocol? Let us assume that two nodes can always merge their versions of data selecting a newest version or preserving both versions for further application-side resolution. \n This problem appears both in data consistency maintenance and in synchronization of a cluster state (propagation of the cluster membership information and so on). Although the problem above can be solved by means of a global coordinator that monitors a database and builds a global synchronization plan or schedule, decentralized databases take advantage of more fault-tolerant approach. The main idea is to use well-studied epidemic protocols [7] that are relatively simple, provide a pretty good convergence time, and can tolerate almost any failures or network partitions. Although there are different classes of epidemic algorithms, we focus on anti-entropy protocols because of their intensive usage in NoSQL databases. \n Anti-entropy protocols assume that synchronization is performed by a fixed schedule \u2013 every node regularly chooses another node at random or by some rule and exchanges database contents, resolving differences. There are three flavors of anti-entropy protocols: push, pull, and push-pull. The idea of the push protocol is to simply select a random peer and push a current state of data to it. In practice, it is quite silly to push the entire database, so nodes typically work in accordance with the protocol which is depicted in the figure below. \n  \n Node A which is initiator of synchronization prepares a digest (a set of checksums) which is a fingerprint of its data. Node B receives this digest, determines the difference between the digest and its local data and sends a digest of the difference back to A. Finally, A sends an update to B and B updates itself. Pull and push-pull protocols work similarly, as it shown in the figure above. \n Anti-entropy protocols provide reasonable good convergence time and scalability. The following figure shows simulation results for propagation of an update in the cluster of 100 nodes. On each iteration, each node contacts one randomly selected peer. \n  \n One can see that the pull style provides better convergence than the push, and this can be proven theoretically [7]. Also, push has a problem with a \u201cconvergence tail\u201d when a small percent of nodes remains unaffected during many iterations, although almost all nodes are already touched. The Push-Pull approach greatly improves efficiency in comparison with the original push or pulls techniques, so it is typically used in practice. Anti-entropy is scalable because the average conversion time grows as a logarithmic function of the cluster size. \n Although these techniques look pretty simple, there are many studies [5] regarding performance of anti-entropy protocols under different constraints. One can leverage knowledge of the network topology to replace a random peer selection by a more efficient schema [10]; adjust transmit rates or use advanced rules to select data to be synchronized if the network bandwidth is limited [9]. Computation of digest can also be challenging, so a database can maintain a journal of the recent updates to facilitate digests computing. \n Eventually Consistent Data Types \n In the previous section we assumed that two nodes can always merge their versions of data . However, reconciliation of conflicting updates is not a trivial task and it is surprisingly difficult to make all replicas to converge to a semantically correct value. A well-known example is that deleted items can resurface in the Amazon Dynamo database [8]. \n Let us consider a simple example that illustrates the problem: a database maintains a logically global counter and each database node can serve increment/decrement operations. Although each node can maintain its own local counter as a single scalar value, but these local counters cannot be merged by simple addition/subtraction. Consider an example: there are 3 nodes A, B, and C and increment operation was applied 3 times, once per node. If A pulls value from B and adds it to the local copy, C pulls from B, C pulls from A, then C ends up with value 4 which is incorrect. One possible way to overcome these issues is to use a data structure similar to vector clock [19] and maintain a pair of counters for each node [1]: \n \nclass Counter {\n int[] plus\n int[] minus\n int NODE_ID\n\n increment() {\n  plus[NODE_ID]++\n }\n\n decrement() {\n  minus[NODE_ID]++\n }\n\n get() {\n  return sum(plus) \u2013 sum(minus)\n }\n\n merge(Counter other) {\n  for i in 1..MAX_ID {\n   plus[i] = max(plus[i], other.plus[i])\n   minus[i] = max(minus[i], other.minus[i])\n  }\n }\n}\n \n Cassandra uses a very similar approach to provide counters as a part of its functionality [11]. It is possible to design more complex eventually consistent data structures that can leverage either state-based or operation-based replication principles. For example, [1] contains a catalog of such structures that includes: \n \n Counters (increment and decrement operations) \n Sets (add and remove operations) \n Graphs (addEdge/addVertex, removeEdge/removeVertex operations) \n Lists (insertAt(position) and removeAt(position) operations) \n \n However, eventually consistent data types are often limited in functionality and impose performance overheads. \n Data Placement \n This section is dedicated to algorithms that control data placement inside a distributed database. These algorithms are responsible for mapping between data items and physical nodes, migration of data from one node to another and global allocation of resources like RAM throughout the database. \n Rebalancing \n Let us start with a simple protocol that is aimed to provide outage-free data migration between cluster nodes. This task arises in situations like cluster expansion (new nodes are added), failover (some node goes done), or rebalancing (data became unevenly distributed across the nodes). Consider a situation that is depicted in the section (A) of the figure below \u2013 there are three nodes and each node contains a portion of data (we assume a key-value data model without loss of generality) that is distributed across the nodes according to an arbitrary data placement policy: \n  \n If one does not have a database that supports data rebalancing internally, he probably will deploy several instances of the database to each node as it is shown in the section (B) of the figure above. This allows one to perform a manual cluster expansion by turning a separate instance off, copying it to a new node, and turning it on, as it is shown in the section (C). Although an automatic database is able to track each record separately, many systems including MongoDB, Oracle Coherence, and upcoming Redis Cluster use the described technique internally, i.e. group records into shards which are minimal units of migration for sake of efficiency. It is quite obvious that a number of shards should be quite large in comparison with the number of nodes to provide the even load distribution. An outage-free shard migration can be done according to the simple protocol that redirects client from the exporting to the importing node during a migration of the shard. The following figure depicts a state machine for get(key) logic as it going to \u00a0be implemented in Redis Cluster: \n  \n It is assumed that each node knows a topology of the cluster and is able to map any key to a shard and a shard to a cluster node. If the node determines that the requested key belongs to a local shard, then it looks it up locally (the upper square in the picture above). If the node determines that the requested key belongs to another node X, than it sends a permanent redirection command to the client (the lower square in the figure above). Permanent redirection means that the client is able to cache the mapping between the shard and the node.\u00a0 If the shard migration is in progress, the exporting and the importing nodes mark this shard accordingly and start to move its records locking each record separately. The exporting node first looks up the key locally and, if not found, redirects the client to the importing node assuming that key is already migrated. This redirect is a one-time and should not be cached. The importing node processes redirects locally, but regular queries are permanently redirected until migration is not completed. \n Sharding and Replication in Dynamic Environments \n The next question we have to address is how to map records to physical nodes. A straightforward approach is to have a table of key ranges where each range is assigned to a node or to use procedures like NodeID = hash(key) % TotalNodes . However, modulus-based hashing does not explicitly address cluster reconfiguration because addition or removal of nodes causes complete data reshuffling throughout the cluster. As a result, it is difficult to handle replication and failover. \n There are different ways to enhance the basic approach from the replication and failover perspectives. The most famous technique is a consistent hashing. There are many descriptions of the consistent hashing technique in the web, so I provide a basic description just for sake of completeness. The following figure depicts the basic ideas of consistent hashing: \n  \n Consistent hashing is basically a mapping schema for key-value store \u2013 it maps keys (hashed keys are typically used) to physical nodes. A space of hashed keys is an ordered space of binary strings of a fixed length, so it is quite obvious that each range of keys is assigned to some node as it depicted in the figure (A) for 3 nodes, namely, A, B, and C. To cope with replication, it is convenient to close a key space into a ring and traverse it clockwise until all replicas are mapped, as it shown in the figure (B). In other words, item Y should be placed on node B because its key corresponds to B\u2019s range, first replica should be placed on C, second replica on A and so on. \n The benefit of this schema is efficient addition and removal of a node because it causes data rebalancing only in neighbor sectors. As it shown in the figures (C), addition of the node D affects only item X but not Y. Similarly, removal (or failure) of the node B affects Y and the replica of X, but not X itself. However, as it was pointed in [8], the dark side of this benefit is vulnerability to overloads \u2013 all the burden of rebalancing is handled by neighbors only and makes them to replicate high volumes of data. This problem can be alleviated by mapping each node not to a one range, but to a set of ranges, as it shown in the figure (D). This is a tradeoff \u2013 it avoids skew in loads during rebalancing, but keeps the total rebalancing effort reasonably low in comparison with module-based mapping. \n Maintenance of a complete and coherent vision of a hashing ring may be problematic in very large deployments. Although it is not a typical problem for databases because of relatively small clusters, it is interesting to study how data placement was combined with the network routing in peer-to-peer networks. A good example is the Chord algorithm [2] that trades completeness of the ring vision by a single node to efficiency of the query routing. The Chord algorithm is similar to consistent hashing in the sense that it uses a concept of a ring to map keys to nodes. However, a particular node maintains only a short list of peers with exponentially growing offset on the logical ring (see the picture below). This allows one to locate a key in several network hops using a kind of binary search: \n  \n This figure depicts a cluster of 16 nodes and illustrates how node A looks up a key that is physically located on node D. Part (A) depicts the route and part (B) depicts partial visions of the ring for nodes A, B, and C. More information about data replication in decentralized systems can be found in [15]. \n Multi-Attribute Sharding \n Although consistent hashing offers an efficient data placement strategy when data items are accessed by a primary key, things become much more complex when querying by multiple attributes is required. A straightforward approach (that is used, for example, in MongoDB) is to distribute data by a primary key regardless to other attributes. As a result, queries that restrict the primary key can be routed to a limited number of nodes, but other queries have to be processed by all nodes in the cluster. This skew in query efficiency leads us to the following problem statement: \n There is a set of data items and each item has a set of attributes along with their values. Is there a data placement strategy that limits a number of nodes that should be contacted to process a query that restricts an arbitrary subset of the attributes? \n One possible solution was implemented in the HyperDex database. The basic idea is to treat each attribute as an axis in a multidimensional space and map blocks in the space to physical nodes. A query corresponds to a hyperplane that intersects a subset of blocks in the space, so only this subset of blocks should be touched during the query processing. Consider the following example from [6]: \n  \n Each data item is a user account that is attributed by First Name, Last Name, and Phone Number. These attributes are treated as a three-dimensional space and one possible data placement strategy is to map each octant to a dedicated physical node. Queries like \u201cFirst Name = John\u201d correspond to a plane that intersects 4 octants, hence only 4 nodes should be involved into processing. Queries that restrict two attributes correspond to a line that intersects two octants as it shown in the figure above, hence only 2 nodes should be involved into processing. \n The problem with this approach is that dimensionality of the space grows as an exponential function of the attributes count. As a result, queries that restrict only a few attributes tend to involve many blocks and, consequently, involve many servers. One can alleviate this by splitting one data item with multiple attributes into multiple sub-items and mapping them to the several independent subspaces instead of one large hyperspace: \n  \n This provides more narrowed query-to-nodes mapping, but complicates coordination because one data item becomes scattered across several independent subspaces with their own physical locations and transactional updates become required. More information about this technique and implementation details can be found in [6]. \n Passivated Replicas \n Some applications with heavy random reads can require all data to fit RAM. In these cases, sharding with independent master-slave replication of each replica (like in MongoDB) typically requires at least double amount of RAM because each chunk of data is stored both on a master and on a slave. A slave should have the same amount of RAM as a master in order to replace the master in case of failure. However, shards can be placed in such a way that amount of required RAM can be reduced, assuming that the system tolerates short-time outages or performance degradation in case of failures. \n The following figure depicts 4 nodes that host 16 shards, primary copies are stored in RAM and replicas are stored on disk: \n  \n The gray arrows highlight replication of shards from node #2. Shards from the other nodes are replicated symmetrically. The red arrows depict how the passivated replicas will be loaded into RAM in case of failure of node #2. Even distribution of replicas throughout the cluster allows one to have only a small memory reserve that will be used to activate replicas in case of failure. In the figure above, the cluster is able to survive a single node failure having only 1/3 of RAM in reserve. It is worth noting that replica activation (loading from disk to RAM) takes some time and cause temporally performance degradation or outage of the corresponding data during failure recovery. \n System Coordination \n In this section we discuss a couple of techniques that relates to system coordination. Distributed coordination is an extremely large area that was a subject of intensive study during several decades. In this article, we, of course, consider only a couple of applied techniques. A comprehensive description of distributed locking, consensus protocols and other fundamental primitives can be found in numerous books or web resources [17, 18, 21]. \n Failure Detection \n Failure detection is a fundamental component of any fault tolerant distributed system. Practically all failure detection protocols are based on a heartbeat messages which are a pretty simple concept \u2013 monitored components periodically send a heartbeat message to the monitoring process (or the monitoring process polls monitored components) and absence of heartbeat messages for a long time is interpreted as a failure. However, real distributed systems impose a number of additional requirements that should be addressed: \n \n Automatic adaptation. Failure detection should be robust to the temporary network failures and delays, dynamic changes in the cluster topology, workload or bandwidth. This is a fundamentally difficult problem because there is no way to distinguish crashed process from a slow one [13]. As a result, failure detection is always a tradeoff between a failure detection time (how long does it take to detect a real failure) and the false-alarm probability. Parameters of this tradeoff should be adjusted dynamically and automatically. \n Flexibility. At first glance, failure detector should produce a boolean output, a monitored process considered to be either live or dead. Nevertheless, it can be argued that boolean output is insufficient in practice. Let us consider an example from [12] that resembles Hadoop MapReduce. There is a distributed application that consists of a master and several workers. The master has a list of jobs and submits them to the workers. The master can distinguish different \u201cdegrees of failure\u201d. If the master starts to suspect that some worker went down, it stops to submit new jobs to this worker. Next, as time goes by and there are no heartbeat messages, the master resubmits jobs that were running on this worker to the other workers. Finally, the master becomes completely confident that the worker is down and releases all corresponding resources. \n Scalability and robustness. Failure detection as a system process should scale up as well as the system does. It also should be robust and consistent, i.e. all nodes in the system should have a consistent view of running and failed processes even in case of communication problems. \n \n A possible way to address the first two requirements is so-called Phi Accrual Failure Detector [12] that is used with some modifications in Cassandra [16]. The basic workflow is as follows (see the figure below): \n \n For each monitored resource, Detector collects arrival times Ti of heartbeat messages. \n Mean and variance are constantly computed for the recent arrival times (on a sliding window of size W) in the Statistics Estimation block. \n Assuming that distribution of arrival times is known (the figure below contains a formula for normal distribution), one can compute the probability of the current heartbeat delay (difference between the current time t_now and the last arrival time Tc). This probability is a measure of confidence in a failure. As suggested in [12], this value can be rescaled using the logarithmic function for sake of usability. In this case output 1 means that the likeness of the mistake is about 10%, output 2 means 1% and so on. \n \n  \n The scalability requirement can be addressed in significant degree by hierarchically organized monitoring zones that prevent flooding of the network with heartbeat messages [14] and synchronization of different zones via gossip protocol or central fault-tolerant repository. This approach is illustrated below (there are two zones and all six failure detectors talk to each other via gossip protocol or robust repository like ZooKeeper): \n  \n Coordinator Election \n Coordinator election is an important technique for databases with strict consistency guarantees. First, it allows one to organize failover of a master node in master-slave systems. Second, it allows one to prevent write-write conflicts in case of network partition by terminating partitions that do not include a majority of nodes. \n Bully algorithm is a relatively simple approach to coordinator election. MongoDB uses a version of this algorithm to elect leaders in replica sets. The main idea of the bully algorithm is that each member of the cluster can declare itself as a coordinator and announce this claim to other nodes. Other nodes can either accept this claim or reject it by entering the competition for being a coordinator. Node that does not face any further contention becomes a coordinator. Nodes use some attribute to decide who wins and who loses. This attribute can be a static ID or some recency metric like the last transaction ID (the most up-to-date node wins). \n An example of the bully algorithm execution is shown in the figure below. Static ID is used as a comparison metric, a node with a greater ID wins. \n \n Initially five nodes are in the cluster and node 5 is a globally accepted coordinator. \n Let us assume that node 5 goes down and nodes 3 and 2 detect this simultaneously. Both nodes start election procedure and send election messages to the nodes with greater IDs. \n Node 4 kicks out nodes 2 and 3 from the competition by sending OK. Node 3 kicks out node 2. \n Imagine that node 1 detects failure of 5 now and an election message to the all nodes with greater IDs. \n Nodes 2, 3, and 4 kick out node 1. \n Node 4 sends an election message to node 5. \n Node 5 does not respond, so node 4 declares itself as a coordinator and announce this fact to all other peers. \n \n  \n Coordinator election process can count a number of nodes that participate in it and check that at least a half of cluster nodes are attend. This guarantees that only one partition can elect a coordinator in case of network partition. \n References \n \n M. Shapiro et al. A Comprehensive Study of Convergent and Commutative Replicated Data Types \n I. Stoica et al. Chord: A Scalable Peer-to-peer\u00a0 Lookup Service\u00a0 for Internet Applications \n R. J. Honicky, E.L.Miller. Replication Under Scalable Hashing: A Family of Algorithms for Scalable Decentralized Data Distribution \n G. Shah. Distributed Data Structures for Peer-to-Peer Systems \n A. Montresor, Gossip Protocols for Large-Scale Distributed Systems \n R. Escriva, B. Wong, E.G. Sirer. HyperDex: A Distributed, Searchable Key-Value Store \n A. Demers et al. Epidemic Algorithms for Replicated Database Maintenance \n G. DeCandia, et al. Dynamo: Amazon\u2019s Highly Available Key-value Store \n R. van Resesse et al. Efficient Reconciliation and Flow Control for Anti-Entropy Protocols \n S. Ranganathan et al. Gossip-Style Failure Detection and Distributed Consensus for Scalable Heterogeneous Clusters \n http://www.slideshare.net/kakugawa/distributed-counters-in-cassandra-cassandra-summit-2010 \n N. Hayashibara, X. Defago, R. Yared, T. Katayama.\u00a0 The Phi Accrual Failure Detector \n M.J. Fischer, N.A. Lynch, and M.S. Paterson. Impossibility of Distributed Consensus with One Faulty Process \n N. Hayashibara, A. Cherif, T. Katayama. Failure Detectors for Large-Scale Distributed Systems \n M. Leslie, J. Davies, and T. Huffman. A Comparison Of Replication Strategies for Reliable Decentralised Storage \n A. Lakshman, P.Malik. Cassandra \u2013 A Decentralized Structured Storage System \n N. A. Lynch.\u00a0 Distributed Algorithms \n G. Tel. Introduction to Distributed Algorithms \n http://basho.com/blog/technical/2010/04/05/why-vector-clocks-are-hard/ \n L. Lamport. Paxos Made Simple \n J. Chase. Distributed Systems, Failures, and Consensus\u00a0 \n W. Vogels. Eventualy Consistent \u2013 Revisited \n J. C. Corbett et al. Spanner: Google\u2019s Globally-Distributed Database"], "link": "http://highlyscalable.wordpress.com/2012/09/18/distributed-algorithms-in-nosql-databases/", "bloglinks": {}, "links": {"http://hyperdex.org/": 1, "http://www.allthingsdistributed.com/": 1, "http://www.slideshare.net/": 1, "http://net.edu.cn/": 1, "http://sbrc2010.ufrgs.br/": 1, "http://hal.inria.fr/": 1, "http://highlyscalable.wordpress.com/": 16, "http://www.harvard.edu/": 1, "http://cs-www.yale.edu/": 1, "http://cassandra-shawn.googlecode.com/": 1, "http://pdos.mit.edu/": 1, "http://static.googleusercontent.com/": 1, "http://research.microsoft.com/": 1, "http://feeds.wordpress.com/": 1, "http://basho.com/blog": 1, "http://www.mcgill.ca/": 1, "http://www.ucsc.edu/": 1, "http://www.cornell.edu/": 2, "http://ddg.ac.jp/": 1, "http://www.ufl.edu/": 1, "http://www.duke.edu/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["We recently worked with one of the Hadoop vendors on the continuous integration system for Hadoop core and other Hadoop-related projects like Pig, Hive, HBase. One of the challenges we faced was very slow automatic tests \u2014 full unit/integration test suite takes more than 2 hours for Hadoop core and more than 9 hours for Apache Pig. Although there are different ways to alleviate this problem (divide tests into suites, optimize tests by tweaking timeouts and sleeps, etc.), we decided to start with a quick solution that immediately and drastically improves CI efficiency \u2014 distributed parallel test execution. In this article I describe a technique we used to speed up a Pig build from 9 hours to 1 hour 30 minutes using 6 Jenkins nodes. This technique is generic and can be considered as a general way to speed up maven or ant builds on Jenkins CI server or other CI systems. \n Solution Overview \n Basically, the problem boils down to the following. There is a number of Jenkins slave nodes, and we have to split all JUnit tests into batches, run all batches in parallel using available slaves, and aggregate test results into a single report. The last two tasks (parallel execution and aggregation) can be solved using built-in Jenkins functionality, namely, multi-configuration jobs also known as matrix builds. Multi-configuration job allows one to configure a standard Jenkins job and specify a set of slave servers this job to be executed on. Jenkins is capable of running an instance of the job on all specified slaves in parallel, passing slave ID as a build parameter, and aggregating JUnit test results into a single report. On our build server, configuration matrix for a job is as simple as this: \n \n Test splitting is a more tricky task. A straightforward approach is to obtain a list of test cases and cut it into equal pieces. This is definitely better than nothing, but execution time can vary significantly from batch to batch especially in presence of long-running tests. Our preliminary experiments showed that parallelization of Pig builds in such a way is not very efficient \u2014 some batches can run two or more times slower than other. To cope with this issue we decided to collect statistics about tests duration and assemble batches such that the difference between expected execution times is minimal and, consequently, the total build time is minimal. The next section is devoted to the implementation details of this approach. \n Build Steps on Jenkins \n One of our goals was to keep an implementation as simple as possible, so we came up with the design where each node executes a number of steps sequentially (as a solid script) and independently from the other nodes. The only information this script receives from Jenkins server is a node ID. Each instance of the multi-configuration job on each node includes the following steps: \n \n A list of available JUnit tests is obtained. \n Statistics about previous test runs is loaded from the central store. \n Available tests are divided into batches according to the statistics. \n A batch is selected according to the node ID and submitted to ant/maven as a build parameter. \n JUnit reports are parsed, test statistics is extracted and saved to the central shared store. \n \n In this section a Python implementation of each step is shown in a simplified form, details like error handling and logging are omitted for sake of readability. \n First, we prepare an initial list of tests by scanning sources in the workspace: \n \n#[ COLLECT A TEST POOL\ntest_pool = set([])\nfor root, dirnames, filenames in os.walk(\"./test\"):\n for filename in fnmatch.filter(filenames, 'Test*.java'):\n  test_name = re.search(r\".*(Test.*)\\.java\", os.path.join(root, filename))\n  test_pool.add(test_name.group(1)) \n#]\n \nSecond, we load test statistics from the shared store. We use MySQL as a database, but one can use version control system to store statistics along with the sources. This statistics is initially empty. \n \n#[ LOAD TEST STATISTICS\njob_name = \"Pig_gd-branch-0.9\"\ndb = MySQLdb.connect(...)\ncursor = db.cursor()\ncursor.execute(\" SELECT test_name, duration FROM test_stat WHERE job_name=%s \", job_name)\ntest_statistics_data = cursor.fetchall()\ntest_statistics = dict(test_statistics_data)\ndb.close()\n#]\n \nThe third step is a scheduling step that selects tests that have to be executed on the current node. We have to split the test pool into a fixed number of disjoint batches such that the difference of their execution times is minimal. We don\u2019t need an optimal solution, a simple greedy algorithm is practically enough. This step produces a set of files with the test names: \n \nrandom.seed(1234) # fix seed to produce identical results on all nodes\n\n#[ PREPARE SPLITS, GREEDY ALGORITHM\ntest_splits = [ [] for i in range(SPLIT_FACTOR) ]\ntest_times = [0] * SPLIT_FACTOR\nfor test in sorted(test_pool, key=lambda test : -test_statistics.get(test, 0)):\n # select a split with minimal expected execution time\n split_index = test_times.index(min(test_times))\n test_duration = test_statistics.get(test, 0)\n if not test_duration: # if statistics is unavailable, select a random split\n  split_index = random.randint(0, SPLIT_FACTOR - 1)  \n test_splits[split_index].append(test)\n test_times[split_index] += test_duration\n\nfor split, id in zip(test_splits, range(SPLIT_FACTOR)):\n f = open(base_dir + 'upar-split.%d' % id, 'w')\n  for test in split: # write ant's include mask to a file\n   f.write(\"**/%s.java\\n\" % test) \n f.close()\n#]\n \nAs soon as splits are ready, the slave name is mapped to the batch ID and the build is executed for this batch (fortunately, Pig\u2019s build system allows to submit a file with test filters as a build parameter). The similar thing can done for maven builds. The following piece of bash code do this part of the work: \n \ncase $SLAVEID in\nSlave-Alpha) JOBID=0;; # Slave-Alpha is a Jenkins node ID\nSlave-Beta) JOBID=1;;\nSlave-Gamma) JOBID=2;;\nSlave-Delta) JOBID=3;;\nSlave-Epsilon) JOBID=4;;\nSlave-Zeta) JOBID=5;;\nesac\nant -Dtest.junit.output.format=xml clean test -Dtest.all.file=upar-split.${JOBID}\n \nThe final step is to parse test results and update test statistics in the DB. This is also quite trivial: \n \n#[ UPDATE TEST STATISTICS\ndb = MySQLdb.connect(...)\ncursor = db.cursor()\npath = \"./build/test/logs/\"\nfor infile in glob.glob( os.path.join(path, 'TEST-*.xml') ):\n f = open(infile)\n text = f.read()\n f.close()\n time = re.search(\n  r\"<testsuite[^>]*time=\\\"([0-9\\.]+)\\\"\", \n  text, flags=re.DOTALL)\n test_name = re.search(r\".*TEST-.*(Test\\w*).xml\", infile).group(1)  \n cursor.execute(\n  \"REPLACE INTO test_stat(job_name,test_name,duration) VALUES(%s,%s,%s)\", \n  (job_name, test_name, float(time.group(1))) )\ndb.close()\n#]\n \n Results \n According to our experiments, the described technique allows one to achieve a very even load distribution among the nodes and, consequently, minimize the total build time. An example of the build duration distribution for Pig build is shown in the screenshot below (monolithic build takes more than 9 hours): \n \nIt should be noted that the real production implementation takes care about a few more issues: \n \n Split stability. Jenkins nodes can differ in performance and vast changes in test-to-node mapping can lead to the unpredictable result. By this reason it\u2019s preferable to have relatively stable mapping procedure, i.e. changes in execution time for a few tests should not lead to a completely new batches. This can be achieved by using thresholds and deliberate coarsening of the statistics that are used in computations. \n Cohesion of artifacts. All instances of the multi-configuration job are executed in parallel and work independently. It is theoretically possible that two nodes can checkout different revisions of artifacts or sources and, consequently, start with different test pools. This can be alleviated in a multiple ways including distribution of the test pool via the central store."], "link": "http://highlyscalable.wordpress.com/2012/08/14/speeding-up-hadoop-builds-distributed-parallel-unit-tests-on-jenkins/", "bloglinks": {}, "links": {"http://highlyscalable.wordpress.com/": 2, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["Intersection of sorted lists is a cornerstone operation in many applications including search engines and databases because indexes are often\u00a0implemented\u00a0using different types of sorted structures.\u00a0At GridDynamics, we recently worked on a custom database for realtime web analytics where fast intersection of very large lists of IDs was a must for good performance. From a functional point of view, we needed mainly a standard boolean query processing, so it was possible to use Solr/Lucene as a platform. However, it was interesting to evaluate performance of alternative approaches. In this article I describe several useful techniques that are based on SSE instructions and provide results of performance testing for Lucene, Java, and C implementations. I\u2019d like to mention that in this study we were focused on a general case when selectivity of the intersection is low or unknown and\u00a0optimization techniques\u00a0like\u00a0 skip list \u00a0are not\u00a0necessarily\u00a0beneficial. \n Scalar Intersection \n Our starting point is a simple element-by-element intersection algorithm (also known as Zipper). Its implementation in C is shown below and do not require lengthy explanations: \n \n#define int32 unsigned int\n\n// A, B - operands, sorted arrays\n// s_a, s_b - sizes of A and B\n// C - result buffer\n// return size of the result C\nsize_t intersect_scalar(int32 *A, int32 *B, size_t s_a, size_t s_b, int32 *C) {\n\tsize_t i_a = 0, i_b = 0;\n\tsize_t counter = 0;\n\n\twhile(i_a < s_a && i_b < s_b) {\n\t\tif(A[i_a] < B[i_b]) {\n\t\t\ti_a++;\n\t\t} else if(B[i_b] < A[i_a]) {\n\t\t\ti_b++;\n\t\t} else {\n\t\t\tC[counter++] = A[i_a];\n\t\t\ti_a++; i_b++;\n\t\t}\n\t}\n\treturn counter;\n}\n \n Performance of this procedure both in C and Java will be evaluated in the last section. I\u00a0believe\u00a0that it is possible to improve this approach using a branchless implementation, but I had no chance to try it out. \n Vectorized Intersection \n It is intuitively clear that performance of intersection may be improved by processing of multiple elements at once using SIMD instructions. Let us start with the following question: how to find and extract common elements in two short sorted arrays (let\u2019s call them segments). SSE instruction set allow one to do a pairwise comparison of two segments of four 32-bit integers each using one instruction\u00a0( _mm_cmpeq intrinsic) that produces a bit mask that highlights positions of equal elements. If one has two 4-element registers, A and B, it is possible to obtain a mask of common elements comparing A with different cyclic shifts of B (the left part of the figure below) and OR-ing the masks produced by each comparison (the right part of the figure): \n  \n The resulting comparison mask highlights the required elements in the segment A. This 128-bit mask can be transformed to a 4-bit value ( shuffling mask index ) using __mm_movemask \u00a0intrinsic.\u00a0\u00a0When this short mask of common elements is\u00a0obtained, we have to efficiently copy out common elements.\u00a0This can be done by\u00a0shuffling of the original elements according to the shuffling mask that can be looked up in the precomputed dictionary using the\u00a0 shuffling mask index ( i.e. each of 16 possible 4-bit\u00a0shuffling mask indexes \u00a0 is mapped to some permutation ) . All common elements should be placed to the beginning of the register, in this case register can be copied in one shot to the output buffer C as it shown in the figure above. \n The described technique gives us a building block that can be used for intersection of long sorted lists. This process is somehow similar to the scalar intersection: \n  \n In this example, during the first cycle we compare first 4-element segments (highlighted in red) and copy out common elements (2 and 11). Similarly to the scalar\u00a0intersection algorithm, we can move forward the pointer for the list B because the tail element of the compared segments is smaller in B (11 vs 14).\u00a0At the second cycle (in green) we compare the first segment of A with the second segment of B, intersection is empty, and we move pointer for A. And so on. In this example, we need 5 comparisons to process two lists of 12 elements each. \n The complete implementation of the described techniques is shown below: \n \nstatic __m128i shuffle_mask[16]; // precomputed dictionary\n\nsize_t intersect_vector(int32 *A, int32 *B, size_t s_a, size_t s_b, int32 *C) {\n\tsize_t count = 0;\n\tsize_t i_a = 0, i_b = 0;\n\n\t// trim lengths to be a multiple of 4\n\tsize_t st_a = (s_a / 4) * 4;\n\tsize_t st_b = (s_b / 4) * 4;\n\n\twhile(i_a < s_a && i_b < s_b) {\n\t\t//[ load segments of four 32-bit elements\n\t\t__m128i v_a = _mm_load_si128((__m128i*)&A[i_a]);\n\t\t__m128i v_b = _mm_load_si128((__m128i*)&B[i_b]);\n\t\t//]\n\n\t\t//[ move pointers\n\t\tint32 a_max = _mm_extract_epi32(v_a, 3);\n\t\tint32 b_max = _mm_extract_epi32(v_b, 3);\n\t\ti_a += (a_max <= b_max) * 4;\n\t\ti_b += (a_max >= b_max) * 4;\n\t\t//]\n\n\t\t//[ compute mask of common elements\n\t\tint32 cyclic_shift = _MM_SHUFFLE(0,3,2,1);\n\t\t__m128i cmp_mask1 = _mm_cmpeq_epi32(v_a, v_b); // pairwise comparison\n\t\tv_b = _mm_shuffle_epi32(v_b, cyclic_shift);  // shuffling\n\t\t__m128i cmp_mask2 = _mm_cmpeq_epi32(v_a, v_b); // again...\n\t\tv_b = _mm_shuffle_epi32(v_b, cyclic_shift);\n\t\t__m128i cmp_mask3 = _mm_cmpeq_epi32(v_a, v_b); // and again...\n\t\tv_b = _mm_shuffle_epi32(v_b, cyclic_shift);\n\t\t__m128i cmp_mask4 = _mm_cmpeq_epi32(v_a, v_b); // and again.\n\t\t__m128i cmp_mask = _mm_or_si128(\n\t\t\t\t_mm_or_si128(cmp_mask1, cmp_mask2),\n\t\t\t\t_mm_or_si128(cmp_mask3, cmp_mask4)\n\t\t); // OR-ing of comparison masks\n\t\t// convert the 128-bit mask to the 4-bit mask\n\t\tint32 mask = _mm_movemask_ps((__m128)cmp_mask);\n\t\t//]\n\n\t\t//[ copy out common elements\n\t\t__m128i p = _mm_shuffle_epi8(v_a, shuffle_mask[mask]);\n\t\t_mm_storeu_si128((__m128i*)&C[count], p);\n\t\tcount += _mm_popcnt_u32(mask); // a number of elements is a weight of the mask\n\t\t//]\n\t}\n\n\t// intersect the tail using scalar intersection\n\t...\n\n\treturn count;\n}\n \n The described implementation uses the\u00a0 shuffle_mask \u00a0dictionary to map the mask of common elements to the\u00a0shuffling\u00a0parameter. Building of this dictionary is straightforward (each bit in the mask corresponds to 4 bytes in the register): \n \n// a simple implementation, we don't care about performance here\nvoid prepare_shuffling_dictionary() {\n\tfor(int i = 0; i < 16; i++) {\n\t\tint counter = 0;\n\t\tchar permutation[16];\n\t\tmemset(permutation, 0xFF, sizeof(permutation));\n\t\tfor(char b = 0; b < 4; b++) {\n\t\t\tif(getBit(i, b)) {\n\t\t\t\tpermutation[counter++] = 4*b;\n\t\t\t\tpermutation[counter++] = 4*b + 1;\n\t\t\t\tpermutation[counter++] = 4*b + 2;\n\t\t\t\tpermutation[counter++] = 4*b + 3;\n\t\t\t}\n\t\t}\n\t\t__m128i mask = _mm_loadu_si128((const __m128i*)permutation);\n\t\tshuffle_mask[i] = mask;\n\t}\n}\n\nint getBit(int value, int position) {\n return ( ( value & (1 << position) ) >> position);\n}\n \n Partitioned Vectorized Intersection \n SSE 4.2\u00a0instruction\u00a0set offers PCMPESTRM \u00a0instruction that allows one to compare two segments of eight 16-bit values each and obtain a bit mask that highlights common elements. This sounds like an\u00a0extremely efficient approach for intersection of sorted lists, but in its basic form this approach is limited by 16-bit values in the lists. This is not the case for many applications, so\u00a0a workaround was recently suggested by Benjamin Schedel et al. in\u00a0 this article . The main idea is to store indexes in the partitioned format, where elements with the same most significant bits are grouped together. This approach also has limited applicability because each partition should contain a sufficient number of elements, i.e. it works well in case or very large lists or favorable distribution of the values. \n Each partition has a header that includes a prefix which represents most significant bits that are common for all elements in the partition and the number of elements in the partition. The following figure illustrates the partitioning process: \n \n The partitioning procedure that coverts 32-bit values into 16-bit values is shown in the code snippet below: \n \n// A - sorted array\n// s_a - size of A\n// R - partitioned sorted array\nsize_t partition(int32 *A, size_t s_a, int16 *R) {\n\tint16 high = 0;\n\tsize_t partition_length = 0;\n\tsize_t partition_size_position = 1;\n\tsize_t counter = 0;\n\tfor(size_t p = 0; p < s_a; p++) {\n\t\tint16 chigh = _high16(A[p]); // upper dword\n\t\tint16 clow = _low16(A[p]); // lower dword\n\t\tif(chigh == high && p != 0) { // add element to the current partition\n\t\t\tR[counter++] = clow;\n\t\t\tpartition_length++;\n\t\t} else { // start new partition\n\t\t\tR[counter++] = chigh; // partition prefix\n\t\t\tR[counter++] = 0;  // reserve place for partition size\n\t\t\tR[counter++] = clow; // write the first element\n\t\t\tR[partition_size_position] = partition_length;\n\t\t\tpartition_length = 1; // reset counters\n\t\t\tpartition_size_position = counter - 2;\n\t\t\thigh = chigh;\n\t\t}\n\t}\n\tR[partition_size_position] = partition_length;\n\n\treturn counter;\n}\n \n A pair of partitions can be intersected using the following procedure that computes a mask of common elements using _mm_cmpestrm \u00a0intrinsic\u00a0and then shuffles these elements similarly to the vectorized intersection procedure what was described in the previous section. \n \nsize_t intersect_vector16(int16 *A, int16 *B, size_t s_a, size_t s_b, int16 *C) {\n\tsize_t count = 0;\n\tsize_t i_a = 0, i_b = 0;\n\n\tsize_t st_a = (s_a / 8) * 8;\n\tsize_t st_b = (s_b / 8) * 8;\n\n\twhile(i_a < st_a && i_b < st_b) {\n\t\t__m128i v_a = _mm_loadu_si128((__m128i*)&A[i_a]);\n\t\t__m128i v_b = _mm_loadu_si128((__m128i*)&B[i_b]);\n\n\t\t__m128i res_v = _mm_cmpestrm(v_b, 8, v_a, 8,\n\t\t\t\t_SIDD_UWORD_OPS|_SIDD_CMP_EQUAL_ANY|_SIDD_BIT_MASK);\n\t\tint r = _mm_extract_epi32(res_v, 0);\n\t\t__m128i p = _mm_shuffle_epi8(v_a, shuffle_mask16[r]);\n\t\t_mm_storeu_si128((__m128i*)&C[count], p);\n\t\tcount += _mm_popcnt_u32(r);\n\n\t\tint16 a_max = _mm_extract_epi16(v_a, 7);\n\t\tint16 b_max = _mm_extract_epi16(v_b, 7);\n\t\ti_a += (a_max <= b_max) * 4;\n\t\ti_b += (a_max >= b_max) * 4;\n\t}\n\n\t// intersect the tail using scalar intersection\n\t...\n\n\treturn count;\n}\n \n The whole intersection algorithm looks similarly to the scalar intersection. It receives two partitioned operands, iterates over headers of partitions and calls intersection of particular partitions if their prefixes match: \n \n// A, B - partitioned operands\nsize_t intersect_partitioned(int16 *A, int16 *B, size_t s_a, size_t s_b, int16 *C) {\n\tsize_t i_a = 0, i_b = 0;\n\tsize_t counter = 0;\n\n\twhile(i_a < s_a && i_b < s_b) {\n\t\tif(A[i_a] < B[i_b]) {\n\t\t\ti_a += A[i_a + 1] + 2;\n\t\t} else if(B[i_b] < A[i_a]) {\n\t\t\ti_b += B[i_b + 1] + 2;\n\t\t} else {\n\t\t\tC[counter++] = A[i_a]; // write partition prefix\n\t\t\tint16 partition_size = intersect_vector16(&A[i_a + 2], &B[i_b + 2],\n\t\t\t\t\t\tA[i_a + 1], B[i_b + 1], &C[counter + 1]);\n\t\t\tC[counter++] = partition_size; // write partition size\n\t\t\tcounter += partition_size;\n\t\t\ti_a += A[i_a + 1] + 2;\n\t\t\ti_b += B[i_b + 1] + 2;\n\t\t}\n\t}\n\treturn counter;\n}\n \n The output of this procedure is also a partitioned vector that can be used in further operations. \n Performance Evaluation \n Performance of the described techniques was evaluated for intersection of sorted lists of size 1 million elements, with average intersection selectivity about 30%. All evaluated methods excepts partitioned vectorized intersection do not require specific properties of the values in the lists. For partitioned vectorized intersection values were selected from range [0, 3M] to provide relatively large partitions. \n In case of Lucene, a corpus of documents with two fields was generated to provide the mentioned index sizes and selectivity; RAMDirectory was used. \u00a0Intersection was done using standard\u00a0Boolean query with top hits limited by 1 to prevent generation of large result set. Of course, this not a fair comparison because Lucene is much more than a list intersector, but it is still interesting to try it out. \n Performance testing was done on the ordinary Linux desktop with 2.8GHz cores. JDK 1.6 and gcc 4.5.2 (with -O3 option) were used."], "link": "http://highlyscalable.wordpress.com/2012/06/05/fast-intersection-sorted-lists-sse/", "bloglinks": {}, "links": {"http://highlyscalable.wordpress.com/": 4, "http://msdn.microsoft.com/": 4, "http://feeds.wordpress.com/": 1, "http://nlp.stanford.edu/": 1, "http://www.adms-conf.org/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["Statistical analysis and mining of huge multi-terabyte data sets is a common task nowadays, especially in the areas like web analytics and Internet advertising. Analysis of such large data sets often requires powerful distributed data stores like Hadoop and heavy data processing with techniques like MapReduce. This approach often leads to heavyweight high-latency\u00a0analytical processes and poor applicability to realtime use cases.\u00a0On the other hand, when one is interested only in simple additive metrics like total page views or average price of conversion, it is obvious that raw data can be efficiently summarized, for example, on a daily basis or using simple in-stream counters. \u00a0Computation of more advanced metrics like a number of unique visitor or most frequent items is more challenging and requires a lot of resources if implemented straightforwardly. In this article, I provide an overview of probabilistic data structures that allow one to estimate these and many other metrics and trade precision of the estimations for the memory consumption. These data structures can be used both as temporary data accumulators in query processing procedures and, perhaps more important, as a compact \u2013 sometimes astonishingly compact \u2013 replacement of raw data in stream-based computing. \n I would like to thank Mikhail Khludnev and Kirill Uvaev, who reviewed this article and provided valuable suggestions. \n Let us start with a simple example that illustrates capabilities of probabilistic data structures: \n  \n Let us have a data set that is simply a heap of ten million random integer values and we know that it contains not more than one million distinct values (there are many duplicates). The picture above depicts the fact that this data set basically occupies 40MB of memory (10 million of 4-byte elements). It is a ridiculous\u00a0size for Big Data applications, but this a reasonable choice to show all structures in scale. Our goal is to convert this data set to compact structures that allow one to process the following queries: \n \n How many distinct elements are in the data set (i.e. what is the cardinality of the data set)? \n What are the most frequent elements (the terms \u201cheavy hitters\u201d and \u201ctop-k elements\u201d are also used)? \n What are the frequencies of the most frequent elements? \n How many elements belong to the specified range (range query, in SQL it looks like \u00a0SELECT count(v) WHERE v >= c1 AND v < c2)? \n Does the data set contain a particular element (membership query)? \n \n The picture above shows (in scale) how much memory different representations of the data set will consume and which queries they support: \n \n A straightforward approach for cardinality computation and membership query processing is to maintain a sorted list of IDs or a hash table. This approach requires at least 4MB because we expect up to 10^6 values, the actual size of the hash table will be even larger. \n A straightforward approach for frequency counting and range query processing is to store a map like (value -> counter) for each element. It requires a table of 7MB that stores values and counters (24-bit counters are sufficient because we have not more than 10^7 occurrences of each element). \n With probabilistic data structures, a membership query can be processed with 4% error rate (false positive answers) using only 0.6MB of memory if data is stored in the Bloom filter. \n Frequencies of 100 most frequent elements can be estimated with 4% precision using Count-Min Sketch structure that uses about 48KB (12k integer counters, based on the experimental result), assuming that data is skewed in accordance with Zipfian distribution that models well natural texts, many types of web events and network traffic. A group of several such sketches can be used to process range query. \n 100 most frequent items can be detected with 4% error (96 of 100 are determined correctly, based on the experimental results) using Stream-Summary structure, also assuming Zipfian distribution of probabilities of the items. \n Cardinality of this data set can be estimated with 4% precision using either Linear Counter or Loglog Counter. The former one uses about 125KB of memory and its size is linear function of the cardinality, the later one requires only 2KB and its size is almost constant for any input. It is possible to combine several linear counters to estimate cardinality of the corresponding union of sets. \n \n A number of probabilistic data structures is described in detail in the following sections, although without excessive theoretical explanations \u2013 detailed mathematical analysis of these structures can be found in the original articles. \u00a0The preliminary remarks are: \n \n For some structures like Loglog Counter or Bloom filter, there exist simple and practical formulas that allow one to determine parameters of the structure on the basis of expected data volume and required error probability. Other structures like Count-Min Sketch or Stream-Summary have complex dependency on statistical properties of data and experiments are the only reasonable way to understand their applicability to real use cases. \n It is important to keep in mind that applicability of the probabilistic data structures is not strictly limited by the queries listed above or by a single data set. On the contrary, structures populated by different data sets can often be combined to process complex queries and other types of queries can be supported by using customized versions of the described algorithms. \n \n Cardinality Estimation: Linear Counting \n Let us start with a very simple technique that is called Linear Counting. Basically, a liner counter is just a bit set and each element in the data set is mapped to a bit. This process is illustrated in the following code snippet: \n \nclass LinearCounter {\n\tBitSet mask = new BitSet(m) // m is a design parameter\n\n\tvoid add(value) {\n\t\tint position = hash(value) // map the value to the range 0..m\n\t\tmask.set(position) // sets a bit in the mask to 1\n\t}\n}\n \n Let\u2019s say that the ratio of a number of distinct items in the data set to m is a load factor . It is intuitively clear that: \n \n If the load factor is much less than 1, a number of collisions in the mask will be low and weight of the mask (a number of 1\u2019s) will be a good estimation of the cardinality. \n If the load factor is higher than 1, but not very high, many different values will be mapped to the same bits. Hence the weight of the mask is not a good estimation of the cardinality. Nevertheless, it is possible that there exist a function that allows one to estimate the cardinality on the basis of weight (real cardinality will always be greater than weight). \n If the load factor is very high (for example, 100), it is very probable that all bits will be set to 1 and it will be impossible to obtain a reasonable estimation of the cardinality on the basis of the mask. \n \n If so, we have to pose the following two questions: \n \n Is there a function that maps the weight of the mask to the estimation of the cardinality and how does this function look like? \n How to choose m on the basis of the expected number of the unique items (or upper bound) and the required estimation error? \n \n Both questions were addressed in [1]. The following table contains key formulas that allow one to estimate cardinality as a function of the mask weight and choose parameter m by required bias or standard error of the estimation: \n  \n The last two equations cannot be solved analytically to express m or load factor as a function of bias or standard error, but it is possible to tabulate numerical solutions. The following plots can be used to determine the load factor (and, consequently, m) for different capacities. \n  \n The rule of thumb is that load factor of 10 can be used for large data sets even if very precise estimation is required, i.e. memory consumption is about 0.1 bits per unique value. This is more than two orders of magnitude more efficient than the explicit indexing of 32- or 64-bit identifiers, but memory consumption grows linearly as a function of the expected cardinality (n), i.e. capacity of counter. \n It is important to note that several independently computed masks for several data sets can be merged as a bitwise OR to estimate the cardinality of the union of the data sets. This opportunity is leveraged in the following case study. \n Case Study \n There is a system that receives events on user visits from different internet sites. This system enables analysis to query a number of unique visitors for the specified date range and site. Linear Counters can be used to aggregate information about registered visitor IDs for each day and site, masks for each day are saved, and a query can be processed using bitwise OR-ing of the daily masks. \n Cardinality Estimation: Loglog Counting \n Loglog algorithm [2] is a much more powerful and much more complex technique than the Linear Counting algorithm. Although some aspects of the Loglog algorithm are pretty complex, the basic idea is simple and ingenious. \n In order to understand principles of the Loglog algorithm we should start one general observation. Let us imagine that we hashed each element in the data set and these hashed values are presented as binary strings. We can expect that about one half of strings will start with 1, one quarter will start with 01, and so on. Let\u2019s denote the number of the leading zeros as a rank. Finally, one or a few values will have some maximum rank r, as it shown in the figure below. \n  \n From this consideration it follows that 2^r can be treated as some kind of the cardinality estimation, but a very unstable estimation \u2013 r is determined by one or few items and variance is very high. However, it is possible to overcome this issue by using multiple independent observations and averaging them. This technique is shown in the code snippet below. Incoming values are routed to a number of buckets by using their first bits as a bucket address. Each bucket maintains a maximum rank of the received values: \n \nclass LogLogCounter {\n\tint H \t\t\t// H is a design parameter\n\tint m = 2^k \t\t// k is a design parameter\n\tetype[] estimators = new etype[m] // etype is a design parameter\n\n\tvoid add(value) {\n\t\thashedValue = hash(value)\n\t\tbucket = getBits(hashedValue, 0, k)\n\t\testimators[bucket] = max(\n \t\t\testimators[bucket],\n \t\t\trank( getBits(hashedValue, k, H) )\n \t\t)\n \t}\n\n \tgetBits(value, int start, int end)\n \trank(value)\n}\n \n This implementation requires the following parameters to be determined: \n \n H \u2013 sufficient length of the hash function (in bits) \n k \u2013 number of bits that determine a bucket, 2^k is a number of buckets \n etype \u2013 type of the estimator (for example, byte), i.e. how many bits are required for each estimator \n \n The auxiliary functions are specified as follows: \n \n hash(value) \u2013 produces H-bit hash of the value \n getBits(value, start, end) \u2013 crop bits between start and end positions from the value and return an integer number that is assembled from this bits \n rank(value) \u2013 compute position of first 1-bit in the value, i.e. rank(1\u2026b) is 1, rank (001\u2026b) is 3, rank (00001\u2026b) is 5 etc. \n \n The following table provides the estimation formula and equations that can be used to determine numerical parameters of the Loglog Counter: \n  \n These formulas are very impressive. One can see that a number of buckets is relatively small for most of the practically interesting values of the standard error of the estimation. For example, 1024 estimators provide a standard error of 4%. At the same time, the length of the estimator is a very slow growing function of the capacity, 5-bit buckets are enough for cardinalities up to 10^11, 8-bit buckets (etype is byte) can support practically unlimited cardinalities. This means that less than 1KB of auxiliary memory may be enough to process gigabytes of data in the real life applications! This is a fundamental phenomenon that was revealed and theoretically analyzed in [7]: It is possible to recover an approximate value of cardinality, using only a (small and) constant memory. \n Loglog counter is essentially a record about a single (rarest) element in the dataset. \n More recent developments on cardinality estimation are described in [9] and [10]. This article \u00a0also provides a good overview of the cardinality estimation techniques. \n Case Study \n There is a system that monitors traffic and counts unique visitors for different criteria (visited site, geography, etc.) The straightforward approaches for implementation of this system are: \n \n Log all events in a large storage like Hadoop and compute unique visitor periodically using heavy MapReduce jobs or whatever. \n Maintain some kind of inverted indexes like (site -> {visitor IDs}) where visitor IDs are stored as a hash table or sorted array. The number of unique users is a length of the corresponding index. \n \n If number of users and criteria is high, both solutions assume very high amount of data to be stored, maintained, or processed. As an alternative, a LoglogCounter structure can be maintained for each criterion. In this case, thousands of criteria and hundreds of millions of visitors can be tracked using a very modest amount of memory. \n Case Study \n There is a system that monitors traffic and counts unique visitors for different criteria (visited site, geography, etc.). It is required to compute 100 most popular sites using a number of unique visitors as a metric of popularity. Popularity should be computed every day on the basis of data for last month, i.e. every day one-day partition added, another one is removed from the scope. Similarly to the previous case study, straightforward solutions for this problem require a lot of resources if data volume is high. On the other hand, one can create a fresh set of per-site Loglog counters every day and maintain this set during 30 days, i.e. 30 sets of counters are active at any moment of time. This approach can be very efficient because of the tiny memory footprint of the Loglog counter, even for millions of unique visitors. \n Frequency Estimation: Count-Min Sketch \n Count-Min Sketches is a family of memory efficient data structures that allow one to estimate frequency-related properties of the data set, e.g. estimate frequencies of particular elements, find top-K frequent elements, perform range queries (where the goal is to find the sum of frequencies of elements within a range), estimate percentiles. \n Let\u2019s focus on the following problem statement: there is a set of values with duplicates, it is required to estimate frequency (a number of duplicates) for each value. Estimations for relatively rare values can be imprecise, but frequent values and their absolute frequencies should be determined accurately. \n The basic idea of Count-Min Sketch [3] is quite simple and somehow similar to Linear Counting. Count-Min sketch is simply a two-dimensional array (d x w) of integer counters. When a value arrives, it is mapped to one position at each of d rows using d different and preferably independent hash functions. Counters on each position are incremented. This process is shown in the figure below: \n  \n It is clear that if sketch is large in comparison with the cardinality of the data set, almost each value will get an independent counter and estimation will precise. Nevertheless, this case is absolutely impractical \u2013 it is much better to simply maintain a dedicated counter for each value by using plain array or hash table. To cope with this issue, Count-Min algorithm estimates frequency of the given value as a minimum of the corresponding counters in each row because the estimation error is always positive (each occurrence of a value always increases its counters, but collisions can cause additional increments). A practical implementation of Count-Min sketch is provided in the following code snippet. It uses simple hash functions as it was suggested in [4]: \n \nclass CountMinSketch {\n\tlong estimators[][] = new long[d][w]\t// d and w are design parameters\n\tlong a[] = new long[d]\n\tlong b[] = new long[d]\n\tlong p \t\t// hashing parameter, a prime number. For example 2^31-1\n\n\tvoid initializeHashes() {\n\t\tfor(i = 0; i < d; i++) {\n\t\t\ta[i] = random(p)\t// random in range 1..p\n\t\t\tb[i] = random(p)\n \t\t}\n\t}\n\n\tvoid add(value) {\n\t\tfor(i = 0; i < d; i++)\n\t\t\testimators[i][ hash(value, i) ]++\n\t}\n\n\tlong estimateFrequency(value) {\n\t\tlong minimum = MAX_VALUE\n\t\tfor(i = 0; i < d; i++)\n\t\t\tminimum = min(\n \t\t\t\tminimum,\n \t\t\t\testimators[i][ hash(value, i) ]\n \t\t\t)\n\t\treturn minimum\n\t}\n\n\thash(value, i) {\n\t\treturn ((a[i] * value + b[i]) mod p) mod w\n\t}\n}\n \n Dependency between the sketch size and accuracy is shown in the table below. It is worth noting that width of the sketch limits the magnitude of the error and height (also called depth) controls the probability that estimation breaks through this limit: \n  \nAccuracy of the Count-Min sketch depends on the ratio between the sketch size and the total number of registered events. This means that Count-Min technique provides significant memory gains only for skewed data, i.e. data where items have very different probabilities. This property is illustrated in the figures below. \n Two experiments were done with the Count-Min sketch of size 3\u00d764, i.e. 192 counters total. In the first case the sketch was populated with moderately skewed data set of 10k elements, about 8500 distinct values (element frequencies follow Zipfian distribution which models, for example, distribution of words in natural texts). The real histogram (for most frequent elements, it has a long flat tail in the right that was truncated in this figure) and the histogram recovered from the sketch are shown in the figure below: \n  \n It is clear that Count-Min sketch cannot track frequencies of 8500 elements using only 192 counters in the case of low skew of the frequencies, so the estimated histogram is very inaccurate. \n In the second case the sketch was populated with a relatively highly skewed data set of 80k elements, also about 8500 distinct values. The real and estimated histograms are presented in the figure below: \n  \n One can see that result is more accurate, at least for the most frequent items. In general, applicability of Count-Min sketches is not a straightforward question and the best thing that can be recommended is experimental evaluation of each particular case. Theoretical bounds of Count-Min sketch accuracy on skewed data and measurements on real data sets are provided in [6]. \n Frequency Estimation: Count-Mean-Min Sketch \n The original Count-Min sketch performs well on highly skewed data, but on low or moderately skewed data it is not so efficient because of poor protection from the high number of hash collisions \u2013 Count-Min sketch simply selects minimal (less distorted) estimator. As an alternative, more careful correction can be done to compensate the noise caused by collisions. One possible correction algorithm was suggested in [5]. It estimates noise for each hash function as the average value of all counters in the row that correspond to this function (except counter that corresponds to the query itself), deduces it from the estimation for this hash function, and, finally, computes the median of the estimations for all hash functions. Having that the sum of all counters in the sketch row equals to the total number of the added elements, we obtain the following implementation: \n \nclass CountMeanMinSketch {\n\t// initialization and addition procedures as in CountMinSketch\n\t// n is total number of added elements\n\n\tlong estimateFrequency(value) {\n\t\tlong e[] = new long[d]\n\t\tfor(i = 0; i < d; i++) {\n\t\t\tsketchCounter = estimators[i][ hash(value, i) ]\n\t\t\tnoiseEstimation = (n - sketchCounter) / (w - 1)\n\t\t\te[i] = sketchCounter \u2013 noiseEstimator\n \t\t}\n\t\treturn median(e)\n\t}\n}\n \n This enhancement can significantly improve accuracy of the Count-Min structure. For example, compare the histograms below with the first histograms for Count-Min sketch (both techniques used a sketch of size 3\u00d764 and 8500 elements were added to it): \n  \n Heavy Hitters: Count-Min Sketch \n Count-Min sketches are applicable to the following problem: Find all elements in the data set with the frequencies greater than k percent of the total number of elements in the data set. The algorithm is straightforward: \n \n Maintain a standard Count-Min sketch during the scan of the data set and put all elements into it. \n Maintain a heap of top elements, initially empty, and a counter N of the total number of already process elements. \n For each element in the data set:\n \n Put the element to the sketch \n Estimate the frequency of the element using the sketch. If frequency is greater than a threshold (k*N), then put the element to the heap. Heap should be periodically or continuously cleaned up to remove elements that do not meet the threshold anymore. \n \n \n \n In general, the top-k problem makes sense only for skewed data, so usage of Count-Min sketches is reasonable in this context. \n Case Study \n There is a system that tracks traffic by IP address and it is required to detect most traffic-intensive addresses. This problem can be solved using the algorithm described above, but the problem is not trivial because we need to track the total traffic for each address, not a frequency of items. Nevertheless, there is a simple solution \u2013 counters in the CountMinSketch implementation can be incremented not by 1, but by absolute amount of traffic for each observation (for example, size of IP packet if sketch is updated for each packet). In this case, sketch will track amounts of traffic for each address and a heap with the most traffic-intensive addresses can be maintained as described above. \n Heavy Hitters: Stream-Summary \n Count-Min Sketch and other similar techniques is not the only family of structures that allow one to estimate frequency-related metrics. Another large family of algorithms and structures that deal with frequency estimation is counter-based techniques. Stream-Summary algorithm [8] belongs to this family. Stream-Summary allows one to detect most frequent items in the dataset and estimate their frequencies with explicitly tracked estimation error. \n Basically, Stream-Summary traces a fixed number (a number of slots) of elements that presumably are most frequent ones. If one of these elements occurs in the stream, the corresponding counter is increased. If a new, non-traced element appears, it replaces the least frequent traced element and this kicked out element become non-traced. \n The figure below illustrates how Stream-Summary with 3 slots works for the input stream {1,2,2,2,3,1,1,4}. Stream-Summary groups all traced elements into buckets where each bucket corresponds to the particular frequency, i.e. to the number of occurrences. Additionally, each traced element has the \u201cerr\u201d field that stores maximum potential error of the estimation. \n \n Initially there is only 0-bucket and there is no elements attached to it. \n Input : 1 . A new bucket for frequency 1 is created and the element is attached to it. Potential error is 0. \n Input : 2 . The element is also attached to the bucket 1. \n Input : 2 . The corresponding slot is detached from bucket 1 and attached to the newly created bucket 2 (element 2 occurred twice). \n Input : 2 . The corresponding slot is detached from bucket 2 and attached to the newly created bucket 3. Bucket 2 is deleted because it is empty. \n Input : 3 . The element is attached to the bucket 1 because it is the first occurrence of 3. \n Input : 1 . The corresponding slot is moved to bucket 2 because this is the second occurrence of the element 1. \n Input : 1 . The corresponding slot is moved to bucket 3 because this is the third occurrence of the element 1. \n Input : 4 . The element 4 is not traced, so it kicks out element 3 and replaces it in the corresponding slot. The number of occurrences of the element 3 (which is 1) becomes a potential estimation error for the element 4. After this the corresponding slot is moved to the bucket 2, just like it was the second occurrence of the element 4. \n \n  \n The estimation procedure for most frequent elements and corresponding frequencies is quite obvious because of simple internal design of the Stream-Summary structure. Indeed, one just need to scan elements in the buckets that correspond to the highest frequencies. Nevertheless, Stream-Summary is able not only to provide estimates, but to answer are these estimates exact (guaranteed) or not. Computation of these guarantees is not trivial, corresponding algorithms are described in [8]. \n Range Query: Array of Count-Min Sketches \n In theory, one can process a range query (something like SELECT count(v) WHERE v >= c1 AND v < c2) using a Count-Min sketch\u00a0 enumerating all points within a range and summing estimates for corresponding frequencies. However, this approach is impractical because the number of points within a range can be very high and accuracy also tends to be inacceptable because of cumulative error of the sum. Nevertheless, it is possible to overcome these problems using multiple Count-Min sketches. The basic idea is to maintain a number of sketches with the different \u201cresolution\u201d, i.e. one sketch that counts frequencies for each value separately, one sketch that counts frequencies for pairs of values (to do this one can simply truncate a one bit of a value on the sketch\u2019s input), one sketch with 4-items buckets and so on. The number of levels equals to logarithm of the maximum possible value. This schema is shown in the right part of the following picture: \n  \n Any range query can be reduced to a number of queries to the sketches of different level, as it shown in right part of the picture above. This approach (called dyadic ranges) allows one to reduce the number of computations and increase accuracy. An obvious optimization of this schema is to replace sketches by exact counters at the lowest levels where a number of buckets is small. \n MADlib (a data mining library for PostgreSQL and Greenplum) implements this algorithm to process range queries and calculate percentiles on large data sets. \n Membership Query: Bloom Filter \n Bloom Filter is probably the most famous and widely used probabilistic data structure. There are multiple descriptions of the Bloom filter in the web, I provide a short overview here just for sake of completeness. Bloom filter is similar to Linear Counting, but it is designed to maintain an identity of each item rather than statistics. Similarly to Linear Counter, the Bloom filter maintains a bitset, but each value is mapped not to one, but to some fixed number of bits by using several independent hash functions. If the filter has a relatively large size in comparison with the number of distinct elements, each element has a relatively unique signature and it is possible to check a particular value \u2013 is it already registered in the bit set or not. If all the bits of the corresponding signature are ones then the answer is yes (with a certain probability, of course). \n The following table contains formulas that allow one to calculate parameters of the Bloom filter as functions of error probability and capacity: \n  \n Bloom filter is widely used as a preliminary probabilistic test that allows one to reduce a number of exact checks. The following case study shows how the Bloom filter can be applied to the cardinality estimation. \n Case Study \n There is a system that tracks a huge number of web events and each event is marked by a number of tags including a user ID this event corresponds to. It is required to report a number of unique users that meet the specified combination of tags (like users from the city C that visited site A or site B). \n A possible solution is to maintain a Bloom filter that tracks user IDs for each tag value and a Bloom filter that contains user IDs that correspond to the final result. A user ID from each incoming event is tested against the per-tag filters \u2013 does it satisfy the required combination of tags or not. If the user ID passes this test, it is additionally tested against the additional Bloom filter that corresponds to the report itself and, if passed, the final report counter is increased. \n References \n \n K. Whang, B. T. Vander-Zaden, H.M. Taylor. A Liner-Time Probabilistic Counting Algorithm for Database Applications \n M. Durand and P. Flajolet. Loglog Counting of Large Cardinalities \n G. Cormode, S. Muthukrishnan. An Improved Data Stream Summary: The Count-Min Sketch and its Applications \n G. Cormode, S. Muthukrishnan. Approximating Data with the Count-Min Data Structure \n F. Deng, D. Rafiei. New Estimation Algorithms for Streaming Data: Count-min Can Do More \n G. Cormode, S. Muthukrishnan. Summarizing and Mining Skewed Data Streams \n P. Flayjolet and N. Martin. Probabilistic counting algorithm for data base applications \n A. Metwally, D. Agrawal, A.E. Abbadi. Efficient Computation of Frequent and Top-K Elements in Data Streams \n P. Flayjolet, E.Fusy, O. Gandouet, F. Meunier. HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm \n P. Clifford, I. Cosma. A Statistical Analysis of Probabilistic Counting Algorithms \n \n It is worth mentioning that simple Java implementations of several structures can be found in\u00a0 stream-lib \u00a0library."], "link": "http://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining/", "bloglinks": {}, "links": {"http://madlib.net/": 1, "https://github.com/": 1, "http://highscalability.com/blog": 1, "http://webdocs.ualberta.ca/": 1, "http://www.harvard.edu/": 1, "http://highlyscalable.wordpress.com/": 13, "http://www.ucsb.edu/": 1, "http://www.rutgers.edu/": 1, "http://arxiv.org/": 1, "http://www.att.com/": 1, "http://feeds.wordpress.com/": 1, "http://algo.inria.fr/": 2, "http://www.emory.edu/": 1, "http://dblab.ac.kr/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["Some time ago I participated in design of a backend for one large online retailer company. From the business logic point of view, this was a pretty typical eCommerce service for hierarchical and faceted navigation, although not without\u00a0peculiarities, but high performance requirements led us to the quite advanced architecture and technical design. In particular, we built this system on top of Oracle Coherence and designed our own data structures and indexes. \n In this article, I describe major architectural decisions we made and\u00a0techniques\u00a0we used. This description should not be\u00a0considered as\u00a0a solid blueprint, but rather a collection of the relatively independent ideas, patterns, and notes that can be used in different combinations and in different applications, not only in eCommerce systems. \n Business Logic: Hierarchical and Faceted Navigation \n I cannot disclose customer\u2019s name, so I will explain business logic using amazon.com as an example, fortunately the basic functionality is very similar. The first piece of functionality is structural or hierarchical navigation\u00a0through\u00a0categories and products, which are the main business entities of the system. Categories are organized in a tree-like structure and the user is provided with several controls that enable him to navigate through this tree starting from the highest categories (like departments on amazon.com ) and going to the lowest ones: \n  Hierarchical Navigation on Amazon.Com \n Each product can be explicitly associated with one or more categories of any level and category contains a product if this product is explicitly associated with it or associated with any of its subcategories. These structural dependencies between categories and products are relatively static (the system refreshes this information daily), but operations team can change separate relations in runtime to fix incorrect data or to inject other urgent changes. Besides this, each product has some transient information like in-stock availability that is a subject of frequent updates (every 5 minutes or so). \n The second important piece of functionality is a faceted navigation. Categories can contain thousands of products and user cannot efficiently search though this array without powerful tools. The most popular way to do this is a faceted navigation that can be thought as a generation of dynamic categories based on product attributes. For example, if the user opens a category that contains clothes, products will be\u00a0characterized by properties like size, brand, color and so on. Available values of these properties (called facets) can be extracted from the product set and shown on the UI\u00a0to enable the user to apply user-select filters, \u00a0which are particular AND-ed or OR-ed combinations of the facet values: \n  Faceted Navigation on Amazon.Com \n Each facet value is often accompanied with cardinality , i.e. number of products that will be in the results set if this filter is applied. When user clicks on a facet, the system automatically applies the selected filters and narrows the product set according to the user interests. It is important that this style of navigation assumes high\u00a0interactivity \u2013 each selection leads to\u00a0recomputing\u00a0of all available facets, their cardinalities, and products in a result set. \n There is a lot of information about faceted search on the web. I can recommend\u00a0 this article by Peter Morville and Jeffrey Callender for further reading. We will also return back to some details of business logic in the section devoted to\u00a0implementation of the faceted navigation. \n From the backend perspective, hierarchical and faced navigation requires the following operations to be implemented: \n \n getProductsAndFacets(CategoryID, UserSelectedFilters) \u2013 return all products within the category filtered in accordance with the user-selected filters, compute available facet values and corresponding cardinalities for the filtered product set. \n traverseCategoryHierarchy(CategoryID) \u2013 return ancestors and descendants of the given category in the tree of categories. Depth of traversal is specified by the frontend. \n getProducts(ProductID[]) \u2013 return a product domain entity that contains product attributes, prices, images etc. This\u00a0information\u00a0is used to populate a page with product and display product details. \n getCategories(CategoryID[]) \u2013 return a category domain entity that contains category attributes and properties. \n getProductsTransientAttributes(ProductID[]), getCategoryTransientAtributes(CategoryID[]) \u00a0-\u00a0 return a short list of attributes that are the subject of frequent changes (the in-stock availability etc.) The rationale behind these methods is that frontend should be able to fetch transient information very efficiently and separately from fetching of heavy-weight domain entities because this information cannot be cached. \n \n System Properties and Major Technical Requirements \n From the technical perspective, the following properties should be highlighted: \n \n All data is initially stored in the relational database, but this database is heavily loaded because it is a master record for many applications. So, the only way was to cache all necessary data to minimize interaction with RDBMS. \n The content that is delivered to users (categories and products) is pretty much static. In such cases, content delivery network (CDN) is typically used to cache majority of the content and shield the system from high workload. Nevertheless, there were two obstacles that decrease efficiency of CDN in this project:\n \n Faceted navigation leads to a high amount of different views because users are able to select arbitrary\u00a0combinations of facets, and,\u00a0consequently, many unique requests should be served. \n Product in-stock availability is transient, especially for the certain periods of eCommerce system life cycle (sales and so on). This means that content \u2013 products and facets \u2013 is sporadically updated every few minutes. \n \n \n Taking into account the previous considerations, performance requirements were set as 1000 faceted navigation requests/second per typical hardware blade. \n Data capacity of the system is not less than\u00a0 1 million products . \n Structural data are completely reloaded from the RDBMS every night. Transient information updates and requests for minor changes of structural information can arrive every few minutes. \n The system is implemented in Java. \n \n Deployment Schema and High-Level Architecture \n The major architectural decision was to use in-memory data grid (IMDG) to shield the master RDBMS from workload during request processing. Oracle Coherence was chosen as an\u00a0implementation. Coherence is used as a platform that provides distributed cache capabilities and can serve as a messaging bus for coordination of all application-level modules on all nodes in the cluster. \n The deployment schema includes three types of nodes \u2013 processing nodes, storage nodes, and maintenance nodes. Processing nodes are responsible for requests serving and act as Coherence clients. Storage nodes are basically Coherence storage nodes. Maintenance nodes are responsible for data indexing and processing of transient information updates. Both Storage and Maintenance nodes do not serve client requests. This deployment schema is shown in the figure below: \n  Deployment Schema \n Nodes can be dynamically added or removed from the cluster. All nodes (processing, storage, maintenance) host the same application that contains all modules for request processing, maintenance operations, and Coherence instance. Basically, deployments on all nodes are identical and can serve both client requests and maintenance operations, although each type of nodes has its own configuration parameters. The rationale behind this architecture can be recognized as a pattern: \n \n \n \n \n Pattern: Homogeneous Cluster Nodes \n \n \n \n Problem \nThere is a clustered system that consist of multiple business services and auxiliary modules (data loaders, administration controls, etc). The deployment process is going to be complex if each module is deployed as a separate artifact with its own deployment schema and configuration. \n \n \n Solution \nDifferent groups of nodes in the cluster can have different roles and serve different needs, but it may be a good idea to create one application and one artifact that will be deployed\u00a0throughout the cluster. Different modules of this application are activated on different nodes depends on explicitly specified configuration (say, property files) or just because of usage pattern (say, certain requests are routed only to particular nodes). \n \n \n Results \nThis approach simplifies deployment and release processes, mitigates risk of incorrect deployment or misconfiguration. Development and QA processes are simplified because one can use either singe node or multiple nodes to run fully functional\u00a0environment. \n \n \n \n Turning to the internals of the application itself, we can see that it includes the following components (these components are depicted in the figure below): \n \n Data Loader. The first role of this component is to fetch data from the master DB, assemble domain entities, and push these entities to Coherence. The second role is to build navigation indexes (these indexes will be described in the further sections), split them into chunks, and flush to Coherence. The rationale behind splitting into chunks is that indexes can be quite large (hundreds of megabytes), and Coherence is not intended for storing of such large entities, transmission of these entities can block Coherence network IO and crash the cluster. The third role of the Loader module is to receive intraday updates and apply patches to the indexes and domain entities. \n Entity Gateway. The role of this module is to return\u00a0information\u00a0about particular entities, products and categories.\u00a0Basically, this module is just a facade for Coherence. It takes domain entities from Coherence, compute fields that depend on transient information using navigation index, and return data to the client. \n Hierarchical Navigation Engine. This engine is responsible for hierarchical navigation and works as a primary navigation service for external clients. Besides this, the navigation index is a master record for transient attributes, so other modules like Entity Gateway request these attributes from the Navigation Engine. Implementation of the engine will be described in the next section. \n Facet Engine. This engine is responsible for computation of facets and for filtering according to user-selected filters.\u00a0Implementation\u00a0of this module will be discussed later. \n \n \n \n  Component Diagram and Data Flows \n Data Loader is active only on the Maintenance nodes where it has a plenty of resources for temporary buffers, index compilation tasks and so on. All updates and indexing requests are routed only to the Maintenance nodes, not to Processing/Storage nodes. Such separation of data loader and other maintenance units can be recognized as a common pattern: \n \n \n \n \n \n Pattern: Maintenance Node \n \n \n \n Problem \nThere is a cluster of nodes where each node is able to serve both business and maintenance requests. Maintenance operations can consume a lot resources and impact performance of business requests. \n \n \n Solution \nMaintenance operations like data indexing can be handled by any cluster node when a distributed platform like IMDG is used. Nevertheless, it is often a good idea to use a dedicated node for this purpose. This node can be identical to other nodes from the deployment point of view (the same application as on the other nodes), but user requests are not routed to it and more powerful hardware can be used in some cases. \n \n \n Results \nOn the one hand, maintenance node provides potentially resource-consuming indexing processes with dedicated hardware capacities. On the other hand,\u00a0maintenance processes do not\u00a0interfere\u00a0with user requests. \n \n \n \n Data Loader loads\u00a0 all active\u00a0 data to Coherence during each daily update, but there is \u201c dark matter \u201d that is not loaded into Coherence but\u00a0occasionally\u00a0requested by some clients. For instance, this matter is obsolete products and categories that are not visible on the site and not available for purchase. Coherence Read-Through feature is used to cope with these entities \u2013 it is acceptable to load them from the RDBMS on demand because the number of such requests is very low. \n Implementation of Data Loader \n Design of Data Loader is influenced by two major factors: \n \n Loader should efficiently fetch and process large data set in a relatively short time. \n There are multiple consumers like index builders or entity saves that should process the same data. \n \n As a result, Data Loader is organized as an\u00a0asynchronous pipeline ( Pipes and Filters design pattern) where batches of entities are loaded from RDBMS by a set of units that work in parallel threads. Loaded entities are submitted to a queue, and each consumer works in its own thread taking batches and processing them independently from the other participants. This schema is shown in the figure below: \n  Data Loading Pipeline \n This schema is relatively simple because there is only one data source and structure of entities is not too complicated. Nevertheless, this pipeline can become more complex if there are multiple data sources and one business entity is assembled using several sources. In this case, a batch of entities can be\u00a0initially\u00a0loaded from a single source and then passed to another loader that\u00a0enriches\u00a0entities by additional attributes and so on. \n \n \n \n \n Pattern: Data Loading Pipeline \n \n \n \n Problem \nA system should be populated with a large data set that come from single or multiple sources. One business entity can depend on multiple sources. There are many consumers of the loaded business entities that index, persist, or process entities. \n \n \n Solution \nAdopt the Pipes and Filters pattern. Implement each operation (loading or indexing) as an isolated unit that produces or consumes entities. Data producers or loaders should be driven by incoming requests that specify data to be loaded. Connect all units via\u00a0asynchronous data\u00a0channels and run multiple instances of each unit as an\u00a0independent\u00a0process. \n \n \n Results \nData Loading Pipeline allows one to organize efficient data loading in a multithreaded environment. All units can work in a batch mode, and more parallel instances can be easily added. A special attention should be paid to the memory consumption because queues with entities can consume a lot of memory if a system is not balanced or misconfigured. \n \n \n \n Data inconsistency during saving of new data to Coherence is practically avoided using techniques that were described in one of my previous articles . \n Implementation of Hierarchical Navigation \n When we first started to work on the navigation procedures, we first tried to do it using standard Coherence capabilities, i.e. filters and entry processors. This attempt was not very\u00a0successful from the performance point of view due to high memory consumption and relatively low performance in general. The next step was to design a compact data structure that supports very fast category tree traversal and extraction of products by Category ID. The structure we created is based on the nested set model , it is shown in the figure below: \n  Hierarchical Navigation Index Structure \n A navigation index represents a huge array of product IDs and their basic attributes that are frequently used in computation and filtering, for example, in-stock availability. In our domain model these attributes are binary, hence we efficiently packed them into integer numbers where each bit is reserved for a particular attribute. Each element of this array corresponds to the product-to-category relation and one product ID can occur in this array multiple times if product is\u00a0associated with multiple categories. Hierarchy itself is stored as an indexed tree of category IDs and each node contains two indexes in product-to-category array. This indexes point to start and end positions of relations that belong to the particular category. \n The second notable feature of this navigation solution is that each Processing Node fetches index from Coherence and entirely caches it in local memory. This allows one to perform navigational operations without touching heavy-weight domain objects. If data volume becomes high, it is possible to partition index into several shards and perform distributed processing, although it was not a case in our application (index with millions of products can be easily handled by one JVM). This technique can be considered as a common pattern (or anti-pattern, it depends on scalability requirements): \n \n \n \n \n Pattern: Replicated Custom Index \n \n \n \n Problem \nThere is an application with a distributed data storage. It is necessary to perform a special type of query that involves limited amount of attributes for each entity, but complex business logic or high performance requirements make standard distributed scans inefficient. \n \n \n Solution \n When a non-standard traversal or querying is required and amount of involved data is limited, each node in the cluster can cache domain-specific index and use it to perform the operation. \n \n \n Results \nThis approach can be very efficient when standard indexes do not work well, but it can turn into scalability bottleneck if implemented incorrectly. If there are reasons to assume that index will become too large to be cached on one node, this is a serious argument against this approach. \n \n \n \n Index propagation throughout the cluster is shown in the figure below. Maintenance Node loads data from the Master DB, builds index, saves it in a serialized partitioned form to Coherence, and then Processing Nodes fetch it and cache locally: \n  Index Building and Propagation \n Implementation of Faceted Navigation \n Faceted Navigation was described in the first section of this article, but it should be mentioned that logic of computation is not always straightforward, but often affected by business rules and\u00a0peculiarities\u00a0of a business model. As an interesting example, we can consider the following use case. Imagine that, according to the business model, product is not a final item of purchase, but a group of such items. For instance, when user looks into the Jeans category, he or she can see Levi\u2019s Jeans 501 as a product, but the actual item to be purchased is a particular instance of\u00a0Levi\u2019s Jeans 501, say\u00a0Levi\u2019s Jeans 501 of size 34\u00d730, white color. Considered as a product domain entity,\u00a0Levi\u2019s Jeans 501 will contain many particular items of a different color and size. From the faceted navigation perspective, this leads to the interesting issue. At the first glance, it is fine to attribute each product with all sizes or colors that can be found\u00a0among\u00a0all its instances and build facets based on this information. Now imagine that there are two instances of Levi\u2019s Jeans 501 \u2013 one is of\u00a0size 34\u00d730 and in white color, another one is\u00a0one is of\u00a0size 30\u00d730 and in white color.\u00a0If the user looks for black jeans of size 34\u00d730, this product will match the filter if it\u00a0is simply attributed by a plain list of instance-level attributes. Nevertheless, there are no\u00a0black jeans of size 34\u00d730 in the store. This situation is illustrated in the figure below: \n  Incorrect Modeling of Products and Instances \n This is just a one example of non-trivial issues with facetization logic. Many more issues and\u00a0merchandiser-driven tweaks can appear in a real system.\u00a0The conclusion is that faceted navigation can be pretty sophisticated and certain implementation flexibility is required. \n To cope with such issues, it was decided to keep the design of a facet index very straightforward and do not use data layouts like inverted indexes . Basically, all products, their instances and higher level groups of items are stored just like nested arrays and maps of objects: \n  Facet Index \n All attributes are mapped to the integer values and these values are compactly stored in open addressing hash sets inside each instance or product. This allows one to iterate over all items within a category, efficiently\u00a0applying\u00a0user selected filter to each item, and increment facet counters for all attributes that are inside accepted items. I provided a detailed description of data structures and algorithms that allow one to do this in my previous post . \n If the user selected filter includes many attributes it may be inefficient to check all these attributes one by one for each item. Performance of filtering can be improved using Bloom filter \u00a0that allows one to apply a filter of several terms to a set of attributes using a couple of processor instructions. Bloom filter is liable to false positives, so it can not completely replace traditional checks using hash sets with attributes, but it can be used as a preliminary test to decrease a number of relatively expensive\u00a0exact checks. This technique is used in a number of well-known systems, Google Big Table and Apache HBase are among them. \n \n \n \n \n Pattern: Probabilistic Test \n \n \n \n Problem \nThere is a large collection of items (domain entities, files, records etc). It is necessary to provide the ability to select items that meet a certain criteria \u2013 simple yes/no predicate or complex filter. \n \n \n Solution \n Items can be grouped into buckets. Each bucket contains one or more items and has a compact signature that allows one to answer the question \u201c is there at least one item inside the bucket that meets the criteria \u201c. This signature is typically a kind of hash that has much smaller memory footprint than the original collection and liable to false positives. Query processor tests bucket\u2019s signature and, if results shows that bucket potentially can contain the requested items, it goes into the bucket and checks all items independently. \n \n \n Results \nProbabilistic testing is good to trade time to memory or IO to memory. It increases memory consumption because of signatures, but allows one to significantly decrease volume of processed data for selective queries. \n \n \n \n Replicated Custom Index pattern is used to distribute Facet Index throughout the cluster, just like Navigation Index. \n Conclusions \n The described design showed the following properties after being in production for a long time: \n \n (+) Computational performance is superior in comparison with the general-purpose databases and third-party products. \n (+) The deployment schema is very efficient at all stages of\u00a0development, functional testing, performance testing, and production maintenance because of its simplicity and flexibility. \n (+) Cost of ownership and development is pretty low in comparison with third-party products usage due to high flexibility and relative simplicity of the used data structures. \n (-) Scalability by data is not a built-in feature of the described design because of non-sharded replicated indexes. Nevertheless, actual capacity is relatively high for eCommerce domain and sharding capabilities can be added. \n (-) In the long term perspective there is a negative tendency to\u00a0over-complicated\u00a0extensions around the core structures that are caused by complication of the business logic."], "link": "http://highlyscalable.wordpress.com/2012/04/02/architecture-of-high-performance-ecommerce-backend/", "bloglinks": {}, "links": {"http://highlyscalable.wordpress.com/": 12, "http://feeds.wordpress.com/": 1, "http://eaipatterns.com/": 2, "http://www.alistapart.com/": 1, "http://en.wikipedia.org/": 2}, "blogtitle": "Highly Scalable Blog"}, {"content": ["NoSQL databases are often compared by various non-functional criteria, such as scalability, performance, and consistency. This aspect of NoSQL is well-studied both in practice and theory because specific non-functional properties are often the main justification for NoSQL usage and\u00a0fundamental results on distributed systems like the\u00a0 CAP theorem \u00a0apply well to NoSQL systems. \u00a0At the same time, NoSQL data modeling is not so well studied and lacks the systematic theory found in relational databases. In this article I provide a short comparison of NoSQL system families from the data modeling point of view and digest several common modeling techniques. \n I would like to thank\u00a0 Daniel Kirkdorffer \u00a0who reviewed the article and cleaned up the grammar. \n To \u00a0explore data modeling techniques, we have to start with a more or less systematic view of NoSQL data models that preferably reveals trends and interconnections. The following figure depicts imaginary \u201cevolution\u201d of the major NoSQL system families, namely, Key-Value stores, BigTable-style databases, Document databases, Full Text Search Engines, and Graph databases: \n  NoSQL Data Models \n First, we should note that SQL and relational model in general were designed long time ago to interact with the end user. This user-oriented nature had vast implications: \n \n The end user is often interested in aggregated reporting information, not in separate data items, and SQL pays a lot of attention to this aspect. \n No one can expect human users to explicitly control concurrency, integrity, consistency, or data type validity. That\u2019s why SQL pays a lot of attention to transactional\u00a0guaranties, schemas, and referential integrity. \n \n On the other hand, it turned out that software applications are not so often interested in in-database aggregation and able to control,\u00a0at least in many cases, integrity and validity themselves. Besides this, elimination of these features had an extremely important influence on the performance and scalability of the stores.\u00a0And this was where a new evolution of data models began: \n \n Key-Value storage is a very simplistic, but very powerful model. Many techniques that are described below are perfectly applicable to this model. \n One of the most significant shortcomings of the Key-Value model is a poor applicability to cases that require processing of key ranges. Ordered Key-Value model overcomes this limitation and significantly improves aggregation capabilities. \n Ordered Key-Value model is very powerful, but it does not provide any\u00a0framework for value modeling. In general, value modeling can be done by an application, but BigTable-style databases go further and model values as a map-of-maps-of-maps, namely, column families, columns, and timestamped versions. \n Document databases advance the BigTable model offering two\u00a0significant improvements. The first one is\u00a0values with\u00a0schemes\u00a0of arbitrary complexity, not just a map-of-maps. The second one is database-managed indexes, at least in some implementations. Full Text Search Engines can be\u00a0considered a related species in the sense that they also offer flexible schema and automatic indexes. The main difference is that Document database group indexes by field names, as opposed to Search Engines that group indexes by field values. It is also worth noting that some Key-Value stores like Oracle Coherence gradually move towards Document databases via addition of indexes and in-database entry processors. \n Finally, Graph data models can be considered as a side branch of evolution that origins from the Ordered Key-Value models. Graph databases allow one model business entities very transparently ( this depends on that ), but hierarchical modeling techniques make other data models very competitive in this area too. Graph databases are related to Document databases because many implementations allow one model a value as a map or document. \n \n General Notes on NoSQL Data Modeling \n The rest of this article describes concrete data modeling techniques and patterns. As a preface, I would like to provide a few general notes on NoSQL data modeling: \n \n NoSQL data modeling often starts from the application-specific queries as opposed to relational modeling:\n \n Relational modeling is typically driven by the structure of available data. The main design theme is \u00a0\u201d What answers do I have?\u201d \u00a0 \n NoSQL data modeling is typically driven by application-specific access patterns, i.e. the types of queries to be supported. The main design theme is \u00a0\u201dWhat questions do I have?\u201d\u00a0 \u00a0 \n \n \n NoSQL data modeling often requires a deeper understanding of data\u00a0structures\u00a0and algorithms than relational database modeling does. In this article I describe several well-known data structures that are not specific for NoSQL, but are very useful in practical NoSQL modeling. \n Data duplication and denormalization are first-class citizens. \n Relational databases are not very convenient for hierarchical or graph-like data modeling and processing. Graph databases are obviously a perfect solution for this area, but actually most of NoSQL solutions are surprisingly strong for such problems. That is why the current article devotes a separate section to hierarchical data modeling. \n \n Although data modeling techniques are basically\u00a0implementation\u00a0agnostic, this is a list of the particular systems that I had in mind while working on this article: \n \n \n Key-Value Stores: Oracle Coherence, Redis, Kyoto Cabinet \n BigTable-style Databases: Apache HBase, Apache Cassandra \n Document Databases: MongoDB, CouchDB \n Full Text Search Engines: Apache Lucene, Apache Solr \n Graph Databases: neo4j, FlockDB \n \n \n Conceptual Techniques \n This section is devoted to the basic principles of NoSQL data modeling. \n (1) Denormalization \n Denormalization can be defined as the copying of the same data into multiple documents or tables in order to simplify/optimize query processing or to fit the user\u2019s data into a particular data model.\u00a0Most techniques described in this article leverage denormalization in one or another form. \n In general, denormalization is helpful for the following trade-offs: \n \n Query data volume \u00a0or IO per query \u00a0VS\u00a0 total data volume . Using denormalization one can group all data that is needed to process a query in one place. This often means that for different query flows the same data will be accessed in different combinations. Hence we need to duplicate data, which increases total data volume. \n Processing complexity VS total data volume . Modeling-time normalization and consequent query-time joins obviously increase complexity of the query processor, especially in distributed systems. Denormalization allow one to store data in a query-friendly structure to simplify query processing. \n \n Applicability : Key-Value Stores, Document Databases, BigTable-style Databases \n (2) Aggregates \n All major genres of NoSQL provide soft schema capabilities in one way or another: \n \n Key-Value Stores and Graph Databases typically do not place constraints on values, so values can be comprised of arbitrary format. It is also possible to vary a number of records for one business entity by using composite keys. For example, a user account can be modeled as a set of entries with composite keys like UserID_name, UserID_email, UserID_messages \u00a0and so on. If a user has no email or messages then a corresponding entry is not recorded. \n BigTable models support soft schema via a variable set of columns within a column family and a variable number of versions for one cell . \n Document databases are inherently schema-less, although\u00a0some of them allow one to validate incoming data\u00a0using a user-defined schema. \n \n Soft schema allows one to form classes of entities with complex internal structures (nested entities) and to vary the structure of particular entities.This feature provides two major facilities: \n \n Minimization of one-to-many relationships by means of nested entities and, consequently, reduction of joins. \n Masking of \u201ctechnical\u201d differences between business entities and modeling of\u00a0heterogeneous business entities using one collection of documents or one table. \n \n These facilities are illustrated in the figure below. This figure depicts modeling of a product entity for an eCommerce business domain. Initially, we can say that all products have an ID, Price, and Description. Next, we discover that different types of products have different attributes like Author for Book or Length for Jeans. Some of these attributes have a one-to-many or many-to-many nature like Tracks in Music Albums. Next, it is possible that some entities can not be modeled using fixed types at all. For example, Jeans attributes are not consistent across brands and specific for each manufacturer. It is possible to overcome all these issues in a relational normalized data model, but solutions are far from elegant. Soft schema allows one to use a single Aggregate (product) that can model all types of products and their attributes: \n \n  Entity Aggregation \n \n \n Embedding with denormalization can\u00a0greatly\u00a0impact updates both in\u00a0performance\u00a0and consistency, so special\u00a0attention should be paid to update flows. \n \n Applicability : Key-Value Stores,\u00a0Document Databases, BigTable-style Databases \n (3) Application Side Joins \n Joins are rarely supported in NoSQL solutions. As a consequence of the \u201cquestion-oriented\u201d NoSQL nature, joins are often handled at design time as opposed to relational models where joins are handled at query execution time. Query time joins almost always mean a performance penalty, but in many cases one can avoid joins using Denormalization and Aggregates, i.e. embedding nested entities. Of course, in many cases joins are inevitable and should be handled by an application. The major use cases are: \n \n Many to many relationships are often modeled by links and require joins. \n Aggregates are often inapplicable when entity internals are the subject of frequent modifications. It is usually better to keep a record that something happened and join the records at query time\u00a0as opposed to changing a value . For example, a messaging system can be modeled as a User entity that contains nested Message entities. But if messages are often appended, it may be better to extract Messages as independent entities and join them to the User at query time:\u00a0 \n \n Applicability : Key-Value Stores, Document Databases, BigTable-style Databases, Graph Databases \n General Modeling Techniques \n In this section we discuss general modeling techniques that applicable to a variety of NoSQL implementations. \n (4) Atomic Aggregates \n Many, although not all, NoSQL solutions have limited transaction support. In some cases one can achieve transactional behavior using distributed locks or application-managed MVCC , but it is common to model data using an Aggregates technique to\u00a0guarantee some of the ACID properties. \n One of the reasons why powerful transactional machinery is an inevitable part of the relational databases is that normalized data typically require multi-place updates. On the other hand, Aggregates allow one to store a single business entity as one document, row or key-value pair and update it atomically: \n  Atomic Aggregates \n Of course, Atomic Aggregates as a data modeling technique is not a complete transactional solution, but if the store provides certain\u00a0guaranties of atomicity, locks, or test-and-set instructions then Atomic Aggregates can be\u00a0applicable. \n Applicability : Key-Value Stores, Document Databases, BigTable-style Databases \n (5) Enumerable Keys \n Perhaps the greatest benefit of an unordered Key-Value data model is that entries can be partitioned across multiple servers by just hashing the key. Sorting makes things more complex, but sometimes an application is able to take some advantages of ordered keys even if storage doesn\u2019t offer such a feature. Let\u2019s consider the modeling of email messages as an example: \n \n Some NoSQL stores provide atomic counters that allow one to generate sequential IDs. In this case one can store messages using userID_messageID \u00a0as a composite key. If the latest message ID is known, it is possible to traverse previous messages. It is also possible to traverse\u00a0preceding\u00a0and\u00a0succeeding messages for any given message ID. \n Messages can be grouped into buckets, for example, daily buckets. This allows one to traverse a mail box backward or forward starting from any specified date or the current date. \n \n Applicability : Key-Value Stores \n (6) Dimensionality Reduction \n Dimensionality Reduction is a technique that allows one to map\u00a0multidimensional data to a Key-Value model or to other non-multidimensional models. \n Traditional geographic information systems use some variation of a Quadtree or R-Tree for indexes. These structures need to be updated in-place and are expensive to manipulate when data volumes are large. An alternative approach is to traverse the 2D structure and flatten it into a plain list of entries. One well known example of this technique is a Geohash. A Geohash uses a Z-like scan to fill 2D space and each move is encoded as 0 or 1 depending on direction. Bits for longitude and latitude moves are interleaved as well as moves. The encoding process is illustrated in the figure below, where black and red bits stand for\u00a0longitude and latitude, respectively: \n  Geohash Index \n An important feature of a Geohash is its ability to estimate distance between regions using bit-wise code proximity, as is shown in the figure. Geohash encoding allows one to store geographical information using plain data models, like sorted key values preserving spatial relationships. The Dimensionality Reduction technique for BigTable was described in [6.1].\u00a0More information about Geohashes and other related techniques can be found in [6.2] and [6.3]. \n Applicability : Key-Value Stores, Document Databases, BigTable-style Databases \n (7) Index Table \n Index Table is a very straightforward technique that allows one to take advantage of indexes in stores that do not support indexes internally. The most important class of such stores is the BigTable-style database. The idea is to create and maintain a special table with keys that follow the access pattern. For example, there is a master table that stores user accounts that can be accessed by user ID. A query that retrieves all users by a specified city can be supported by means of an additional table where city is a key: \n  Index Table Example \n An Index table can be updated for each update of the master table or in batch mode. Either way, it results in an additional performance penalty and become a consistency issue. \n Index Table can be considered as an analog of materialized views in relational databases. \n Applicability : BigTable-style Databases \n (8) Composite Key Index \n Composite key is a very generic technique, but it is\u00a0extremely beneficial when a store with ordered\u00a0keys is used. Composite keys in conjunction with secondary sorting allows one to build a kind of multidimensional index which is\u00a0fundamentally similar to the previously described Dimensionality Reduction technique. For example, let\u2019s take a set of records where each record is a user statistic. If we are going to aggregate these statistics by a region the user came from, we can use keys in a format\u00a0 (State:City:UserID) \u00a0that allow us to iterate over records for a particular state or city if that store supports the selection of key ranges by a partial key match (as BigTable-style systems do): \n \nSELECT Values WHERE state=\"CA:*\"\nSELECT Values WHERE city=\"CA:San Francisco*\"\n \n  Composite Key Index \n Applicability : BigTable-style Databases \n (9) Aggregation with Composite Keys \n Composite keys may be used not only for indexing, but for different types of grouping. Let\u2019s consider an example. There is a huge array of log records with information about internet users and their visits from different sites ( click stream ). The goal is to count the number of unique users for each site. This is similar to the following SQL query: \n \nSELECT count(distinct(user_id)) FROM clicks GROUP BY site\n \n We can model this situation using composite keys with a UserID prefix: \n  Counting Unique Users using Composite Keys \n The idea is to keep all records for one user collocated, so it is possible to fetch such a frame into memory (one user can not produce too many events) and to eliminate site duplicates using hash table or whatever. An alternative technique is to have one entry for one user and append sites to this entry as events arrive. Nevertheless, entry modification is generally less efficient than entry insertion in the majority of\u00a0implementations. \n Applicability : Ordered Key-Value Stores, BigTable-style Databases \n (10) Inverted Search \u2013 Direct Aggregation \n This technique is more a data processing pattern, rather than data modeling. Nevertheless, data models are also impacted by usage of this pattern. The main idea of this technique is to use an index to find data that meets a criteria, but aggregate data using original representation or full scans. Let\u2019s consider an example. There are a number of log records with information about internet users and their visits from different sites ( click stream ). Let assume that each record contains user ID, categories this user belongs to (Men, Women, Bloggers, etc), city this user came from, and visited site. The goal is to describe the audience that meet some criteria (site, city, etc) in terms of unique users for each category that occurs in this audience (i.e. in the set of users that meet the criteria). \n It is quite clear that a search of users that meet the criteria can be efficiently done using inverted indexes like {Category -> [user IDs]} or {Site -> [user IDs]} . Using such indexes, one can intersect or unify corresponding user IDs (this can be done very efficiently if user IDs are stored as sorted lists or bit sets) and obtain an audience. But describing an audience which is similar to an aggregation query like \n \nSELECT count(distinct(user_id)) ... GROUP BY category\n \n cannot be handled\u00a0efficiently using an inverted index if the number of categories is big.\u00a0To cope with this, one can build a direct index of the form {UserID -> [Categories]} and iterate over it in order to build a final report. This schema is depicted below: \n  Counting Unique Users using Inverse and Direct Indexes \n And as a final note, we should take into account that random retrieval of records for each user ID in the audience can be inefficient. One can grapple with this problem by leveraging batch query processing. This means that some number of user sets can be precomputed (for different criteria) and then all reports for this batch of audiences can be computed in one full scan of direct or inverse index. \n Applicability : Key-Value Stores, BigTable-style Databases, Document Databases \n Hierarchy Modeling Techniques \n (11) Tree Aggregation \n Trees or even\u00a0arbitrary\u00a0graphs (with the aid of denormalization) can be modeled as a single record or document. \n \n This techniques is efficient when the tree is accessed at once (for example, an entire tree of blog comments is fetched to show a page with a post). \n Search and arbitrary access to the entries may be problematic. \n Updates are\u00a0inefficient in most NoSQL implementations (as compared to independent nodes). \n \n  Tree Aggregation \n Applicability : Key-Value Stores, Document Databases \n \u00a0(12) Adjacency Lists \n Adjacency Lists are a straightforward way of graph modeling \u2013 each node is modeled as an independent record that contains arrays of direct ancestors or descendants. It allows one to search for nodes by identifiers of their parents or children and, of course, to traverse a graph by doing one hop per query. This approach is usually inefficient for\u00a0getting an entire subtree for a given node, for deep or wide traversals. \n Applicability : Key-Value Stores, Document Databases \n (13) Materialized Paths \n Materialized Paths is a technique that helps to avoid recursive traversals of tree-like\u00a0structures. This technique can be considered as a kind of denormalization. The idea is to attribute each node by identifiers of all its parents or children, so that it is possible to determine all descendants or predecessors of the node without traversal: \n  Materialized Paths for eShop Category Hierarchy \n This technique is especially helpful for Full Text Search Engines because it allows one to convert hierarchical structures into flat documents. One can see in the figure above that all products or subcategories within the\u00a0 Men\u2019s Shoes category can be retrieved using a short query which is simply a category name. \n Materialized Paths can be stored as a set of IDs or as a single string of concatenated IDs. The latter option allows one to search for nodes that meet a certain partial path criteria using regular expressions. This option is illustrated in the figure below (path includes node itself): \n  Query Materialized Paths using RegExp \n Applicability : Key-Value Stores, Document Databases, Search Engines \n (14) Nested Sets \n Nested sets is a standard technique for modeling tree-like structures. It is widely used in relational databases, but it is perfectly applicable to Key-Value Stores and Document Databases. The idea is to store the leafs of the tree in an array and to map each non-leaf node to a range of leafs using start and end indexes, as is shown in the figure below: \n  Modeling of eCommerce Catalog using Nested Sets \n This structure is pretty efficient for immutable data because it has a small memory footprint and allows one to fetch all leafs for a given node without traversals. Nevertheless, inserts and updates are quite costly because the addition of one leaf causes an extensive update of indexes. \n Applicability : Key-Value Stores, Document Databases \n (15)\u00a0Nested Documents Flattening: Numbered Field Names \n Search Engines typically work with flat documents, i.e. each document is a flat list of fields and values. The goal of data modeling is to map business entities to plain documents and this can be challenging if the entities\u00a0have a complex internal structure. One typical challenge mapping documents with a hierarchical structure, i.e. documents with nested documents inside. Let\u2019s consider the following example: \n  Nested Documents Problem \n Each business entity is some kind of resume. It contains a person\u2019s name and a list of his or her skills with a skill level. An obvious way to model such an entity is to create a plain document with Skill and Level fields. This model allows one to search for a person by skill or by level, but queries that combine both fields are liable to result in false matches, as depicted in the figure above. \n One way to overcome this issue was suggested in [4.6]. The main idea of this technique is to\u00a0index each skill and corresponding level as a dedicated pair of fields\u00a0 Skill_i \u00a0and\u00a0 Level_i,\u00a0 and to search for all these pairs\u00a0simultaneously (where the number of OR-ed terms in a query is as high as the maximum number of skills for one person): \n  Nested Document Modeling using Numbered Field Names \n This approach is not really scalable because query complexity grows rapidly as a function of the number of nested structures. \n Applicability : Search Engines \n (16) Nested Documents Flattening: Proximity Queries \n The problem with nested documents can be solved using another technique that were also described in [4.6]. The idea is to use proximity queries that limit the acceptable distance between words in the document. In the figure below, all skills and levels are indexed in one field, namely, SkillAndLevel, and the query indicates that the words \u201cExcellent\u201d and \u201cPoetry\u201d should follow one another: \n  Nested Document Modeling using Proximity Queries \n [4.3] describes a success story for this\u00a0technique used on top of Solr. \n Applicability : Search Engines \n (17) Batch Graph Processing \n Graph databases like neo4j are exceptionally good for exploring the neighborhood of a given node or exploring relationships between two or a few nodes. Nevertheless, global processing of large\u00a0graphs is not very efficient because general purpose graph databases do not scale well. Distributed graph processing can be done using MapReduce and the Message Passing pattern that was described, for example, in one of my previous articles . This approach makes\u00a0Key-Value stores, Document databases, and BigTable-style databases suitable for processing large graphs. \n Applicability : Key-Value Stores, Document Databases, BigTable-style Databases \n References \n Finally, I provide a list of useful links related to NoSQL data modeling: \n \n Key-Value Stores:\n \n http://www.devshed.com/c/a/MySQL/Database-Design-Using-KeyValue-Tables/ \n http://antirez.com/post/Sorting-in-key-value-data-model.htm l \n http://stackoverflow.com/questions/3554169/difference-between-document-based-and-key-value-based-databases \n http://dbmsmusings.blogspot.com/2010/03/distinguishing-two-major-types-of_29.html \n \n \n BigTable-style Databases:\n \n http://www.slideshare.net/ebenhewitt/cassandra-datamodel-4985524 \n http://www.slideshare.net/mattdennis/cassandra-data-modeling \n http://nosql.mypopescu.com/post/17419074362/cassandra-data-modeling-examples-with-matthew-f-dennis \n http://s-expressions.com/2009/03/08/hbase-on-designing-schemas-for-column-oriented-data-stores/ \n http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable \n \n \n Document Databases:\n \n http://www.slideshare.net/mongodb/mongodb-schema-design-richard-kreuters-mongo-berlin-preso \n http://www.michaelhamrah.com/blog/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks-and-denormalizing-data-for-efficiency/ \n http://seancribbs.com/tech/2009/09/28/modeling-a-tree-in-a-document-database/ \n http://www.mongodb.org/display/DOCS/Schema+Design \n http://www.mongodb.org/display/DOCS/Trees+in+MongoDB \n http://blog.fiesta.cc/post/11319522700/walkthrough-mongodb-data-modeling \n \n \n Full Text Search Engines:\n \n http://www.searchworkings.org/blog/-/blogs/query-time-joining-in-lucene \n http://www.lucidimagination.com/devzone/technical-articles/solr-and-rdbms-basics-designing-your-application-best-both \n http://blog.griddynamics.com/2011/07/solr-experience-search-parent-child.html \n http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/ \n http://blog.mgm-tp.com/2011/03/non-standard-ways-of-using-lucene/ \n http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene \n http://mysolr.com/tips/denormalized-data-structure/ \n http://sujitpal.blogspot.com/2010/10/denormalizing-maps-with-lucene-payloads.html \n http://java.dzone.com/articles/hibernate-search-mapping-entit \n \n \n Graph Databases:\n \n http://docs.neo4j.org/chunked/stable/tutorial-comparing-models.html \n http://blog.neo4j.org/2010/03/modeling-categories-in-graph-database.html \n http://skillsmatter.com/podcast/nosql/graph-modelling \n http://www.umiacs.umd.edu/~jimmylin/publications/Lin_Schatz_MLG2010.pdf \n \n \n Demensionality Reduction:\n \n http://www.slideshare.net/mmalone/scaling-gis-data-in-nonrelational-data-stores \n http://blog.notdot.net/2009/11/Damn-Cool-Algorithms-Spatial-indexing-with-Quadtrees-and-Hilbert-Curves \n http://www.trisis.co.uk/blog/?p=1287"], "link": "http://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/", "bloglinks": {}, "links": {"http://docs.neo4j.org/": 1, "http://s-expressions.com/": 1, "http://mysolr.com/": 1, "http://www.umd.edu/": 1, "http://stackoverflow.com/": 1, "http://highlyscalable.wordpress.com/": 18, "http://www.devshed.com/": 1, "http://blog.mgm-tp.com/": 1, "http://www.michaelhamrah.com/blog": 1, "http://blog.fiesta.cc/": 1, "http://nosql.mypopescu.com/": 1, "http://skillsmatter.com/": 1, "http://www.lucidimagination.com/blog": 1, "http://feeds.wordpress.com/": 1, "http://antirez.com/": 1, "http://www.slideshare.net/": 5, "http://jimbojw.com/": 1, "http://sujitpal.blogspot.com/": 1, "http://www.co.uk/blog": 1, "http://www.kirkdorffer.com/": 1, "http://java.dzone.com/": 1, "http://blog.griddynamics.com/": 1, "http://dbmsmusings.blogspot.com/": 1, "http://www.mongodb.org/": 2, "http://blog.neo4j.org/": 1, "http://www.searchworkings.org/blog": 1, "http://seancribbs.com/": 1, "http://en.wikipedia.org/": 2, "http://www.lucidimagination.com/": 1, "http://blog.notdot.net/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["Java was initially designed as a safe, managed environment. Nevertheless, Java HotSpot VM contains a \u201cbackdoor\u201d that provides a number of low-level operations to manipulate memory and threads directly. This backdoor \u2013\u00a0 sun.misc.Unsafe \u00a0\u2013 is widely used by JDK itself in the packages like\u00a0 java.nio \u00a0or\u00a0 java.util.concurrent . It is hard to imagine a Java developer who uses this backdoor in any regular development because this API is extremely dangerous, non portable, and volatile. Nevertheless,\u00a0 Unsafe \u00a0provides an easy way to look into HotSpot JVM internals and do some tricks. Sometimes it is simply funny, sometimes it can be used to study VM internals without C++ code debugging, sometimes it can be leveraged for profiling and development tools. \n Obtaining Unsafe \n The\u00a0 sun.misc.Unsafe \u00a0class is so unsafe that JDK developers added special checks to restrict access to it. Its constructor is private and caller of the factory method\u00a0 getUnsafe() \u00a0should be loaded by Bootloader (i.e. caller should also be a part of JDK): \n \npublic final class Unsafe {\n ...\n private Unsafe() {}\n private static final Unsafe theUnsafe = new Unsafe();\n ...\n public static Unsafe getUnsafe() {\n  Class cc = sun.reflect.Reflection.getCallerClass(2);\n  if (cc.getClassLoader() != null)\n   throw new SecurityException(\"Unsafe\");\n  return theUnsafe;\n }\n ...\n}\n \n Fortunately there is\u00a0 theUnsafe \u00a0field that can be used to retrieve\u00a0 Unsafe \u00a0instance. We can easily write a helper method to do this via reflection: \n \npublic static Unsafe getUnsafe() {\n try {\n   Field f = Unsafe.class.getDeclaredField(\"theUnsafe\");\n   f.setAccessible(true);\n   return (Unsafe)f.get(null);\n } catch (Exception e) { /* ... */ }\n}\n \n In the next sections we will study several tricks that become possible due to the following methods of\u00a0 Unsafe : \n \n long getAddress(long address) \u00a0and\u00a0 void putAddress(long address, long x) \u00a0that allows to read and write dwords directly from memory. \n int getInt(Object o, long offset) \u00a0,\u00a0 void putInt(Object o, long offset, int x) , and other similar methods that allows to read and write data directly from C structure that represents Java object. \n long allocateMemory(long bytes) \u00a0which can be considered as a wrapper for C\u2019s malloc(). \n \n \n sizeof() Function \n The first trick we will do is C-like sizeof() function, i.e. function that returns shallow object size in bytes. Inspecting JVM sources of JDK6 and JDK7, in particular\u00a0 src/share/vm/oops/oop.hpp \u00a0and\u00a0 src/share/vm/oops/klass.hpp , and reading comments in the code, we can notice that size of class instance is stored in\u00a0 _layout_helper \u00a0which is the fourth field in C structure that represents Java class. Similarly,\u00a0 /src/share/vm/oops/oop.hpp \u00a0shows that each instance (i.e. object) stores pointer to a class structure in its second field. For 32-bit JVM this means that we can first take class structure address as 4-8 bytes in the object structure and next shift by 3\u00d74=12 bytes inside class structure to capture _layout_helper \u00a0field which is instance size in bytes. These structures are shown in the picture below: \n  \n As so, we can implement sizeof() as follows: \n \n \npublic static long sizeOf(Object object) {\n Unsafe unsafe = getUnsafe();\n return unsafe.getAddress( normalize( unsafe.getInt(object, 4L) ) + 12L );\n}\n\npublic static long normalize(int value) {\n if(value >= 0) return value;\n return (~0L >>> 32) & value;\n}\n \n We need to use normalize() function because addresses between 2^31 and 2^32 will be automatically converted to negative integers, i.e. stored in complement form. Let\u2019s test it on 32-bit JVM (JDK 6 or 7): \n \n// sizeOf(new MyStructure()) gives the following results:\n\nclass MyStructure { } // 8: 4 (start marker) + 4 (pointer to class)\nclass MyStructure { int x; } // 16: 4 (start marker) + 4 (pointer to class) + 4 (int) + 4 stuff bytes to align structure to 64-bit blocks\nclass MyStructure { int x; int y; } // 16: 4 (start marker) + 4 (pointer to class) + 2*4\n \n This function will not work for array objects, because\u00a0 _layout_helper \u00a0field has another meaning in that case. Although it is still possible to generalize sizeOf() to support arrays. \n Direct Memory Management \n Unsafe \u00a0allows to allocate and deallocate memory explicitly via\u00a0 allocateMemory \u00a0and\u00a0 freeMemory \u00a0methods. Allocated memory is not under GC control and not limited by maximum JVM heap size. In general, such functionality is safely available via NIO\u2019s off-heap bufferes. But the interesting thing is that it is possible to map standard Java reference to off-heap memory: \n \nMyStructure structure = new MyStructure(); // create a test object\nstructure.x = 777;\n\nlong size = sizeOf(structure);\nlong offheapPointer = getUnsafe().allocateMemory(size);\ngetUnsafe().copyMemory(\n    structure,  // source object\n    0,    // source offset is zero - copy an entire object\n    null,   // destination is specified by absolute address, so destination object is null\n    offheapPointer, // destination address\n    size\n); // test object was copied to off-heap\n\nPointer p = new Pointer(); // Pointer is just a handler that stores address of some object\nlong pointerOffset = getUnsafe().objectFieldOffset(Pointer.class.getDeclaredField(\"pointer\"));\ngetUnsafe().putLong(p, pointerOffset, offheapPointer); // set pointer to off-heap copy of the test object\n\nstructure.x = 222; // rewrite x value in the original object\nSystem.out.println( ((MyStructure)p.pointer).x ); // prints 777\n\n....\n\nclass Pointer {\n Object pointer;\n}\n \n So, it is virtually possible to manually allocate and deallocate real objects, not only byte buffers. Of course, it\u2019s a big question what may happen with GC after such cheats. \n Inheritance from Final Class and void* \n Imagine the situation when one has a method that takes a string as an argument, but it is necessary to pass some extra payload. There are at least two standard ways to do it in Java: put payload to thread local or use static field. With\u00a0 Unsafe \u00a0another two possibilities appears: pass payload address as a string and inherit payload class from String class. The first approach is pretty close to what we see in the previous section \u2013 one just need obtain payload address using Pointer and create a new Pointer to payload inside the called method. In other words, any argument that can carrier an address can be used as analog of void* in C. In order to explore the second approach we start with the following code which is compilable, but obviously produces ClassCastException in run time: \n \nCarrier carrier = new Carrier();\ncarrier.secret = 777;\n\nString message = (String)(Object)carrier; // ClassCastException\nhandler( message );\n\n...\n\nvoid handler(String message) {\n System.out.println( ((Carrier)(Object)message).secret );\n}\n\n...\n\nclass Carrier {\n int secret;\n}\n \n To make it work, one need to modify Carrier class to simulate inheritance from String. A list of superclasses is stored in Carrier class structure starting from position 28, as it shown in the figure. Pointer to object goes first and pointer to Carrier itself goes after it (at position 32) since Carrier is inherited from Object directly. In principle, it is enough to add the following code before the line that casts Carrier to String: \n \nlong carrierClassAddress = normalize( unsafe.getInt(carrier, 4L) );\nlong stringClassAddress = normalize( unsafe.getInt(\"\", 4L) );\nunsafe.putAddress(carrierClassAddress + 32, stringClassAddress); // insert pointer to String class to the list of Carrier's superclasses\n \n Now cast works fine. Nevertheless, this transformation is not correct and violates VM contracts. More careful approach should include more steps: \n \n Position 32 in Carrier class actually contains a pointer to Carrier class itself, so this pointer should be shifted to position 36, not simply overwritten by the pointer to the String class. \n Since Carrier is now inherited from String, final markers in String class should be removed. \n \n \n Conclusion \n sun.misc.Unsafe \u00a0provides almost unlimited capabilities for exploring and modification of VM\u2019s runtime data structures. Despite the fact that these capabilities are almost inapplicable in Java development itself, Unsafe is a great tool for anyone who want to study HotSpot VM without C++ code debugging or need to create ad hoc profiling instruments."], "link": "http://highlyscalable.wordpress.com/2012/02/02/direct-memory-access-in-java/", "bloglinks": {}, "links": {"http://hg.java.net/": 3, "http://highlyscalable.wordpress.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["In this article I\u00a0digested\u00a0a number of MapReduce patterns and algorithms to give a systematic view of the different techniques that can be found on the web or scientific articles. Several practical case studies are also provided. All descriptions and code snippets use the standard Hadoop\u2019s MapReduce model with Mappers, Reduces, Combiners, Partitioners, and sorting. This framework is depicted in the figure below. \n  MapReduce Framework \n Basic MapReduce Patterns \n Counting and Summing \n Problem Statement: \u00a0There is a number of documents where each document is a set of terms. It is required to calculate a total number of\u00a0occurrences of each term in all documents. Alternatively, it can be an arbitrary function of the terms. For instance, there is a log file where each record contains a response time and it is required to calculate an average response time. \n Solution: \n Let start with something really simple. The code snippet below shows Mapper that simply emit \u201c1\u2033 for each term it processes and Reducer that goes through the lists of ones and sum them up: \n \nclass Mapper\n method Map(docid id, doc d)\n  for all term t in doc d do\n   Emit(term t, count 1)\n\nclass Reducer\n method Reduce(term t, counts [c1, c2,...])\n  sum = 0\n  for all count c in [c1, c2,...] do\n   sum = sum + c\n  Emit(term t, count sum)\n \n The obvious disadvantage of this approach is a high amount of dummy counters emitted by the Mapper. The Mapper\u00a0can decrease a number of counters via summing counters for each document: \n \nclass Mapper\n method Map(docid id, doc d)\n  H = new AssociativeArray\n  for all term t in doc d do\n   H{t} = H{t} + 1\n  for all term t in H do\n   Emit(term t, count H{t})\n \n In order to accumulate counters not only for one document, but for all documents processed by one Mapper node, it is possible to leverage Combiners: \n \nclass Mapper\n method Map(docid id, doc d)\n  for all term t in doc d do\n   Emit(term t, count 1)\n\nclass Combiner\n method Combine(term t, [c1, c2,...])\n  sum = 0\n  for all count c in [c1, c2,...] do\n   sum = sum + c\n  Emit(term t, count sum)\n\nclass Reducer\n method Reduce(term t, counts [c1, c2,...])\n  sum = 0\n  for all count c in [c1, c2,...] do\n   sum = sum + c\n  Emit(term t, count sum)\n \n Applications: \n Log Analysis, Data Querying \n \n Collating \n Problem Statement: \u00a0There is a set of items and some function of one item. It is required to save all items that have the same value of function into one file or perform some other computation that requires all such items to be processed as a group. The most typical example is building of inverted indexes. \n Solution: \n The solution is straightforward. Mapper computes a given function for each item and emits value of the function as a key and item itself as a value. Reducer obtains all items grouped by function value and process or save them. In case of inverted indexes, items are terms (words) and function is a document ID where the term was found. \n Applications: \n Inverted Indexes, ETL \n \n Filtering (\u201cGrepping\u201d), Parsing, and Validation \n Problem Statement: \u00a0There is a set of records and it is required to collect all records that meet some condition or transform each record (independently from other records) into another representation. The later case includes such tasks as text parsing and value extraction, conversion from one format to another. \n Solution: \u00a0 Solution is absolutely straightforward \u2013 Mapper takes records one by one and emits accepted items or their transformed versions. \n Applications: \n Log Analysis, Data Querying, ETL, Data Validation \n \n Distributed Task Execution \n Problem Statement: \u00a0There is a large computational problem that can be\u00a0divided into multiple parts and results from all parts can be combined together to obtain a final result. \n Solution: \u00a0 Problem description is split in a set of specifications and specifications are stored as input data for Mappers. Each Mapper takes a specification, performs corresponding computations and emits results. Reducer combines all emitted parts into the final result. \n Case Study: Simulation of a Digital Communication System \n There is a software simulator of a digital communication system like WiMAX that passes some volume of random data through the system model and computes error probability of throughput. Each Mapper runs simulation for specified amount of data which is 1/Nth of the required sampling and emit error rate. Reducer computes average error rate. \n Applications: \n Physical and Engineering Simulations, Numerical Analysis, Performance Testing \n \n Sorting \n Problem Statement: \u00a0There is a set of records and it is required to sort these records by some rule or process these records in a certain order. \n Solution: \u00a0Simple sorting is absolutely straightforward \u2013 Mappers just emit all items as values associated with the sorting keys that are assembled as function of items. Nevertheless, in practice sorting is often used in a quite tricky way, that\u2019s why it is said to be a heart of MapReduce (and Hadoop). In particular, it is very common to use composite keys to achieve secondary sorting and grouping. \n Sorting in MapReduce is originally intended for sorting of the emitted key-value pairs by key, but there exist techniques that leverage Hadoop\u00a0implementation\u00a0specifics to achieve sorting by values. See this\u00a0 blog \u00a0for more details. \n It is worth noting that if MapReduce is used for sorting of the original (not intermediate) data, it is often a good idea to\u00a0continuously maintain data in sorted state using BigTable concepts. In other words, it can be more efficient to sort data once during insertion than sort them for each MapReduce query. \n Applications: \n ETL, Data Analysis \n \n Not-So-Basic MapReduce Patterns \n Iterative Message Passing (Graph Processing) \n Problem Statement: \u00a0There is a network of entities and relationships between them. It is required to calculate a state of each entity on the basis of properties of the other entities in its neighborhood. This state can represent a distance to other nodes, \u00a0indication that there is a neighbor with the certain properties, characteristic of neighborhood density and so on. \n Solution: \u00a0A network is stored as a set of nodes and each node contains a list of adjacent node IDs. Conceptually, MapReduce jobs are performed in iterative way and at each iteration each node sends messages to its neighbors. Each neighbor updates its state on the basis of the received messages. Iterations are terminated by some condition like fixed maximal number of iterations (say, network diameter) or negligible changes in states between two consecutive iterations.\u00a0From the technical point of view, Mapper emits messages for each node using ID of the adjacent node as a key. As result, all messages are grouped by the incoming node and reducer is able to recompute state and rewrite node with the new state. This algorithm is shown in the figure below: \n \nclass Mapper\n method Map(id n, object N)\n  Emit(id n, object N)\n  for all id m in N.OutgoingRelations do\n   Emit(id m, message getMessage(N))\n\nclass Reducer\n method Reduce(id m, [s1, s2,...])\n  M = null\n  messages = []\n  for all s in [s1, s2,...] do\n   if IsObject(s) then\n    M = s\n   else    // s is a message\n    messages.add(s)\n  M.State = calculateState(messages)\n  Emit(id m, item M)\n \n It should be emphasized that state of one node rapidly propagates across all the network of network is not too sparse because all nodes that were \u201cinfected\u201d by this state start to \u201cinfect\u201d all their neighbors. This process is illustrated in the figure below: \n  \n Case Study: Availability Propagation Through The Tree of Categories \n Problem Statement: \u00a0This problem is inspired by real life eCommerce task. There is a tree of categories that branches out from large categories (like Men, Women, Kids) to smaller ones (like Men Jeans or Women Dresses), and eventually to small end-of-line categories (like Men Blue Jeans). End-of-line category is either available (contains products) or not. Some high level category is available if there is at least one available end-of-line category in its subtree. The goal is to calculate availabilities for all categories if availabilities of end-of-line categories are know. \n Solution: \u00a0This problem can be solved using the framework that was described in the previous section. We define getMessage and calculateState methods as follows: \n \nclass N\n State in {True = 2, False = 1, null = 0}, initialized 1 or 2 for end-of-line categories, 0 otherwise\n\nmethod getMessage(object N)\n return N.State\n\nmethod calculateState(state s, data [d1, d2,...])\n return max( [d1, d2,...] )\n \n Case Study: Breadth-First Search \n Problem Statement: \u00a0There is a graph and it is required to calculate distance (a number of hops) from one source node to all other nodes in the graph. \n Solution: \u00a0Source node emits 0 to all its neighbors and these neighbors propagate this counter incrementing it by 1 during each hope: \n \nclass N\n State is distance, initialized 0 for source node, INFINITY for all other nodes\n\nmethod getMessage(N)\n return N.State + 1\n\nmethod calculateState(state s, data [d1, d2,...])\n min( [d1, d2,...] )\n \n Case Study: PageRank and Mapper-Side Data Aggregation \n This algorithm was suggested by Google to calculate relevance of a web page as a function of authoritativeness (PageRank) of pages that have links to this page. The real algorithm is quite complex, but in its core it is just a propagation of weights between nodes where each node\u00a0calculates its weight as a mean of the incoming weights: \n \nclass N\n State is PageRank\n\nmethod getMessage(object N)\n return N.State / N.OutgoingRelations.size()\n\nmethod calculateState(state s, data [d1, d2,...])\n return ( sum([d1, d2,...]) )\n \n It is worth mentioning that the schema we use is too generic and doesn\u2019t take advantage of the fact that state is a numerical value. In most of practical cases, we can perform aggregation of values on the Mapper side due to virtue of this fact. This optimization \u00a0is illustrated in the code snippet below (for the\u00a0PageRank algorithm): \n \nclass Mapper\n method Initialize\n  H = new AssociativeArray\n method Map(id n, object N)\n  p = N.PageRank / N.OutgoingRelations.size()\n  Emit(id n, object N)\n  for all id m in N.OutgoingRelations do\n   H{m} = H{m} + p\n method Close\n  for all id n in H do\n   Emit(id n, value H{n})\n\nclass Reducer\n method Reduce(id m, [s1, s2,...])\n  M = null\n  p = 0\n  for all s in [s1, s2,...] do\n   if IsObject(s) then\n    M = s\n   else\n    p = p + s\n  M.PageRank = p\n  Emit(id m, item M)\n \n Applications: \n Graph Analysis, Web Indexing \n \n Distinct Values (Unique Items Counting) \n Problem Statement: There is a set of records that contain fields F and G. Count the total number of unique\u00a0values of filed F for each subset of records that have the same G (grouped by G). \n The problem can be a little bit generalized and formulated in terms of faceted search: \n Problem Statement: There is a set of records. Each record has field F and arbitrary number of category labels G = {G1, G2, \u2026} . Count the total number of unique\u00a0values of filed F for each subset of records for each value of any label. Example: \n \nRecord 1: F=1, G={a, b}\nRecord 2: F=2, G={a, d, e}\nRecord 3: F=1, G={b}\nRecord 4: F=3, G={a, b}\n\nResult:\na -> 3 // F=1, F=2, F=3\nb -> 2 // F=1, F=3\nd -> 1 // F=2\ne -> 1 // F=2\n \n Solution I: \n The first approach is to solve the problem in two stages. At the first stage Mapper emits dummy counters for each pair of F and G; Reducer calculates a total number of\u00a0occurrences\u00a0for each such pair. The main goal of this phase is to\u00a0guarantee uniqueness of F values.\u00a0At the second phase pairs are grouped by G and the total number of items in each group is calculated. \n Phase I: \n \nclass Mapper\n method Map(null, record [value f, categories [g1, g2,...]])\n  for all category g in [g1, g2,...]\n   Emit(record [g, f], count 1)\n\nclass Reducer\n method Reduce(record [g, f], counts [n1, n2, ...])\n  Emit(record [g, f], null )\n \n Phase II: \n \nclass Mapper\n method Map(record [f, g], null)\n  Emit(value g, count 1)\n\nclass Reducer\n method Reduce(value g, counts [n1, n2,...])\n  Emit(value g, sum( [n1, n2,...] ) )\n \n Solution II: \n The second solution requires only one MapReduce job, but it is not really scalable and its applicability is limited. The algorithm is simple \u2013 Mapper emits values and categories, Reducer excludes duplicates from the list of categories for each value and increment counters for each category. The final step is to sum all counter emitted by Reducer. This approach is applicable if th number of record with the same f value is not very high and total number of categories is also limited. For instance, this approach is applicable for processing of web logs and classification of users \u2013 total number of users is high, but number of events for one user is limited, as well as a number of categories to classify by. It worth noting that Combiners can be used in this schema to exclude duplicates from category lists before data will be\u00a0transmitted\u00a0to Reducer. \n \nclass Mapper\n method Map(null, record [value f, categories [g1, g2,...] )\n  for all category g in [g1, g2,...]\n   Emit(value f, category g)\n\nclass Reducer\n method Initialize\n  H = new AssociativeArray : category -> count\n method Reduce(value f, categories [g1, g2,...])\n  [g1', g2',..] = ExcludeDuplicates( [g1, g2,..] )\n  for all category g in [g1', g2',...]\n   H{g} = H{g} + 1\n method Close\n  for all category g in H do\n   Emit(category g, count H{g})\n \n Applications: \n Log Analysis, Unique Users Counting \n \n Cross-Correlation \n Problem Statement: There is a set of tuples of items. For each possible pair of items calculate a number of tuples where these items co-occur. If the total number of items is N then N*N values should be reported. \n This problem appears in text analysis (say, items are words and tuples are sentences), market analysis (customers who buy\u00a0 this \u00a0tend to also buy\u00a0 that ). If N*N is quite small and such a matrix can\u00a0fit in the memory of a single machine, then implementation is straightforward. \n Pairs Approach \n The first approach is to emit all pairs and dummy counters from Mappers and sum these counters on Reducer. The shortcomings are: \n \n The benefit from combiners is limited, as it is likely that all pair are distinct \n There is no in-memory accumulations \n \n \nclass Mapper\n method Map(null, items [i1, i2,...] )\n  for all item i in [i1, i2,...]\n   for all item j in [i1, i2,...]\n   Emit(pair [i j], count 1)\n\nclass Reducer\n method Reduce(pair [i j], counts [c1, c2,...])\n  s = sum([c1, c2,...])\n  Emit(pair[i j], count s)\n \n Stripes Approach \n The second approach is to group data by the first item in pair and maintain an associative array (\u201cstripe\u201d) where counters for all adjacent items are accumulated. Reducer receives all stripes for leading item i, merges them, and emits the same result as in the Pairs approach. \n \n Generates fewer intermediate keys. Hence the framework has less sorting to do. \n Greately benefits from combiners. \n Performs in-memory accumulation. This can lead to problems, if not properly\u00a0implemented. \n More complex implementation. \n In general, \u201cstripes\u201d is faster than \u201cpairs\u201d \n \n \nclass Mapper\n method Map(null, items [i1, i2,...] )\n  for all item i in [i1, i2,...]\n   H = new AssociativeArray : item -> counter\n   for all item j in [i1, i2,...]\n   H{j} = H{j} + 1\n   Emit(item i, stripe H)\n\nclass Reducer\n method Reduce(item i, stripes [H1, H2,...])\n  H = new AssociativeArray : item -> counter\n  H = merge-sum( [H1, H2,...] )\n  for all item j in H.keys()\n   Emit(pair [i j], H{j})\n \n Applications: \n Text Analysis, Market Analysis \n References: \n \n Lin J. Dyer C. Hirst G.\u00a0 Data Intensive Processing MapReduce \n \n \n Relational MapReduce Patterns \n In this section we go though the main relational operators and discuss how these operators can implemented in MapReduce terms. \n Selection \n \nclass Mapper\n method Map(rowkey key, tuple t)\n  if t satisfies the predicate\n   Emit(tuple t, null)\n \n Projection \n Projection is just a little bit more complex than selection, but we should use a Reducer in this case to eliminate possible duplicates. \n \nclass Mapper\n method Map(rowkey key, tuple t)\n  tuple g = project(t) // extract required fields to tuple g\n  Emit(tuple g, null)\n\nclass Reducer\n method Reduce(tuple t, array n) // n is an array of nulls\n  Emit(tuple t, null)\n \n Union \n Mappers are fed by all records of two sets to be united. Reducer is used to eliminate duplicates. \n \nclass Mapper\n method Map(rowkey key, tuple t)\n  Emit(tuple t, null)\n\nclass Reducer\n method Reduce(tuple t, array n) // n is an array of one or two nulls\n  Emit(tuple t, null)\n \n Intersection \n Mappers are fed by all records of two sets to be intersected. Reducer emits only records that\u00a0occurred twice. It is possible only if both sets contain this record because record includes primary key and can occur in one set only once. \n \nclass Mapper\n method Map(rowkey key, tuple t)\n  Emit(tuple t, null)\n\nclass Reducer\n method Reduce(tuple t, array n) // n is an array of one or two nulls\n  if n.size() = 2\n   Emit(tuple t, null)\n \n Difference \n Let\u2019s we have two sets of records \u2013 R and S. We want to compute difference R \u2013 S. Mapper emits all tuples and tag which is a name of the set this record came from. Reducer emits only records that came from R but not from S. \n \nclass Mapper\n method Map(rowkey key, tuple t)\n  Emit(tuple t, string t.SetName) // t.SetName is either 'R' or 'S'\n\nclass Reducer\n method Reduce(tuple t, array n) // array n can be ['R'], ['S'], ['R' 'S'], or ['S', 'R']\n  if n.size() = 1 and n[1] = 'R'\n   Emit(tuple t, null)\n \n GroupBy and Aggregation \n Grouping and aggregation can be performed in one MapReduce job as follows. Mapper extract from each tuple values to group by and aggregate and emits them. Reducer receives values to be aggregated already grouped and calculates an aggregation function. Typical\u00a0aggregation\u00a0functions like sum or max can be calculated in a streaming fashion, hence don\u2019t require to handle all values\u00a0simultaneously. Nevertheless, in some cases two phase MapReduce job may be required \u2013 see pattern\u00a0 Distinct Values as an example. \n \nclass Mapper\n method Map(null, tuple [value GroupBy, value AggregateBy, value ...])\n  Emit(value GroupBy, value AggregateBy)\nclass Reducer\n method Reduce(value GroupBy, [v1, v2,...])\n  Emit(value GroupBy, aggregate( [v1, v2,...] ) ) // aggregate() : sum(), max(),...\n \n \n Joining \n Joins are perfectly possible in MapReduce framework, but there exist a number of techniques that differ in efficiency and data volumes they are oriented for. In this section we study some basic approaches. The references section contains links to detailed studies of join techniques. \n Repartition Join (Reduce Join, Sort-Merge Join) \n This algorithm joins of two sets R and L on some key k. Mapper goes through all tuples from R and L, extracts key k from the tuples, marks tuple with a tag that indicates a set this tuple came from (\u2018R\u2019 or \u2018L\u2019), and emits tagged tuple using k as a key. Reducer receives all tuples for a particular key k and put them into two buckets \u2013 for R and for L. When two buckets are filled, Reducer runs nested loop over them and emits a cross join of the buckets. Each emitted tuple is a concatenation R-tuple, L-tuple, and key k. This approach has the following disadvantages: \n \n Mapper emits absolutely all data, even for keys that\u00a0occur only in one set and have no pair in the other. \n Reducer should hold all data for one key in the memory. If data doesn\u2019t fit the memory, its Reducer\u2019s responsibility to handle this by some kind of swap. \n \n Nevertheless, Repartition Join is a most generic technique that can be successfully used when other optimized techniques are not applicable. \n \nclass Mapper\n method Map(null, tuple [join_key k, value v1, value v2,...])\n  Emit(join_key k, tagged_tuple [set_name tag, values [v1, v2, ...] ] )\n\nclass Reducer\n method Reduce(join_key k, tagged_tuples [t1, t2,...])\n  H = new AssociativeArray : set_name -> values\n  for all tagged_tuple t in [t1, t2,...]  // separate values into 2 arrays\n   H{t.tag}.add(t.values)\n  for all values r in H{'R'}     // produce a cross-join of the two arrays\n   for all values l in H{'L'}\n   Emit(null, [k r l] )\n \n Replicated Join (Map Join, Hash Join) \n In practice, it is typical to join a small set with a large one (say, a list of users with a list of log records). Let\u2019s assume that we join two sets \u2013 R and L, R is relative small. If so, R can be distributed to all Mappers and each Mapper can load it and index by the join key. The most common and efficient indexing technique here is a hash table. After this, Mapper goes through tuples of the set L and joins them with the corresponding tuples from R that are stored in the hash table. This approach is very effective because there is no need in sorting or transmission of the set L over the network, but set R should be quite small to be distributed to the all Mappers. \n \nclass Mapper\n method Initialize\n  H = new AssociativeArray : join_key -> tuple from R\n  R = loadR()\n  for all [ join_key k, tuple [r1, r2,...] ] in R\n   H{k} = H{k}.append( [r1, r2,...] )\n\n method Map(join_key k, tuple l)\n  for all tuple r in H{k}\n   Emit(null, tuple [k r l] )\n \n References: \n \n Join Algorithms using Map/Reduce \n Optimizing Joins in a MapReduce Environment \n \n Machine Learning and Math MapReduce Algorithms \n \n C. T. Chu et al provides an excellent\u00a0description\u00a0of \u00a0machine learning algorithms for MapReduce in the article\u00a0 Map-Reduce for Machine Learning on Multicore . \n FFT using MapReduce:\u00a0 http://www.slideshare.net/hortonworks/large-scale-math-with-hadoop-mapreduce \n MapReduce for integer factorization:\u00a0 http://www.javiertordable.com/files/MapreduceForIntegerFactorization.pdf \n Matrix multiplication with MapReduce:\u00a0 http://csl.skku.edu/papers/CS-TR-2010-330.pdf \u00a0and\u00a0 http://www.norstad.org/matrix-multiply/index.html"], "link": "http://highlyscalable.wordpress.com/2012/02/01/mapreduce-patterns/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://csl.skku.edu/": 1, "http://highlyscalable.wordpress.com/": 2, "http://www.slideshare.net/": 1, "http://www.ac.uk/": 1, "http://www.javiertordable.com/": 1, "http://www.stanford.edu/": 1, "http://www.amazon.com/": 1, "http://www.norstad.org/": 1, "http://www.riccomini.name/": 1, "http://infolab.stanford.edu/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["ACID transactions are one of the most widely used software engineering techniques, a\u00a0cornerstone of \u00a0the relational databases, and an integral part of the enterprise middleware where transactions are often offered as the black-box primitives. Notwithstanding all these and many other cases, the old-fashion approach to transactions cannot be maintained in a variety of modern large scale systems and NoSQL storages because of high requirements on performance, data volumes, and availability. In such cases, traditional transactions are not rarely replaced by a customized model that assumes implementation of transactional\u00a0or semi-transactional operations on the top of units that are not transactional by\u00a0themselves. \n In this post we consider implementation of lock-free transactional operations on the top of \u00a0Key-Value storages, although these techniques are generic and can be used in any database-like system. In GridDynamics, we recently used some of these techniques to implement a lightweight nonstandard transactions on the top of Oracle Coherence. In the first section we take a look at two simple approaches that suitable for some\u00a0important\u00a0use cases, in the second section we study more generic approach that resembles PostgreSQL\u2019s MVCC implementation. \n Atomic Cache Switching, Read\u00a0Committed\u00a0Isolation \n Let\u2019s start with simple and easy-to-implement techniques that are intended for relatively infrequent updates in read-mostly systems, for instance, daily data reload in eCommerce systems, administrative operations like repair of invalid items, or cache refreshes. \n The most trivial case is reload of all data in the cache (or key space). We wrap cache interface by a proxy that intercepts all cache operation like get() or put() . This proxy is backed by two caches, namely, A and B, and works in accordance with the following simple logic (Fig.1): \n \n At any moment of time, only one cache is active and proxy routes all user request to it (Fig.1.1) \n Refresh process load new data into inactive cache (Fig.1.2) \n Refresh process switches a global flag that shared by all proxies that participate in refresh and this flag\u00a0defines which cache is active (Fig.1.3). Proxy starts to dispatch all new read transactions to the new active cache. \n Transactions that are in progress at the moment of cache\u00a0switching can be handled\u00a0differently\u00a0depending on required level of consistency and isolation. If non-repeatable reads\u00a0are acceptable (some transaction can read data partially from the old state and partially from the new one) then switch is straightforward and old data can be cleaned up\u00a0immediately. Otherwise, the proxy should maintain a list of active transactions and route each one to the cache it was initially assigned. Old data can be purged only when all attached transaction were committed or aborted. \n \n  Fig.1 Cache Switch \n The similar technique can be used for partial updates. It can be implemented\u00a0differently depends on the\u00a0underlying\u00a0storage, we consider one simple strategy with three caches. The framework is similar to the previous one, but proxy acts in the following way (Fig.2): \n \n User requests are routed to the PRIMARY cache (Fig.2.1) \n New and updated items are loaded into the NEW cache, keys of deleted items are stored to DELETED cache \u00a0(Fig.2.2) \n Commit process begins with switching of the global flag. This flag instructs the proxies to look up requested keys in NEW and DELETED caches first and, if not found, look up the same key in the PRIMARY cache (Fig.2.3). In other words, all user request are switched to the new data at this step. \n Commit process starts to propagate changes from NEW and DELETED caches to the PRIMARY cache, i.e. replace/add/remove items in the PRIMARY cache one by one in non-atomic way (Fig.2.4). \n Finally, the commit process switches the global flag back and requests are routed to the PRIMARY cache (Fig.2.5). \n Old data can be copied to another cache during step 4 in order to provide rollback possibility. In-progress transactions can be handled as for full refreshes. \n \n  Fig.2 Partial Cache Switch \n Thus, from the\u00a0examples above, we can conclude that attachment of read transactions to the snapshot of data and avoiding of interference from the commitment of the update transactions is one of the main sources of complexity. This is obviously a case for write-intensive\u00a0environments. In the next section we consider very powerful technique that helps to solve gracefully\u00a0this problem. \n MVCC Transactions, Repeatable Reads Isolation \n Isolation between transactions can be\u00a0achieved using versioning of separate items in the Key-Value space.\u00a0There are different ways to implement this technique, here we discuss an approach that is very similar to how PostgreSQL handles transactions. \n As it was said in the previous section, each transaction should be attached to a particular data snapshot which is a set of items in the cache. At the same time, each item has its own life span \u2013 from the moment it was added to the cache till the moment it was removed or updated, i.e. replace by a new version. So, isolation can be achieved via marking each item two time stamps, each transaction by its start time, and checking that transaction sees only items that were alive at the moment the transaction began. In practice of course global monotonically\u00a0increasing counters are usually used instead of time stamps. More formally: \n \n When a new transaction is started, it is associated with:\n \n Its Transaction ID or XID which is unique for each transaction and grows monotonically. \n A list of XIDs of all transactions that are currently in-progress. \n \n \n Each item in the cache is marked with two values, xmin and xmax. \u00a0Values are assigned as follows:\n \n When item is created by some transaction,\u00a0 xmin is set to XID of this transaction, \u00a0xmax is empty. \n When item is removed by some transaction, xmin is not changed, xmax is set to XID. The item is not actually removed from the cache, it is merely marked as deleted. \n When item is updated by some transaction, old version is preserved in the cache, its\u00a0 xmax is set to XID; new version is inserted with xmin =XID and empty xmax . In other words this is equivalent to remove + insert. \n \n \n Item is visible for transaction with XID = txid\u00a0 if the following two statements are true:\n \n xmin is a XID of the committed transaction and\u00a0 xmin is less or equal than txid \n xmax is blank, or XID of the non-committed (aborted or in-progress)\u00a0transaction, or\u00a0greater than txid \n \n \n Each xmin and xmax can store two bit flags that indicate wherever transaction aborted or committed in order to perform checks described in the previous point. \n \n This\u00a0logic is illustrated\u00a0in the following graphic: \n  Fig.3 PostgeSQL-like MVCC \n The disadvantage of this approach is a quite complex procedure of the obsolete versions removal.\u00a0Because different transactions will have visibility to a different set items and versions, it is not straightforward to determine a moment when particular version becomes invisible and may be eliminated. There at least two different techniques to do this, the first one is used in PostgreSQL, the second one in the Oracle Database: \n \n \u00a0All versions are stored in the same key-value space and there is no fixed limit on how many versions may be maintained. Old versions are collected by a background\u00a0process that is executed\u00a0continuously, by schedule, or triggered by reads or writes. \n Primary key-value space stores only the last versions, the previous versions are stored in another fixed size storage. The last versions have references to the previous versions and particular version can be traced back by transactions that require them. Because size of the storage is limited, oldest versions may be eliminated to free space for the \u201cnew old\u201d items. If some transaction is not able to find a required version it fails."], "link": "http://highlyscalable.wordpress.com/2012/01/07/mvcc-transactions-key-value/", "bloglinks": {}, "links": {"http://highlyscalable.wordpress.com/": 3, "http://feeds.wordpress.com/": 1}, "blogtitle": "Highly Scalable Blog"}, {"content": ["In web applications, it is a very common task \u00a0to sort some set of items according to the user-selected\u00a0criteria and \u00a0return only the first or N-th page of the sorted result. The page size can be much less than the total number of items, hence it is typically not\u00a0reasonable to sort the entire set and crop a one page; it\u2019s much more efficient to extract this page on the fly, running through the initial unsorted set. Sorting with priority queue is well know solution for this problem. In this post I present analysis of priority queue sorting for page-oriented use cases. \n Let us assume that sorting unit iterates over the unsorted list of items and maintains a sorted page (a queue) of the selected maximums. If the unit meets an item which is greater than the minimal element in the queue then it removes the minimal element from the page and inserts the current item into the queue. This logic is illustrated below: \n  \n Let we have\u00a0 N \u00a0randomly permuted items and we need to extract\u00a0 p \u00a0maximums. If we have set of\u00a0 k \u00a0items\u00a0 x 1 , ...,x k \u00a0and\u00a0 x k \u00a0is p-th largest element in this list then we say that\u00a0 rank(x k )=p . In other words rank(x k ) \u00a0is a position of\u00a0 x k\u00a0 in a list after sorting. Let us consider the following case: we scanned\u00a0 k-1 > p \u00a0items and the page contains \u00a0 p\u00a0 largest items among\u00a0 x 1 , ..., x k-1 . \u00a0In this case, next item\u00a0 x k \u00a0will be inserted in the page with the following probability (assuming flat distribution of all possible permutations): \n To see this note that\u00a0we have\u00a0 k! \u00a0possible permutations of\u00a0k\u00a0items and k-th item will be inserted if it is a largest element ( (k-1)! \u00a0 permutations) or second largest element ( (k-1)! \u00a0permutations), \u2026 or p-th largest element ( (k-1)! \u00a0permutations) .\u00a0Now we can estimate an average number of item comparisons for page extraction. This number consist of two components. The first one is an initial effort of scanning of first p of N elements \u2013 the list that represents the extracted page is initially empty, so we simply add first p items into it and sort them \u2013 this requires about p*log p \u00a0comparisons. The second component is an effort of scanning of the remaining items. It requires\u00a0 N-p-1 comparisons with the page minimal element (\u201c 1 + Pr{.. \u201d term in the formula below) and insertion into the page with complexity log p , but only in the cases when item has a higher rank than a minimal item in the currently extracted page. Combining all this into one formula and reducing it we obtain that total complexity is about\u00a0 p log p ln N + N :  \n  \n At the same time direct sorting of all items is estimated in\u00a0 N*logN \u00a0comparisons. For example, if we select first 48 items (second page of size 24) from 5000 items then simple sorting will take\u00a0 ~61500 \u00a0comparisons and page-oriented sorting will take\u00a0 ~6470 \u00a0comparisons. \n The plots below depict complexity (number of comparisons) of page-aware sorting and sorting of entire item set for different values of p and N ."], "link": "http://highlyscalable.wordpress.com/2012/01/02/sorting-with-pagination/", "bloglinks": {}, "links": {"http://highlyscalable.wordpress.com/": 4, "http://feeds.wordpress.com/": 1}, "blogtitle": "Highly Scalable Blog"}]