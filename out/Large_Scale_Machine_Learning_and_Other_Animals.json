[{"blogurl": "http://bickson.blogspot.com\n", "blogroll": [], "title": "Large Scale Machine Learning and Other Animals"}, {"content": ["A super cool micro computer running Linux for 25$ : \nhttp://www.raspberrypi.org/ \n\n\n \nFor all the students looking for cool projects, why not implement some ML methods on top? \n \nToday I met Tal Anker , who told me that Raspberry pi is for amateurs and the serious \nguys use Stratus , which comes assembled with some additional features."], "link": "http://bickson.blogspot.com/feeds/640297823569740162/comments/default", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://www.ionicsplug.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Recently an interesting paper named distributed convex BP was published in CVPR 2011. I asked Alex Schwing (ETH Zurich) to give a brief overview of convex BP and his paper. \n \n \n \n \n \n \n \n What is convex belief propagation? \n \n The convex variant of belief propagation, named \"convex belief propagation,\" is a message passing algorithm which naturally extends a variety of approaches (loopy belief propagation, tree-reweighted message passing) by respectively choosing counting numbers. For the original loopy belief propagation algorithm those counting numbers are chosen such that the resulting distribution is exact for a graphical model corresponding to a tree. For loopy graphical models the true distribution is commonly approximated by using counting numbers that correspond to a tree-structured graphical model. It turns out that the resulting cost function which is optimized by the message passing algorithm is non-convex, and loopy belief propagation is not guaranteed to converge. Instead of approximating the true distribution with counting numbers originating from a tree-like model, \"convex belief propagation\" employs counting numbers that ensure a convex cost function. Hence there is a global optimum and the respective message passing algorithm is guaranteed to converge. \n What are counting numbers? \n \n Counting numbers specify how often entropy terms for different subsets of variables are counted in the entropy approximation used in the energy functional. \n If I need to read one paper about convex BP what should I read? \n \n One of the first papers introducing convex belief propagation is T. Hazan and A. Shashua, \" Convergent message-passing algorithms for inference over general graphs with convex free energy ,\" UAI 2008. It is also explained within the related work section of the CVPR 2011 work on distributed convex belief propagation . \n \n What are the applications for convex BP? \n \n Convex belief propagation is applicable to any problem that can be addressed by loopy belief propagation. Its computational and memory complexity are identical to the loopy variant and recent applications that benefit from (distributed) convex belief propagation are: \n \n \n \n M. Salzmann and R. Urtasun; \"Beyond Feature Points: Structured Prediction for Monocular Non-rigid 3D Reconstruction\" ECCV 2012 \n \n \n \n \n \n K. Yamaguchi, T. Hazan, D. McAllester and R. Urtasun; \"Continuous Markov Random Fields for Robust Stereo Estimation\"; ECCV 2012 \n \n \n \n \n J. Yao, S. Fidler and R. Urtasun; \"Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation\"; CVPR 2012 \n \n \n \n \n A.G. Schwing, T. Hazan, M. Pollefeys and R. Urtasun; \"Efficient Structured Prediction for 3D Indoor Scene Understanding\"; CVPR 2012 \n \n \n \n \n \n \n \n Efficient implementation for larger models \n \n Distributed convex belief propagation (dcBP) is an inference algorithm that allows the user to partition the task (graph) and solve the sub-problems independently and in parallel on distributed memory compute environments. Efficiency is improved by partitioning the computation onto multiple cluster nodes/machines connected, e.g., by a standard LAN. Most importantly, convergence guarantees of convex belief propagation are maintained by occasionally exchanging information between different machines and efficiency is improved due to additional computation power. Within the paper we compare the distributed computation to a non-distributed version and we observe an improvement almost equivalent to the number of additional machines (as expected). We also show that it is important to reduce communication overhead between machines, even when working on a very common 4-connected grid-like graph. We also compare the algorithm to loopy belief propagation and other message passing implementations."], "link": "http://bickson.blogspot.com/feeds/3306949044816845648/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://alexander-schwing.de/": 2, "http://feedads.doubleclick.net/": 2, "http://arxiv.org/": 1, "http://www.alexander-schwing.de/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Note: the MPI section of this toturial is based on this excellent tutorial . \n \n \nPreliminaries: \n \n \n Mpi should be installed \n \n \n \n \nStep 0: Install GraphLab on one of your cluster nodes. \n \nUsing the instructions here on your master node (one of your cluster machines) \n \n \n \nStep 1: start MPI \na) Create a file called .mpd.conf in your home directory (only once) \nThis file should contain a secret password using the format: \n secretword=some_password_as_you_wish \n \nb) Verify that MPI is working by running the daemon (only once) \n $ mpd &  # run the MPI daemon \n $ mpdtrace # lists all active MPI nodes \n $ mpdallexit # kill MPI \n \nc) Spawn the cluster . Create a file named machines with the list of machines you like to deploy. \nRun: \n $ mpd -f machines -n XX # where XX is the number of MPI nodes you like to spawn. \n \nd) Copy GraphLab files to all machines. \nOn the node you installed GraphLab on, run the following commands to copy GraphLab files to the rest of the machines: \n \nd1) Verify you have the machines files from section 1c) in your root folder. \n \n d2) Copy the GraphLab files \n cd ~/graphlabapi/release/toolkits; ~/graphlabapi/scripts/mpirsync \n  cd ~/graphlabapi/deps/local; ~/graphlabapi/scripts/mpirsync  \n \n Step 2: Run GraphLab ALS \n \n This step runs ALS (alternating least squares) in a cluster using small netflix susbset. \n It first downloads the data from the web: http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.train and http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.validate , and runs 5 alternating least squares iterations. After the run is completed, you can login into any of the nodes and view the output files in the folder ~/graphlabapi/release/toolkits/collaborative_filtering/ \n \n The algorithm operation is explained in detail  here . \n \n \n cd /some/ns/folder/ \n mkdir smallnetflix \n cd smallnetflix/ \n wget http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.train \n wget http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.validate \n \nNow run GraphLab: \n \n mpiexec -n 2 /path/to/als --graph /some/ns/folder/ --max_iter=3 --ncpus=1 \n \n \nWhere -n is the number of MPI nodes, and --ncpus is the number of deployed cores on each MPI node. \n \nNote: this section assumes you have a network storage (ns) folder where the input can be stored. \nAlternatively, you can split the input into several disjoint files, and store the subsets on the cluster machines. \n \nNote: Don't forget to change /path/to/als and /some/ns/folder to your actual folder path!"], "link": "http://bickson.blogspot.com/feeds/4740472937989521643/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 2, "http://docs.graphlab.org/": 1, "http://graphlab.org/": 1, "http://feedads.doubleclick.net/": 2, "http://source.ac.uk/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I got the following instructions from my collaborator Jay (Haijie Gu) who spent some time learning Spark cluster deployment and adapted those useful scripts to be used in GraphLab.\n \nThis tutorial will help you spawn a GraphLab distributed cluster, run alternating least squares task, collect the results and shutdown the cluster. \n \nThis tutorial is very new beta release. Please contact me if you are brave enough to try it out.. \n\n \n \n \n \nStep 0: Requirements \n1) You should have Amazon EC2 account eligible to run on us-east-1a zone. \n2) Find out using the Amazon AWS console your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY \n3) Download your private/public key pair (called here graphlab.pem) \n4) Download Graphlab 2.1 using the instructions here .\n \n \n \n \nStep 1: Environment setup \nEdit your .bashrc or .bash_profile (remember to source it after editing) \n export AWS_ACCESS_KEY_ID=[ Your access key ] \n export AWS_SECRET_ACCESS_KEY=[ Your access key secret ] \n\n \n \n \n \nStep 2: Start the cluster \n $ cd ~/graphlabapi/scripts/ec2 \n\n $ ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey -z us-east-1a -s 1 launch launchtest \n (In the above command, we created a 2-node cluster in us-east-1a zone. -s is the number of slaves, and launch is the action, and launchtest is the name of the cluster)\n\nonly once when starting the image.\n \n \n \n \nStep 2.2: Start Hadoop (mandatory when using HDFS) \nThis operation is needed when you want to work with HDFS \n $ ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey start-hadoop launchtest \n \n \n \n \nStep 3: Run alternating least squares demo \nThis step runs ALS (alternating least squares) in a cluster using small netflix susbset. \nIt first downloads the data from the web: http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.train and http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.validate , copy it into HDFS, and run 5 alternating least squares iterations:\n \n \n ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey als_demo launchtest \n After the run is completed, you can login into the master node and view the output files in the folder ~/graphlabapi/release/toolkits/collaborative_filtering/\nThe algorithm and exact format is explained here .\n \n \n \n \nStep 4: shutdown the cluster \n $ ./gl-ec2 -i ~/.ssh/graphlab.pem -k grpahlabkey destroy launchtest\n\n \n \n \n \nAdvanced functionality: \n \nStep 5: Login into the master node \n $ ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey login launchtest \n\n\n \n \nStep 6: Manual building of GraphLab code \nOn the master: \n \n cd ~/graphlabapi/release/toolkits\n\nhg pull; hg update; \n make \n /* Sync the binary folder to slaves */ \n cd ~/graphlabapi/release/toolkits; ~/graphlabapi/scripts/mpirsync \n \n /* Sync the local dependency folder to slaves */\ncd ~/graphlabapi/deps/local; ~/graphlabapi/scripts/mpirsync\n\n \n \nManual run of ALS demo \n \n Login into the master node \n cd graphlabapi/release/toolkits/collaborative_filtering/ \n mkdir smallnetflix \n cd smallnetflix/ \n wget http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.train \n wget http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm.validate \n cd .. \n hadoop fs -copyFromLocal smallnetflix/ / \n mpiexec -n 2 ./als --matrix hdfs://`hostname`/smallnetflix --max_iter=3 --ncpus=1\n\n \n \nTroubleshooting \nKnown Errors: \n Starting the dfs:\nnamenode running as process 1302. Stop it first. \n localhost: datanode running as process 1435. Stop it first. \n ip-10-4-51-142: secondarynamenode running as process 1568. Stop it first. \n Starting map reduce:\njobtracker running as process 1647. Stop it first. \n localhost: tasktracker running as process 1774. Stop it first. \n \n Solution:\nKill hadoop and restart it again using the commands:\n\n \n \n ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey stop-hadoop launchtest \n \n \n ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey start-hadoop launchtest \n \n \n \n \nError: \n 12/10/20 13:37:18 INFO ipc.Client: Retrying connect to server: domU-12-31-39-16-86-CC/10.96.133.54:8020. Already tried 0 time(s). \n \nSolution: run jps to verify that one of the Hadoop nodes failed. \n \n ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey jps launchtest \n > jps \n 1669 TaskTracker \n 2087 Jps \n 1464 SecondaryNameNode \n 1329 DataNode \n 1542 JobTracker \nIn the above example, NameNode is missing (not running). Stop hadoop execution using stop-hadoop command line. \n \nError: \n mpiexec was unable to launch the specified application as it could not access \n or execute an executable: \n \n Executable: /home/ubuntu/graphlabapi/release/toolkits/graph_analytics/pagerank \n Node: domU-12-31-39-0E-C8-D2 \n \n while attempting to start process rank 0. \n \nSolution: \nExecutable is missing. Run update: \n \n \nError: \n \n # \n # A fatal error has been detected by the Java Runtime Environment: \n # \n # SIGILL (0x4) at pc=0x000000000056c0be, pid=1638, tid=140316305243104 \n # \n # JRE version: 6.0_26-b03 \n # Java VM: Java HotSpot(TM) 64-Bit Server VM (20.1-b02 mixed mode linux-amd64 compressed oops) \n # Problematic frame: \n # C [als+0x16c0be] graphlab::distributed_ingress_base<vertex_data, edge_data>::finalize()+0xe0e \n # \n # An error report file with more information is saved as: \n # /home/ubuntu/graphlabapi/release/toolkits/collaborative_filtering/hs_err_pid1638.log \n # \n # If you would like to submit a bug report, please visit: \n # http://java.sun.com/webapps/bugreport/crash.jsp \n # The crash happened outside the Java Virtual Machine in native code. \n # See problematic frame for where to report the bug. \n # \n \n \n \n Solution: \n 1) Update the code: \n \n \n $./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey update launchtest \n \n2) If the problem still persists submit a bug report to GraphLab users list. \n \n \n \n \nError: \n \n \n bickson@thrust:~/graphlab2.1/graphlabapi/scripts/ec2$ ./gl-ec2 -i ~/.ssh/graphlab.pem -k graphlabkey login launchtest \n \n ERROR: The environment variable AWS_ACCESS_KEY_ID must be set \n \n \n \n \nSolution: \n \nNeed to set environment variables, as explained in step 1."], "link": "http://bickson.blogspot.com/feeds/6177279821292330804/comments/default", "bloglinks": {}, "links": {"http://www.linkedin.com/": 1, "http://docs.graphlab.org/": 1, "https://github.com/": 1, "http://graphlab.org/": 1, "http://feedads.doubleclick.net/": 2, "http://www.cmu.edu/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Yesterday I stumbled upon this company: quid.com \n \n \n \nLike quantified.com it is a company initiated by a few physicists who like to analyze data and to find some structure in the data. When looking at their website I found this\ninteresting talk: \"Sean Gourley on the mathematics of war\" from TED:\n\nAt first it looked like a very exciting intuition. But in a second thought the power low structure is not that surprising. For example, recall all the times you ate in a restaurant in the last 5 years. Typically, you eat in a small group, maybe yourself or with your wife. Occasionally you take out a few friends. Quite rarely you go out with 15 people out for dinner, maybe for your grandfather birthday or something. So it is quite clear that most times you go in small groups and a few times in a large group. With this intuition will you be able to get funding from the pentagon? Probably not.."], "link": "http://bickson.blogspot.com/feeds/7211190037481975073/comments/default", "bloglinks": {}, "links": {"http://2.blogspot.com/": 1, "http://quid.com/": 1, "http://quantified.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Yesterday I learned from my collaborator Joey Gonzalez about InfiniteGraph software from Objectivity. It is both a distributed graph database and an analytics software on top of graphs. \n \nI quickly looked through their website and found this hilarious video: \n \n \nIt seems InifiniteGraph has a great popularity in the defense/security sectors . \nWhile the basic barebones version is free, I hear that the full features are rather expensive. \n \nHere is a video with a more comprehensive technical description of the architecture:"], "link": "http://bickson.blogspot.com/feeds/7430629046366253926/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1, "http://objectivity.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I got this from my collaborator Erik Eurell . I love the Nordic countries \nthis could be a great experience for anyone who wants to relocate for a year or two: \n \n Call for Applications: Several Postdoc / Research Associate positions in \n the Finnish Center of Excellence in Computational Inference COIN \n \n Call deadline 1 November 2012, 3:00 pm EET \n \n Keywords: Machine learning, computational statistics, Bayesian statistics, \n information-theoretic learning, satis\ufb01ability checking, proactive \n interfaces, computational biology \n \n The Finnish Center of Excellence in Computational Inference Research \n (COIN, Aalto University, University of Helsinki, Helsinki Institute for \n Information Technology HIIT) announces several postdoc or research \n associate positions in Computational Inference. The objective of COIN, \n started on 1 January 2012, is to push the boundaries of inference in the \n data-rich world of today and tomorrow. The COIN consortium includes the \n creators of world standards and software such as Independent Component \n Analysis. \n \n Six postdocs / research associates were hired at the start of COIN, and we \n now seek several additional new recruitments to complete our team. \n Successful candidates will work on fundamental questions of inference and \n in applications in Intelligent Information Access, Computational Molecular \n Biology and Medicine, Computational History, Computational Climate, \n Computational Neuroscience and other directions yet to be determined. \n Applicants with suitable background in machine learning, mathematics, \n statistics, computational logic, combinatorial optimization or statistical \n physics are encouraged to apply. \n \n COIN is a joint research center of the Aalto University School of Science, \n Department of Information and Computer Science, the University of \n Helsinki, Department of Computer Science and Department of Mathematics and \n Statistics, and Helsinki Institute for Information Technology HIIT. COIN \n studies the following research topics: \n \n C1: Learning models from massive data \n \n C2: Learning from multiple data sources \n \n C3: Statistical inference in highly structured stochastic models \n \n C4: Extremely fast inference \n \n F1: Intelligent information access \n \n F2: Computational molecular biology and medicine \n \n In the four core methodological challenges C1\u2013C4 we develop methods for \n transforming the data produced in the current data revolution into useful \n information. In COIN F1 Intelligent Information Access \ufb02agship the \n challenge is to make use of massive interrelated information sources, \n whether in everyday life or in science, and select what information to \n present to the user. The inference needs to be done on-line, learning \n relevance from the user\u2019s responses. In COIN F2 Computational Molecular \n Biology and Medicine \ufb02agship we develop methods for maximally utilizing \n the novel measurement databases and structured stochastic models in making \n data-driven biology cumulative. We especially welcome applications from \n researchers who have solid knowledge and research background in one or \n more of the four core methodological challenges C1\u2013C4 and experience and \n vision on applying those methodologies on either one of the two \ufb02agship \n applications F1 and F2 or the other areas referred to above or of interest \n to COIN. \n \n The duration of the fixed term contract period is three years at most, \n starting on 1 December 2012 at the earliest. In addition to personal \n research work, the postdoc or research associate is expected to \n participate in specialist teaching and student supervision in the \n indicated areas of research, in collaboration with other COIN members. \n Good command of English is a necessary prerequisite. The salary level for \n beginning postdoctoral researchers in COIN is typically between 3200 and \n 3600 \u20ac/month, depending on experience and quali\ufb01cations. The contract \n includes occupational health benefits and Finland has a comprehensive \n social security system. \n \n Application procedure \n \n The positions will remain open until \ufb01lled. Deadline for the call is 1 \n November 2012, 3 PM EET. Please send an email to registry@aalto.fi , \n indicating that you are applying for a postdoc position at COIN, \n containing in one pdf file the following information: \n \n \u2022 Your contact information \n \n \u2022 Names and contact information of two senior academics available for \n reference \n \n \u2022 A research statement of at most \ufb01ve pages outlining planned work and \n connections to the four core methodological challenges C1\u2013C4 and the two \n \ufb02agship applications F1\u2013F2. \n \n \u2022 Curriculum vit\u00e6. \n \n \u2022 List of publications, with pointers to openly available online versions \n of at most three of the most relevant publications. \n \n \u2022 Degree certi\ufb01cate of the PhD degree, including a transcript of the \n doctoral studies. In case the doctoral degree is still pending, an \n up-to-date study transcript and a plan for completion of the degree should \n be provided. \n \n In addition to the application sent to Aalto registry, please fill in a \n separate application form at \n https://elomake.helsinki.fi/lomakkeet/37840/lomake.html . \n \n Candidates should also arrange for reference letters from the two \n indicated senior academics to be sent separately to HR Coordinator, Mr. \n Stefan Ehrstedt, e-mail firstname.lastname@aalto.fi by the deadline of 15 \n November 2012. Shortlisted candidates may be invited for an interview. In \n the review process, particular emphasis is put on the quality of the \n candidate\u2019s previous research and international experience, together with \n the substance, innovativeness, and feasibility of the research plan, and \n its relevance to COIN\u2019s mission. The candidate must have completed his or \n her PhD degree before the start of the contract period, and e\ufb03cient and \n successful completion of studies is considered an additional merit. \n \n Further information \n \n For further information, please visit http://research.ics.aalto.fi/coin \n and contact: \n \n \u2022 HR Coordinator, Mr. Stefan Ehrstedt, e-mail firstname.lastname@aalto.fi \n (application process) \n \n \u2022 Director of COIN, Prof. Erkki Oja, e-mail firstname.lastname@aalto.fi \n \n \u2022 Deputy Director of COIN, Prof. Samuel Kaski, e-mail \n firstname.lastname@aalto.fi \n \n \u2022 Prof. Erik Aurell, e-mail firstname.lastname@aalto.fi \n \n \u2022 Prof. Jukka Corander, e-mail firstname.lastname@helsinki.fi \n \n \u2022 Dr. Jorma Laaksonen, e-mail firstname.lastname@aalto.fi \n \n \u2022 Prof. Petri Myllym\u00e4ki, e-mail firstname.lastname@cs.helsinki.fi \n \n \u2022 Prof. Ilkka Niemel\u00e4, e-mail firstname.lastname@aalto.fi \n \n About Aalto University \n \n Aalto University is a new university created in 2010 from the merger of \n the Helsinki University of Technology TKK, the Helsinki School of \n Economics, and the University of Art and Design Helsinki. The University\u2019s \n cornerstones are its strengths in education and research, with 20,000 \n basic degree and graduate students, and a staff of 4,500 of whom 350 are \n professors. For further information, see http://www.aalto.fi/en . \n \n About University of Helsinki \n \n The University of Helsinki is the most versatile institution of science, \n education, and intellectual renewal in Finland, a pioneering builder of \n the future. It operates to actively promote the wellbeing of humanity and \n a just society. The University of Helsinki is one of the best \n multidisciplinary research universities in the world. The high-quality \n research carried out by the university creates new knowledge for educating \n diverse specialists in various \ufb01elds, and for utilisation in social \n decision-making and the business sector. For further information, see \n http://www.helsinki.fi/university . \n \n \n Additional postdoc position I got from Florent Krzakala , my former collaborator in the Evergrow project: \n  \n \n \n I would like to invite applications for two postdoctoral positions funded by the European Research Council Starting Grant program, in the context of the project SPARCS (Statistical Physics Approach to Reconstruction in Compressed Sensing) in my group in ESPCI in Paris. The appointments are intended to start in the fall of 2013 (or sooner) and will be for 2 years with a possibility of extension on 3 years. \n The candidates can come from different areas (Statistical Physics, Signal Processing, Applied Mathematics, Error correcting codes, Information Theory, Inference and Machine learning) and are expected to bring their expertise. Successful candidates will thus conduct a vigorous research program within the scope of the project, and are expected to show independence and team working attitude at the same time. \n For more information, please visit the post-doc announcement page: http://www.pct.espci.fr/~florent/postdoc.html"], "link": "http://bickson.blogspot.com/feeds/8453154175327562576/comments/default", "bloglinks": {}, "links": {"http://www.kth.se/": 1, "http://www.helsinki.fi/": 1, "https://elomake.helsinki.fi/": 1, "http://www.aalto.fi/": 1, "http://research.aalto.fi/": 1, "http://www.espci.fr/": 2, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["The all connected Prof. Joe Hellerstein from Berkeley does not rest for a minute. He has now a new startup company called Trifacta . What they do is very secret. But as hint you can look at my related older blog post . \n \nThis week it was announced Trifacta raised 4.3M $: \n http://gigaom.com/data/how- trifacta -wants-to-teach-humans-and-data-to-work-together/ \n http://allthingsd.com/20121004/ trifacta -aims-to-make-big-data-useful-lands-4-3-million-from-accel-partners/ \n http://venturebeat.com/2012/10/04/big-data-startup- trifacta -comes-out-of-stealth-with-4-3m-in-accel-funding/ \n http://techcrunch.com/2012/10/04/accel-partners-big-data-fund-backs- trifacta -with-4-3-million-investment/ \n \nTrifacta further has one for the most impressive management and advisory boards out there. I am looking forward to hearing more about the company soon. By the way, Trifacta is hiring. If you apply tell Joe I sent you.. :-)"], "link": "http://bickson.blogspot.com/feeds/3863909719785864868/comments/default", "bloglinks": {}, "links": {"http://bickson.co.il/": 1, "http://techcrunch.com/": 1, "http://gigaom.com/": 1, "http://trifacta.com/": 2, "http://venturebeat.com/": 1, "http://allthingsd.com/": 1, "http://2.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["How big is Facebook data? I got this update from my collaborator Aapo Kyrola: \n \n \nThis morning, there are more than one billion people using Facebook actively each month.... \n \nFacebook has also shared a number of key metrics with users along with the announcement, including 1.13 trillion Likes since its 2009 launch (note that this is actually probably higher, since the official press document contained a note accidentally left in from an editor about rolling back the number because of info shared previously with Businessweek), 140.3 billion friend connections, 219 billion photos uploaded, 17 billion location-tagged posts and 62.6 million songs played some 22 billion times.\n\n http://techcrunch.com/2012/10/04/facebook-tops-1-billion-monthly-users-ceo-mark-zuckerberg-shares-a-personal-note/ \n \nI got the following 10 patterns for research in Machine learning from Tianqi Chen . The list is by John Langford in his blog : \n \n Separate code from data. \n Separate input data, working data and output data. \n Save everything to disk frequently. \n Separate options from parameters. \n Do not use global variables. \n Record the options used to generate each run of the algorithm. \n Make it easy to sweep options. \n Make it easy to execute only portions of the code. \n Use checkpointing. \n Write demos and tests. \n \n Following John's good practice, Tianqi used some of those ideas for competing in KDD CUP. And here is a summary of his experience. Specifically, Tianqi uses Makefiles for managing multiple and complex execution scripts."], "link": "http://bickson.blogspot.com/feeds/9211736595679933802/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://hunch.net/": 1, "http://svdfeature.apexlab.org/": 1, "http://apex.edu.cn/": 1, "http://techcrunch.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I got the following interesting email from Denis Parra, a PhD student @ University of Pittsburgh : \n \n \n \n \n Danny, \nFollowing the post on evaluation metrics in your blog, we would be glad to help you testing new evaluation metrics for GraphChi. Not long ago (this year, actually), with Sherry we wrote a book Chapter on recommender systems focusing on sources of knowledge and evaluation metrics. In section 7.4 we explain some of these evaluation metrics. \n \n \nFor instance, we describe a metric that has become to be popular for evaluating recommendations based on implicit feedback called MPR (Mean Percentile Ranking) that some authors call Percentile Ranking. This is the method used by Hu et al. in \"Collaborative filtering for implicit feedback datasets\" (2008) and by Quercia et al. in \"Recommending social events from mobile phone location data\" (2010) \n \nCheers, \n Denis \n \n PS: In case you want to cite the book chapter, you can use \n @incollection{Denis2012, \n chapter = {7}, \n title = {Recommender Systems: Sources of Knowledge and Evaluation Metrics}, \n editor = { J.D. Vel{\\' a}squez et al. (Eds.)}, \n author = {Parra, D. and Sahebi, S. }, \n booktitle = {Advanced Techniques in Web Intelligence-2: Web User Browsing Behaviour and Preference Analysis}, \n publisher = {Springer-Verlag}, \n address = {Berlin Heidelberg}, \n pages = {149\u2013-175}, \n year = {2013} \n } \n \nI think this book chapter is going to become highly useful overview for anyone who is working on recommender system. As a \"teaser\" until the book comes out, I asked Denis to shortly summarize the different metrics by giving a reference to each one. The book itself will include much detailed explanation of each metrics and its usage. \n \nDenis was very kind to provide me the following list to share in my blog: \n \n Though many of these metrics are described in the seminal paper \"Evaluating collaborative filtering recommender systems\" by Herlocker et al.,\nthis is a subset of an updated list of metrics used to evaluate Recommender Systems in the latest years:\n \n For rating \n \n MAE (Mean Absolute Error) \n Breese, J.S., Heckerman, D., Kadie, C.: Empirical analysis of predictive algorithms for\ncollaborative filtering. In: 14th Conference on Uncertainty in Artificial Intelligence, pp. 43\u201352 (1998) \n MSE (Mean Squared Error) \n Shardanand, U., Maes, P.: Social information filtering: algorithms for automating word of mouth. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI 1995, pp. 210\u2013217. ACM Press/Addison-Wesley Publishing Co., New York (1995) \n RMSE (Root mean Squared Error) \n Bennett, J., Lanning, S., Netflix, N.: The netflix prize. In: KDD Cup and Workshop in Conjunction with KDD (2007) \n Evaluating lists of recommendation (based on relevancy levels) \n \n Precision@n \n Le, Q. V. & Smola, A. J. (2007), 'Direct Optimization of Ranking Measures', CoRR abs/0704.3359 \n Recall \n Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. 2010. Performance of recommender algorithms on top-n recommendation tasks. In Proceedings of the fourth ACM conference on Recommender systems (RecSys '10) \n MAP: Mean Average Precision \n Manning, C.D., Raghavan, P., Schtze, H.: Introduction to Information Retrieval. Cambridge University Press, New York (2008) \n nDCG: normalized Discounted Cummulative Gain \n J\u00a8arvelin, K., Kek\u00a8al\u00a8ainen, J.: Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst. 20, 422\u2013446 (2002) \n Diversity \n \n Intra-list Similarity \n Ziegler, C.-N., McNee, S.M., Konstan, J.A., Lausen, G.: Improving recommendation lists through topic diversification. In: Proceedings of the 14th International Conference on World Wide Web, WWW 2005, pp. 22\u201332. ACM, New York (2005) \n Lathia's Diversity \n Lathia, N., Hailes, S., Capra, L., Amatriain, X.: Temporal diversity in recommender systems. In: Proceeding of the 33rd International ACMSIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010, pp. 210\u2013217. ACM, New York\n(2010)\n \n Implicit Feedback \n \n Mean Percentage Ranking \n Hu, Y., Koren, Y., Volinsky, C.: Collaborative filtering for implicit feedback datasets. In: Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, pp. 263\u2013272. IEEE Computer Society, Washington, DC (2008)\n \n User-Centric Evaluation Frameworks \n Knijnenburg, B.P., Willemsen, M.C., Kobsa, A.: A pragmatic procedure to support the user-centric evaluation of recommender systems. In: Proceedings of the Fifth ACM Conference on Recommender Systems, RecSys 2011, pp. 321\u2013324. ACM, New York (2011)\n\nPu, P., Chen, L., Hu, R.: A user-centric evaluation framework for recommender systems.\nIn: Proceedings of the Fifth ACM Conference on Recommender Systems, RecSys 2011, pp. 157\u2013164. ACM, New York (2011)"], "link": "http://bickson.blogspot.com/feeds/7101351007589132070/comments/default", "bloglinks": {}, "links": {"http://www.pitt.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I got the following from Prof. Scott Kirkpatrick. \n \n \n \nWrite a 250-words research project and get access within a week to the largest \never released mobile phone datasets: datasets based on 2.5 billion records, \ncalls and text messages exchanged between 5 million anonymous users over 5 \nmonths. \nParticipation rules: http://www.d4d.orange.com/ \n \nDescription of the datasets: http://arxiv.org/abs/1210.0137 \n \nThe \"Terms and Conditions\" by Orange allows the publication of results \nobtained from the datasets even if they do not directly relate to the \nchallenge. \n \nCash prizes for winning participants and an invitation to present the results \nat the NetMob conference be held in May 2-3, 2013 at the Medialab at MIT \n( www.netmob.org ). \nDeadline: October 31, 2012 \n \n \n \n \nSome more information about this dataset: \n \nThe data collection took place in Cote d\u2019Ivoire over a five-month period, from December 2011 to April 2012. The original dataset contains 2.5 billion records, calls and text messages exchanged between 5 million users. The customer identifier was anonymized by Orange Cote d\u2019Ivoire. All subsequent data processing was completed by Orange Labs in Paris. \n \nWe will release four datasets in order to offer a large spectrum of possible analyses: \n \n Aggregate communication between cell towers; \n Mobility traces: fine resolution dataset; \n Mobility traces: coarse resolution dataset; \n Communication sub-graphs."], "link": "http://bickson.blogspot.com/feeds/6805247757378562404/comments/default", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://www.netmob.org/": 1, "http://feedads.doubleclick.net/": 2, "http://4.blogspot.com/": 1, "http://www.orange.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Item based collaborative filtering is one of the most popular collaborative filtering methods used in more than 70% of the companies I am talking to. Following my mega collaborator Justin Yan 's advice, I have started to implement some item based similarity methods in GraphChi. \n \nItem based methods compare all pairs of items together, for computing similarity metric between each pair of items. This task becomes very quickly computation intensive. For example, Netflix data has around 17K movies. If we want to compute all pairs of movies to find the most similar ones, we need to compare around 290M pairs! \n \nIf we use a symmetric similarity measure, the distance between movie A and B, is similar to the distance between movie B and A. Thus for the Netflix example we have around 145M pairs to compute. To reduce the work furthermore, we only compare movies which where watched together by at least X users, for example X=5. Otherwise, those movies are not considered similar. \n \nWhen the dataset is big, it is not possible to load it fully into memory at a single machine. That is where GraphChi comes in. My preliminary implementation of the item similarity computes similarity between all pairs without fully reading the dataset into memory. The idea is to load a chunk of the items (called pivots) into memory, and then stream over the rest of the items by comparing the pivots to the rest of the items. \n \nThe simplest distance metric I have implemented is Jackard distance . The distance of items i and j is computed as follows: \n   wi = number of users who watched movie i \n   wj = number of users who watched movie j \n   wij = number of users who watched both movie i and movie j \n   Dij = wij / ( wi + wj - wij ) \n \nIt is clear that Dij is a number between zero and one. \n Additional distance functions are found here . \n \nAs always, I am looking for brave beta testers who want to try it out ! \nFor full Netflix data, it takes around 1200 seconds to compute distances of around 130M item pairs. (Around 1000 item pairs in a second, using a 24 core machine, with only 800MB memory used). \n \nPing me if you are interested in other distance metric so I could add them as well. \n \nHow to try it out: \n1) Follow GraphChi installation instructions steps 1-4. \n2) Download smallnetflix_mm \n3) Run itemcf on smallnetflix data using: \n \n bickson@thrust:~/graphchi$ ./toolkits/collaborative_filtering/itemcf --training=smallnetflix_mm \n WARNING: itemcf.cpp(main:344): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n [training] => [smallnetflix_mm] \n INFO:  chifilenames.hpp(find_shards:204): Detected number of shards: 2 \n INFO:  chifilenames.hpp(find_shards:205): To specify a different number of shards, use command-line parameter 'nshards' \n INFO:  io.hpp(convert_matrixmarket:165): File smallnetflix_mm was already preprocessed, won't do it again. \n INFO:  io.hpp(convert_matrixmarket:171): Opened matrix size: 95526 x 3561 Global mean is: 3.5992 time bins: 0 Now creating shards. \n INFO:  graphchi_engine.hpp(graphchi_engine:146): Initializing graphchi_engine. This engine expects 4-byte edge data. \n INFO:  stripedio.hpp(io_thread_loop:662): Thread for multiplex :0 starting. \n INFO:  stripedio.hpp(io_thread_loop:662): Thread for multiplex :0 starting. \n INFO:  graphchi_engine.hpp(load_vertex_intervals:842): shard: 0 - 97296 \n INFO:  graphchi_engine.hpp(load_vertex_intervals:842): shard: 97297 - 99086 \n INFO:  stripedio.hpp(open_session:333): Opened write-session: 0(29) for smallnetflix_mm_degs.bin \n INFO:  graphchi_engine.hpp(run:560): GraphChi starting \n INFO:  graphchi_engine.hpp(run:561): Licensed under the Apache License 2.0 \n INFO:  graphchi_engine.hpp(run:562): Copyright Aapo Kyrola et al., Carnegie Mellon University (2012) \n ... \n INFO:  graphchi_engine.hpp(run:677): Start updates \n INFO:  itemcf.cpp(update:258): 20.0583)  2000000 pairs compared \n INFO:  itemcf.cpp(update:258): 27.2468)  3000000 pairs compared \n INFO:  itemcf.cpp(update:258): 34.8232)  4000000 pairs compared \n INFO:  itemcf.cpp(update:258): 43.5588)  5000000 pairs compared \n INFO:  itemcf.cpp(update:258):  50.926)  6000000 pairs compared \n \n INFO:  itemcf.cpp(main:395): Total item pairs compaed: 6330705 total written to file: 4740938 \n INFO:  itemcf.cpp(main:400): Created output files with the format: smallnetflix_mmXX.out, where XX is the output thread number \n \nNow let's examine one of the output files: \n \n bickson@thrust:~/graphchi$ head smallnetflix_mm.out0 \n 14 1 0.0084507 \n 14 2 0.0127042 \n 14 4 0.0289017 \n 14 6 0.00530206 \n 14 7 0.0102339 \n 14 9 0.0211538 \n 14 11 0.00369004 \n 14 13 0.00761421 \n 38 1 0.0221654 \n 38 2 0.0116618 \n \n \n \n \nThe format is rather simple. In each row we have: \n <item A> <item B> <similarity> \nThere are input files as the number of cores in your system. If you like to sort them, you can do it with the linux sort command: \n sort -k 1,1 -k 2,2 -g outfilename > outfilename.sorted \n \n \nCommand line arguments \n --training - input file name in sparse matrix market format \n --min_allowed_intersection=XX - filter out item pairs with less than XX users who rated them jointly. \n --quiet=1 run with less verbose traces \n FOR itemcf: --distance=XX, 0 = Jackard index, 1 = AA, 2 = RA \n FOR itemcf2: --distance=XX, 3=PEARSON, 4=COSINE, 5=CHEBYCHEV, 6=MANHATTEN, 7=TANIMOTO, 8=LOG_LIKELIHOOD \n \n \nUseful GraphChi command line arguments: \n execthreads XX - set the number of execution threads \n membudget_mb XX - fix the size of used memory to XX mb \n \n \n \n \nAdditional metrics: \n \nAdamic/Adar (AA): \n Just got an email from Timmy Wilson , our man in Ohio: \n \" There are a lot of similarity functions out there -- Adamic / Adar is another good one \" \n \n \n \n I looked it up an indeed it is a very simple cost function: \n \n Distance_item_i_j = sum over users which rated both items i j (1 / log (user_rating) ) \n \n see equation (2) in the paper: http://arxiv.org/abs/0907.1728 \"Role of Weak Ties in Link Prediction of Complex Networks\". The equation gives larger weight to users which rated both items but have relatively small number of rated items. \n \nI have also added the RA cost function, which is same like AA but without the log. \n \n \nPearson Correlation \nI got a request from additional reader \" silly snail \" to implement Pearson Correlation as well. Let m be a vector of size M (users) which hold the user rating mean. \n \n Let a be a sparse vector holding user rating for item a, and the same for item b. \n \n Pearson_correlation_i_j = ((a - mean)' * (b-mean)) / (stddev(a) * stddev(b)) \n \n And stddev(a) = sum((a-mean).^2) / (n - 1) \n where n = length(a). \n \n To run pearson correlation, use the program ./toolkits/collaborative_filtering/preason \n \n A toy example: \n \n Prepare the file \"stam\" with the content: \n %%MatrixMarket matrix coordinate real general \n 3 4 9 \n 1 1 3 \n 1 2 4 \n 2 3 1 \n 2 4 1 \n 2 1 2 \n 3 1 1 \n 3 2 2 \n 3 3 3 \n 3 4 4 \n \n Namely there are 3 users, 4 items, and 9 ratings. \n Now run pearson correlation: \n ./toolkits/collaborative_filtering/pearson --training=stam execthreads 1 \n \n Lets examine the output file: \n head stam.out0 \n 2 1 0.337405 \n ... \n \n \nNamely, the pearson correlation between items 1 and 2 is 0.337405. \nNow let's do the same computation in matlab: \nmatlab \n \n >> a=full( mmread ('stam')); % load the matrix into memory \n \n \n a = \n \n \n \n  3  4  0  0 \n \n  2  0  1  1 \n \n  1  2  3  4 \n \n \n \n >> means=mean(a') % compute the item vectors mean \n \n \n \n means = \n \n \n \n  1.7500 1.0000 2.5000 \n \n \n \n \n \n >> A=a(:,1)-means' % compute item 1 vector minus mean \n \n \n \n A = \n \n \n \n  1.2500 \n \n  1.0000 \n \n -1.5000 \n \n \n \n >> B=a(:,2)-means' % compute item 2 vector minus mean \n \n \n \n B = \n \n \n \n  2.2500 \n \n -1.0000 \n \n -0.5000 \n \n \n \n >> dist = A'*B % multiply the result \n \n \n \n ans = \n \n \n \n  2.5625 \n \n \n \n >> stddeva=sum(A.^2)/2 % compute stddev item 1 \n \n \n \n stddeva = \n \n \n \n  2.4062 \n \n \n \n >> stddevb=sum(B.^2/2) % compute stddev item 2 \n \n \n \n stddevb = \n \n \n \n  3.1562 \n \n \n \n >> dist/(stddeva*stddevb) % compute pearson correlation \n \n \n \n ans = \n \n \n \n  0.3374 \n \n \n \n \n \n Luckily we got the same result! namely pearson correlation between the two first items is 0.3374. \n \n Cosine Distance \n \nSee: http://reference.wolfram.com/mathematica/ref/CosineDistance.html \n \nManhattan Distance \nSee http://en.wikipedia.org/wiki/Taxicab_geometry \n \nLog Similarity Distance \n \nSee http://tdunning.blogspot.co.il/2008/03/surprise-and-coincidence.html \n \nChebychev Distance \n http://en.wikipedia.org/wiki/Chebyshev_distance \n \n \nTanimoto Distance \nSee http://en.wikipedia.org/wiki/Jaccard_index \n \n Additional reading: \n \n Matrix factorization based collaborative filtering with GraphChi . \n \n \n \n Acknowledgements \n \n \n \n \n Tao Ye, Pandora Internet Radio , for beta testing itemcf. \n Timmy Wilson , Smarttypes.org, for proposing additional metrics \n Denis Parra , Univ. of Pittsburgh, for proposing additional metrics"], "link": "http://bickson.blogspot.com/feeds/2241757437702574037/comments/default", "bloglinks": {}, "links": {"http://cn.linkedin.com/": 1, "http://bickson.co.il/": 4, "http://select.cmu.edu/": 2, "http://tdunning.co.il/": 1, "http://www.sillysnail.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.smarttypes.org/": 2, "http://arxiv.org/": 1, "http://www.pitt.edu/": 1, "http://www.blogger.com/blog": 1, "http://en.wikipedia.org/": 6}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Every now and then I am getting some strange emails with all kinds of job offers. \nYesterday I got an email with the title: \"URGENT: Invitation from Portuguese Foundation for Science and Technology\". I was flattered to learn from Vanessa, that the Portuguese Foundation for Science and Technology is interested that I will review some project proposal in the area of P2P networks. \n \nSo far nothing bizarre here.\n\nI continued reading and I got to the compensation part. I was glad to learn that \"Furthermore, FCT pays \u20ac 70 per proposal evaluated, from which a tax deduction will be applied as required by Portuguese law.\" There was also an attached link with a 22 page tutorial on how to rate proposals. \n \nThe email was sent on Thursday night. I need to return an answer until Sunday if I am willing to rate the proposal. I am almost certain Vanessa does not work on weekends. \n \nAny of my blog readers is Portugese? Let be conservative and assume assume there is 20% tax. We remain with 56 Euro. Which are 72$. Further assume I need one hour to read the proposal ratings guidelines and three hours to read and rate the proposal. So we are talking about 18$ an hour gig! \n \nNow, let's be more realistic. Depositing a check in Euro and converting the money will cost probably 20$ fee. So we remain with 52$ gig. Reporting a foreign income in end of your tax report will result in overhead of at least 50$ for the accountants. So we remain with a 2$ fee for 4 hours of work. \n \nAnd now I ask Vanessa. Are you serious? Why don't you ask me to volunteer to review your proposal? It will be much cheaper for me.. :-) \n \n \nI asked Vanessa to find out what is the applied tax. It may be that the income tax is lower than 20%. In that case all my calculations are wrong. And then I need to reconsider... \n \n \nAny of my readers which have an advice what to do please comment ASAP. We have only until Sunday to deploy the wisdom of crowdsourcing."], "link": "http://bickson.blogspot.com/feeds/8203422606564300101/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I got this email from Danielle: \n \nOn October 19, 2012, the New York Academy of Sciences will present the conference, \" 7th Annual Machine Learning Symposium \" at the Academy headquarters in New York City. This symposium features keynote speakers in applied and theoretical machine learning as well as \"spotlight\" talks selected from poster abstract submissions. I write to enquire if you could include a conference announcement about the event on your blog as your professional community would be the ideal audience for this event. Thank you in advance for your time and consideration. I look forward to hearing from you in the near future. Best regards, Danielle M. Haynes-Amran Marketing Coordinator: Programs  \nI looked over their website and indeed it seems they have a serious program. Their keynote speakers are Peter Bartlett , Vladimir Vapnik and William Freeman ."], "link": "http://bickson.blogspot.com/feeds/6509341749904789605/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://people.mit.edu/": 1, "http://www.nyas.org/": 1, "http://www.nec-labs.com/": 1, "http://www.berkeley.edu/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I met Troy Raeder at ACM KDD CUP in Beijing and asked him to tell me a little about \n m6d.com : \n \n \n From a business perspective, we do targeted display advertising. Essentially, consumer brands contract with us to find strong targets for their advertising and show display ads (banner or video) to these targets on the web and mobile devices. Within display advertising, there are two common approaches: \"retargeting\", which is advertising to people who have already visited your brand's website; and \"prospecting\", which requires finding people who have never been to your site but are good candidates for your brand. Retargeting is fairly popular. You've probably noticed it, where you visit a commerce site and then ads for that site follow you elsewhere on the web. Prospecting, by contrast, can have impressive scale (obviously there are more non-visitors than visitors to your website), and that is the main focus of our business. More specifically, we use the retargeting population (site visitors) as a seed set from which to build a classification model for predicting brand affinity in the population of potential prospects. \n From a machine learning perspective, we solve a huge sparse classification problem, where features are URLs that people have visited and the class (outcome) is some sort of brand action. Usually we use visits to the brand site as a proxy for brand affinity because it is more common than purchase conversions but less random than clicks. The coolest thing about our system, at least to me, is that it is automatic and dynamic on a number of levels. We re-score browsers regularly as we get new data on them, so the set of good prospects for a particular brand is accurate pretty much in real time, and everything we do -- rescoring browsers, retraining models, and adding new brands -- happens very automatically. \n \n We have a number of papers that describe our algorithms and systems in greater detail, including two that will be presented at KDD in China this year. If you read these, you'll know about as much as you\u2019d care to know about our system. \n \n This is the original paper describing our methods: \n http://pages.stern.nyu.edu/~fprovost/Papers/kdd_audience.pdf \n \n Our measurement methodology: \n http://www.m6d.com/blog/wp-content/uploads/2011/08/CausalKDDWorkShopPaper.pdf \n \n Our large-scale classification system: \n Design Principles of Massive, Robust Prediction Systems  \n \n And our bid modeling strategy \n Bid Optimizing and Inventory Scoring in Targeted Online Advertising  \n \n While many companies are very hush hush about their applied ML methods, I find it very useful that m6d actually describes their ML approaches. It helps us to get up-to-date what is happening in the industry."], "link": "http://bickson.blogspot.com/feeds/1850937281760580116/comments/default", "bloglinks": {}, "links": {"http://www.linkedin.com/": 1, "http://m6d.com/blog": 2, "http://feedads.doubleclick.net/": 2, "http://pages.nyu.edu/": 1, "http://www.m6d.com/blog": 1, "http://2.blogspot.com/": 1, "http://m6d.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Here is a quick Q/A session with Marcoz Sainz , a fellow CMUer who is data scientist at ModCloth. ModCloth has an office in Pittsburgh and was one of the companies who attended our July workshop. I did the unfortunate mistake of showing my wife their website, an error which immediately cost me 200$.. :-) \n \n \n \n \n \n What is the special market / what make modcloth unique?\n \n \nModCloth\u2019s mission is to democratize the fashion industry. We seek to \ndo so by empowering our community of shoppers through a social commerce\nplatform that brings products to market with customer feedback and validation. \nPrograms like Be the Buyer\u2122 allow customers to vote items from emerging \ndesigners into production.\n\nModCloth has built a loyal community through engaging, interactive contests\non our blog and an active involvement in social networks such as Twitter and\nFacebook, and we\u2019re gaining even more attention for our use of new social\nplatforms, such as Instagram and Pinterest. \n \nWhat is the size of the company? where are you based?\n \n \nModCloth was founded in 2002 when Eric & Susan Koger were only 18 and \n17 respectively.\n\nIn just several years, we've grown from our humble beginnings in a Carnegie\nMellon University dorm room to \u201cAmerica\u2019s Fastest-Growing Retailer,\u201d with\noffices in Pittsburgh, San Francisco, and Los Angeles.\n\nIn 4 years, ModCloth has grown from 3 to over 300 employees (and \ngrowing)! \n \nModCloth has more than 500,000 fans on Facebook, over 80,000 followers\non Twitter, works with over 700 independent designers, and averages over\n500 product reviews daily. \n\nOur Be the Buyer\u2122 program has received over 11 million votes so far. \nWhat are some interesting machine learning challenges you face?\n \n \nA particularly noteworthy challenge we face is the \"cold start\" problem as\nit relates to our lean and ever-changing supply chain. In plain words, by the\ntime we have gathered enough data about any given product that we sell on\nour site, it is too late to use that data for inference and prediction for that\nproduct.\n\nTo make things more exciting, mix in the fact that fashion is a relatively\nvolatile and difficult-to-quantify concept. Change is the only constant. \nWould you like to share anything about your solution methods / mode of work ?\n \n \nCentral to the success of some of our Data Science initiatives has been the\ncreation of a Data Warehouse or centralized repository of data originating from\nmultiple systems. Having all or most relevant data in one place in an \neasily-consumable format makes the life of data analysts/scientists\nmuch easier."], "link": "http://bickson.blogspot.com/feeds/7873770023446238242/comments/default", "bloglinks": {}, "links": {"http://www.linkedin.com/": 1, "http://2.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["One of my greatest pleasures is keeping in touch with our user community and finding out what interesting projects are out there. Here are some small examples from the LAST WEEK. I hope this will encourage further people to contact me!! \n \n \n \n Phillip Trotter , ptav: \n \n This is just a short note to say huge thanks for the GraphChi posts on your blog. \n \n I have small start up developing complex systems simulation and visualization tools and we are in the early stages of using GraphChi for both feature identification in network graphs and for data validation following simulations. We have potentially a lot of other uses for GraphChi and I am really enjoying getting to know the code base. A big part of that has been your articles and i just wanted to say thank you for the posts - they have been incredibly helpful. \n \n Ben Mabey , Red brain labs \n \n  Thanks for all your work on graphlab. I think it is one of the most interesting ML projects in a long time. Between graphlab's CF library and libfm I haven't been this excited about recsys since the FunkSVD. :) At my work we're excited about the performance of graphlab since it allows for faster experimentation and cheaper builds. As a side hobby project I'm trying to develop a recsys for github repositories. The majority of the user feedback is implicit (e.g. commits to repos, opened issues, comments, etc) with the only explicit feedback (staring of a repo) is binary. Since GraphLab's CF toolkit already implements some of the papers that I had read dealing with implicit feedback it seemed like a great way to get started. \n \n \n \n Norman Graham: \n \n thanks for the good work on the GraphLab project. --Norm \n \n \n  \n Paul Lefkopoulos, 55.com : \n Thanks for the work. We are a fresh French startup specialized in web analytics and digital media optimization. One of our main aim is to increase the website conversion rate of our retailing customers. We currently use your tool to build a recommender system for a small retailer in order to increase its cross-sell. \n \n \n \n Marilyn Wulfekuhler \n \n I really appreciate such a responsive and helpful forum! Thank you, thank you. \n \n \n \n Shingo Takamatsu, Sony Japan \n \n I have read your papers on GraphLab (and GraphChi) and implemented a few algorithms on GraphLab (Danny\u2019s blog was great help!). \n \n \n \n Xiaobing Liu, Tencent China \n \n \n \n \n I am a senior engineer at Tencent.com.inc which is largest internet companies in \n China. I am interested in distributed/parallel large scale machine learning algorithms and applications in search advertising targeting and the relevance of the search engine. So for, I implement parallel logistic regression in batch learning way and in online learning way, which are applied in search ad-ctr prediction and also I implemented some topic models, such as LDA and PLSA all of algorithm bases on MPI. I am quite excited about parallel framework, such as pregel and graphlab.Currently, I am implementing the PLSA and logistic regression base on graphlab. \n \n \n \n Zygmunt Zaj\u0105c \nI've been using Graphlab mainly for data sport at Kaggle, currently I'm in TOP 100 among 55 thousand registered users. I also write about Kaggle competitions and machine learning in general at fastml.com .\n\n\nI generally like trying new machine learning software and I found Graphlab to be fast, relatively easy to use and comprehensive (a lot of options and algorithms, built-in validation etc.).\n \n  Ayman M Shalaby, University of Waterloo, Canada \n I've read heard about the Graphlab project and the project sounds very promising. \n I'm particularly interested in using Graphlab to implement large scale Dynamic Bayesian Networks. \n \n S teffen Rendle , Author of the libFM collaborative filtering package  \n \n \n I am impressed by the size of the Graphlab project and the efforts you make to scale the algorithms to multiple cores/ machines. I think this will get a very important topic with future increasing parallelism of computers. \n \n \n \n \n \n And how can we forget our mega collaborator JustinYan ? \n \n \n I am Qiang (Justin) Yan ,a master student from the Chinese Academy of Sciences. My main focus is large scale data mining and Collaborative Filtering. I was working as an intern to build Recommender system for Baidu and Hulu in last couple of years. Recently I focus on implementing Collaborative Filtering Models on GraphLab and GraphChi."], "link": "http://bickson.blogspot.com/feeds/6047498294565941294/comments/default", "bloglinks": {}, "links": {"http://cn.linkedin.com/": 1, "http://www.linkedin.com/": 3, "http://3.blogspot.com/": 1, "http://fr.linkedin.com/": 1, "http://feedads.doubleclick.net/": 2, "http://cms.uni-konstanz.de/": 1, "http://1.blogspot.com/": 3, "http://www.libfm.org/": 1, "http://www.fifty-five.com/": 1, "http://4.blogspot.com/": 1, "http://2.blogspot.com/": 2, "http://www.kaggle.com/": 1, "http://fastml.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Consecutive IDS parser \nIf you like to use GraphChi collaborative filtering library, you first need to parse your data into consecutive user integer ids and consecutive item integer ids. \nFor example, assume you have a user movie rating database with the following format: \n Johny Walker,The Matrix (Director's Cut),5 \n Abba Babba,James Bond VII,3 \n Bondy Jr.,Titanic,1 \n \nNamely, each row contains one ratings with the format \n<user name>,<item name>,<rating> \n \n \nAnd you like to convert it to sparse matrix market format: \n \n \n 1 1 5 \n \n 2 2 3 \n \n 3 3 1 \n \n \n \n \nNamely user 1 rated item 1 with rating of 5, etc. \n \n \nAdditionally, you can also convert files of the format: \n 12032 12304-0323-X 3 \nWhere for example 12032 is the first user ID, 12304-0323-X is the ISBN of the rated book and 3 is the rating value. \n \n \nYou can use the consecutive_matrix_market parser to create the appropriate format from your data files. \n \n \n \nThe input to the parser is either CSV file, TSV file or a matrix market input file with non consecutive integer ids. The user and items can be either strings or integers. \n \n \n \nThere are several outputs to the consecutive IDS parser: \n \n1) filename.out - a rating file with the format [user] [ movie] [rating] where both the user ids and item ids are consecutive integers. In our example \n \n \n \n \n 1 1 5 \n \n 2 2 3 \n \n 3 3 1 \n \n \n \n \n2) Mapping between user names/ids to consecutive integers. In our example: \n \n Abba Babba 2 \n Bondy Jr. 3 \n Johny Walker 1 \n3) Mapping between movie names/ids to consecutive integers. In our example: \n \n James Bond VII 2 \n The Matrix (Director's Cut) 1 \n Titanic 3 \n \n4+5) The same maps but in reverse, namely mapping back between unsigned integers to string ids. \n6) Matrix market header file - in our examples there are 3 users, 3 movies and 3 ratings: \n \n %%MatrixMarket matrix coordinate integer general \n 3 3 3 \n \n To run the consecutive IDS parser you should prepare a file with the input file name within it. \n For example, the file name \"movies\" contains: \n \n \n \n Johny Walker,The Matrix (Director's Cut),5 \n \n Abba Babba,James Bond VII,3 \n \n Bondy Jr.,Titanic,1 \n \n \n \n Next, you prepare a file named list, with movies in the first line: \n \n #> cat list \n \n movies \n \n \nFinally, you run the parser: \n \n ./toolkits/collaborative_fitering/consecutive_matrix_market --file_list=list --csv=1 \n \n \n \n The supported command line flags are: \n \n --csv=1 - csv format \n \n --tsv=1 - tsv format \n \n -> otherwise - sparse separated format \n \n --file_list=list - list of files to parse \n \nLDA Parser \nTo the request of Baoqiang Cao I have started a parsers toolkits in GraphChi to be used for \npreparing data to be used in GraphLab/ Graphchi. The parsers should be used as template which can be easy customized to user specific needs. \nLDA parser code is found here . \n \nExample input file: \n \n I was not a long long ago here \n because i am not so damn smart. \n as you may have thought \n \n \n \nThe assumption is that each line holds words from a certain document. We would like to assign the strings unique ids, and count how many words appear in each document. \n \n \n \nThe input to GraphLab's LDA is in the format \n \n [docid] [wordid] [word count] \n \n \n \nExample run: \n \n1) Create a file \"stamfile\" with the example input above. \n \n2) Create a file \"a\" which has the file name \"stamfile\" in its first line. \n \n >>head d \n \n stamfile \n \n3) Run the parser: \n \n \n bickson@thrust:~/graphchi$ ./toolkits/parsers/texttokens --file_list=a \n \n WARNING: texttokens.cpp(main:146): GraphChi parsers library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n \n [file_list] => [a] \n \n INFO:  texttokens.cpp(parse:138): Finished parsing total of 4 lines in file stamfile \n \n total map size: 16 \n \n Finished in 0.024819 \n \n INFO:  texttokens.cpp(save_map_to_text_file:56): Wrote a total of 16 map entries to text file: amap.text \n \n INFO:  texttokens.cpp(save_map_to_text_file:68): Wrote a total of 16 map entries to text file: areverse.map.text \n \n \n \n \nThe output of the parser is a file named stamfile.out \n \n \n bickson@thrust:~/graphchi$ cat stamfile.out \n \n 2 1 1 \n \n 2 2 1 \n \n 2 3 1 \n \n 2 4 2 \n \n 2 5 1 \n \n 2 6 1 \n \n 3 11 1 \n \n 3 3 1 \n \n 3 7 1 \n \n 3 8 1 \n \n 3 9 1 \n \n 3 10 1 \n \n 4 12 1 \n \n 4 13 1 \n \n 4 14 1 \n \n 4 15 1 \n \n 4 16 1 \n \n \n \n \nAnd the mapping files: \n \n \n bickson@thrust:~/graphchi$ cat amap.text \n \n may 14 \n \n long 4 \n \n am 8 \n \n because 7 \n \n you 13 \n \n as 12 \n \n so 9 \n \n here 6 \n \n have 15 \n \n not 3 \n \n i 1 \n \n smart 11 \n \n thought 16 \n \n damn 10 \n \n was 2 \n \n ago 5 \n \n \n \n \n bickson@thrust:~/graphchi$ cat areverse.map.text \n \n \n 1 i \n \n 2 was \n \n 3 not \n \n 4 long \n \n 5 ago \n \n 6 here \n \n 7 because \n \n 8 am \n \n 9 so \n \n 10 damn \n \n 11 smart \n \n 12 as \n \n 13 you \n \n 14 may \n \n 15 have \n \n 16 thought \n \n \n \n \n \nTwitter parser \n \nA second parser, which is slightly more fancy, go over user twitter tweets in the following format: \n \n \n \n /* \n * Twitter input format is: \n * \n * T 2009-06-11 16:56:42 \n * U http://twitter.com/tiffnic85 \n * W Bus is pulling out now. We gotta be in LA by 8 to check into the Paragon. \n * \n * T 2009-06-11 16:56:42 \n * U http://twitter.com/xlamp \n * W \u7070\u3092\u7070\u76bf\u306b\u843d\u3068\u305d\u3046\u3068\u3059\u308b\u3068\u9ad8\u78ba\u7387\u3067\u30d8\u30c3\u30c9\u30bb\u30c3\u30c8\u306e\u7dda\u3092\u6839\u6027\u713c\u304d\u3059\u308b\u5f62\u306b\u306a\u308b\u3093\u3060\u304c \n * \n * T 2009-06-11 16:56:43 \n * U http://twitter.com/danilaselva \n * W @carolinesweatt There are no orphans...of God! :) Miss your face! \n * \n */ \n \nAnd extracts graph links between the retweets. \n \n \n \nAggregate parser \nThis parser computes the aggregate of the requested column. \nThe input is a sorted graph: \n 1 2 4 \n 1 2 4 \n 2 3 3 \n 3 4 2 \n 3 4 2 \n \nAnd the output is the aggregate of the requested column (column 3 in the example): \n \n 1 2 8 \n 2 3 3 \n 3 4 2 \n 3 4 4 \n \n \nOperating the parsers \n \n1) Install graphchi as explained in steps 1+2 here . \n \n2) Compile with \" make parsers \" \n \n3) Run using ./toolkits/parsers/texttoken --file_list=files \n \n \n \nCascading several parsers together \nI got this from Stefan Weigert : \nI would like to use GraphLab for a top-k algorithm.\nThere is a communication-log which i want to process as input-file.\nAssume, the log contains the following entries : \n \n YVjAeZQjnVA IfrTTVlatui 050803 156 \n YVjAeZQjnVA IfrTTVlatui 050803 12 \n GNgrmichxmG GNgriWokEhN 050803 143 \n YnRdCKZkLao MHexzaXWCPL 050803 0 \n RGNReqpKcZw RGNRSTDdqew 050803 0 \n LPHSeuGhYkN ZFwbovKzAxY 050803 1 \n sijmyRRfkwl XtqJaHYFEPqbZqNGPCr 050803 68 \n RGNReqpKcZw RGNRSTDdqew 050805 6 \n RGNReqpKcZw RGNRSTDdqew 050805 6 \n sijmyRRfkwl XtqJaHYFEPqbZqNGPCr 050805 12 \n \n \n \nWhere each line has the following format: \n \n [ caller id] [ receiver id ] [date ] [call duration] \n \n \n \nWhat i wanted to do is \n1) load the first line, add an edge YVjAeZQjnVA -> IfrTTVlatui with 156 as attribute \n2) load the second line, see that YVjAeZQjnVA -> IfrTTVlatui exists already and only change the edge-attrib to 168 (168 = 156+12)\n \n \n \nProposed solution: \n1) Run consecutive ids parser to convert the strings to consecutive ids. \n a) Assume the input file is named \"test\". Prepare a list file named \"files\" with the string \"test\" in its first line. \n b) run consecutive parser with: \n \n ./toolkits/parsers/consecutive_matrix_market --file_list=files \n WARNING: consecutive_matrix_market.cpp(main:177): GraphChi parsers library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n [file_list] => [a] \n INFO:  consecutive_matrix_market.cpp(parse:169): Finished parsing total of 10 lines in file test \n total map size: 6 \n Finished in 0.000117 \n INFO:  consecutive_matrix_market.cpp(save_map_to_text_file:69): Wrote a total of 6 map entries to text file: auser.map.text \n INFO:  consecutive_matrix_market.cpp(save_map_to_text_file:81): Wrote a total of 6 map entries to text file: auser.reverse.map.text \n INFO:  consecutive_matrix_market.cpp(save_map_to_text_file:69): Wrote a total of 6 map entries to text file: amovie.map.text \n INFO:  consecutive_matrix_market.cpp(save_map_to_text_file:81): Wrote a total of 6 map entries to text file: amovie.reverse.map.text \n INFO:  consecutive_matrix_market.cpp(main:225): Writing matrix market header into file: matrix_market.info \n \n c) The output is saved into the file: test.out \n \n \n 1 1 050803 156 \n \n 1 1 050803 12 \n \n 2 2 050803 143 \n \n 3 3 050803 0 \n \n 4 4 050803 0 \n \n 5 5 050803 1 \n \n 6 6 050803 68 \n \n 4 4 050805 6 \n \n 4 4 050805 6 \n \n 6 6 050805 12 \n \n \n \n \n2) Sort the output file: \n \n \n sort -k 1,1 -k 2,2 -g test.out > test.sorted \n \n \n The sorted file is: \n \n \n 1 1 050803 156 \n \n 1 1 050803 12 \n \n 2 2 050803 143 \n \n 3 3 050803 0 \n \n 4 4 050803 0 \n \n ... \n \n \n \n \n3) Run the aggregator parser: \n \n a) edit the file \"files\" to contain the string test.sorted in the first line. \n \n b) Run the aggregator parser: \n \n   bickson@thrust:~/graphchi$ ./toolkits/parsers/aggregator --file_list=a --col=4 \n \n WARNING: aggregator.cpp(main:151): GraphChi parsers library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n \n [file_list] => [a] \n \n [col] => [4] \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 6.4e-05 edges: 1 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 8.6e-05 edges: 2 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 9.6e-05 edges: 3 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 0.000103 edges: 4 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 0.000109 edges: 5 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 0.000114 edges: 6 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 0.000119 edges: 7 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 0.000124 edges: 8 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 0.000129 edges: 9 \n \n INFO:  aggregator.cpp(parse:137): Hash map size: 0 at time: 0.000134 edges: 10 \n \n INFO:  aggregator.cpp(parse:143): Finished parsing total of 11 lines in file test.sorted \n \n total map size: 0 \n \n Finished in 0.000559 \n \nc) Now the output is: \n \n \n 1 1 168 \n \n 2 2 143 \n \n 3 3 0 \n \n 4 4 12 \n \n 5 5 1 \n \n 6 6 80 \n \n \n \n \nWe are done! \n \n \n \n \n \n Acknowledgements \n \n \n \n \n Brad Cox, Technica , for helping debug the parser library. \n Baoqiang Cao, Networked Insights , for suggesting LDA parser."], "link": "http://bickson.blogspot.com/feeds/3684228535006424530/comments/default", "bloglinks": {}, "links": {"http://bickson.co.il/": 1, "http://www.linkedin.com/": 3, "http://feedads.doubleclick.net/": 2, "http://code.google.com/": 1, "http://de.linkedin.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I got today a question by Timmy Wilson , our man in Ohio, about the paper: Energy Models for Graph Clustering by Andreas Noack. This paper has a nice treatment for power law graph visualization (like social networks). In traditional layouts, the popular nodes which have a lot of links have larger importance and thus are visualized in the center, so we get a messy layout: \n \n \nThe paper proposes to use edge repulsion model instead of node repulsion model, thus highly connected nodes are no longer stacked in the middle: \n \n \nThis example is taken from the above paper and visualizes links between terms in an online dictionary. \n \nThe layout cost function is: \n \n min sum_over_edges_u,v || p(u) - p(v) || - sum_over_all_pairs_u,v deg(u) deg(v) log || p(u) - p(v) || \n \nWhere ||x|| is the Euclidian distance, and p(u) is the layout position of node u, deg(u) is the degree of node u. \nThe cost function is composed of two terms. The first tries to pull visualized nodes which are closed in the graph to be close together. The second term tries to place all pairs of nodes as far as possible from each other. The trick is that high degree nodes should be more far away since their degree is high and thus two close high degree nodes have a lot of effect on the cost function. \n \nRegarding the solution, it is rather straightforward to implement the method using gradient descent. The problem is that the first term in the cost function fits very nicely to graphlab/graphchi since it \nis local over the graph edges. However, the second term computes distance over all pairs of nodes, this is not a local operation and does not fit well within graphlab."], "link": "http://bickson.blogspot.com/feeds/8226030167106781470/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://www.linkedin.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www-sst.tu-cottbus.de/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["My collaborators Joey and Yucheng are organizing this year NIPS big learning workshop. They also mentioned that some skying may be involved.. :-) \n \n  \n \n Big Learning 2012: Algorithms, Systems and Tools \n NIPS 2012 Workshop http://www.biglearn.org   Organizers: Sameer Singh < sameer@cs.umass.edu > (UMass Amherst) John Duchi < jduchi@eecs.berkeley.edu > (UC Berkeley) Yucheng Low < ylow@cs.cmu.edu > (Carnegie Mellon University) Joseph Gonzalez < jegonzal@eecs.berkeley.edu > (UC Berkeley)  Submissions are solicited for a one day workshop on December 7-8 in Lake Tahoe, Nevada.  This workshop will address algorithms, systems, and real-world problem domains related to large-scale machine learning (\u201cBig Learning\u201d). With active research spanning machine learning, databases, parallel and distributed systems, parallel architectures, programming languages and abstractions, and even the sciences, Big Learning has attracted intense interest. This workshop will bring together experts across these diverse communities to discuss recent progress, share tools and software, identify pressing new challenges, and to exchange new ideas. Topics of interest include (but are not limited to):  Big Data: Methods for managing large, unstructured, and/or streaming data; cleaning, visualization, interactive platforms for data understanding and interpretation; sketching and summarization techniques; sources of large datasets.  Models & Algorithms: Machine learning algorithms for parallel, distributed, GPGPUs, or other novel architectures; theoretical analysis; distributed online algorithms; implementation and experimental evaluation; methods for distributed fault tolerance.  Applications of Big Learning : Practical application studies and challenges of real-world system building; insights on end-users, common data characteristics (stream or batch); trade-offs between labeling strategies (e.g., curated or crowd-sourced).  Tools, Software & Systems : Languages and libraries for large-scale parallel or distributed learning which leverage cloud computing, scalable storage (e.g. RDBMs, NoSQL, graph databases), and/or specialized hardware.  Submissions should be written as extended abstracts, no longer than 4 pages (excluding references) in the NIPS latex style. Relevant work previously presented in non-machine-learning conferences is strongly encouraged, though submitters should note this in their submission.  Submission Deadline: October 17th, 2012 . Please refer to the website for detailed submission instructions: http://biglearn.org"], "link": "http://bickson.blogspot.com/feeds/2363939485525937778/comments/default", "bloglinks": {}, "links": {"http://www.biglearn.org/": 1, "http://feedads.doubleclick.net/": 2, "http://biglearn.org/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["I got this link from Igor Caron. The famous compressed sensing and matrix factorization blogger . \n \n \n \nIn one of the thread, there was a discussion about recommender capabilities. Since we were looking at Arxaliv.org as a model (this is a Reddit clone), I went to the reddit discussion of the development of that open source platform and found that they, Reddit, actually are looking for a recommeder system and they have a nice dataset . \n \n There are 23,091,688 votes from 43,976 users over 3,436,063 links in 11,675 reddits. (Interestingly these ~44k users represent almost 17% of our total votes). The dump is 2.2gb uncompressed, 375mb in bz2. A reddit is a category. A link is a subject (in Arxaliv it would be a paper) so that matrix (43976 x 3436063) is pretty sparsely filled (1.5e-5). Some SVD has been tried but I am sure they haven't looked at low rank solvers . Since Reddit is such a massive platform, if your algorithm provides good results, it will get to be known beyond your expectations."], "link": "http://bickson.blogspot.com/feeds/4421451559947251502/comments/default", "bloglinks": {}, "links": {"http://www.reddit.com/": 2, "http://feedads.doubleclick.net/": 2, "https://groups.google.com/": 1, "http://arxaliv.org/": 2, "https://sites.google.com/": 1, "http://nuit-blanche.blogspot.fr/": 1, "http://nuit-blanche.blogspot.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["A couple of weeks ago I covered GraphChi by Aapo Kyrola in my blog. \nHere is a quick tutorial for trying out GraphChi collaborative filtering toolbox that I wrote. Currently it supports ALS (alternating least squares), SGD (stochastic gradient descent), bias-SGD (biased stochastic gradient descent) , SVD++ , NMF (non-negative matrix factorization), SVD (restarted lanczos, and one sided lanczos) but I am soon going to implement several more algorithms. \n \n \nReferences \nHere are papers which explain the algorithms in more detail: \n \n \n Alternating Least Squares (ALS) Yunhong Zhou, Dennis Wilkinson, Robert Schreiber and Rong Pan. Large-Scale Parallel Collaborative Filtering for the Netflix Prize. Proceedings of the 4th international conference on Algorithmic Aspects in Information and Management. Shanghai, China pp. 337-348, 2008.\n \n \n Stochastic gradient descent (SGD) Matrix Factorization Techniques for Recommender Systems Yehuda Koren, Robert Bell, Chris Volinsky In IEEE Computer, Vol. 42, No. 8. (07 August 2009), pp. 30-37. \nTak\u00e1cs, G, Pil\u00e1szy, I., N\u00e9meth, B. and Tikk, D. (2009). Scalable Collaborative Filtering Approaches for Large Recommender Systems. Journal of Machine Learning Research, 10, 623-656.\n \n \n Bias stochastic gradient descent (Bias-SGD) Y. Koren. Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model. ACM SIGKDD 2008. Equation (5). \n \n SVD++ Y. Koren. Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model. ACM SIGKDD 2008. \n \n \n \n Weighted-ALS Collaborative Filtering for Implicit Feedback Datasets Hu, Y.; Koren, Y.; Volinsky, C. IEEE International Conference on Data Mining (ICDM 2008), IEEE (2008). \n \n \n \n Sparse-ALS Xi Chen, Yanjun Qi, Bing Bai, Qihang Lin and Jaime Carbonell. Sparse Latent Semantic Analysis. In SIAM International Conference on Data Mining (SDM), 2011. \nD. Needell, J. A. Tropp CoSaMP: Iterative signal recovery from incomplete and inaccurate samples Applied and Computational Harmonic Analysis, Vol. 26, No. 3. (17 Apr 2008), pp. 301-321. \n \n \n \n NMF Lee, D..D., and Seung, H.S., (2001), 'Algorithms for Non-negative Matrix\nFactorization', Adv. Neural Info. Proc. Syst. 13, 556-562. \n \n SVD (Restarted Lanczos & One sided Lanczos) V. Hern\u00b4andez, J. E. Rom\u00b4an and A. Tom\u00b4as. STR-8: Restarted Lanczos Bidiagonalization for the SVD in SLEPc. \n \n \n \n tensor-ALS Tensor Decompositions, Alternating Least Squares and other Tales. P. Comon, X. Luciani and A. L. F. de Almeida. Special issue, Journal of Chemometrics. In memory of R. Harshman.\nAugust 16, 2009 \n \n \n \n \n \nTarget \nThe benefit of using GraphChi is that it requires a single multicore machine and can scale up to very large models, since at no point the data is fully read into memory. In other words, GraphChi is very useful for machine with limited RAM since it streams over the dataset. It is also possible to configure how much RAM to use during the run. \n \nHere are some performance numbers: \n \n \n \n \n \nThe above graph shows 6 iterations of SGD (stochastic gradient descent) on the full Netflix data. \nNetflix has around 100M ratings so the matrix has 100M non-zeros. The size of the decomposed \nmatrix is about 480K users x 10K movies. I used a single multicore machine with 8 threads, where GraphChi memory consumption was limited to 800Mb, using 8 cores. The factorized matrix has a width of D=20. In total it takes around 80 seconds per 6 iterations, which is around 14 seconds per iteration. \n \nPreprocessing the matrix is done once, and take around 35 seconds. \n \nThe input to GraphChi ALS/SGD/bias-SGD is the sparse matrix A in sparse matrix market format. The output are two matrices U and V s.t. A ~= U*V' and both U and V have \na lower dimension D. \n \n \nRunning and setup instructions \nLet's start with an example: \n \n1) Download graphchi from mercurial using the instructions here . \n \n2) Change directory to graphchi \n  cd graphchi \n \n3) Download Eigen and put the header files inside the ./src directory \n  wget http://bitbucket.org/eigen/eigen/get/3.1.1.tar.bz2 \n tar -xjf 3.1.1.tar.bz2 \n mv eigen-eigen-43d9075b23ef/Eigen ./src \n \n4) Compile using \n  make cf \n \n5a) For ALS/SGD/bias-SGD/SVD++/SVD Download Netflix synthetic sample file. The input is in sparse matrix market format . \n  wget http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mm \n wget http://www.select.cs.cmu.edu/code/graphlab/datasets/smallnetflix_mme \n  \n 5b) For WALS Download netflix sample file including time: \n \n wget http://www.select.cs.cmu.edu/code/graphlab/datasets/time_smallnetflix \n wget http://www.select.cs.cmu.edu/code/graphlab/datasets/time_smallnetflixe \n \n6a) Run ALS on the Netflix example: \n  ./toolkits/collaborative_filtering/als --training= smallnetflix_mm --validation=smallnetflix_mme --lambda=0.065 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \n \n \n At the first time, the input file will be preprocessed into an efficient binary representation on disk and then 6 ALS iterations will be run. \n \n6b) Run SGD on the Netflix example: \n ./ toolkits/collaborative_filtering /sgd --training= smallnetflix_mm --validation= smallnetflix_mme --sgd_lambda=1e-4 --sgd_gamma=1e-4 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \n6c) Run bias-SGD on the Netflix example: \n ./ toolkits/collaborative_filtering/ biassgd --training= smallnetflix_mm --validation=smallnetflix_mme --biassgd_lambda=1e-4 --biassgd_gamma=1e-4 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \n6d) Run SVD++ on Netflix example: \n \n ./ toolkits/collaborative_filtering/ svdpp --training= smallnetflix_mm --validation=smallnetflix_mme --biassgd_lambda=1e-4 --biassgd_gamma=1e-4 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \n \n \n6e) Run weighted-ALS on the Netflix time example: \n ./ toolkits/collaborative_filtering/w als --training= time_smallnetflix --validation= time_smallnetflixe --lambda=0.065 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \n6f) Run NMF on the reverse Netflix example: \n \n ./ toolkits/collaborative_filtering/ nmf --training= reverse_netflix.mm --minval=1 --maxval=5 --max_iter=20 --quiet=1 \n \n \n6g) Run one sided SVD on the Netflix example: \n ./ toolkits/collaborative_filtering/ svd_onesided --training= smallnetflix_mm --nsv=3 --nv=10 --max_iter=1 --quiet=1 --tol=1e-1 \n \n6h) Run tensor-ALS on Netflix time example \n ./ toolkits/collaborative_filtering/ als_tensor --training= time_smallnetflix --validation= time_smallnetflixe --lambda=0.065 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \n7) View the output. \n \n \nFor ALS, SGD, bias-SGD, WALS, SVD++ and NMF \nTwo files are created: filename_U.mm and filename_V.mm \n The files store the matrices U and V in sparse matrix market format . \n  head smallnetflix_mm_U.mm \n \n %%MatrixMarket matrix array real general \n 95526 5 \n 0.693370 \n 1.740420 \n 0.947675 \n 1.328987 \n 1.150084 \n 1.399164 \n 1.292951 \n 0.300416 \n \n \n \nFor tensor-ALS \nAdditional output file named filename_T.mm is created. Prediction is computed as the tensor product of U_i * V_j * T_k (namely r_ijk = sum_l( u_il * v_jl * t_kl )). \n \nFor bias-SGD, SVD++ \nAdditional three files are created: filename_U_bias.mm, filename_V_bias.mm and filename_global_mean.mm. Bias files include the bias for each user (U) and item (V). \nThe global mean file includes the global mean of the rating. \n \n \nFor SVD \nFor each singular vector a file named filename.U.XX is created where XX is the number of the singular vector. The same with filename.V.XX. Additionally a singular value files is also saved. \n \n \n \n Algorithms \n  Here is a table summarizing the properties of the different algorithms in the collaborative filtering library:  \n \n   \n ALGORITHM Tensor? (Supports time of rating input) Sparse output? Multiple ratings between the same user/item pair? Monte Carlo sampling method? \n ALS     \n Sparse-ALS   V   \n SGD      \n bias-SGD      \n SVD     \n NMF     \n RBM (not implemented yet)     V \n SVD++     \n LIBFM (not implemented yet) V    \n PMF (not implemented yet)     V \n time-SVD++ V    \n BPTF (not implemented yet) V    V \n     \n WALS V    \n \n \n \n  Note: for tensor algorithms, you need to verify you have both the rating and its time. Typically the exact time is binned into time bins (a few tens up to a few hundreds). Having too fine granularity over the time bins slows down computation and does not improve prediction. Using matrix market format, you need to specify each rating using 4 fields: [user] [item] [time bin] [rating]  \n \n \n  \n \n \n \n \nCommon command line options (for all algorithms) \n \n --training - the training input file \n --validation - the validation input file (optional). Validation is data with known ratings which not used for training. \n --test - the test input file (optional). Test input file is used for computing predictions to a predefined list of user/item pairs. \n \n --minval - min allowed rating (optional). It is highly recommended to set this value since it improves prediction accuracy. \n --maxval - max allowed rating (optional). It is highly recommended to set this value since it improves prediction accuracy. \n \n --max_iter - number of iterations to run \n \n --quiet =1 run with less traces. (optional, default = 0). \n \n --halt_on_rmse_increase=1 (optional, default = 0). Stops execution when validation error goes up. \n \n --load_factors_from_file - (optional, default = 0). This options allows for two functionalities. Instead of starting with a random state, you can start from any predefined state for the algorithm. This also allows for running a few iterations, saving the results to disk for fault tolerance, and running later FROM THE SAME EXACT state. \n \n NLATENT - width of the factorized matrix. Default is 20. To change it you need to edit the file toolkits/collaborative_filtering/XXX.hpp (where XXX is the algorithm name), and change the macro #define NLATENT to some other value and recompile. \n  \n \n  \n \n \n \n \n ALS (Alternating least squares) \n \n Pros: Simple to use, not many command line arguments Cons: intermediate accuracy, higher computational overhead  ALS is a simple yet powerful algorithm. In this model the prediction is computed as:  r_ui = p_u * q_i Where r_ui is a scalar rating of user u to item i, and p_u is the user feature vector of size D, q_i is the item feature vector of size D and the product is a vector product. The output of ALS is two matrices: filename_U.mm and filename_V.mm The matrix U holds the user feature vectors in each row. (Each vector has exactly D columns). The matrix V holds the feature vectors for each time (Each vector has again exactly D columns). In linear algebra notation the rating matrix R ~ UV   Below are ALS related command line options:  \n \n  \n   \n Basic \nConfirmation  --lambda=XX Set regularization. Regularization helps to prevent overfitting. \n   \n \n \n \n \n \n \n \n Stochastic gradient descent (SGD) \n Pros: fast method Cons: need to tune step size, more iterations are needed relative to ALS.  SGD is a simple gradient descent algorithm. Prediction in SGD is done as in ALS:  r_ui = p_u * q_i Where r_ui is a scalar rating of user u to item i, and p_u is the user feature vector of size D, q_i is the item feature vector of size D and the product is a vector product. The output of SGD is two matrices: filename.U and filename.V. The matrix U holds the user feature vectors in each row. (Each vector has exactly D columns). The matrix V holds the feature vectors for each time (Each vector has again exactly D columns). In linear algebra notation the rating matrix R ~ UV \n --lambda - regularization (optional). Default value 1e-3. \n --gamma - gradient step size (optional).Default value 1e-3. \n --sgd_step_dec - multiplicative step decrement (optional). Default is 0.9. \n \n \n \n Bias-SGD \n Pros: fast method Cons: need to tune step size  Bias-SGD is a simple gradient descent algorithm, where besides of the feature vector we also compute item and user biases (how much their average rating differs from the global average). Prediction in bias-SGD is done as follows:  r_ui = global_mean_rating + b_u + b_i + p_u * q_i  Where global_mean_rating is the global mean rating, b_u is the bias of user u, b_i is the bias of item i and p_u and q_i are feature vectors as in ALS. You can read more about bias-SGD in reference [N].  The output of bias-SGD consists of two matrices: filename.U and filename.V. The matrix U holds the user feature vectors in each row. (Each vector has exactly D columns). The matrix V holds the feature vectors for each time (Each vector has again exactly D columns). Additionally, the output consists of two vectors: bias for each user, bias for each item. Last, the global mean rating is also given as output. \n \n bias-SGD command line arguments: \n --biassgd_lambda -regularization (optional). Default value 1e-3. \n --biassgd_gamma -gradient step size (optional). Default value 1e-3. \n --biassgd_step_dec - multiplicative gradient step decrement (optional). Default is 0.9. \n \n \n \n Koren\u2019s SVD++ \n Pros: more accurate method than SGD once tuned, relatively fast method Cons: a lot of parameters for tuning, immune to numerical errors when parameters are out of scope.  Koren SVD++ is an algorithm which is slightly more fancy than bias-SGD and give somewhat better prediction results. \n \n Prediction in Koren\u2019s SVD++ algorithm is computed as follows:  r_ui = global_mean_rating + b_u + b_i + q_u * ( p_i + w_i ) Where r_ui is the scalar rating for user u to item i, global_mean_rating is the global mean rating, b_u is a scalar bias for user u, b_i is a scalar bias for item i, q_u is a feature vectors of length D for user u, p_i is a feature vector of length D for item i, and w_i is an additional feature vector of length D (the weight). The product is a vector product.  The output of Koren\u2019s SVD++ is 5 output files: Global mean ratings - include the scalar global mean rating. user_bias - includes a vector with bias for each user movie_bias - includes a vector with bias for each movie matrix U - includes in each row the feature vector q_u of size D. matrix V - includes in each row the sum of feature vectors p_i + w_i of size D. \n \n  \n \n SVD++ command line arguments: \n- -svdpp_item_bias_step, --svdpp_user_bias_step, --svdpp_user_factor_step, --svdpp_user_factor2_step - gradient step size (optional). Default value 1e-4. \n --svdpp_item_bias_reg, --svdpp_user_bias_reg, --svdpp_user_factor_reg, --svdpp_user_factor2_reg - regularization (optional). Default value 1e-4. \n --svdpp_step_dec - multiplicative gradient step decrement (optional). Default is 0.9. \n \n \n \n Weighted Alternating Least Squares (WALS) \n \n \n Pros: allows weighting of ratings (can be thought of as confidence in rating), almost the same computational cost like in ALS. Cons: worse modeling error relative to ALS  Weighted ALS is a simple extension for ALS where each user/item pair has an additional weight. In this sense, WALS is a tensor algorithm since besides of the rating it also maintains a weight for each rating. Algorithm is described in references [I, J].  Prediction in WALS is computed as follows: r_ui = w_ui * p_u * q_i  The scalar value r for user u and item i is computed by multiplying the weight of the rating w_ui by the vector product p_u * q_i. Both p and q are feature vectors of size D.  \nNote: for weighted-ALS, the input file has 4 columns: \n \n[user] [item] [weight] [rating]. See example file in section 5b). \n \n \n --lambda - regularization \n \n \n Alternating least squares with sparse factors \n Pros: excellent for spectral clustering Cons: less accurate linear model because of the sparsification step  This algorithm is based on ALS, but an additional sparsifying step is performed on either the user feature vectors, the item feature vectors or both. This algorithm is useful for spectral clustering: first the rating matrix is factorized into a product of one or two sparse matrices, and then clustering can be computed on the feature matrices to detect similar users or items.  The underlying algorithm which is used for sparsifying is CoSaMP. See reference [K1].  Below are sparse-ALS related command line options:  \n \n  \n Basic configuration --user_sparsity=XX A number between 0.5 to 1 which defines how sparse is the resulting user feature factor matrix \n  --movie_sparsity=XX A number between 0.5 to 1 which defines how sparse is the resulting movie feature factor matrix \n  --algorithm=XX \nThere are three run modes: \n \n SPARSE_USR_FACTOR = 1 \n \n SPARSE_ITM_FACTOR = 2 \n \n SPARSE_BOTH_FACTORS = 3 \n \n \n \n \n  Prediction in sparse-ALS is computing like in ALS. \n \n \n \n \n \n \n \n \n \nExample running sparse-ALS: \n \n \n \n bickson@thrust:~/graphchi$ ./bin/sparse_als.cpp --training=smallnetflix_mm --user_sparsity=0.8 --movie_sparsity=0.8 --algorithm=3 --quiet=1 --max_iter=15 \n \n WARNING: sparse_als.cpp(main:202): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n \n [training] => [smallnetflix_mm] \n \n [user_sparsity] => [0.8] \n \n [movie_sparsity] => [0.8] \n \n [algorithm] => [3] \n \n [quiet] => [1] \n \n [max_iter] => [15] \n \n 0) Training RMSE: 1.11754 Validation RMSE: 3.82345 \n \n 1) Training RMSE: 3.75712 Validation RMSE:  3.241 \n \n 2) Training RMSE: 3.22943 Validation RMSE: 2.03961 \n \n 3) Training RMSE: 2.10314 Validation RMSE: 2.88369 \n \n 4) Training RMSE: 2.70826 Validation RMSE: 3.00748 \n \n 5) Training RMSE: 2.70374 Validation RMSE: 3.16669 \n \n 6) Training RMSE: 3.03717 Validation RMSE:  3.3131 \n \n 7) Training RMSE: 3.18988 Validation RMSE: 2.83234 \n \n 8) Training RMSE: 2.82192 Validation RMSE: 2.68066 \n \n 9) Training RMSE: 2.29236 Validation RMSE: 1.94994 \n \n 10) Training RMSE: 1.58655 Validation RMSE: 1.08408 \n \n 11) Training RMSE:  1.0062 Validation RMSE: 1.22961 \n \n 12) Training RMSE: 1.05143 Validation RMSE:  1.0448 \n \n 13) Training RMSE: 0.929382 Validation RMSE: 1.00319 \n \n 14) Training RMSE: 0.920154 Validation RMSE: 0.996426 \n \n \n \n \n \n \n \n tensor-ALS \n \n \n \n \n \nNote: for tensor-ALS, the input file has 4 columns: \n \n[user] [item] [time] [rating]. See example file in section 5b). \n \n --lambda - regularization \n \n \n \n \n \n \n Non-negative matrix factorization (NMF) \n Non-negative matrix factorization (NMF) is based on Lee and Seung [reference H]. Prediction is computed like in ALS:  r_ui = p_u * q_i  Namely the scalar prediction r of user u is composed of the vector product of the user feature vector p_u (of size D), with the item feature vector q_i (of size D). The only difference is that both p_u and q_i have all nonnegative values. The output of NMF is two matrices: filename.U and filename.V. The matrix U holds the user feature vectors in each row. (Each vector has exactly D columns). The matrix V holds the feature vectors for each time (Each vector has again exactly D columns). In linear algebra notation the rating matrix R ~ UV, U>=0, V>=0. \n \nNMF Has no special command line arguments. \n \n \n \nSVD \n \n SVD is implemented using the restarted lanczos algorithm . The input is a sparse matrix market format input file. The output are 3 files: one file containing the singular values, and two dense matrix market format files containing the matrices U and V.  Note: for larger models, it is advised to use svd_onesided since it significantly saved memory.  Here is an example Matrix Market input file for the matrix A2:  <235|0>bickson@bigbro6:~/ygraphlab/graphlabapi/debug/toolkits/parsers$ cat A2 %%MatrixMarket matrix coordinate real general 3 4 12 1 1 0.8147236863931789 1 2 0.9133758561390194 1 3 0.2784982188670484 1 4 0.9648885351992765 2 1 0.9057919370756192 2 2 0.6323592462254095 2 3 0.5468815192049838 2 4 0.1576130816775483 3 1 0.1269868162935061 3 2 0.09754040499940952 3 3 0.9575068354342976 3 4 0.9705927817606157   Here is an for running SVD : \n \n bickson@thrust:~/graphchi$ ./bin/svd --training=A2 --nsv=4 --nv=4 --max_iter=4 --quiet=1 \nWARNING: svd.cpp(main:329): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n[training] => [A2]\n[nsv] => [4]\n[nv] => [4]\n[max_iter] => [4]\n[quiet] => [1]\nLoad matrix \nset status to tol\n Number of computed signular values 4\nSingular value 0  2.16097 Error estimate: 2.35435e-16\nSingular value 1  0.97902 Error estimate: 1.06832e-15\nSingular value 2  0.554159 Error estimate: 1.56173e-15\nSingular value 3  9.2673e-65 Error estimate: 6.51074e-16\nLanczos finished 7.68793  \n \n   \n \n Listing the output files: \n \n \n #> ls -lrt \n \n -rw-r--r-- 1 bickson bickson  2728 2012-09-20 01:57 graphchi_metrics.txt \n \n -rw-r--r-- 1 bickson bickson  2847 2012-09-20 01:57 graphchi_metrics.html \n \n -rw-r--r-- 1 bickson bickson  188 2012-09-20 01:57 A2.V.3 \n \n -rw-r--r-- 1 bickson bickson  179 2012-09-20 01:57 A2.V.2 \n \n -rw-r--r-- 1 bickson bickson  179 2012-09-20 01:57 A2.V.1 \n \n -rw-r--r-- 1 bickson bickson  177 2012-09-20 01:57 A2.V.0 \n \n -rw-r--r-- 1 bickson bickson  208 2012-09-20 01:57 A2.U.3 \n \n -rw-r--r-- 1 bickson bickson  195 2012-09-20 01:57 A2.U.2 \n \n -rw-r--r-- 1 bickson bickson  195 2012-09-20 01:57 A2.U.1 \n \n -rw-r--r-- 1 bickson bickson  194 2012-09-20 01:57 A2.U.0 \n \n -rw-r--r-- 1 bickson bickson  271 2012-09-20 01:57 A2.singular_values \n \n \n Verifying solution accuracy in matlab \n \n \n >>A2=mmread('A2'); \n \n >> full(A2)' \n \n \n \n ans = \n \n \n \n  0.8147 0.9058 0.1270 \n \n  0.9134 0.6324 0.0975 \n \n  0.2785 0.5469 0.9575 \n \n  0.9649 0.1576 0.9706 \n \n \n \n Now we read graphchi output using: \n \n \n % read the top 3 singular values: \n \n \n \n >> sigma=mmread('A2.singular_values'); \n \n >> sigma=sigma(1:3); \n \n \n \n Read the top 3 vectors v: \n \n \n \n >> v0=mmread('A2.V.0'); \n \n >> v1=mmread('A2.V.1'); \n \n >> v2=mmread('A2.V.2'); \n \n \n \n Read the top 3 vectors u: \n \n \n \n >> u0=mmread('A2.U.0'); \n \n >> u1=mmread('A2.U.1'); \n \n >> u2=mmread('A2.U.2'); \n \n \n \n \n Compute an approximation to A2: \n \n \n >> [u0 u1 u2] * diag(sigma) * [v0 v1 v2]' \n \n \n \n ans = \n \n \n \n  0.8147 0.9058 0.1270 \n \n  0.9134 0.6324 0.0975 \n \n  0.2785 0.5469 0.9575 \n \n  0.9649 0.1576 0.9706 \n \n \n \n \nAs you can see we got an identical result to A2. \n \n \n \nwhere \n \n \n \n \n >> [u0 u1 u2] \n \n \n \n ans = \n \n \n \n  0.5047 0.5481 0.2737 \n \n  0.4663 0.4726 -0.2139 \n \n  0.4414 -0.4878 0.7115 \n \n  0.5770 -0.4882 -0.6108 \n \n \n \n >> [v0 v1 v2]' \n \n \n \n ans = \n \n \n \n  0.7019 0.5018 0.5055 \n \n  0.2772 0.4613 -0.8428 \n \n -0.6561 0.7317 0.1847 \n \n \n \n >> diag(sigma) \n \n \n \n ans = \n \n \n \n  2.1610   0   0 \n \n   0 0.9790   0 \n \n   0   0 0.5542 \n \n \n \n \n \n Command line arguments  \n  \n \n \n Basic configuration --training Input file name (in sparse matrix market format ) \n \n  --nv Number of inner steps of each iterations. Typically the number should be greater than the number of singular values you look for. \n \n  --nsv Number of singular values requested. Should be typically less than --nv \n  --ortho_repeats Number of repeats on the orthogonalization step. Default is 1 (no repeats). Increase this number for higher accuracy but slower execution. Maximal allowed values is 3. \n \n  --max_iter Number of allowed restarts. The minimum is 2= no restart. \n  --save_vectors=0 \n  Disable saving the factorized matrices U and V to file. On default save_vectors=1. \n  --tol Convergence threshold. For large matrices set this number set this number higher (for example 1e-1, while for small matrices you can set it to 1e-16). As smaller the convergence threshold execution is slower. \n \n \n \n \n \n \n \n Understanding the error measure \n \n \n Following Slepc, the error measure is computed by a combination of: sqrt( ||Av_i - sigma(i) u_i ||_2^2 + ||A^Tu_i - sigma(i) V_i ||_2^2 ) / sigma(i)   Namely, the deviation of the approximation sigma(i) u_i from Av_i , and vice versa.  \n \n \n Scalability \n \n Currently the code was tested with up to 3.5 billion non-zeros on a 24 core machine. Each Lanczos iteration takes about 30 seconds.  \n \n \n Difference to Mahout \n \n Mahout SVD solver is implemented using the same Lanczos algorithm. However, there are several differences 1) In Mahout there are no restarts, so quality of the solution deteriorates very rapidly, after 5-10 iterations the solution is no longer accurate. Running without restarts can be done using our solution with the --max_iter=2 flag. 2) In Mahout there is a single orthonornalization step in each iteration while in our implementation there are two (after computation of u_i and after v_i ). 3) In Mahout there is no error estimation while we provide for each singular value the approximated error. 4) Our solution is typically x100 times faster than Mahout. \n  \n \n Notes about parameter tuning (In case not enough singular vectors have converged): \n \n \nSVD have a few tunable parameters you need to play with. \n \n1) --tol=XX, this is the tolerance. When not enough singular vectors converge to a desired \n \ntolerance you can increase it, for example from 1e-4 to 1e-2, etc. \n \n2) --nv=XX this number should be larger than nsv. Typically you can try 20% more or even larger. \n \n3) --nsv=XX this is the number of the desired singular vectors \n \n4) --max_iter=XX - this is the number of restarts. When the algorithm does not converge you can increase the number of restarts. \n \n \n \n \n \n \nPost processing of the output \n \n \n \nExample 1: Load output in Matlab, for computing recommendations for ALS/SGD/NMF \n \n \na) run ALS  ./toolkits/collaborative_filtering/als --training= smallnetflix_mm --validation=smallnetflix_mme --lambda=0.065 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \nb) download the script mmread.m . \n \nc) # matlab \n \n  >> A=mmread('smallnetflix_mm'); \n \n  >> U=mmread('smallnetflix_mm_U.mm'); \n \n  >> V=mmread('smallnetflix_mm_V.mm'); \n \n \n  >> whos \n \n Name   Size     Bytes Class  Attributes \n \n \n \n A   95526x3561    52799104 double sparse  \n \n U   95526X5    3821040 double    \n \n V   3561X5     142480 double   \n c) compute prediction for user 8 movie 12: \n >> U(8,:)*V(12,:)' \n d) compute approximation error  \n \n \n  >> norm(A-U*V') % may be slow... depending on problem size \n \n \n \nExample 2: Load output in Matlab, for verifying bias-SGD results \na) run the command line: \n ./toolkits/collaborative_filtering/biassgd --training= smallnetflix_mm --validation=smallnetflix_mme --biassgd_lambda=1e-4 --biassgd_gamma=1e-4 --minval=1 --maxval=5 --max_iter=6 --quiet=1 \n \n \nb) download the script mmread.m . \n \n \n \n \n \n \n c) # matlab \n \n >> V=mmread('smallnetflix_mm_V.mm');     % read item matrix V \n \n >> U=mmread('smallnetflix_mm_U.mm');     % read user matrix U \n \n >> m=mmread(' smallnetflix_mm_global_mean.mm ');  % read global mean \n \n >> bV=mmread('smallnetflix_mm_V_bias.mm');   % read user bias \n \n >> bU=mmread('smallnetflix_mm_U_bias.mm');   % read item bias \n \n >> pairs = load('pairs');       % read user/item pairs \n \n \n \n >> A=mmread('smallnetflix_mme');      % read rating matrix \n \n \n >> \n \n >> rmse = 0; \n \n >> for r=1:545177,         % run over each rating \n \n              % compute bias-SGD prediction \n \n    % using the prediction rule: \n \n    % prediction = global_mean + bias_user + bias_item + vector_user*vector_item \n \n pred = m + bU(pairs(r,1)) + bV(pairs(r,2)) + U(pairs(r,1),:)*V(pairs(r,2),:)'; \n \n pred = min( 5, pred );        % truncate prediction [1,5] \n \n pred = max( 1, pred ); \n \n obs = A( pairs(r,1), pairs(r,2) );     \n \n rmse = rmse + (pred - obs).^2;      % compute RMSE by (observation-  \n \n              % prediction)^2  \n \n end    \n \n >>     \n \n >> sqrt( rmse/545177.0 )        % print RMSE \n \n ans = \n \n \n \n \n \n \n \n \n 1.1239           % compare the training RMSE to g  \n \n \n \n \n              % graphchi output \n \n \n \n \n \n \n \n \n \n Computing recommendations \n \nIt is possible to compute recommendations using the rating command. Currently it supports: ALS, sparse-ALS, NMF, SGD. For weighted-ALS use the rating4 command. \n \n \n \nFirst you need to run one of the above methods (ALS, SGD, NMF etc.) . Next, the the command rating (or rating4 for weighted-ALS), for example: \n \n \n bickson@thrust:~/graphchi/toolkits/collaborative_filtering$ ./rating --training=smallnetflix_mm --num_ratings=5 --quiet=1 \n \n WARNING: rating.cpp(main:376): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n \n [training] => [smallnetflix_mm] \n \n [num_ratings] => [5] \n \n [quiet] => [1] \n \n Computing recommendations for user 1 at time: 0.717385 \n \n Computing recommendations for user 40001 at time: 0.730287 \n \n Computing recommendations for user 56001 at time: 0.745666 \n \n Computing recommendations for user 70001 at time: 0.781473 \n \n Computing recommendations for user 72001 at time: 0.789523 \n \n Computing recommendations for user 76001 at time: 0.803709 \n \n Computing recommendations for user 87001 at time: 0.827568 \n \n Computing recommendations for user 91001 at time: 0.838071 \n \n Computing recommendations for user 95001 at time: 0.861837 \n \n Computing recommendations for user 1001 at time: 0.924312 \n \n Computing recommendations for user 2001 at time: 0.98236 \n \n Computing recommendations for user 3001 at time: 1.0347 \n \n Computing recommendations for user 4001 at time: 1.08348 \n \n Computing recommendations for user 5001 at time: 1.12427 \n \n Computing recommendations for user 6001 at time: 1.16263 \n \n Computing recommendations for user 7001 at time: 1.2002 \n \n Computing recommendations for user 8001 at time: 1.23634 \n \n Computing recommendations for user 9001 at time: 1.2701 \n \n Computing recommendations for user 10001 at time: 1.30333 \n \n Computing recommendations for user 11001 at time: 1.33622 \n \n \n \n \n \n \nThe output of the rating algorithm are two files. The first one is more useful. \n \n1) filename.ids - includes recommended item ids for each user. \n \n2) filename.ratings - includes scalar ratings of the top K items \n \n \n \n \n bickson@thrust:~/graphchi/toolkits/collaborative_filtering$ head smallnetflix_mm.ids \n \n %%MatrixMarket matrix array real general \n \n %This file contains item ids matching the ratings. In each row i the top K item ids for user i. \n \n 95526 5 \n \n 3424 1141 1477 2151 2012 \n \n 2784 1900 516 1835 1098 \n \n 1428 3450 2284 2328 58 \n \n 209 1073 3285 60 1271 \n \n 132 1702 2575 1816 2284 \n \n 2787 1816 3024 2514 985 \n \n 3078 375 168 2514 2460 \n \n \n \n \n bickson@thrust:~/graphchi/toolkits/collaborative_filtering$ head smallnetflix_mm.ratings %%MatrixMarket matrix array real general %This file contains user scalar rating. In each row i, K top scalar ratings of different items recommended for user i. 95526 5 7.726248219530e+00 7.321665743778e+00 7.023083603761e+00 7.008616274552e+00 6.670937980807e+00 1.222724647853e+01 1.162004403228e+01 1.144299819709e+01 1.133374751034e+01 1.061483854315e+01 7.497070438026e+00 7.187132667285e+00 6.686989429238e+00 6.550680427186e+00 6.542147872641e+00 1.158861203665e+01 9.885307642785e+00 9.045366124418e+00 8.801333430322e+00 8.713271980918e+00 \n\n \n \n \n \n  Command line arguments  \n \n  \n \n \n Basic configuration --training Input file name (in sparse matrix market format ) for training data \n \n  --num_ratings Number of top items to recommend \n \n  --knn_sample_percent A value between (0,1]. When the dataset is big and there are a lot of user/item pairs it may not be feasible to compute all possible pairs. knn_sample_percent tells the program how many pairs to sample \n  --minval Truncate allowed ratings in range \n \n  --maxval Truncate allowed ratings in range \n  --quiet \n  Less verbose \n   \n \n \n \n \n \n \n \n Limitations:  \n \n The rating command does not support yet all algorithms. Contact me if you like to add additional algorithms like SVD++ etc. \n \n \n Handling implicit ratings \n Implicit rating handles the case where we have only positive examples (for example when a user bought a certain product) but we never have indication when a user DID NOT buy another product. The paper [ Pan, Yunhong Zhou, Bin Cao, Nathan N. Liu, Rajan Lukose, Martin Scholz, and Qiang Yang. 2008. One-Class Collaborative Filtering. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining (ICDM '08). IEEE Computer Society, Washington, DC, USA, 502-511.  ] proposes to add negative examples at random for unobserved user/item pairs. Implicit rating is implemented in the collaborative filtering library and can be used with any of the algorithms explained above.    \n \n Basic configuration --implicitratingtype=1  Adds implicit ratings at random \n \n  --implicitratingpercentage  A number between 1e-8 to 0.8 which determines what is the percentage of edges to add to the sparse model. 0 means none while 1 means fully dense model . \n \n  --implicitratingvalue  The value of the rating added. On default it is zero, but you can change it. \n \n  --implicitratingweight  Weight of the implicit rating (for WALS) OR \n Time of the explicit rating (for tensor algorithms) \n \n \n \n \n \n Example for implicit rating addition: \n \n ./toolkits/collaborative_filtering/sgd --training=smallnetflix_mm --implicitratingtype=1 --implicitratingvalue=-1 --implicitratingpercentage=0.00001 WARNING: sgd.cpp(main:182): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com [training] => [smallnetflix_mm] [implicitratingtype] => [1] [implicitratingvalue] => [-1] [implicitratingpercentage] => [0.00001] INFO:  sharder.hpp(start_preprocessing:164): Started preprocessing: smallnetflix_mm --> smallnetflix_mm.4B.bin.tmp INFO:  io.hpp(convert_matrixmarket:190): Starting to read matrix-market input. Matrix dimensions: 95526 x 3561, non-zeros: 3298163 INFO:  implicit.hpp(add_implicit_edges:71): Going to add: 3401 implicit edges. INFO:  implicit.hpp(add_implicit_edges:79): Finished adding 3401 implicit edges. \n \n... \n \n \n \n \nComputing test predictions \n It is possible to compute test predictions: namely entering a list of user / movie pairs and getting predictions for each item in the list. For creating such a list, create a sparse matrix market format file with the user/movie pair list in each row (and for the unknown prediction put a zero or any other number). \n  \n Here is an example for generating predictions on the user/movie pair list on Netflix data: \n bickson@thrust:~/graphchi$ ./toolkits/collaborative_filtering/biassgd --training=smallnetflix_mm --validation=smallnetflix_mme --test=smallnetflix_mme --quiet=1 WARNING: biassgd.cpp(main:210): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n \n [training] => [smallnetflix_mm] \n \n [validation] => [smallnetflix_mme] \n \n [test] => [smallnetflix_mme] \n \n [ quiet] => [1]  0.726158) \n \n Iteration: 0 Training RMSE: 1.40926 Validation RMSE:  1.1636  1.61145) Iteration: 1 Training RMSE: 1.07647 Validation RMSE: 1.09299   2.536) Iteration: 2 Training RMSE: 1.02413 Validation RMSE: 1.05944  3.41652) Iteration: 3 Training RMSE: 0.996051 Validation RMSE: 1.03869  4.29683) Iteration: 4 Training RMSE: 0.977975 Validation RMSE: 1.02426  5.15537) Iteration: 5 Training RMSE: 0.965243 Validation RMSE: 1.01354 Finished writing 545177 predictions to file: smallnetflix_mme.predict \n \n \n The input user/movie pair list is specified using the --test=filename command. \n The output predictions is found in the file smallnetflix_mme.predictions : \n \n\n bickson@thrust:~/graphchi$ head smallnetflix_mme.predict \n %%MatrixMarket matrix coordinate real general \n %This file contains user/item pair predictions. In each line one prediction. The first column is user id, second column is item id, third column is the computed prediction. \n 95526 3561 545177 \n 135 1 3.6310739 \n 140 1 3.7827248 \n 141 1 3.5731169 \n 154 1 3.9835398 \n 162 1 3.9378759 \n 167 1 3.9865881 \n 169 1 3.6489052 \n 171 1 4.0544691 \n ... \n\n  \n \n  \n  Common errors and their meaning \n \n File not found error: \n \n bickson@thrust:~/graphchi$ ./bin/example_apps/matrix_factorization/als_vertices_inmem file smallnetflix_mm \n INFO:  sharder.hpp(start_preprocessing:164): Started preprocessing: smallnetflix_mm --> smallnetflix_mm.4B.bin.tmp \n ERROR: als.hpp(convert_matrixmarket_for_ALS:153): Could not open file: smallnetflix_mm, error: No such file or directory \n \n \n \nSolution: \n \nInput file is not found, repeat step 5 and verify the file is in the right folder \n \n \n \nEnvironment variable error: \n \n \n bickson@thrust:~/graphchi/bin/example_apps/matrix_factorization$ ./als_vertices_inmem \n \n ERROR: Could not read configuration file: conf/graphchi.local.cnf \n \n Please define environment variable GRAPHCHI_ROOT or run the program from that directory. \n \n \n \n \nSolution: \n \n export GRAPHCHI_ROOT=/path/to/graphchi/folder/ \n \n \n \n \nError: \n \nF ATAL: io.hpp(convert_matrixmarket:169): Failed to read global mean from filesmallnetflix_mm.gm \n \nSolution: remove all temporary files created by the preprocessor, verify you have write permissions to your working folder and try again. \n \n \nAdding fault tolerance \nFor adding fault tolerance, use the command line flag - -load_factors_from_file=1 when continuing any previous run. \n \n \nThe following algos are supported: ALS, WALS, sparse_ALS, tensor_ALS, NMF, SGD, bias-SGD and SVD++. \n \n \n \nHere is an example for bias-SGD. \n \n1) Run a few rounds of the algo: \n \n \n bickson@thrust:~/graphchi$ ./toolkits/collaborative_filtering/biassgd --training=smallnetflix_mm --max_iter=3 --quiet=1 \n \n WARNING: biassgd.cpp(main:210): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n \n [training] => [smallnetflix_mm] \n \n [max_iter] => [3] \n \n [quiet] => [1] \n \n  1.5052) Iteration: 0 Training RMSE: 1.40926 Validation RMSE:  1.1636 \n \n 3.30333) Iteration: 1 Training RMSE: 1.07647 Validation RMSE: 1.09299 \n \n 5.28362) Iteration: 2 Training RMSE: 1.02413 Validation RMSE: 1.05944 \n \n \n \n === REPORT FOR biassgd-inmemory-factors() === \n \n .. \n \n \n \n2) Now continue from the same run, after the 3 iterations: \n \n \n \n bickson@thrust:~/graphchi$ ./toolkits/collaborative_filtering/biassgd --training=smallnetflix_mm --max_iter=3 --quiet=1 --load_factors_from_file=1 \n \n WARNING: biassgd.cpp(main:210): GraphChi Collaborative filtering library is written by Danny Bickson (c). Send any comments or bug reports to danny.bickson@gmail.com \n \n [training] => [smallnetflix_mm] \n \n [max_iter] => [3] \n \n [quiet] => [1] \n \n [load_factors_from_file] => [1] \n \n 2.63894) Iteration: 0 Training RMSE: 0.996053 Validation RMSE: 1.03869 \n \n 4.07894) Iteration: 1 Training RMSE: 0.977975 Validation RMSE: 1.02427 \n \n  5.6297) Iteration: 2 Training RMSE: 0.965245 Validation RMSE: 1.01355 \n \n .. \n \n \n \n \n \n \nAs you can see the second runs, starts from the state of the first run. \n \n \n \n \nItem based similarity methods \n \n \nItem based similarity methods documentation is found here . \n \n \nAcknowledgements \n \n  Liang Xiong, CMU for providing the Matlab code of BPTF, numerous discussions and infinite support!! Thanks!! \n  Timmy Wilson, Smarttypes.org for providing twitter network snapshot example, and Python scripts for reading the output. Sanmi Koyejo , from the University of Austin, Texas, for providing Python scripts for preparing the inputs. Dan Brickely, from VU University Amsertdam , for helping debugging installation and prepare the input in Octave. Nicholas Ampazis, University of the Aegean , for providing his SVD++ source ode. Yehuda Koren, Yahoo! Research , for providing his SVD++ source code implementation. Marinka Zitnik, University of Ljubljana, Slovenia , for helping debugging ALS and suggesting NMF algos to implement. Joel Welling from Pittsburgh Supercomputing Center , for optimizing GraphLab on BlackLight supercomputer and simplifying installation procedure. Sagar Soni from Gujarat Technological University and Hasmukh Goswami College of Engineering for helping testing the code. Young Cha, UCLA for testing the code. Mohit Singh for helping improve documentation. Nicholas Kolegraff for testing our examples. Theo Throuillon, Ecole Nationale Superieure d'Informatique et de Mathematiques Appliquees de Grenoble for debugging NMF. Qiang Yan, Chinese Academy of Science for providing time-svd++, bias-SVD, RBM and LIBFM code that the Graphlab/GraphChi version is based on. Ramakrishnan Kannan, Georgia Tech , for helping debugging and simplifying usage. Charles Martin, GLG , for debugging NMF. \n Izhar Wallach, University of Toronoto , for debugging bias-SGD. \n Aleksandr Krushnyakov, Moscow State University, for debugging GraphCHi CF package \n Alessandro Vitale, Optimist AI SRL, for detecting a bug in item based cf \n Jacob Kesinger, quid.com , for reporting a bug in item based cf"], "link": "http://bickson.blogspot.com/feeds/2689797507305873095/comments/default", "bloglinks": {}, "links": {"http://www.smarttypes.org/": 1, "http://3.blogspot.com/": 1, "http://www.blogger.com/blog": 2, "https://sites.google.com/": 1, "http://labs.aegean.gr/": 1, "http://hgce.org/": 1, "http://cn.linkedin.com/": 1, "http://staff.psc.edu/": 1, "http://www.upv.es/": 1, "http://quid.com/": 1, "http://www.notube.tv/": 1, "http://bickson.co.il/": 6, "http://select.cmu.edu/": 15, "http://sagarksoni.blogspot.com/": 1, "http://smallnetflix_mm_global_mean.mm/": 1, "http://www.utexas.edu/": 1, "https://code.google.com/": 2, "http://www.linkedin.com/": 4, "http://www.toronto.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://www.cmu.edu/": 1, "http://www.autonlab.com/": 1, "http://gtu.ac.in/": 1, "http://marinka.zitnik.si/": 1, "http://research.yahoo.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Here is a quick explanation on how to install GraphLab 2.1 on Mountain Lion. \n \n \n \n1) Install XCODE 4.4 \n \n2) After XCODE is installed, go to XCode-> Preferences->Downloads \n \nand install \"command line tools\". \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n3) Install mercurial . \n \n4) Go to Graphlab download page and follow the link to the mercurial code page . \n \n Open a new terminal and use the command to checkout the code: \n \n  hg clone https://... \n \n5) cd graphlabapi \n \n ./configure \n \n \n6) For compiling the collaborative filtering library: \n cd release/toolkits/collaborative_filtering/ \n make -j4"], "link": "http://bickson.blogspot.com/feeds/974610541097577411/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://graphlab.org/": 1, "http://feedads.doubleclick.net/": 2, "http://mercurial.selenic.com/": 1, "http://code.google.com/": 1, "https://developer.apple.com/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Just learned a new thing from my mega collaborator Justin Yan . The Harry Potter effect is \na term in psychology for a phenomena which is very popular (everyone likes Harry Potter). \n \nWhen computing recommendations (for example movies) we can compare movie similarity \nto recommend to the user similar movies to the one she saw. The problem, when we have an item which is liked by everyone, it \"covers\" any other subset of the data, and thus it will have the greatest similarity to all other items. For example, the movie Tarzan was watched 10 times. Harry Potter was watched 1,000,000 times. Every single time a viewer watched Tarzan, she watched also Harry Potter! So Harry Potter is the most similar movie to Tarzan. And to all the other movies as well... \n \nTo prevent this Harry Potter effect, we normalize by the total number of ratings in the data. Thus if we had an overlap of 10 watches of Tarzan with Harry Potter, we divide it to 1,000,000 occurrences of Harry Potter ratings, and thus we diminish the \"Harry Potter\" effect."], "link": "http://bickson.blogspot.com/feeds/3152798600125882008/comments/default", "bloglinks": {}, "links": {"http://cn.linkedin.com/": 1, "http://www.psychologytoday.com/blog": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Large Scale Machine Learning and Other Animals"}, {"content": ["Another new thing I learned from Brad Cox , Rapid Miner software seemed to be heavily used in the homeland security / defense sectors. \n \n Here is a short introductory video.\n \n \nIt seems it does not scale well to large models but have an excellent UI which helps visualize the data. \n \nAs a follow up, I got this from Dave Laxer: \n \n \nDid you know about RADoop? RADoop = RapidMiner + Hadoop. \n \n http://www.radoop.eu/ \n \n \n \nIt seems it is an effort to scale rapid miner to larger models. \n \nSlightly related, I got a link to Myrrix from Charles Martin . It seems to be a recommendation engine on top of Mahout headed by Sean Owen ."], "link": "http://bickson.blogspot.com/feeds/4513293132891035306/comments/default", "bloglinks": {}, "links": {"http://myrrix.com/": 1, "http://www.blogger.com/": 1, "http://www.linkedin.com/": 2, "http://feedads.doubleclick.net/": 2, "http://rapid-i.com/": 1, "http://www.radoop.eu/": 1}, "blogtitle": "Large Scale Machine Learning and Other Animals"}]