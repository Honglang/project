[{"blogurl": "http://www.andrewgelman.com\n", "blogroll": [], "title": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Bayesian inference, conditional on the model and data, conforms to the likelihood principle. But there is more to Bayesian methods than Bayesian inference. See chapters 6 and 7 of Bayesian Data Analysis for much discussion of this point. \n It saddens me to see that people are still confused on this issue."], "link": "http://andrewgelman.com/2012/10/it-not-necessary-that-bayesian-methods-conform-to-the-likelihood-principle/", "bloglinks": {}, "links": {"http://errorstatistics.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["I\u2019m sorry I don\u2019t have any new zombie papers in time for Halloween. Instead I\u2019d like to be a little monster by reproducing a mini-rant from this article on experimental reasoning in social science: \n I will restrict my discussion to social science examples. Social scientists are often tempted to illustrate their ideas with examples from medical research. When it comes to medicine, though, we are, with rare exceptions, at best ignorant laypersons (in my case, not even reaching that level), and it is my impression that by reaching for medical analogies we are implicitly trying to borrow some of the scientific and cultural authority of that field for our own purposes. Evidence-based medicine is the subject of a large literature of its own (see, for example, Lau, Ioannidis, and Schmid, 1998)."], "link": "http://andrewgelman.com/2012/10/social-scientists-who-use-medical-analogies-to-explain-causal-inference-are-i-think-implicitly-trying-to-borrow-some-of-the-scientific-and-cultural-authority-of-that-field-for-our-own-purposes/", "bloglinks": {}, "links": {"http://www.columbia.edu/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Jay Livingston writes : \n I know that in art, quality and value are two very different things. Still, I had to stop and wonder when I read about \n Domenico and Eleanore De Sole, who in 2004 paid $8.3 million for a painting attributed to Mark Rothko that they now say is a worthless fake. \n One day a painting is worth $8.3 million; the next day, the same painting \u2013 same quality, same capacity to give aesthetic pleasure or do whatever it is that art does \u2013 is \u201cworthless.\u201d* Art forgery also makes me wonder about the buyer\u2019s motive. If the buyer wanted only to have and to gaze upon something beautiful, something with artistic merit, then a fake Rothko is no different than a real Rothko. It seems more likely that what the buyer wants is to own something valuable \u2013 i.e., something that costs a lot. Displaying your brokerage account statements is just too crude and obvious. What the high-end art market offers is a kind of money laundering. Objects that are rare and therefore expensive, like a real Rothko, transform money into something more acceptable \u2013 personal qualities like good taste, refinement, and sophistication. \n I\u2019m in sympathy with Livingston\u2019s general point\u2014I too am happy to mock people who happen to have more money than I do\u2014and Rothko\u2019s art has always seemed pretty pointless to me. I mean, sure, it can look fine on the wall, but it hardly seems like something special to me. \n But I think Livingston\u2019s going too far, in that he\u2019s forgetting the natural human desire not to get ripped off. \n Let\u2019s set Rothko aside and consider something I really want: a 10-hour clock: \n I\u2019m interested in this not because I\u2019m some sort of French-revolution buff but just because I love clocks. We have a 24-hour clock, a backwards clock (I took a regular old-style AC wall clock and flipped around a plastic gear), and a clock where the big hand does the hours and the little hand does the minutes (which I made by sawing off the end of the big hand and gluing it onto the end of the little hand; amazingly enough, it looks just fine, you don\u2019t notice the join at all), and a neon (actually, one of those other gases, the green one, argon maybe?) diner-style clock that says Probability on the top and Statistics on the bottom. So when I saw the 10-hours-a-day, 100-minutes-an-hour, 100-seconds-a-minute clock in the Muse\u00e9 des Arts et M\u00e9tiers, I had to have it. \n  \n Malheureusement, there aren\u2019t a lot of these clocks floating around, and they cost a lot. But suppose I find a beat-up one of these and decide to plunk down $10,000 for it and proudly place it on my wall, partly for the joy of having a 10-hour clock to look at, and partly for the thrill of having this old object. Then some art expert comes by our apartment and tells me it\u2019s a fake. Damn right I\u2019d be mad! Not because the clock is \u201ca form of money laundering\u201d but because somebody ripped me off."], "link": "http://andrewgelman.com/2012/10/real-rothko-fake-rothko/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://montclairsoci.blogspot.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["The Journal of the Royal Statistical Society publishes papers followed by discussions. Lots of discussions, each can be no more than 400 words. Here\u2019s my most recent discussion: \n The authors are working on an important applied problem and I have no reason to doubt that their approach is a step forward beyond diagnostic criteria based on point estimation. An attempt at an accurate assessment of variation is important not just for statistical reasons but also because scientists have the duty to convey their uncertainty to the larger world. I am thinking, for example, of discredited claims such as that of the mathematician who claimed to predict divorces with 93% accuracy (Abraham, 2010). \n Regarding the paper at hand, I thought I would try an experiment in comment-writing. My usual practice is to read the graphs and then go back and clarify any questions through the text. So, very quickly: I would prefer Figure 1 to be displayed in terms of standard deviations, not variances. I find variances difficult to interpret, and I\u2019m always taking mental square roots (0.09 is 0.3 squared, and so forth). Figure 3 is appealing but I don\u2019t like the visual emphasis of the endpoints of the 95% intervals. From a Bayesian standpoint, there is nothing special about the 2.5th and 97.5th percentiles of the posterior distribution, and I think it goes against the spirit of the article to emphasize these arbitrary endpoints. I also think that, with some care, the graphs in Figures 3, 4, and 5 could be compactly re-expressed to show comparisons more effectively (as in Gelman, Pasarica, and Dodhia, 2002). Tables 2 and 3 I think are useless: why should a reader care that the 10th percentile point of the distribution for a particular probability os 0.164 or whatever? Again, this seems to me to contradict the decision-analytic focus of the applied research. \n These brusque comments on display may seem peripheral but to me they are important. Communication is a central task of statistics, and ideally a state-of-the-art data analysis can have state-of-the-art displays to match. \n References \n Abraham, Laurie (2010). Can you really predict the success of a marriage in 15 minutes? Slate, 8 March. http://www.slate.com/articles/double_x/doublex/2010/03/can_you_really_predict_the_success_of_a_marriage_in_15_minutes.html \n Gelman, Andrew, Pasarica, C., and Dodhia, R. (2002). Let\u2019s practice what we preach: turning tables into graphs. American Statistician 56, 121-130."], "link": "http://andrewgelman.com/2012/10/communication-is-a-central-task-of-statistics-and-ideally-a-state-of-the-art-data-analysis-can-have-state-of-the-art-displays-to-match/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Charlie Saunders writes: \n A study has recently been published in the New England Journal of Medicine (NEJM) which uses survival analysis to examine long-acting reversible contraception (e.g. intrauterine devices [IUDs]) vs. short-term commonly prescribed methods of contraception (e.g. oral contraceptive pills) on unintended pregnancies. \n The authors use a convenience sample of over 7,000 women. I am not well versed-enough in sampling theory to determine the appropriateness of this but it would seem that the use of a non-probability sampling would be a significant drawback. If you could give me your opinion on this, I would appreciate it. \n The NEJM is one of the top medical journals in the country. Could this type of sampling method coupled with this method of analysis be published in a journal like JASA? \n My reply: There are two concerns, first that it is a convenience sample and thus not representative of the population, and second that the treatments are chosen rather than randomly assigned, hence there will be pre-treatment differences between the groups. That said, perhaps this descriptive information is valuable. From a statistical perspective, the strengths of the study are realism of the conditions and accuracy of the measurements."], "link": "http://andrewgelman.com/2012/10/a-convenience-sample-and-selected-treatments/", "bloglinks": {}, "links": {"http://www.nejm.org/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Sunday, October 28, 2012, 4:00 pm, Larchmont Public Library, 121 Larchmont Ave, Larchmont, NY . \n I\u2019m picturing a roomful of people like this: \n \n On the other hand, when I spoke at the New Jersey Institute of Technology in Newark a few years ago, it was nothing like the Sopranos. . . ."], "link": "http://andrewgelman.com/2012/10/my-talk-at-the-larchmont-public-library-this-sunday/", "bloglinks": {}, "links": {"http://larchmont.patch.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Oof!"], "link": "http://andrewgelman.com/2012/10/steven-levitt-says-that-he-has-a-good-indicator-that-aaron-edlin-noah-kaplan-nate-silver-and-i-are-not-so-smart/", "bloglinks": {}, "links": {"http://themonkeycage.org/blog": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["In an article provocatively entitled, \u201cWill Ohio State\u2019s football team decide who wins the White House?\u201d, Tyler Cowen and Kevin Grier report : \n It is statistically possible that the outcome of a handful of college football games in the right battleground states could determine the race for the White House. \n Economists Andrew Healy, Neil Malhotra, and Cecilia Mo make this argument in a fascinating article in the Proceedings of the National Academy of Science. They examined whether the outcomes of college football games on the eve of elections for presidents, senators, and governors affected the choices voters made. They found that a win by the local team, in the week before an election, raises the vote going to the incumbent by around 1.5 percentage points. When it comes to the 20 highest attendance teams\u2014big athletic programs like the University of Michigan, Oklahoma, and Southern Cal\u2014a victory on the eve of an election pushes the vote for the incumbent up by 3 percentage points. That\u2019s a lot of votes, certainly more than the margin of victory in a tight race. \n I took a look at the study (I felt obliged to, as it combined two of my interests) and it seemed reasonable to me. There certainly could be some big selection bias going on that the authors (and I) didn\u2019t think of, but I saw no obvious problems. So for now I\u2019ll take their result at face value and will assume a 2 percentage-point effect. I\u2019ll assume that this would be +1% for the incumbent party and -1% for the other party, I assume. \n I agree with Cowen and Grier that this sort of pattern among voters is disturbing, similar to Chris Achen and Larry Bartels\u2019s famous study of the electoral effects of shark attacks. \n Not as bad as it sounds \n That said, I\u2019d like to defend democracy a bit and argue that the college football effect isn\u2019t quite as bad as Cowen and Grier imply. Here\u2019s what they say : \n The key to victory could come down to . . . Florida, Ohio, and Virginia. On Oct. 27th, a little more than a week before the election, the Ohio State Buckeyes have a big football game against Penn State. The University of Florida Gators have a huge match up against the University of Georgia Bulldogs. If the election remains razor close, these games in these two key battleground states could affect who sits in the White House for the next four years. Can you imagine Ohio State head coach Urban Meyer getting a late night call from the Obama campaign suggesting a particular blitz package? Or maybe Romney has some advice for how the Gators can bottle up Georgia\u2019s running game. The decision of whether to punt or go for it on that crucial fourth down could affect the job prospects of more than just the football team\u2019s coaching staff. \n Unless I\u2019m misunderstanding something, I think Romney\u2019s staff should be giving advice to the Bulldogs, not the Gators (unless Cowen and Grier are subtly riffing last month\u2019s \u201chapless Romney\u201d meme to imply that the candidate\u2019s best chance to help the Florida team lose is to give them advice). Details aside, though, I don\u2019t think this adds up to so much. \n I have two reasons for making this argument, even conditional on assuming that a local win really does count for 2 percentage points of the vote. My reasons are: \n 1. Locality. My quick reading of the Healy, Malhotra, and Mo is that all their analysis is at the county level. Ohio State University is in Columbus, Ohio, which is in Franklin county, which according to Wiki has 1.2 million people. 1.2 million is a lot, but it\u2019s only 10% of the population of the state. So a difference of +/- 1 percentage points in Franklin county corresponds to only +/- 0.1 percentage points in the state of Ohio. An increase or decrease of 0.1 percentage points isn\u2019t nothing, but even swing states are unlikely to be divided that closely. (Just to calibrate, 0.1% of 5 million votes is 5000 votes, which is very close for a state election.) \n As for Florida . . . the Gators are in Gainesville, in Alachua county, population 250,000, that\u2019s less than 1.5% of the population of the state. OK, maybe there\u2019s some influence on other nearby counties but then you have to be careful, as you\u2019re going beyond the bounds of the research study, also one might expect the effect to decline as you move away from the location of the game. \n 2. Averaging. The article under discussion includes two Ohio teams\u2014Ohio State and Cincinnati. According to the schedule, the Bearcats are playing Louisville this Friday and Syracuse the Saturday after that. And, after to their upcoming Penn State game, the Buckeyes are playing Illinois before the election. So these are 4 Ohio games, not just one. Florida has three teams (Florida, Florida State, and Miami). Even lowly Virginia has two teams in the list: Virginia and Virginia Tech. And here are a few more football teams in swing states: Colorado, Iowa, and Iowa State. \n There are multiple games in multiple weeks in several states, each of which, according to the analysis, operates on the county level and would have at most a 0.2% effect in any state. So there\u2019s no reason to believe that any single game would have a big effect, and any effects there are would be averaged over many games. \n In summary, it is indeed disturbing that people are more likely to vote for the incumbent party if their local team wins\u2014sure, 2% isn\u2019t much, but it\u2019s a nontrivial proportion of the undecided voters. But the claim, \u201cIt is statistically possible that the outcome of a handful of college football games in the right battleground states could determine the race for the White House,\u201d while literally true (if the election happens to be extremely close) is overstated. \n P.S. In addition, Avi suggests that the electoral effect of a football game is likely to be smaller in an intense election such as Ohio 2012, compared to an average election in the dataset."], "link": "http://andrewgelman.com/2012/10/college-football-voting-and-the-law-of-large-numbers/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.slate.com/": 1, "http://www.pnas.org/": 1, "http://marginalrevolution.com/": 1, "http://www.princeton.edu/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Lee Seachrest points to an article , \u201cLife expectancy and disparity: an international comparison of life table data,\u201d by James Vaupel, Zhen Zhang, and Alyson van Raalte. This paper has killer graphs. Here are their results: \n In 89 of the 170 years from 1840 to 2009, the country with the highest male life expectancy also had the lowest male life disparity. This was true in 86 years for female life expectancy and disparity. In all years, the top several life expectancy leaders were also the top life disparity leaders. Although only 38% of deaths were premature, fully 84% of the increase in life expectancy resulted from averting premature deaths. The reduction in life disparity resulted from reductions in early-life disparity, that is, disparity caused by premature deaths; late-life disparity levels remained roughly constant. \n  \n The authors also note: \n Reducing early-life disparities helps people plan their less-uncertain lifetimes. A higher likelihood of surviving to old age makes savings more worthwhile, raises the value of individual and public investments in education and training, and increases the prevalence of long-term relationships. Hence, healthy longevity is a prime driver of a country\u2019s wealth and well-being. While some degree of income inequality might create incentives to work harder, premature deaths bring little benefit and impose major costs. \n They also write something that puzzles me: \n Russia, the USA and other laggards can learn much from research on the reasons why various countries (including Japan, France, Italy, Spain, Sweden and Switzerland) have been more successful in reducing premature deaths. The reasons involve healthcare, social policies, personal behaviour (especially cigarette smoking and alcohol abuse), and the safety and salubriousness of the environment. \n I don\u2019t know much about salubriousness, but I thought the smoking rate among men is lower in the U.S. than in Japan and much of Europe, and maybe we have less alcohol abuse here too. So I\u2019m not sure how the above paragraph can make sense. \n \nSechrest also sent along this article , \u201cDifferences in life expectancy due to race and educational differences are widening, and many may not catch up,\u201d by S. Jay Olshansky and 14 (!) others: \n It has long been known that despite well-documented improvements in longevity for most Americans, alarming disparities persist among racial groups and between the well-educated and those with less education. . . . in 2008 US adult men and women with fewer than twelve years of education had life expectancies not much better than those of all adults in the 1950s and 1960s. When race and education are combined, the disparity is even more striking. In 2008 white US men and women with 16 years or more of schooling had life expectancies far greater than black Americans with fewer than 12 years of education\u201414.2 years more for white men than black men, and 10.3 years more for white women than black women. These gaps have widened over time and have led to at least two \u201cAmericas,\u201d if not multiple others, in terms of life expectancy, demarcated by level of education and racial-group membership. \n That\u2019s pretty scary! But I\u2019m suspicious of the causal reasoning that leads to the final sentence in their abstract: \n The message for policy makers is clear: implement educational enhancements at young, middle, and older ages for people of all races, to reduce the large gap in health and longevity that persists today."], "link": "http://andrewgelman.com/2012/10/health-disparities-are-associated-with-low-life-expectancy/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://bmjopen.bmj.com/": 1, "http://content.healthaffairs.org/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["A few years ago I suggested a research project to study how Americans define themselves in terms of regional identity. For example, if you grew up in South Dakota but live in Washington, D.C., do you you call yourself a midwesterner, a westerner, a southerner, or what? The analogy is to the paper by Michael Hout on \u201cHow 4 million Irish immigrants became 40 million Irish Americans.\u201d Contrary to expectations, it wasn\u2019t about prolific breeding, it was about how people of mixed background choose to classify themselves."], "link": "http://andrewgelman.com/2012/10/hey-has-anybody-done-this-study-yet/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["(1) Hop the Q-Train ! That is, the Columbia/NYU Quantitative Training Program, funded by the Institute of Education Sciences to create a cohort of postdoctoral scholars both to develop the new statistical methods required to meet future research challenges and to effectively train and consult with other education researchers. You\u2019ll be working with Jennifer Hill, Marc Scott, and me on our exciting research projects, some of which are here ! Candidates must be United States citizens or permanent residents. For best consideration applications must be submitted before 15 Nov 2012. Please direct administrative inquiries to Jonathan Winters at jonathan.winters@nyu.edu and substantive inquiries to Jennifer or me. \n  \n (2) The Earth Institute Postdoctoral Fellows program ! Every year a select group of recent Ph.D.s in a variety of fields come to the Earth Institute for this two-year research fellowship. I\u2019d love to see more statisticians applying. To apply, candidates must complete the online application and submit a proposal for research that would contribute to the goal of global sustainable development. In addition to submitting the application and proposal, candidates are encouraged to identify and contact their desired multidisciplinary mentoring team: two or more senior faculty members or research scientists/scholars at Columbia University with whom they would like to work during their appointment. We\u2019ve had two statisticians in this program in the past (Kenny Shirley and Leontine Alkema) and they fit in really well with us and with the EI group. If you\u2019re interested, just contact me directly. And this one has a hard deadline of 15 Nov 2012."], "link": "http://andrewgelman.com/2012/10/two-postdoc-opportunities-to-work-with-our-research-group-apply-by-15-nov-2012/", "bloglinks": {}, "links": {"http://steinhardt.nyu.edu/": 2, "http://www.columbia.edu/": 1, "http://andrewgelman.com/": 2}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["The other day we had a fun little discussion in the comments section of the sister blog about the appropriateness of stating forecast probabilities to the nearest tenth of a percentage point. \n It started when Josh Tucker posted this graph from Nate Silver : \n  \n My first reaction was: this looks pretty but it\u2019s hyper-precise. I\u2019m a big fan of Nate\u2019s work, but all those little wiggles on the graph can\u2019t really mean anything. And what could it possibly mean to compute this probability to that level of precision? \n In the comments, people came at me from two directions. From one side, Jeffrey Friedman expressed a hard core attitude that it\u2019s meaningless to give a probability forecast of a unique event: \n What could it possibly mean, period, given that this election will never be repeated? . . . I know there\u2019s a vast literature on this, but I\u2019m still curious, as a non-statistician, what it could mean for there to be a meaningful 65% probability (as opposed to a non-quantifiable likelihood) that a one-time outcome will occur. If 65.7 is too precise, why isn\u2019t 65? My [Friedman] hypothesis is that we\u2019re trying to homogenize inherently heterogenous political events to make them tractable to statistical analysis. But presidential elections are not identical balls drawn randomly from an urn, and they are not lottery numbers randomly picked by a computer. \n This one was in my wheelhouse, and I responded: \n Probabilities can have a lot of meaning, even for an event that will never be repeated. For example, suppose you want to make a decision where the outcome is contingent on who wins the election. It can make sense to quantify your uncertainty using probability. Neumann and Morgenstern wrote a book about this! But at some point the quantification becomes meaningless. \u201c60%,\u201d sure. \u201c65%,\u201d maybe. \u201c65.7%,\u201d no way. . . . The different events being analyzed (in this case, elections) are not modeled as identical balls drawn from an urn. A better analogy you might keep in your mind is business forecasting. You might have some uncertainty about the price of oil next year, or whether the price will exceed $X a barrel. It\u2019s an uncertain event but you know something about it. Then you get some information, for example a new oil field is discovered or a new refinery somewhere is built. This changes your probability. A number such as 65% could express this. Similarly, I might describe someone as being 5 feet 8 inches tall, or even 5 feet 8 1/2 inches tall, but it would be silly to call him 5 feet 8.34 inches tall, given that his height changes by a large fraction of an inch during the day. \n From the other direction, commenter Paul wrote: \n I disagree with Andrew that 65.7% is too precise. One reason is that intrade prices are quoted to 3 digit precision \u2014 viz., ranging from $0.00 to $10.00, with bid-ask spreads as low as 1 cent. So, for example, if I believed Nate\u2019s formula was more accurate than Intrade, and the current quote was $6.56 bid, $6.57 ask, I would like Nate to provide more precision in order to determine whether or not to buy \u2018Barack Obama to be re-elected on 2012\u2032. Even with today\u2019s low interest rates, I would still need Nate to forecast at least a 65.74% probability in order to believe purchasing \u2018Obama 2012\u2032 at $6.57 would outperform a 0.90% money market rate over the next 18 days. \n Paul makes a good point about market pricing. I can see why Intrade would want that precision. But I can\u2019t see the point of Nate giving probabilities to that level of precision, given that he\u2019s not working for Intrade or for a trader. Or, to put it another way, I can can see why Nate might want to report fractional percentage-point probabilities on the New York Times website: more detail means more fluctuations which means more news. (In the above image, that 65.7% was listed as \u201c-2.2 since Oct 10.\u201d I don\u2019t think this \u201c-2.2\u2033 means very much but it represents a change, i.e. news.) But from a statistical standpoint I don\u2019t see the value. \n Crunching the numbers \n Let\u2019s do a quick calibration. Currently Nate gives Obama a 67.6% change of winning, with a 50.0% to 48.9% lead in the popular vote. That\u2019s a 50.55% share of the 2-party vote. Nate\u2019s page doesn\u2019t give a standard error, but let\u2019s suppose that his forecast for Obama\u2019s popular vote share is a normal distribution with mean 50.55% and standard deviation 1.5%. That is, there\u2019s a 95% chance that Obama will get within 47.5% and 53.5% of the 2-party vote. That seems in the right ballpark to me. Then the probability Obama will win the popular vote is pnorm((50.55-50)/1.5) = 0.643. Not quite Nate\u2019s 65.7%; I can attribute the difference to the electoral college. \n This is getting interesting. A big lead in the probability (65%-35%) corresponds to a liny lead in the vote (50.5%-49.5%). Now suppose that our popular vote forecast is off by one-tenth of a percentage point. Given all our uncertainties, it would seem pretty ridiculous to claim we could forecast to that precision anyway, right? If we bump Obama\u2019s predicted 2-party vote share up to 50.65%, we get a probability Obama wins of pnorm((50.65-50)/1.5) = 0.668. If we ratchet Obama\u2019s expected vote share down to 50.45%, his probability of winning goes down to pnorm((50.45-50)/1.5) = 0.612. \n Thus, a shift of 0.1% in Obama\u2019s expected vote share corresponds to a change of 2.5 percentage points in his probability of winning. \n Now let\u2019s do it the other way. If Obama\u2019s expected vote share is 50.65%, his probability of winning is 0.6676 (keeping that extra digit to avoid roundoff issues). If his probability of winning goes up by 0.1 percentage points, then his expected percentage of the two-party vote must be qnorm(0.6686,50,1.5) = 50.654. That\u2019s right: a change in 0.1 of win probability corresponds to a 0.004 percentage point share of the two-party vote. I can\u2019t see that it can possibly make sense to imagine an election forecast with that level of precision. Even multiplying everything by ten\u2014specifying win probabilities to the nearest percentage point\u2014corresponds to specifying expected vote shares to within 0.04% of the vote, which remains ridiculous. \n Really, I think it would be just fine to specify win probabilities to the nearest 10%, which will register shifts of 0.4% in expected vote share. Probabilities to the nearest 10%: if it\u2019s good enough for the National Weather Service, it\u2019s good enough for me. \n P.S. Just to emphasize: I think Nate\u2019s great, and I can understand the reasons (in terms of generating news and getting eyeballs on the webpage) that he gives probabilities such as \u201c65.7%.\u201d I just don\u2019t think they make sense from a statistical point of view, any more than it would make sense to describe a person as 5 feet 8.34 inches tall. Nate\u2019s in a tough position: on one hand, once you have a national and state-level forecast, there\u2019s not much you can say, day-to-day or week-to-week. On the other hand, people want news, hence the pressure to report essentially meaningless statistics such as a change in probability from 65.7% to 67.6%, etc. \n P.P.S. I think the above calculations are essentially valid even though Nate\u2019s forecast is at a state-by-state level. See my comment here . \n To see this in another way, imagine that your forecast uncertainty about the election is summarized by 1000 simulations of the election outcome, that is, a 1000 x 51 matrix of simulated vote shares by state. If Pr(Obama wins) = 0.657, this corresponds to 657 out of 1000 simulations adding up to an Obama win. Now suppose there is a 1% shift in win probability, then this bumps 657 up to 667. What shift in the vote would carry just 10 out of 1000 simulations over the bar? Given that vote swings are largely national, it will come to approximately 0.04% (that is, 4 hundredths of a percentage point)."], "link": "http://andrewgelman.com/2012/10/is-it-meaningful-to-talk-about-a-probability-of-65-7-that-obama-will-win-the-election/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 2, "http://fivethirtyeight.nytimes.com": 1, "http://themonkeycage.org/blog": 3}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["As we get more data, we can fit more model. But at some point we become so overwhelmed by data that, for computational reasons, we can barely do anything at all. Thus, the curve above could be thought of as the product of two curves: a steadily increasing curve showing the statistical ability to fit more complex models with more data, and a steadily decreasing curve showing the computational feasibility of doing so."], "link": "http://andrewgelman.com/2012/10/model-complexity-as-a-function-of-sample-size/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["We have lots of models for overdispersed count data but we rarely see underdispersed data. But now I know what example I\u2019ll be giving when this next comes up in class. From a book review by Theo Tait: \n A number of shark species go in for oophagy, or uterine cannibalism. Sand tiger foetuses \u2018eat each other in utero, acting out the harshest form of sibling rivalry imaginable\u2019. Only two babies emerge, one from each of the mother shark\u2019s uteruses: the survivors have eaten everything else. \u2018A female sand tiger gives birth to a baby that\u2019s already a metre long and an experienced killer,\u2019 explains Demian Chapman, an expert on the subject. \n That\u2019s what I call underdispersion. E(y)=2, var(y)=0. Take that, M. Poisson!"], "link": "http://andrewgelman.com/2012/10/a-statistical-model-for-underdispersion/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.co.uk/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Mark Johnstone writes: \n I\u2019ve recently been investigating a new European Court of Justice ruling on insurance calculations (on behalf of MoneySuperMarket) and I found something related to statistics that caught my attention. . . . The ruling (which comes into effect in December 2012) states that insurers in Europe can no longer provide different premiums based on gender. Despite the fact that women are statistically safer drivers, unless it\u2019s biologically proven there is a causal relationship between being female and being a safer driver, this is now seen as an act of discrimination (more on this from the Wall Street Journal). \n However, where do you stop with this? What about age? What about other factors? And what does this mean for the application of statistics in general? Is it inherently unjust in this context? \n One proposal has been to fit \u2018black boxes\u2019 into cars so more individual data can be collected, as opposed to relying heavily on aggregates. \n For fans of data and statistics, the law poses some interesting challenges. And I\u2019d love to see somebody digging into this further from a statistical point-of-view. \n I don\u2019t have much to add here, beyond the usual Bayesian point that, if we have enough data on individuals, this will be more important than average rates, and also the usual political point that good information might not get used if the rulemakers have particular sympathy for unsafe drivers."], "link": "http://andrewgelman.com/2012/10/statistical-discrimination-again/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["David Pennock writes: \n http://PredictWiseQ.com is our (beta) prediction contest which aims to estimate not just the marginal probabilities of election outcomes this November, but millions of correlations among outcomes as well, like the chance Obama will win both Ohio and Florida, or the chance Romney will win if the September jobs numbers are negative. It\u2019s a working example of a combinatorial prediction market design we published this summer in the conference ACM EC\u201912. \n And here\u2019s Pennock\u2019s blog, which supplies more background."], "link": "http://andrewgelman.com/2012/10/in-a-sense-predictwiseq-is-intrade-to-the-57th-power/", "bloglinks": {}, "links": {"http://blog.oddhead.com/": 1, "http://PredictWiseQ.com": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Andrew Perrin nails it : \n Twice a year, like clockwork, the ethics cops at the IRB [institutional review board, the group on campus that has to approve research involving human subjects] take a break from deciding whether or not radioactive isotopes can be administered to prison populations to cure restless-leg syndrome to dream up some fancy new way in which participating in an automated telephone poll might cause harm. \n Perrin adds: \n The list of exemptions to IRB review is too short and, more importantly, contains no guiding principle as to what makes exempt. . . . [and] Even exemptions require approval by the IRB. \n He also voices a thought I\u2019ve had many times, which is that there are all sorts of things you or I or anyone else can do on the street (for example, go up to people and ask them personal questions, drop objects and see if people pick them up, stage fights with our friends to see the reactions of bystanders, etc etc etc) but for which we have to go through an IRB in our roles as researchers, teachers, or students. \n Recently a high school student contacted me about a research project he has, involving online surveys. He had some interesting ideas and it looked like a great project. But it might never get done. Why? His high school has no IRB. But he can\u2019t do it through the Columbia IRB because he\u2019s not a Columbia student (and it\u2019s his project, not mine, so it doesn\u2019t help that I work here.) \n Bottom line: Everybody\u2019s afraid of getting sued. That\u2019s the problem with living in a country that\u2019s run by lawyers. \n Two other things: \n 1. At the university, endless hours are wasted on getting permissions to do innocuous surveys. Meanwhile, what about dangerous medical experiments, the kind of study where a drug company crams some illegal aliens into a bunch of Miami hotel rooms ? Don\u2019t worry, those guys use commercial IRB\u2019s that approve everything. \n 2. I do have problems with the ethics of surveys that don\u2019t pay their participants. But that has nothing to do with the questions that are being asked. Instead, the IRB has all this \u201cminimal risk\u201d B.S. \n Of course this is not the world\u2019s most important problem, but it really is pretty bogus."], "link": "http://andrewgelman.com/2012/10/irb-nightmares/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.newyorker.com/": 1, "http://scatter.wordpress.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["I happened to be referring to the path sampling paper today and took a look at Appendix A.2: \n \n \n \n \nI\u2019m sure I could reconstruct all of this if I had to, but I certainly can\u2019t read this sort of thing cold anymore."], "link": "http://andrewgelman.com/2012/10/rust/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 3, "http://www.columbia.edu/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Behavioral and Brain Sciences"], "link": "http://andrewgelman.com/2012/10/100/", "bloglinks": {}, "links": {"http://www.columbia.edu/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Rohin Dhar writes : \n While bike theft is an epidemic in major US cities, most people seem resigned that it\u2019s just a fact of life. . . . at Priceonomics, we thought we\u2019d take a crack at trying to reduce bike theft. Could we use software to help people fight back against bike thieves? \n Professional bike thieves exist because they can make a profit. Luckily, this author went to business school and remembers exactly one equation from the experience: \n Profit = Revenue \u2013 Cost \n From a criminal\u2019s perspective, the \u201cCost\u201d of bike theft is about zero. The odds of getting caught are negligible and the penalty is about zero as well. Most commentators suggest that in order to prevent bike theft, the government should increase the penalties to make it a less attractive crime. As we stated earlier, we somehow doubt government intervention is going to happen any time soon. \n We decided to focus on the revenue half of the equation. Could we make it harder for bike thieves to turn their contraband into cash? If you make it nearly impossible to sell a stolen bike, fewer bikes should get stolen because it\u2019s more difficult to turn a profit. . . . \n So that\u2019s what we decided to build. A tool to help you track down your bike when it gets stolen and make it really hard for criminals to flip it for a profit by selling it online. A dragnet for stolen bikes. \n What we\u2019ve built is a search engine of almost every bike for sale on the web. If someone out there is trying to sell your stolen bike, you can catch them here: \n  \n If your bike is stolen in San Francisco and the thief is selling it in Los Angeles, now you have a way to find it. If they\u2019ve stripped the bike down to its frame and are trying to auction it off, you can find it too. You can search on the site or set an alert and we\u2019ll notify when bikes matching the description pop up. . . . \n As people who love bicycles, we\u2019re not very pleased with Craigslist and eBay. Bikes get stolen and then flipped for a profit on marketplaces like these. The whole reason bikes get stolen is because reselling them online is so easy to do. \n So, we created an alternative marketplace for people that just want a nice place to buy and sell bikes. If the current marketplaces are cesspools of stolen bikes, then it would be nice if there were at least an alternative. A place where the community could prevent stolen bikes from being sold and just generally be excellent to each other. Here it is: \n  \n The harder you make it to profit from bike theft, the less prevalent this crime will be become."], "link": "http://andrewgelman.com/2012/10/using-economics-to-reduce-bike-theft/", "bloglinks": {}, "links": {"https://racklove.com/": 2, "http://blog.priceonomics.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Bill Harris writes: \n On pp. 250-251 of BDA second edition, you write about multiple comparisons, and you write about stepwise regression on p. 405. How would you look at stepwise regression analyses in light of the multiple comparisons problem? Is there an issue? \n My reply: \n In this case I think the right approach is to keep all the coefs but partially pool them toward 0 (after suitable transformation). But then the challenge is coming up with a general way to construct good prior distributions. I\u2019m still thinking about that one! Yet another approach is to put something together purely nonparametrically as with Bart."], "link": "http://andrewgelman.com/2012/10/bayesian-analogue-to-stepwise-regression/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["A few years ago I asked what happened to Matthew Klam, a talented writer who has a bizarrely professional-looking webpage but didn\u2019t seem to be writing anymore. \n Good news! He published a new story in the New Yorker! Confusingly, he wrote it under the name \u201cJustin Taylor,\u201d but I\u2019m not fooled (any more than I was fooled when that posthumous Updike story was published under the name \u201c Antonya Nelson \u201c). I\u2019m glad to see that Klam is back in action and look forward to seeing some stories under his own name as well."], "link": "http://andrewgelman.com/2012/10/the-strange-reappearance-of-matthew-klam/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 2, "http://www.newyorker.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Michael Collins ( here\u2019s a link but I don\u2019t know the original source) picked up on my article about statistics and the cigarette companies, which mentioned a consulting job of famed Stanford statistician Ingram Olkin, and noticed that Olkin was recently in the news as a coauthor of a report on organic food: \n A widely publicized study claiming that there is no demonstrated difference in nutritional value between organically and conventionally grown foods just appeared in the Annals of Internal Medicine. Broad mainstream media coverage produced headlines like Stanford Scientists Cast Doubt on Advantages of Organic Meat and Produce. The media failed to mention one point that may be of major interest. . . . The article co-author with recognized expertise in meta-analysis, Ingram Olkin, applied for a grant from Council of Tobacco Research (CTR) in 1976. . . . Olkin applied to the CTR to conduct a project on the statistical methods used in the Framingham Heart Study, the landmark project linking cigarette smoking with increased risk of heart disease. . . . \n Olkin\u2019s history of being paid to work for the tobacco companies is being used to discredit his current work on organic vegetables. \n Is this fair? I don\u2019t know. On one sense, it seems pretty rough to slam a guy based some past consulting job. I\u2019d be pretty annoyed if that were to happen to me. \n On the other hand, there\u2019s a symmetry to the accusation. When they hired Olkin, the cigarette companies were, to some extent, buying his reputation as a leading applied statistician. So maybe it\u2019s appropriate for the inference to go the other way: being funded by cigarette companies based on \u201cconsiderations other than practical scientific merit\u201d can affect one\u2019s reputation as a statistician. Olkin got the $12,000 but now he\u2019s paying for it (a bit). \n But this is not a deterministic relation; just \u2018cos someone is a distinguished statistician, it doesn\u2019t mean that his cigarette-funded report on the Framingham Heart Study is to be trusted; and just \u2018cos someone took cigarette $ to write that report, it doesn\u2019t mean that his other work can\u2019t be trusted. \n P.S. Just to be clear: This post is not intended to be an attack on Olkin. Rather, it\u2019s an exploration of the ways in which people use statistical inference in judging someone\u2019s work."], "link": "http://andrewgelman.com/2012/10/if-x-is-correlated-with-y-then-y-is-correlated-with-x/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.thepeoplesvoice.org/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Actually, $100,000 auction. I learned about it after seeing the following email which was broadcast to a couple of mailing lists: \n Dear all, \n I am now writing about something completely different! \n I need your help \u201cvoting\u201d for our project, and sending this e-mail to others so that they can also vote for our project. As you will see from the video, the project would fund *** \n Project: \n I am a finalist for a $100,000 prize from Brigham and Women\u2019s Hospital. My project is to understand how ***. Ultimately, we want to develop a ***. We expect that this ** can be used to *** \n Here are the instructions: \n 1. Go to the web page: http://brighamandwomens.org/research/BFF/default.aspx \n 2. scroll to the bottom and follow the link to \u201cVote\u201d \n 3. select project #** \n 4. FORWARD THIS E-MAIL TO AS MANY PEOPLE AS YOU CAN. \n Best regards, \n ** \n I love that step 4 is in ALL CAPS, just to give it that genuine chain-letter aura. \n Isn\u2019t this weird? First, that this foundation would give out $100,000 based on an internet ballot where anyone can vote, second that this guy who I\u2019ve never met would spam me to vote for him. \n I think this guy\u2019s gonna lose, though. He\u2019s spamming academic lists, but I bet one of the other contestants is paying 100,000 people on Mechanical Turk to vote for him at a cost of 10 cents each. That savvy dude will net $90,000 on the deal. \n Which brings us to the dollar auction. I think what the foundation should do is set up the voting so that each vote corresponds to a 10-cent contribution to the fund. And then make public the total votes for each proposal. Then each contestant will be motivated to get just a few more votes, just a few more. . . . and eventually the foundation will raise hundreds of thousands of dollars from these guys, all competing for the fixed $100,000 prize. It\u2019s a win-win situation, no? Much better than someone spamming me to \u201cFORWARD THIS E-MAIL TO AS MANY PEOPLE AS YOU CAN.\u201d"], "link": "http://andrewgelman.com/2012/10/a-real-life-dollar-auction-game/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["It\u2019s good to remember that wikis aren\u2019t just for looking up Dylan lyrics and the plots of old Three\u2019s Company episodes."], "link": "http://andrewgelman.com/2012/10/elderpedia/", "bloglinks": {}, "links": {"http://www.elderpedia.org": 2}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Psychology researcher Alison Gopnik discusses the idea that some of the systematic problems with human reasoning can be explained by systematic flaws in the statistical models we implicitly use. \n I really like this idea and I\u2019ll return to it in a bit. But first I need to discuss a minor (but, I think, ultimately crucial) disagreement I have with how Gopnik describes Bayesian inference. She writes: \n The Bayesian idea is simple, but it turns out to be very powerful. It\u2019s so powerful, in fact, that computer scientists are using it to design intelligent learning machines, and more and more psychologists think that it might explain human intelligence. Bayesian inference is a way to use statistical data to evaluate hypotheses and make predictions. These might be scientific hypotheses and predictions or everyday ones. \n So far, so good. Next comes the problem (as I see it). Gopnik writes: \n Here\u2019s a simple bit of Bayesian election thinking. In early September, the polls suddenly improved for Obama. It could be because the convention inspired and rejuvenated Democrats. Or it could be because Romney\u2019s overly rapid response to the Benghazi attack turned out to be a political gaffe. Or it could be because liberal pollsters deliberately manipulated the results. How could you rationally decide among those hypotheses? . . . \n Combining your prior beliefs about the hypotheses and the likelihood of the data can help you . . . In this case, the inspiring convention idea is both likely to begin with and likely to have led to the change in the polls, so it wins out over the other two. \n I have no problem with the general message here (which is why I label it as only \u201cslightly\u201d garbled) but I\u2019d like to make one correction on the details. (I\u2019m a statistician; I care about details.) I\u2019m (slightly) unhappy with Gopnik\u2019s framing of the problem as A, or B, or C. It\u2019s really A, B, and C. The appropriate claim, I believe, is not that A is more likely than B or C, but rather that the continuous effect A is probably larger than B or C. \n As noted, this point is minor\u2013I have no problem with Gopnik\u2019s summary that one of the hypotheses \u201cwins out over the other two.\u201d But I think we are led to confusion if we place ourselves in an either/or setting. (This is probably a good place for me to plug my article with Kari Lock from a couple years ago on Bayesian combination of state polls and election forecasts, where we use continuous weighting.) \n Blame the discrete models, not the priors \n One way this seemingly minor point can matter is when we follow Gopnik\u2019s suggestion that Bayesian inference \u201cmight explain human intelligence.\u201d I agree that we naturally think discretely. But discrete thinking does not describe how much of the biological social world works. Out there are lots and lots of varying effects of varying sizes. If we, as humans, take these continuous phenomena and try to model them discretely, we will trip up, in predictable ways\u2013even if we use (discrete) Bayesian methods. \n  \n To put it another way: what if Josh Tenenbaum and his colleagues (not mentioned in Gopnik\u2019s article but you can search for them here on the blog) are right that our brains use some sort of approximate discrete Bayesian reasoning to make decisions and perform inferences about the world? Then this should imply some predictable errors. \n Gopnik asks, \u201cIf kids are so smart, why are adults so stupid?\u201d She\u2019s referring to this experiment done in her lab: \u201cWe gave 4-year-olds and adults evidence about a toy that worked in an unusual way. The correct hypothesis about the toy had a low \u2018prior\u2019 but was strongly supported by the data. The 4-year-olds were actually more likely to figure out the toy than the adults were.\u201d \n In that example, Gopnik might well be correct: it seems reasonable to suspect that a kid will have a better prior than an adult on how a toy works. \n More generally, though, I think we should avoid the temptation to think that, when a Bayesian inference goes wrong, it has to be a problem with the prior. That\u2019s old-fashioned thinking, the idea that the likelihood is God-given and known perfectly, leaving us all to fight over our priors. In many cases, the model matters (for example, in our discussion above about natural-seeming but flawed discrete models). Even if the data model generally makes sense, its details can matter: as I point out to my students, the prior only counts once in the posterior, but the likelihood comes in over and over again, once for each data point. \n If, as I think is the case, our brains like discrete models (perhaps they can be more quickly coded and computed) but the world is continuous and varying, this suggests interesting systematic ways that our brains might be misunderstanding the world in everyday reasoning. (Conversely, if discrete models really do have major computational advantages, maybe statisticians like myself should be giving them a second look.) \n P.S. This post had been titled, \u201cI notice a (slightly) garbled version of Bayesian inference, which provokes some thoughts on the applicability of Bayesian models of human reasoning.\u201d But I think the new title is better!"], "link": "http://andrewgelman.com/2012/10/i-notice-a-slightly-garbled-version-of-bayesian-inference-which-provokes-some-thoughts-on-the-applicability-of-bayesian-models-of-human-reasoning/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.columbia.edu/": 1, "http://www.slate.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Cord Blomquist, who did a great job moving us from horrible Movable Type to nice nice WordPress, writes: \n I [Cord] wanted to share a little news with you related to the original work we did for you last year. When ReadyMadeWeb converted your Movable Type blog to WordPress, we got a lot of other requestes for the same service, so we started thinking about a bigger market for such a product. After a bit of research, we started work on automating the data conversion, writing rules, and exceptions to the rules, on how Movable Type and TypePad data could be translated to WordPress. \n After many months of work, we\u2019re getting ready to announce TP2WP.com , a service that converts Movable Type and TypePad export files to WordPress import files, so anyone who wants to migrate to WordPress can do so easily and without losing permalinks, comments, images, or other files. By automating our service, we\u2019ve been able to drop the price to just $99. \n I recommend it (and, no, Cord is not paying me for this blurb, nor is he giving me a discount)."], "link": "http://andrewgelman.com/2012/10/migrating-your-blog-from-movable-type-to-wordpress/", "bloglinks": {}, "links": {"http://TP2WP.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Stan: open-source Bayesian inference \n Speaker: Andrew Gelman, Columbia University \nDate: Thursday, October 11 2012 \nTime: 4:00PM to 5:00PM \nLocation: 32-D507 \nHost: Polina Golland, CSAIL \nContact: Polina Golland, 6172538005, polina@csail.mit.edu \n Stan ( mc-stan.org ) is an open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. We discuss how Stan works and what it can do, the problems that motivated us to write Stan, current challenges, and areas of planned development, including tools for improved generality and usability, more efficient sampling algorithms, and fuller integration of model building, model checking, and model understanding in Bayesian data analysis. \n P.S. Here\u2019s the talk ."], "link": "http://andrewgelman.com/2012/10/my-talk-at-mit-on-thurs-11-oct/", "bloglinks": {}, "links": {"http://www.mit.edu/": 1, "http://www.columbia.edu/": 1, "http://mc-stan.org": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["John Cook considers how people justify probability distribution assumptions: \n Sometimes distribution assumptions are not justified. \n Sometimes distributions can be derived from fundamental principles [or] . . . on theoretical grounds. For example, large samples and the central limit theorem together may justify assuming that something is normally distributed. \n Often the choice of distribution is somewhat arbitrary, chosen by intuition or for convenience, and then empirically shown to work well enough. \n Sometimes a distribution can be a bad fit and still work well, depending on what you\u2019re asking of it. \n Cook continues: \n The last point is particularly interesting. It\u2019s not hard to imagine that a poor fit would produce poor results. It\u2019s surprising when a poor fit produces good results. \n And then he gives an example of an effective but inaccurate model used to model survival times in a clinical trial. Cook explains: \n The [poorly-fitting] method works well because of the question being asked. The method is not being asked to accurately model the distribution of survival times for patients in the trial. It is only being asked to determine whether a trial should continue or stop, and it does a good job of doing so. As the simulations in this paper show, the method makes the right decision with high probability, even when the actual survival times are not exponentially distributed. \n This is an excellent point, and I\u2019d like to elaborate by considering a different way in which a bad model can work well. \n An example where a bad model works well because of its implicit assumptions \n In Section 9.3 of Bayesian Data Analysis (second edition) , we compare several different methods for estimating a population total from a random sample in an artificial problem in which the population is the set of all cities and towns in a state. The data are skewed\u2014some cities have much more population than others\u2014but if you use standard survey-sampling estimates and standard errors, you get OK inferences. The inferences are not perfect\u2014in particular, the confidence interval can include negative values because the brute-force approach doesn\u2019t \u201cknow\u201d that the data (city populations) are all positive\u2014but the intervals make sense and have reasonable coverage properties. In contrast, as Don Rubin showed when he first considered this example, comparable analyses applying the normal distribution to log or power-transformed data can give horrible answers. \n What\u2019s going on? How come the interval estimates based on these skewed data have reasonable coverage we use the normal distribution, while inferences based on the much more sensible lognormal or power-transformed models are so disastrous? \n A quick answer is that the normal-theory method makes implicit use of the central limit theorem, but then this just pushes the question back one step: Why should the central limit theorem apply here? Why indeed. The theorem applies for this finite sample (n=100, in this case) because, although the underlying distribution is skewed, there are no extreme outliers. By using the normal-based interval, we are implicitly assuming a reasonable upper bound in the population. And, in fact, if we put an upper bound into the power-transformed model, it works even better. \n The moral of the story? Sometimes an ill-fitting model works well because, although it doesn\u2019t fit much of the data, it includes some assumption that is relevant to inferences, some aspect of the model that would be difficult to ascertain from the data alone. And, once we identify what that assumption is, we can put it directly into an otherwise better-fitting model and improve performance."], "link": "http://andrewgelman.com/2012/10/another-reason-why-you-can-get-good-inferences-from-a-bad-model/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.johndcook.com/blog": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["See if you can interpolate the talk from the slides . \n The background is: I was invited to speak in this seminar on \u201cbig data.\u201d I said I didn\u2019t know anything about big data, I worked on little data. They said that was ok. Actually it was probably a crowd-pleasing move to tell these people that little-data ideas remain relevant."], "link": "http://andrewgelman.com/2012/10/%ef%bf%bclittle-data-how-traditional-statistical-ideas-remain-relevant-in-a-big-data-world/", "bloglinks": {}, "links": {"http://www.columbia.edu/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["I opened the paper today and saw this from Paul Krugman, on \n Jack Welch, the former chairman of General Electric, who posted an assertion on Twitter that the [recent unemployment data] had been cooked to help President Obama\u2019s re-election campaign. His claim was quickly picked up by right-wing pundits and media personalities. \n It was nonsense, of course. Job numbers are prepared by professional civil servants, at an agency that currently has no political appointees. But then maybe Mr. Welch \u2014 under whose leadership G.E. reported remarkably smooth earnings growth, with none of the short-term fluctuations you might have expected (fluctuations that reappeared under his successor) \u2014 doesn\u2019t know how hard it would be to cook the jobs data. \n I was curious so I googled *general electric historical earnings*. It was surprisingly difficult to find the numbers! Most of the links just went back to 2011, or to 2008. Eventually I came across this blog by Barry Ritholtz that showed this graph: \n  \n That looks pretty fishy, indeed. Also a link to a news article from 2002 (shortly after Welch stepped down from running GE) that said: \n GE used to feature on university courses as a model of probity. These days, it crops up in the seminars about earnings-manipulation. Everyone agrees that GE practises one form of earnings management: it times one-off asset sales to coincide with one-off write-downs or restructurings. . . . Beyond this, the amount of profits-smoothing that GE indulges in is a matter of speculation. GE also manages expectations about its earnings by managing its analysts. . . . managers who are in the habit of smoothing earnings have an especially strong motive to keep the good news coming, whether or not the business warrants it. \n Yup. Also this from Marie Leone and Tim Reason: \n [In 2009,] after a four-year investigation, GE settled accounting fraud charges with the SEC for allegedly misleading investors with improper hedge accounting and revenue recognition schemes. Specifically, GE was charged with violating accounting rules when it changed its original hedge documentation to avoid recording fluctuations in the fair value of interest rates swaps, which would have dragged down the company\u2019s reported earnings-per-share estimates. \n In addition, the SEC charged GE with concocting schemes to accelerate the recognition of revenue from its locomotive and aircraft spare parts business, to make the company\u2019s financial results appear healthier than they actually were. \n Without admitting or denying guilt, GE paid a fine of $50 million, and agreed to remedial action related to internal control enhancements. \u201cGE bent the accounting rules beyond the breaking point,\u201d noted Robert Khuzami, director of the SEC\u2019s Division of Enforcement, in a statement. \n OK, fine. This isn\u2019t my area of expertise and, in any case, our circulation is on the order of 1/10,000th of Krugman\u2019s, so why report it here? \n What\u2019s interesting to me are the different attitudes on statistical manipulation. The Bureau of Labor Statistics, Census Bureau, etc., take their data pretty seriously and I agree with Krugman that it\u2019s hard for me to imagine them manipulating the numbers in any way. For one thing, don\u2019t have much incentive to do so: as Krugman notes, they are civil service workers, not political appointees. And it\u2019s not like better numbers would increase their budget line. (In contrast, I can understand the motivation for those military guys who faked the data on missile tests: success can lead to more funding.) Beyond this, it just doesn\u2019t seem that this sort of fraud is part of the culture of government statistics in the United States. \n In contrast, Leone and Reason report: \n The SEC complaint relates several instances of round-robin email discussions among GE accountants, internal auditors, executives, and the company\u2019s external auditor, KPMG, debating whether aggressive accounting would past muster with regulators. \n So, it\u2019s not about Welch being some sort of data sociopath; rather, data manipulation is part of corporate culture. And, indeed, these guys have lots of motivation to fake the numbers (i.e., \u201caggressive accounting\u201d). The executives and accountants personally make millions of dollars from it. Millions of dollars in win, very little personal risk if you get caught (it was GE that paid the fine, right? Jack Welch is still at loose on Twitter): that\u2019s what I call an incentive. \n One reason this interests me is the connection to ethics in the scientific literature. Jack Welch has experience in data manipulation and so, when he sees a number he doesn\u2019t like, he suspects it\u2019s been manipulated. In academia, Steven Levitt has seen economists \u201cwork behind the scenes constantly trying to undermine each other\u201d and been involved with a journal that suppressed unwelcome research results and so, when he has a paper rejected (by a different journal), he suspects the rejection was for illegitimate reasons. I don\u2019t think such a thing would be done in the field of statistics, because I think we have more of a tradition of going with the data."], "link": "http://andrewgelman.com/2012/10/ethical-standards-in-different-data-communities/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 3, "http://www.economist.com/": 1, "http://www.nytimes.com/": 1, "http://www.cfo.com/": 1, "http://www.ritholtz.com/blog": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["I was just reading an old post and came across this example which I\u2019d like to share with you again: \n Here\u2019s a story of R-squared = 1%. Consider a 0/1 outcome with about half the people in each category. For.example, half the people with some disease die in a year and half live. Now suppose there\u2019s a treatment that increases survival rate from 50% to 60%. The unexplained sd is 0.5 and the explained sd is 0.05, hence R-squared is 0.01."], "link": "http://andrewgelman.com/2012/10/r-squared-of-1/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Juli writes: \n I\u2019m helping a professor out with an analysis, and I was hoping that you might be able to point me to some relevant literature\u2026 She has two studies that have been completed already (so we can\u2019t go back to the planning stage in terms of sampling, unfortunately). Both studies are based around the population of adults in LA who attended LA public high schools at some point, so that is the same for both studies. Study #1 uses random digit dialing, so I consider that one to be SRS. Study #2, however, is a convenience sample in which all participants were involved with one of eight community-based organizations (CBOs). \n Of course, both studies can be analyzed independently, but she was hoping for there to be some way to combine/compare the two studies. Specifically, I am working on looking at the civic engagement of the adults in both studies. In study #1, this means looking at factors such as involvement in student government. In study #2, this means looking at involvement in CBOs\u2026but they were all involved in those. \n I know I can\u2019t blindly combine the two studies. I also know that not having a control group (i.e., not in CBOs) in study #2 is a problem, as is the convenience sampling, but I can\u2019t change those things. I was trying to see if I could somehow use study #1 (or part of it \u2013 participants who look similar based on a variety of factors) to act as the control group for study #2 and do some sort of matching, but I\u2019m not sure that\u2019s okay. Then I was trying to see if I could combine the studies and act as though they are different strata, one with SRS and one with quota sampling (I think \u2013 per Lohr\u2019s book, chapter on stratified sampling). But I\u2019m still not sure if it\u2019s okay to compare them that way. \n I know that overall, generalizability is going to be nearly impossible here. But it would be really nice to come up with a creative way to make this work. I have a sneaking suspicion that this might be useful for others \u2013 which then made me wonder if this has been tackled before. Any thoughts? \n My reply: \n It\u2019s funny this comes up, because we were just having a discussion on the blog with a student at UCLA who was asking about the use of hierarchical models for causal inference, combining different data sources. \n My generic advice is to set up a regression model controlling for as many background variables as possible, then it\u2019s possible that within each poststrat cell, the two groups can be considered to be equivalent to a natural experiment in which one group is involved with the CBO and the other isn\u2019t. Since you can\u2019t control for everything, the next step is to include in the model an unobserved variable representing unknown differences (that is, selection effects). How exactly to do this, though, I don\u2019t know. On this subject, I\u2019m all talk and no action. \n My more constructive suggestion would be to talk with Jennifer or, since you\u2019re at UCLA, to Sander Greenland in the epidemiology department. This sort of thing is right up his alley."], "link": "http://andrewgelman.com/2012/10/comparing-people-from-two-surveys-one-of-which-is-a-simple-random-sample-and-one-of-which-is-not/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Solomon Hsiang writes : \n I [Hsiang] have posted about high temperature inducing individuals to exhibit more violent behavior when driving, playing baseball and prowling bars. These cases are neat anecdotes that let us see the \u201cpure aggression\u201d response in lab-like conditions. But they don\u2019t affect most of us too much. But violent crime in the real world affects everyone. Earlier, I posted a paper by Jacob et al. that looked at assault in the USA for about a decade \u2013 they found that higher temperatures lead to more assault and that the rise in violent crimes rose more quickly than the analogous rise in non-violent property-crime, an indicator that there is a \u201cpure aggression\u201d component to the rise in violent crime. \n A new working paper \u201cCrime, Weather, and Climate Change\u201d by recent Harvard grad Matthew Ranson puts together an impressive data set of all types of crime in USA counties for 50 years. The results tell the aggression story using street-level data very clearly [click to view the graphs]: \n \n All crime increases as temperatures rise from 0 F to about 50 F. It seems reasonable to hypothesize that a lot of this pattern comes from \u201clogistical constraints\u201d, eg. it\u2019s hard to steal a car when it\u2019s covered in snow. But above 60 F, only the violent crimes continue to go up: murder, rape, and assault. The comparison between murder and manslaughter is elegantly telling, as manslaughter should be less motivated by malicious intent. \n This seems important to me. Just one graphics tip (to Ronson, not to Hsiang, who is merely reporting this work): The graphs are great, but they\u2019d be even better if they were rearranged slightly. First, put the labels on the top rather than bottom of the graphs. As it is, when I first looked, I thought that the results for Murder were in the second row, not the first row. You can make more space for the titles by removing the boxes around the graphs (in R notation, bty=\u201dl\u201d rather than bty=\u201do\u201d). The x-axes are too busy, it would be enough to label temperature every 20 degrees rather than 10. (I\u2019d actually prefer Celsius but that\u2019s more of a judgment call.) The zero line should be gray rather than black so as not to so strongly distract from the results. The y-labels would be improved by (redundantly) naming the crime: thus, Murder Rate, Manslaughter Rate, Rape Rate, etc., rather than the identical \u201cNumber of Crimes\u201d for each. \n Finally, I\u2019d prefer the scales of the y-axis to be directly interpretable. Instead of presenting changes in monthly crime rate per 100,000 persons, I\u2019d present changes in crime rates per crime . For example, if there were 15,000 murders in the U.S. in one year, that\u2019s 15,000/(12*100,000)=0.0125 murders per 100,000 people per month. A change in 0.002 in the upper-left graph then corresponds to 0.16 or 16% of the murder rate. Maybe I got the numbers wrong here; in any case, my point is that percentages of the murder rate will be much more relevant than crime rate per person."], "link": "http://andrewgelman.com/2012/10/high-temperatures-cause-violent-crime-and-implications-for-climate-change/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.fight-entropy.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["I\u2019m already on record as saying that Ronald Reagan was a statistician so I think this is ok too . . . \n  \n Here\u2019s what Columbo does. He hears the killer\u2019s story and he takes it very seriously (it\u2019s murder, and Columbo never jokes about murder), examines all its implications, and finds where it doesn\u2019t fit the data. Then Columbo carefully examines the discrepancies, tries some model expansion, and eventually concludes that he\u2019s proved there\u2019s a problem. \n OK, now you\u2019re saying: Yeah, yeah, sure, but how does that differ from any other fictional detective? The difference, I think, is that the tradition is for the detective to find clues and use these to come up with hypotheses, or to trap the killer via internal contradictions in his or her statement. I see Columbo is different\u2014and more in keeping with chapter 6 of Bayesian Data Analysis\u2014in that he is taking the killer\u2019s story seriously and exploring all its implications. That\u2019s the essence of predictive model checking: you take advantage of the fact that you\u2019re working with a generative model, and you generate anything and everything you can."], "link": "http://andrewgelman.com/2012/10/columbo-does-posterior-predictive-checks/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 2}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["When we suggest a new method, we are duty-bound to not just demonstrate that it works better than existing approaches (or is superior in some other way such as simplicity or cost). We also need to explain why, if this new method is so great, people aren\u2019t already using it. \n Various answers are possible, for example: \n - The new idea is technically advanced, requiring a level of mathematical or engineering complexity such that it could not easily have been discovered by accident. Hence its novelty can be explained as a product of some particular historical process. \n - The new idea is clever and unexpected, as with the mechanical device underlying Rubik\u2019s Cube. \n - The new idea could only exist given recent technological developments (perhaps hardware developments such as a new composite material or an ultralight battery, or software developments such as a new MCMC algorithm). \n - The new idea usually isn\u2019t so impressive but it shows its virtues in some previously hidden domain (for example, you wouldn\u2019t have much need for relativity theory if you\u2019re modeling sub-relativistic velocities). \n - The new idea violates some principle or taboo (for example, until recently there wasn\u2019t much formal research on weakly informative prior distributions because the literature on Bayesian statistics was divided between quests for noninformativity and assertions of complete subjective informativity). \n - The new idea was actually out there all along but people didn\u2019t use it because until recently they had no need for its benefits (for example, a method that offers a 10% improvement in speed but at the cost of requiring an elaborate computational implementation, this might not be worth it for a desktop regression but can come in handy if you\u2019re analyzing billions of data points). \n You can probably come up with some more. My point is that when we promote a new idea, we must\u2014explicitly or implicitly\u2014explain why our brilliant predecessors did not already discover and use it. \n I thought about the above after reading this from John Cook: \n According a recent biography of Henri Poincar\u00e9, \n Poincar\u00e9 \u2026 worked regularly from 10 to 12 in the morning and from 5 till 7 in the late afternoon. He found that working longer seldom achieved anything \u2026 \n Poincar\u00e9 made tremendous contributions to math and physics. His two-hour work sessions must have been sprints, working with an intensity that could not be sustained much longer. \n I [Cook] expect most of us would accomplish more if we worked harder when we worked, rested more, and cut out half-work. \n I agree, but . . . I\u2019ve thought this for a long time, as I\u2019m sure has almost anybody who works at a flexible job. It\u2019s long been my goal to work intensely for whatever number of hours a week is possible and then relax the rest of the time, rather than spending hours and hours each week rearranging my files, responding to email, etc. \n Yet this doesn\u2019t always happen (hence it\u2019s long been my goal etc.). I occasionally make some progress (for example, my strategy of reviewing journal articles immediately, just reading the article and writing the report in 15 minutes, or my strategy of not reading email before 4pm), but I still spend lots of time doing essentially nothing yet still hanging out at work. (Not to mention blogging, but that at least serves some socially useful purposes.) \n Which brings us to the question: if this is such good advice, and such obviously good advice, why aren\u2019t we doing it already? I think one reason is that it\u2019s hard to work intensely, and once you\u2019re at work it\u2019s easier to spend your time quasi-goofing-off. I was once at a workshop where the person next to me was checking email on the laptop, literally more than once per minute. Seems pretty boring, but it beats working! \n I\u2019m reminded of the advice in the classic book, How to Talk So Your Kids Will Listen and Listen So Your Kids Will Talk. It\u2019s all such clearly good advice, but somehow so difficult to carry out in real time. \n P.S. One of the comments on Cook\u2019s post led me to this website by computer scientist Cal Newport which is full of advice for students, along the lines of: \u201c\u2018follow your passion\u2019 is bad advice if your goal is to end up loving what you do.\u201d I like what Newport has to say. Unlike the usual in-your-face internet self-help gurus , he doesn\u2019t seem to feel the need to be obnoxious or to supply having-it-all parables. He does do a little of that B.S.\u2014for example, a post entitled, \u201cHow to Get Into Stanford with B\u2019s on Your Transcript\u201d\u2014but, even there, the substance of the post is interesting. Gladwellian, one might say, and I mean that in the best possible sense of the word."], "link": "http://andrewgelman.com/2012/10/advice-thats-so-eminently-sensible-but-so-difficult-to-follow/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://www.johndcook.com/blog": 1, "http://calnewport.com/blog": 2}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Faten Sabry writes: \n We are looking to hire full time analysts at the undergraduate and graduate levels. The work involves extensive econometric analysis and handling of large databases. The analysts will be part of a team working to address various empirical microeconomic issues. \n I worked with Faten and her colleagues on a consulting project once, and they seemed like reasonable people to me."], "link": "http://andrewgelman.com/2012/10/job/", "bloglinks": {}, "links": {"http://www.nera.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Following a recent email exchange regarding path sampling and thermodynamic integration (sadly, I\u2019ve gotten rusty and haven\u2019t thought seriously about these challenges for many years), a correspondent referred to the marginal distribution of the data under a model as \u201cthe evidence.\u201d \n I hate that expression! As we discuss in chapter 6 of BDA, for continuous-parametered models, this quantity can be completely sensitive to aspects of the prior that have essentially no impact on the posterior. In the examples I\u2019ve seen, this marginal probability is not \u201cevidence\u201d in any useful sense of the term. \n When I told this to my correspondent, he replied, \n I actually don\u2019t find \u201cthe evidence\u201d too bothersome. I don\u2019t have BDA at home where I\u2019m working from at the moment, so I\u2019ll read up on chapter 6 later, but I assume you refer to the problem of the marginal likelihood being strongly sensitive to the prior in a way that the posterior typically isn\u2019t, thereby diminishing the value of the marginal likelihood as a model selection statistic for problems in which there is no well motivated way to choose the prior. If so, I understand, but I think you might be fighting a losing battle as \u201cthe evidence\u201d is seemingly now popular in the stats literature as well as the physics \u2026 \n I replied that I\u2019ll fight that battle forever. I really really hate the use of linguistically-loaded terms such as \u201cbias,\u201d \u201cevidence,\u201d \u201cempirical Bayes,\u201d etc."], "link": "http://andrewgelman.com/2012/10/fighting-a-losing-battle/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Rachel Schutt (the author of the Taxonomy of Confusion) has a blog! for the course she\u2019s teaching at Columbia, \u201cIntroduction to Data Science.\u201d It sounds like a great course\u2014I wish I could take it! \n Her latest post is \u201cOn Inspiring Students and Being Human\u201d: \n  \n Of course one hopes as a teacher that one will inspire students . . . But what I actually mean by \u201cinspiring students\u201d is that you are inspiring me; you are students who inspire: \u201cinspiring students\u201d. This is one of the happy unintended consequences of this course so far for me. \n She then gives examples of some of the students in her class and some of their interesting ideas: \n Phillip is a PhD student in the sociology department . . . He\u2019s in the process of developing his thesis topic around some of the themes we\u2019ve been discussing in this class, such as the emerging data science community. \n Arvi works at the College Board and is a part time student . . . He analyzes user-level data of students who have signed up for (and taken) the SATs and has lots of interesting data around where those students hope to go to college; and longitudinal data sets that allow him and his colleagues to examine trends . . . \n Adam, Christina and Eurry (respectively, (1)sociology PhD student, (2)data scientist at Nielsen, and (3)\u201daspring data visualizer\u201d (better term?) from the QMSS program) have taken on the challenge of polling the students and then developing an algorithm to automatically find optimal data science teams and a corresponding visualization. \n Matt is a history of science professor who wrote the Curse of Dimensionality post a week ago, and is starting to think about (or revisit) how exploratory text classification could be used in his research. \n Jed works as a data analyst at Case Commons, a nonprofit that builds web apps and and databases for state-wide foster care agencies. . . . he read this paper on using Naive Bayes to classify suicide notes, and now has some early ideas of ways he might apply this approach in his own work. \n Maryanne is the Executive Director for the Center for Innovation Through Data Intelligence in Mayor Bloomberg\u2019s office. Her office deals with data about the juvenile justice system, homelessness and poverty and she too is thinking about how analyzing data sets could be used to prioritize social worker interventions. \n Then let\u2019s not forget the Biomedical Informatics (or variation of that) students/post-doc, Hojjat, Albert and Heather; or Kaushik, the student from operations research interested in journalism; or Yegor, the business school student who has an interest in urban planning and architecture . . . \n The comments on the blog from various students are also starting to become interesting. Also let me add Jared\u2019s (our lab instructor) study of his own text messages, after he broke up with his girlfriend, which he just told me about tonight. \n This all reminds me of a comment Seth made once, many years ago, that the usual goal (even if not explicitly stated) of a class is for the students to become replicas of the instructor, whereas he (Seth) liked to teach in such a way that each student could bring in his or her special knowledge, interests, and abilities. I don\u2019t know how good a teacher Seth actually is\u2014I have lots of innovative teaching ideas too, but I\u2019m not such a great teacher, in fact in a large part it\u2019s my crappiness as a teacher that inspires me to come up with new teaching ideas (the entire book Teaching Statistics: A Bag of Tricks arose out of my difficulties in getting students actively engaged in class)\u2014but I think his idea is interesting. In my own teaching, sadly, I pretty much have the goal of turning the students into mini-me\u2019s. I mean, sure, I don\u2019t want them all to become me, I want them to develop their own talents, but implicitly I\u2019m acting as if the best way to do so is to first become as much like me as possible. \n I think Rachel\u2019s shout-outs above are great, not just because it\u2019s a nice thing to do, but because the act of writing these details about the students helps to bring these project to life, as well as to inspire others. \n Rachel also has some thoughts about statistics education: \n Traditional Statistics Pedagogy \n First, allow me to describe the way traditional statistics classes/textbooks present data analysis. A standard homework problem would be: one is presented with a clean data set, and told to run a regression with y=weight and x=height, for example. I look unfavorably upon this because it takes the creativity and life out of everything, and doesn\u2019t resemble in the least what it\u2019s like to actually be a researcher or statistician in the real world. As mentioned previously, in the \u201creal world\u201d (no offense, classrooms aren\u2019t the real world), no one is going to hand you a nice clean data set (and if they did, I\u2019d be skeptical! Also where\u2019d it come from?), and no one is going to tell you what method to use. (Why are we even running this regression in the first place? What questions are we even trying to answer?). The homework problem might then have some questions about interpreting or understanding the model: \u201cinterpret the coefficients\u201d and let\u2019s face it, most people think these are blow-off questions, don\u2019t take them all that seriously, and there are no consequences if they mis-interpret on a homework problem, so they may think about it for . . . a minute, and then write some plausible interpretation. \n Oof! Those are the kind of homework assignments that I write. Rachel continues with a visual: \n  \n She continues with her preferred model: \n  \n followed by: \n  \n This all makes sense to me, and it looks a lot like a bunch of notes I took back in 1997 or so, after teaching applied statistics to the Columbia graduate students. I gave them open-ended homework assignments, then in class spent some time giving them the background they needed to know to attack the problems, and spent a lot of time going over the homeworks they had just turned in. The students liked the class, and I had thoughts of trying to write up a general approach to data analysis, trying to formalize what I did and what I taught. After a few years of this, though, I became dissatisfied because I felt that, although the class was a good learning experience for the students, they didn\u2019t actually end up with many useful new skills. And, over the years, I\u2019ve moved to a more structured, textbook-based style of teaching. This semester, in fact, I\u2019m pretty much just going through Bayesian Data Analysis section by section. We do have some open-ended homework assignments on applied statistics\u2014I think this is important\u2014but maybe Rachel is right that the students aren\u2019t getting a clear message on where to go with these."], "link": "http://andrewgelman.com/2012/10/on-inspiring-students-and-being-human/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 4, "http://columbiadatascience.wordpress.com/": 2}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["John Mount provides some useful background and follow-up on our discussion from last year on computational instability of the usual logistic regression solver. \n Just to refresh your memory, here\u2019s a simple logistic regression with only a constant term and no separation, nothing pathological at all: \n > y \n> display (glm (y ~ 1, family=binomial(link=\"logit\"))) \nglm(formula = y ~ 1, family = binomial(link = \"logit\")) \n   coef.est coef.se \n(Intercept) 0.69  0.55 \n--- \n n = 15, k = 1 \n residual deviance = 19.1, null deviance = 19.1 (difference = 0.0) \n And here\u2019s what happens when we give it the not-outrageous starting value of -2: \n > display (glm (y ~ 1, family=binomial(link=\"logit\"), start=-2)) \nglm(formula = y ~ 1, family = binomial(link = \"logit\"), start = -2) \n   coef.est coef.se \n(Intercept)  71.97 17327434.18 \n--- \n n = 15, k = 1 \n residual deviance = 360.4, null deviance = 19.1 (difference = -341.3) \nWarning message: \nglm.fit: fitted probabilities numerically 0 or 1 occurred \n As I discussed in the comments to my original post, there were reasons we were running glm with prespecified starting points\u2014this blow-up occurred originally in a real problem and then I stripped it down to get to this simple and clear example. \n Mount explains what\u2019s going on: \n From a theoretical point of view the logistic generalized linear model is an easy problem to solve. The quantity being optimized (deviance or perplexity) is log-concave. This in turn implies there is a unique global maximum and no local maxima to get trapped in. Gradients always suggest improving directions. However, the standard methods of solving the logistic generalized linear model are the Newton-Raphson method or the closely related iteratively reweighted least squares method. And these methods, while typically very fast, do not guarantee convergence in all conditions. . . . \n The problem is fixable, because optimizing logistic divergence or perplexity is a very nice optimization problem (log-concave). But most common statistical packages do not invest effort in this situation. \n Mount points out that, in addition to patches which will redirect exploding Newton steps, \u201cmany other optimization techniques can be used\u201d: \n - stochastic gradient descent \n- conjugate gradient \n- EM (see \u201cDirect calculation of the information matrix via the EM.\u201d D Oakes, Journal of the Royal Statistical Society: Series B (Statistical Methodology), 1999 vol. 61 (2) pp. 479-482). \nOr you can try to solve a different, but related, problem: \u201cExact logistic regression: theory and examples\u201d, C R CR Mehta and N R NR Patel, Statist Med, 1995 vol. 14 (19) pp. 2143-2160. \n There\u2019s also the new algorithm of Polson and Scott , which looks pretty amazing, I should really try it out on some examples and report back to all of you on this. \n Maybe we should also change what we write in Bayesian Data Analysis about how to fit a logistic regression."], "link": "http://andrewgelman.com/2012/09/computational-problems-with-glm-etc/", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://andrewgelman.com/": 1, "http://www.win-vector.com/blog": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Research psychologist John Jost reviews the recent book, \u201cThe Righteous Mind,\u201d by research psychologist Jonathan Haidt. Some of my thoughts on Haidt\u2019s book are here . And here\u2019s some of Jost\u2019s review: \n Haidt\u2019s book is creative, interesting, and provocative. . . . The book shines a new light on moral psychology and presents a bold, confrontational message. From a scientific perspective, however, I worry that his theory raises more questions than it answers. Why do some individuals feel that it is morally good (or necessary) to obey authority, favor the ingroup, and maintain purity, whereas others are skeptical? (Perhaps parenting style is relevant after all.) Why do some people think that it is morally acceptable to judge or even mistreat others such as gay or lesbian couples or, only a generation ago, interracial couples because they dislike or feel disgusted by them, whereas others do not? Why does the present generation \u201ccare about violence toward many more classes of victims today than [their] grandparents did in their time\u201d (p. 134)? Haidt dismisses the possibility that this aspect of liberalism, which prizes universal over parochial considerations (the justice principle of impartiality), is in fact a tremendous cultural achievement\u2014a shared victory over the limitations of our more primitive ancestral legacy. In this spirit, he spurns the John Lennon song, \u201cImagine\u201d: \n Imagine if there were no countries, and no religion too. If we could just erase the borders and boundaries that divide us, then the world would \u2018be as one.\u2019 It\u2019s a vision of heaven for liberals, but conservatives believe it would quickly descend into hell. I think conservatives are on to something. (p. 311) \n Throughout the book Haidt mocks the liberal vision of a tolerant, pluralistic, civil society, but, ironically, this is precisely where he wants to end up, quoting Isaiah Berlin with evident approval at the end of his book: \u201cI came to the conclusion that there is a plurality of ideals, as there is a plurality of cultures and of temperaments\u201d (p. 320). \n Good point. Jost also writes: \n Haidt draws sparingly on the details of contemporary research in social and political psychology, usually as a foil for his ostensibly above-the-fray approach. Consider this passage: \n I began by summarizing the standard explanations that psychologists had offered for decades: Conservatives are conservative because they were raised by overly strict parents, or because they are inordinately afraid of change, novelty, and complexity, or because they suffer from existential fears and therefore cling to a simple worldview with no shades of gray. These approaches all had one feature in common: they used psychology to explain away conservatism. They made it unnecessary for liberals to take conservative ideas seriously because these ideas are caused by bad childhoods or ugly personality traits. I suggested a very different approach: start by assuming that conservatives are just as sincere as liberals, and then use Moral Foundations Theory to understand the moral matrices of both sides. (pp. 166-167) \n This paragraph illustrates both the slipperiness of Haidt\u2019s prose and the extent to which key issues are unresolved by his theory. First, there is a great deal of empirical evidence indicating that conservatives are in fact less open to change, novelty, and complexity and are more likely to perceive the world as a dangerous place than liberals (Carney et al., 2008; Gerber et al., 2010; Jost et al., 2003). Rather than attempting to grapple with these findings, which are uncomfortable for his view of political ideology, Haidt characterizes them with argumentative language ( \u201coverly,\u201d \u201cinordinately,\u201d \u201csuffer,\u201d \u201ccling,\u201d \u201cbad childhoods,\u201d and \u201cugly personality traits\u201d) to suggest that these claims have to be false because they sound so . . . pejorative. Second, he claims that past researchers have \u201cused psychology to explain away conservatism,\u201d as if there is no difference between explaining something and explaining it away. Third, Haidt switches at the last moment from discussing the origins and characteristics of liberals and conservatives to the issue of sincerity, as if it were impossible to sincerely believe something that is rooted in childhood or other psychological experiences. Psychological scientists recognize that questions about the social, cognitive, and motivational underpinnings of a belief system are distinct from questions about its validity (and whether it should be taken \u201cseriously,\u201d which is not a scientific question at all). \n Jost is arguing that Haidt has some interesting things to say but trips up when trying to insert all of this into a particular political message about liberals and conservatives."], "link": "http://andrewgelman.com/2012/09/jost-haidt/", "bloglinks": {}, "links": {"http://themonkeycage.org/": 1, "http://andrewgelman.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Lee Wilkinson sends me this amusing ad for his new software, AdviseStat: \n \n The ad is a parody, but the software is real !"], "link": "http://andrewgelman.com/2012/09/advisestat-47-campaign-ad/", "bloglinks": {}, "links": {"https://adviseanalytics.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Political campaigns are commonly understood as random walks, during which, at any point in time, the level of support for any party or candidate is equally likely to go up or down. Each shift in the polls is then interpreted as the result of some combination of news and campaign strategies. \n A completely different story of campaigns is the mean reversion model in which the elections are determined by fundamental factors of the economy and partisanship; the role of the campaign is to give voters a chance to reach their predetermined positions. \n The popularity of the random walk model for polls may be partially explained via analogy to the widespread idea that stock prices reflect all available information, as popularized in Burton Malkiel\u2019s book, A Random Walk Down Wall Street. Once the idea has sunk in that short-term changes in the stock market are inherently unpredictable, it is natural for journalists to think the same of polls. For example, political analyst Nate Silver wrote in 2010: \n In races with lots of polling, instead, the most robust assumption is usually that polling is essentially a random walk, i.e., that the polls are about equally likely to move toward one or another candidate, regardless of which way they have moved in the past. \n But, as I discussed then in the context of remarks by Silver and Justin Wolfers , polls are not stock markets: for many races, a forecast from the fundamentals gives a pretty good idea about where the polls are going to end up. For example, in the 1988 presidential election campaign, even when Michael Dukakis was up 10 points in the polls, informed experts were pretty sure that George Bush was going to win. Congressional races can have predictable trends too. Political scientists Erikson, Bafumi, and Wlezien have found predictable changes in the generic opinion polls in the year leading up to an election, with different patterns in presidential years and off years. Individual polls are noisy, though, and predictability will generally only be detectable with a long enough series. \n Noah Kaplan, David Park, and I have a paper on the topic (to appear in Presidential Studies Quarterly) reviewing the literature and analyzing data from polls during the 2000, 2004, and 2008 elections. We show that, as the campaign progresses, vote preferences become more predictable based on fundamental variables such as political ideology and party identification. This is consistent with a \u201cmean reversion\u201d model in which the campaign serves to solidify latent preferences, but it is not consistent with a random walk model in which a campaign is an accretion of unpredictable shocks. \n To many of the readers of this blog, the above is not news. Political scientists have been talking about \u201cthe fundamentals\u201d for awhile, to the extent that journalists and other observers sometimes overestimate the importance of the economy in determining the election (for example, here\u2019s a clueless history professor likening the predictability elections to \u201cthe law of gravity\u201d). As John Sides explained reasonably, you have to be careful when translating economic numbers into vote predictions. \n Still, a bit of old-fashioned random-walk thinking remains in the old-fashioned news media. For example, Michael Kinsley recently wrote : \n In 1988, Michael Dukakis, who was ahead in the polls just after the Democratic convention, declared in his acceptance speech: \u201cThis election isn\u2019t about ideology. It\u2019s about competence.\u201d Then he proceeded to blow a large lead and lose to George Bush the Elder, who turned out to be a tougher old bird than anyone suspected. \n This sort of understanding of campaigns was pretty standard a few decades ago, back when Kinsley was editor of the New Republic, but nowadays we wouldn\u2019t frame Dukakis as having \u201cblown a large lead\u201d but rather that he lost a lead that was effectively unsustainable, given the economic and political conditions of 1988. Nor would we need to characterize Bush Senior as a \u201ctough old bird\u201d for winning this election; it was more about being in the right place at the right time. \n To say that Dukakis blew a lead is not quite to buy into a random-walk model, but I think it is close. Given what we know about elections, I think it would be more accurate to say that the 1988 election was Bush\u2019s to lose (and he didn\u2019t). \n Anyway, that Kinsley quote is an example of why I think this blog post could be helpful. I\u2019m hoping that, by explicitly stating the random-walk and mean-reversion scenarios, I can make people more aware of the implicit models that underly their stories about campaigns and elections."], "link": "http://andrewgelman.com/2012/09/a-non-random-walk-down-campaign-street/", "bloglinks": {}, "links": {"http://freakonomics.nytimes.com/": 1, "http://themonkeycage.org/blog": 2, "http://andrewgelman.com/": 1, "http://www.columbia.edu/": 1, "http://fivethirtyeight.nytimes.com/": 1, "http://www.bloomberg.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Kaiser asks: \n Trying to figure out what are some keywords to research for this problem I\u2019m trying to solve. I need to estimate seasonality but without historical data. What I have are multiple time series of correlated metrics (think department store sales, movie receipts, \netc.) but all of them for 52 weeks only. I\u2019m thinking that if these metrics are all subject to some underlying seasonality, I should be able to estimate that without needing prior years data. \n My reply: \n Can I blog this and see if the hive mind responds? I\u2019m not an expert on this one. \n My first thought is to fit an additive model including date effects, with some sort of spline on the date effects along with day-of-week effects, idiosyncratic date effects (July 4th, Christmas, etc.), and possible interactions. \n Actually, I\u2019d love to fit something like that in Stan, just to see how it turns out. It could be a tangled mess but it could end up working really well!"], "link": "http://andrewgelman.com/2012/09/estimating-seasonality-with-a-data-set-thats-just-52-weeks-long/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Felipe Nunes writes: \n I have many friends working with data that they claim to be considered as a \u2018population\u2019. For example, the universe of bills presented in a Congress, the roll call votes of all deputies in a legislature, a survey with all deputies in a country, the outcomes of an election, or the set of electoral institutions around the world. Because of the nature of these data, we do not know how to interpret the p-value. I have seen many arguments been made, but I have never seen a formal response to the question. So I don\u2019t know what to say. The most common arguments among the community of young researchers in Brazil are: (1) don\u2019t interpret p-value when you have population, but don\u2019t infer anything either; (2) interpret the p-value because of error measurement which is also present, (3) there is no such a thing as a population, so always look at p-values, (4) don\u2019t worry about p-value, interpret the coefficients substantively, and (5) if you are frequentist you interpret p-value, if you are bayesian you don\u2019t. \n If you have a paper or any other reference that can help with this discussion, please refer to me as well. \n Here\u2019s my reply."], "link": "http://andrewgelman.com/2012/09/what-do-statistical-p-values-mean-when-the-sample-the-population/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Hogg writes: \n At the end this article you wonder about consistency. Have you ever considered the possibility that utility might resolve some of the problems? I have no idea if it \nwould\u2014I am not advocating that position\u2014I just get some kind of intuition from phrases like \u201cJudgment is required to decide\u2026\u201d. Perhaps there is a coherent and objective description of what is\u2014or could be\u2014done under a coherent \u201cutility\u201d model (like a utility that could be objectively agreed upon and computed). Utilities are usually subjective\u2014true\u2014but priors are usually subjective too. \n My reply: \n I\u2019m happy to think about utility, for some particular problem or class of problems going to the effort of assigning costs and benefits to different outcomes. I agree that a utility analysis, even if (necessarily) imperfect, can usefully focus discussion. For example, if a statistical method for selecting variables is justified on the basis of cost, I like the idea of attempting to quantify the costs of gathering and handling predictors, as compared to the costs of errors in predictions for new data. \n But the problem of incoherence as discussed at the end of my article\u2014that\u2019s something different. Here I\u2019m referring to two fundamental problems with Bayesian data analysis as I practice it: \n 1. I prefer continuous model expansion to discrete model averaging\u2014but the former can be seen as just a limiting case of the latter. So really I need a better understanding of what sorts of model expansions work well and what sorts run into trouble. From a Bayesian perspective, the trouble typically arises from the joint prior distribution over the larger, expanded space. Default choices such as prior independence often create problems that were not so obvious when the model was set up. \n 2. My procedure of model building, inference, and model checking requires outside human intervention. How could a computer do it, if you wanted to program a computer to do Bayesian data analysis? How can our brains do anything approximating Bayesian data analysis? Neither the computer nor the brain has a \u201chomunculus\u201d that can sit outside, make graphs, and do posterior predictive checks. I don\u2019t have a great answer to this right now, but I suspect that the natural or artificial intelligence actually would need some external module to check model fit. This connects to the familiar \u201caha\u201d feeling and to the fractal nature of scientific revolutions."], "link": "http://andrewgelman.com/2012/09/incoherence-of-bayesian-data-analysis/", "bloglinks": {}, "links": {"http://www.columbia.edu/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Via Tom LaGatta, Boris Glebov writes: \n My labmates have statistics problem. We are all experimentalists, but need an input on a fine statistics point. \n The problem is as follows. The data set consists of photon counts measured at a series of coordinates. The number of input photons is known, but the system transmission (T) is not known and needs to be estimated. The number of transmitted photons at each coordinate follows a binomial distribution, not a Gaussian one. \n The spatial distribution of T values it then fit using a Levenberg-Marquart method modified to use weights for each data point. \n At present, my labmates are not sure how to properly calculate and use the weights. The equations are designed for Gaussian distributions, not binomial ones, and this is a problem because in many cases the photon counts are near the edge (say, zero), where a Gaussian width is nonsensical. \n Could you recommend a source they could use to guide their calculations? \n My reply: \n I don\u2019t know anything about this (although I assume I could figure out a good answer easily enough if I knew more about the model). I just thought this was worth sharing, partly because maybe some readers have a good answer and partly as an example of the wide variety of terminology used in different statistical applications. \n My general advice in this sort of problem is to forget about the weights as weights and instead think about where they came from, and include in the model (typically in a likelihood function or a poststratification summary) the information that went into the weights."], "link": "http://andrewgelman.com/2012/09/analyzing-photon-counts/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Even within the realm of writing-about-statistics, there are things I can say in a blog that are much more difficult to include in an academic article. Blogging gives me freedom. \n But I want to distinguish between two different sorts of frankness. \n 1. Obnoxiousness: In a blog I can write, \u201cI hate X\u201d as rudely as I\u2019d like without needing to justify myself. \n 2. Openness: In a blog I can write about the limitations of my work. It\u2019s a real challenge to discuss limitations in a scholarly article, as we\u2019re always looking over our shoulder at what referees might think. Sure, sometimes I can get away with writing \u201cSurvey weighting is a mess,\u201d but my impression is that most scholarly articles are relentlessly upbeat. Sort of like how a magazine article typically will have a theme and just plug it over and over. In a blog we can more easily admit uncertainty. \n Overall, I think blogs are more celebrated for feature 1 above (the freedom to say what you really feel, to be rude, partisan, and politically incorrect), but I think feature 2 (the freedom to express uncertainty) is important too."], "link": "http://andrewgelman.com/2012/09/speaking-frankly/", "bloglinks": {}, "links": {"http://www.columbia.edu/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["My upstairs colleague Blattman writes : \n The trend is unsurprising. Schools have every incentive to move to the highest four or five piles [grades] possible. . . . Then grade inflation will stop because . . there will be nowhere to go. . . . So why resist the new equilibrium? \n I don\u2019t have any argument for resisting, but I don\u2019t think everything\u2019s quite so simple as Chris is saying. \n First, you can easily get compression so that almost everyone gets the same grade (\u201cA\u201d). So, no four or five piles. \n Second, the incentives for grading high have been there for awhile. To me, the more interesting question is: how is it that grade inflation hasn\u2019t gone faster? Why would it take so many decades to reach a natural and obvious equilibrium? Here\u2019s what I wrote last year : \n \nAs a teacher who, like many others, assigns grades in an unregulated environment (that is, we have no standardized tests and no rules on how we should grade), all the incentives to toward giving only A\u2019s: When I give A\u2019s, students are happier and complain less, I get to feel like a nice person, and I give my own students (whom I generally have somewhat warm feelings toward) a benefit in their future lives. Back when I used to organize a class with several different section leaders, each instructor wanted to give his or her students higher grades. We had common assignments and a common final exam; even so, each instructor had a reason why his or her students deserved some exemption from the grading cutoffs. \n So the real question is, why have grades been going up so slowly ? I assume that back in the 1940s, a prof couldn\u2019t really just give all A\u2019s to his or her classes: someone would probably notice and say something. But now we really can, and it\u2019s been that way for awhile. \n The fact that profs don\u2019t give all A\u2019s, even though they can, is interesting to me. My explanation for this behavior is as follows: college professors typically got high grades themselves in college. Getting high grades is part of how we defined ourselves when we were students. So, now that we\u2019re giving out the grades, we don\u2019t want to devalue this currency. It\u2019s not a matter of self-interest\u2013if I give out a bunch of A\u2019s to my students, it\u2019s not going to retroactively tarnish my college grade-point average. Rather, I think it\u2019s just that profs see grades as important in themselves. Sort of like rich people who don\u2019t want to debase the currency, just as a matter of principle. \n I remember looking at grading records for undergraduate classes back when I taught at Berkeley in the early 1990s. There was lots of variation in average grades by instructor, even for different sections of the same class. I didn\u2019t do a formal study, but I remember when flipping through the sheets that average grade seemed to be correlated with niceness. The profs who were generally pleasant people tended to give lots of A\u2019s, while the jerks were giving lower grades. Again, no standardized tests so no way to judge whether the average grades were informative, but I doubt it. \n At the institutional level, these problems with grades would be fixed using standardized tests or with some sort of statistical correction such as proposed by statistician Val Johnson, who writes : \n \n There are two approaches that might be taken in reforming our grading system. The first is to encourage faculty to modify their grading practices and adhere to a \u201ccommon\u201d grading standard. The second is to make post-hoc adjustments to assigned grades to account for differences in faculty grading policies. \n The beauty of Val\u2019s approach is that it does three things: \n 1. By statically correcting for grading practices, Val\u2019s method produces adjusted grades that are more informative measures of student ability. \n 2. Since students know their grades will be adjusted, they can choose and evaluate their classes based on what they expect to learn and how they expect to perform; they don\u2019t have to worry about the extraneous factor of how easy the grading is. \n 3. Since instructors know the grades will be adjusted, they can assign grades for accuracy and not have to worry about the average grade. (They can still give all A\u2019s but this will no longer be a benefit to the individual students after the course is over.)"], "link": "http://andrewgelman.com/2012/09/grade-inflation-why-werent-the-instructors-all-giving-all-as-already/", "bloglinks": {}, "links": {"http://andrewgelman.com/": 2, "http://www.dukechronicle.com/": 1, "http://chrisblattman.com/": 1, "http://projecteuclid.org/": 1}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}, {"content": ["Dan Silitonga writes: \n I was wondering whether you would have any advice on building a regression model on a very small datasets. I\u2019m in the midst of revamping the model to predict tax collections from unincorporated businesses. But I only have 27 data points, 27 years of annual data. Any advice would be much appreciated. \n My reply: \n This sounds tough, especially given that 27 years of annual data isn\u2019t even 27 independent data points. \n I have various essentially orthogonal suggestions: \n 1 [added after seeing John Cook's comment below]. Do your best, making as many assumptions as you need. In a Bayesian context, this means that you\u2019d use a strong and informative prior and let the data update it as appropriate. In a less formal setting, you\u2019d start with a guess of a model and then alter it to the extent that your data contradict your original guess. \n 2. Get more data. Not by getting information on more years (I assume you can\u2019t do that) but by breaking up the data you do have, for example by geography, or class of business, or size of business, or some other factor. Or could each business be a data point? What I\u2019m getting at is, it seems that you must have a lot more than 27 pieces of information you could analyze. \n 3. With a small n and many predictors, you often can\u2019t come to a good story about what is happening but you can still rule out a lot of potential stories. For example, suppose you have 20 candidate predictors. You can\u2019t just throw these into a regression. But you can correlate each of the predictors with the outcome, one at a time, and discover either a very close predictive relation with one or more of the separate predictors, or no such relation. Either way, you\u2019ve learned something. It ain\u2019t nothing to know that none of these 20 inputs determines the output all by itself. \n 4. You can combine predictors. For example, if you have 5 similar predictors, each measuring some aspect of a common input, you can average them (after rescaling, if necessary) and then use that average as a single predictor. Bill James did that sort of thing in his baseball analyses. Instead of throwing all his variables into a regression, he\u2019d use theory (of a sort) and some data analysis to compute composite scores such as \u201cruns created\u201d and then use these composites in his further analyses."], "link": "http://andrewgelman.com/2012/09/building-a-regression-model-with-only-27-data-points/", "bloglinks": {}, "links": {}, "blogtitle": "Statistical Modeling, Causal Inference, and Social Science"}]