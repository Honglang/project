[{"blogurl": "http://biostatmatt.com\n", "blogroll": [], "title": "BioStatMatt"}, {"content": ["My picture made the front page of The Reporter , the Vanderbilt University Medical Center weekly paper, for an article about our department's biostatistics clinics . By sheer luck, I was wearing a tie that day. Unfortunately, it appears that I am gesturing crudely toward Bob Johnson. Although I was not making a crude gesture, Bob appears to be making a face in reaction. \n Alas, my name doesn't appear anywhere in the article or image caption. Also pictured (from left to right; those that I can name off the top of my head) are Nate Mercaldo, Dave Afshartous, Frank Harrell, and Sarah Fletcher."], "link": "http://biostatmatt.com/archives/2217", "bloglinks": {}, "links": {"http://news.vanderbilt.edu/": 1, "http://biostatmatt.com/": 1}, "blogtitle": "BioStatMatt"}, {"content": ["Thanks to Anthony Damico who alerted me to an error in the Bank of America's 1% cash rewards figure in my last post. The plot shows the true reward percentage as a function of purchase amount. A key \"feature\" of the BoA reward program is that the full 1% is only awarded for purchases in whole dollar amounts. But, in the figure I neglected to plot the function at each whole dollar amount. Hence, the function was a bit misleading. Anthony sent the code below that corrects this; the figure is below. \n \nx <- seq(1,100,length.out=9901)\nplot(x, trunc(x)/x, type=\"l\",\n  xlab=\"Purchase Amount ($)\",\n  ylab=\"BoA Cash Reward (%)\")\nabline(h=1, lty=2)"], "link": "http://biostatmatt.com/archives/2209", "bloglinks": {}, "links": {"http://biostatmatt.com/": 2}, "blogtitle": "BioStatMatt"}, {"content": ["Bank of America (BoA) has a \"Cash Rewards\" credit card that pays \"1% cash back everywhere, every time\" 1 . But if you read the fine print, it's clear that the reward is almost always less than 1%. Here's the relevant sentence from the terms and conditions 2 : \n \nFractions are truncated at the 100th decimal place, and are subject to verification...\n \n This sentence is cryptic, and the context only helps a little bit. It means that for each purchase amount m, the reward is the value of m * 0.01 truncated at the 100th decimal place. For example, suppose that m = $10.59. One percent of m is $0.1059. The reward is then $0.10. \n This means that rewards are not paid on the fractional part of purchase amounts; that the full 1% is paid only for whole dollar amounts. Otherwise, the reward is less than 1%. As evidenced by my own transaction history, fractional purchase amounts are common. Hence, the full 1% cash reward is almost never achieved. \n The actual cash reward percentage that BoA pays on each purchase depends on (1) the fractional purchase amount, and (2) the total purchase amount. For factional dollar amounts, the reward approaches 1% as the total amount becomes larger. And again, for whole dollar amounts, the percentage is exactly 1%. As a function of purchase amount, the cash reward percentage has a \"saw tooth\" shape: \n \nx <- seq(1.00, 100, length.out=1000)\nplot(x, trunc(x)/x, type=\"l\", xlab=\"Purchase Amount ($)\", ylab=\"BoA Cash Reward (%)\")\nabline(h=1, lty=2)\n \n  \n Consumers who tend to make small purchases will generally receive a smaller cash reward percentage than those who make larger purchases. To illustrate, consider spending $1000 dollars in 50 small purchases versus spending all $1000 in two large purchases. We can simulate these two strategies by drawing the proportions spent at each transaction from the Dirichlet distribution. \n \nsimulate \n In 95% of these simulations, the reward amount for the first strategy (50 small purchases) was between $9.71 (0.971%) and $9.79 (0.979%), and between $9.99 (0.999%) and $9.99 (0.999%) for the second strategy (2 large purchases). \n 1 https://www.bankofamerica.com/credit-cards/products/cash-back-credit-card.go \n 2 https://www.bankofamerica.com/credit-cards/credit-cards-terms-and-conditions.go?cid=2083811&po=AS"], "link": "http://biostatmatt.com/archives/2191", "bloglinks": {}, "links": {"http://biostatmatt.com/": 3, "https://www.bankofamerica.com/": 2}, "blogtitle": "BioStatMatt"}, {"content": ["I have sometimes heard complaints from collaborators that it will be impossible to have their work published in the mainstream literature unless a p-value is reported. This post is to report yet another counterexample that was recently published; a meta-analysis for the odds of perioperative bleeding complications in patients taking one of several anticoagulant/antiplatelet drugs. In this study 1 (published by Circulation: Arrhythmia and Electrophysiology ), the statistical evidence was reported using Bayesian point estimates and credible intervals. The referees had no problem with the absence of p-values. However, they did require some additional explanation of the credible interval relative to the confidence interval, and of the Bayesian approach in general because they felt that these concepts would be less familiar to their readers. Below is the text that we used. Perhaps readers here can help refine it for future work. \n \nBayesian analysis formalizes the notion of prior evidence about quantities under study, that is, the evidence at hand before an experiment is carried out. In this meta-analysis, the quantities under study are log-odds of perioperative bleeding. Prior evidence may be updated in light of experimental data to yield the posterior evidence. The Bayesian method encodes evidence as a probability distribution. Hence, a prior distribution is specified to reflect prior evidence, and the posterior evidence is summarized using a posterior distribution.\n \n Another complication that arises when we take a Bayesian approach is justification of the selected prior distribution: \n \nThe log-odds were each assigned a normal-gamma prior distribution with mean zero, and small (0.1) shape and rate parameters. The resulting prior distribution was heavy-tailed and symmetric about zero. This choice of prior distribution reflected conservative (zero mean) but weak (heavy-tailed) prior evidence regarding the effects of treatment and study on the log-odds of bleeding.\n \n Indeed, the marginal distribution of the log-odds, with these parameter values, has infinite kurtosis. Kurtosis is a measure of the degree to which a distribution is \"heavy-tailed\". But, since this manuscript was intended for medical professionals, we opted for a more relevant description: \n \nTo further illustrate, suppose that an antiplatelet/anticoagulant therapy is considered \"safe\" with regard to bleeding complications when the estimated log-odds is zero, and there is strong evidence that the estimate is accurate to within 5 events per 100 patients (i.e., that the log-odds is confined within the interval -0.2-0.2 with high probability). Although the prior estimate is zero, there is weak prior evidence (prior probability \n Log-odds of -0.2 and 0.2 correspond roughly to probabilities 0.45 and 0.55 respectively. Hence, if there were strong prior evidence that the log-odds occurred within this interval, then we would be confident that the prior estimate of the log-odds (zero) was accurate to within 5 patients per 100. However, the prior probability that the log-odds occur within (-0.2, 0.2) is small. We can easily form a Monte Carlo estimate of this probability using R \n \n> # Sample from the prior distribution on the log-odds\n> plo <- rnorm(1e5, mean=0, sd=1/sqrt(rgamma(1e5, shape=0.1, rate=0.1)))\n> # Estimate the prior probability that the log-odds occur in (-0.2, 0.2)\n> mean(plo >= -0.2 & plo < 0.2)\n[1] 0.07544\n \n I think presenting the evidence in terms of (credible or confidence) intervals will prevail as the preferred method for reporting scientific results. I another post, I will try to enumerate some of the advantages of this approach over hypothesis and significance testing. \n 1 Michael L. Bernard, Matthew Shotwell, Paul J. Nietert, and Michael R. Gold. 2012. Meta-Analysis of Bleeding Complications Associated with Cardiac Rhythm Device Implantation. Circulation: Arrhythmia and Electrophysiology. Pre-print, April 24 2012, doi:10.1161/CIRCEP.111.969105."], "link": "http://biostatmatt.com/archives/2178", "bloglinks": {}, "links": {"http://biostatmatt.com/": 1, "http://circep.ahajournals.org/": 1}, "blogtitle": "BioStatMatt"}, {"content": ["Some time ago, I had an ineloquent and less-than-cordial online discussion with a commenter on this site, partially about how statisticians define the term \"parameter\". This post is just to quote a relevant passage from \"Bootstrap Methods and Their Application\", by Davison and Hinkley (1997), that better articulates a point I had made earlier. \n \n2.1.1 Statistical Functions \nMany simple statistics can be thought of in terms of properties of the EDF [empirical distribution function]. For example the sample average $\\bar{y} = n^{-1} \\sigma y_j$ is the mean of the EDF. More generally, the statistic of interest $t$ will be a symmetric function of $y_1,\\ldots,y_n$, meaning that $t$ is unaffected by reordering the data. This implies that $t$ depends on the ordered values $y_{(1)} \\leq \\cdots \\leq y_{(n)}$, or equivalently on the EDF $\\hat{F}$. Often this can be expressed simply as $t = t(\\hat{F})$, where $t(\\cdot)$ is a statistical function - essentially just a mathematical expression of the algorithm for computing $t$ from $\\hat{F}$. Such a statistical function is of central importance in the nonparametric case because it also defines the parameter of interest $\\theta$ through the \"algorithm\" $\\theta = t(F)$. This corresponds to the qualitative idea that $\\theta$ is a characteristic of the population described by $F$..."], "link": "http://biostatmatt.com/archives/2166", "bloglinks": {}, "links": {"http://biostatmatt.com/": 1}, "blogtitle": "BioStatMatt"}, {"content": ["Forwarded from Frank Harrell: \n DEADLINES FAST APPROACHING \u2013 8th Annual International R User Conference useR! 2012, Nashville, Tennessee USA \n Registration Deadlines: \nEarly Registration: Passed \nRegular Registration: Mar 1- May 12 \nLate Registration: May 13 \u2013 June 4 \nOn-Site Registration: June 12 \u2013 June 15 \n Please note: Nashville is offering several large entertainment events the month of June, and hotels are quickly selling out. It's imperative that you make your hotel accommodations for the conference as soon as possible. For those of you who have submitted abstracts we will be \nnotifying you this week regarding acceptance as an oral presentation or as a poster. \n Students: A limited number of $500 reimbursements for registration and travel expenses are available, based on merit and need. Please apply by sending an application to Tatsuki Koyama at tatsuki.koyama@Vanderbilt.Edu by April 15. Include a brief CV, a copy of your abstract if one was submitted, a statement that demonstrates your need for financial assistance, and a letter of support from your supervisor. \n Please join us at the 8th Annual International R User Conference useR! 2012 in Nashville, Tennessee. For more conference details, please visit http://biostat.mc.vanderbilt.edu/wiki/Main/useR-2012"], "link": "http://biostatmatt.com/archives/2150", "bloglinks": {}, "links": {"http://biostat.vanderbilt.edu/": 2}, "blogtitle": "BioStatMatt"}, {"content": ["That's a mouthful! I presented this topic to a group of Vandy statisticians a few days ago. My notes (essentially reproduced in this post) are recorded at the Dept. of Biostatistics wiki: HowToBootstrapCorrelatedData . The presentation covers some bootstrap strategies for hierarchically structured (correlated) data, but focuses on the multi-stage bootstrap; an extension of that described by Davison and Hinkley (ISBN 978-0-521-57471-6). \n The multi-stage bootstrap mimics the data generating mechanism by resampling in a nested fashion. For example, resample first among factors at the highest level of hierarchy. Then, for each resampled factor, further resample among factors at the next lower level, and so forth. Each level may be resampled with or without replacement. Furthermore, some levels of hierarchy may be ignored completely, if considered to have little or no effect on the data correlation structure. Whether to ignore a level of hierarchy, or to sample with replacement are important bootstrap design considerations. \n The resample function below implements a multi-stage bootstrap recursively. That is, levels of hierarchy are traversed by nested calls to resample . The dat argument is a dataframe with factor fields for each level of hierarchy ( e.g. , hospital, patient, measurement), and a numeric field of measured values. The cluster argument is a character vector that identifies the hierarchy in order from top to bottom ( e.g. , c('hospital','patient','measurement') ). The replace argument is a logical vector that indicates whether sampling should be with replacement at the corresponding level of hierarchy ( e.g. , c(TRUE,FALSE,FALSE) ). \n \nresample <- function(dat, cluster, replace) {\n \n # exit early for trivial data\n if(nrow(dat) == 1 || all(replace==FALSE))\n  return(dat)\n \n # sample the clustering factor\n cls <- sample(unique(dat[[cluster[1]]]), replace=replace[1])\n \n # subset on the sampled clustering factors\n sub <- lapply(cls, function(b) subset(dat, dat[[cluster[1]]]==b))\n \n # sample lower levels of hierarchy (if any)\n if(length(cluster) > 1)\n sub <- lapply(sub, resample, cluster=cluster[-1], replace=replace[-1])\n \n # join and return samples\n do.call(rbind, sub)\n\n}\n \n The following block of R code simulates a dataset with 5 correlated (rho = 0.4) repeat measurements on each of 10 patients, from each of 5 hospitals. Hence, there are 250 simulated measurements and 50 patients in total. Patients are simulated independently ( i.e. , the hospital level of hierarchy has no affect on the correlation structure). The functions covimage and datimage generate a levelplot representations of the covariance and data matrices for the simulated data, respectively. \n \n# simulate correlated data\nrho <- 0.4\ndat <- expand.grid(\n measurement=factor(1:5),\n patient=factor(1:10),\n hospital=factor(1:5))\nsig <- rho * tcrossprod(model.matrix(~ 0 + patient:hospital, dat))\ndiag(sig) <- 1\ndat$value <- chol(sig) %*% rnorm(250, 0, 1)\n\nlibrary(\"lattice\")\n\ncovimage <- function(x)\n levelplot(as.matrix(x), aspect=\"fill\", scales=list(draw=FALSE),\n  xlab=\"\", ylab=\"\", colorkey=FALSE, col.regions=rev(gray.colors(100, end=1.0)),\n  par.settings=list(axis.line=list(col=NA,lty=1,lwd=1)))\n \ndatimage <- function(x) {\n mat <- as.data.frame(lapply(x, as.numeric))\n levelplot(t(as.matrix(mat)), aspect=\"fill\", scales=list(cex=1.2, y=list(draw=FALSE)),\n  ylab=\"\", xlab=\"\", colorkey=FALSE, col.regions=gray.colors(100),\n  par.settings=list(axis.line=list(col=NA,lty=1,lwd=1)))\n}\n\ndatimage(dat)\ncovimage(sig)\n \n The images below result from calls to datimage(dat) and covimage(dat) respectively. \n  \n  \n The next block of R code generates several boostrap distributions for the sample mean, and approximates the 'true' sampling distribution by Monte Carlo. The final series of boxplots (shown below) illustrate that bootstrap design greatly impacts the inferred distribution of the sample mean (and presumably for other sample statistics). Hence, it's important to think carefully about bootstrap design for hierarchically structured data, and ensure that it closely reflects the 'true' data generating mechanism. \n \n# bootstrap ignoring hospital and patient levels\ncluster <- c(\"measurement\")\nsystem.time(mF <- replicate(200, mean(resample(dat, cluster, c(F))$val)))\nsystem.time(mT <- replicate(200, mean(resample(dat, cluster, c(T))$val)))\n#boxplot(list(\"F\" = mF, \"T\" = mT))\n\n# bootstrap ignoring hospital level\ncluster <- c(\"patient\",\"measurement\")\nsystem.time(mFF <- replicate(200, mean(resample(dat, cluster, c(F,F))$val)))\nsystem.time(mTF <- replicate(200, mean(resample(dat, cluster, c(T,F))$val)))\nsystem.time(mTT <- replicate(200, mean(resample(dat, cluster, c(T,T))$val)))\n#boxplot(list(\"FF\" = mFF, \"TF\" = mTF, \"TT\" = mTT))\n\n# bootstrap accounting for full hierarchy\ncluster <- c(\"hospital\",\"patient\",\"measurement\")\nsystem.time(mFFF <- replicate(200, mean(resample(dat, cluster, c(F,F,F))$val)))\nsystem.time(mTFF <- replicate(200, mean(resample(dat, cluster, c(T,F,F))$val)))\nsystem.time(mTTF <- replicate(200, mean(resample(dat, cluster, c(T,T,F))$val)))\nsystem.time(mTTT <- replicate(200, mean(resample(dat, cluster, c(T,T,T))$val)))\n#boxplot(list(\"FFF\" = mFFF, \"TFF\" = mTFF, \"TTF\" = mTTF, \"TTT\" = mTTT))\n\n# Monte Carlo for the true sampling distribution\nsystem.time(mMC <- replicate(200, mean(chol(sig) %*% rnorm(250, 0, 1))))\n#boxplot(list(\"MC\" = mMC))\n\nboxplot(list(\"MC\" = mMC,\n    \"F\" = mF, \"T\" = mT,\n    \"FF\" = mFF, \"TF\" = mTF, \"TT\" = mTT,\n    \"FFF\" = mFFF, \"TFF\" = mTFF, \"TTF\" = mTTF, \"TTT\" = mTTT))\n \n The following figure presents boxplots for the distribution of sample means under the above sequence of bootstrap strategies. The \"MC\" boxplot summarizes the 'true' distribution of the sample mean (estimated using Monte Carlo). The remaining boxplots are labeled according to the bootstrap strategy used. For instance, the \"TF\" boxplot corresponds to a multi-stage bootstrap of patients with replacement and measurements-within-patients without replacement (this is commonly called the \"cluster bootstrap\"), but that ignores the hospital factor. This strategy most closely reflects the data generating mechanism. Notice that sampling all levels of hierarchy without replacement ( e.g. , \"FFF\") simply permutes the indices of the resampled data, and does not confer any variability on the sample mean."], "link": "http://biostatmatt.com/archives/2125", "bloglinks": {}, "links": {"http://biostatmatt.com/": 4, "http://biostat.vanderbilt.edu/": 1}, "blogtitle": "BioStatMatt"}, {"content": [], "link": "http://biostatmatt.com/archives/2115", "bloglinks": {}, "links": {"http://biostatmatt.com/": 1}, "blogtitle": "BioStatMatt"}, {"content": ["useR! 2012 is just around the corner. The deadline for talk and poster abstract submissions is today! Submit your abstract here ."], "link": "http://biostatmatt.com/archives/2108", "bloglinks": {}, "links": {"http://biostat.vanderbilt.edu/": 3}, "blogtitle": "BioStatMatt"}, {"content": ["The early registration deadline for useR! 2012 is tomorrow! Visit the Online Registration Website . The fees for registration increase March 1 st ."], "link": "http://biostatmatt.com/archives/2097", "bloglinks": {}, "links": {"http://biostat.vanderbilt.edu/": 2}, "blogtitle": "BioStatMatt"}]