[{"blogurl": "http://www.drewconway.com/zia\n", "blogroll": [], "title": "Zero Intelligence Agents"}, {"content": ["In a post on dataists almost two years ago , John Myles White and I posed the question: \u201cHow would you rank the popularity of a programming language?\u201d. \n From the original post: \n One way to do so is to count the number of projects using each language, and rank those with the most projects as being the most popular. Another might be to measure the size of a language\u2019s \u201ccommunity,\u201d and use that as a proxy for its popularity. Each has their advantages and disadvantages. Counting the number of projects is perhaps the \u201cpurest\u201d measure of a language\u2019s popularity, but it may overweight languages based on their legacy or use in production systems. Likewise, measuring community size can provide insight into the breadth of applications for a language, but it can be difficult to distinguish among language with a vocal minority versus those that are actually have large communities. \n So, we spent an evening at Princeton hacking around on Github and StackOverflow to get data on the number of projects and questions tagged, per programming language, respectively. The result was a scatter plot showing the linear relationship between these two measures . As with any post comparing programming languages, it was great bait for the Internet masses to poke holes in, and since then Stephen O\u2019Grady at Redmonk has been re-running the analysis to show changes in the relative position of languages over time. \n Today I am giving a talk at Monktoberfest on the importance of pursuing good questions in data science. As an example, I wanted to revisits the problem of ranking programming languages. For a long time I have been unsatisfied with the outcome of the original post, because the chart does not really address the original question about ranking. \n Scatter plots are a really poor way of displaying rank correlation. In reality, the chart from the original post is a great first step in the exploration of the data. Clearly there is a relationship between the two metrics that is strongly linear; we can see there are many outliers on either side, but, it doesn\u2019t help us understand how to rank the languages based on these measures. So, I decided to pull the data again, and create a new visualization. \n  \n If you have not seen one before, this is a Tufte-style \u201cslopegraph,\u201d which Tufte developed to show the change in gradients over time. Here I am using the technique to compare two independent rankings of the same list. A couple of things to note about the graph: \n \n The languages that have been bumped out into the margins by brackets represent ties in the ranking. This happens much more often in StackOverflow data because we must convert the highly skewed number of tags to a rank. Unlike the Github data, where the rank is the value provided.\n The colors of the lines correspond to a simple k-means clustering of the languages\u2019 ranks using five clusters. For the k-means, the distance is based on the among languages in the original scatter plot. \n The descending grey bands represent the rank quartiles, e.g., the top band is the 100th percentiles, then 80th percentile, etc. \n \n Not, perfect, but I can get much more information on the ranking by looking at this chart \n \n By adding the k-means clusters and the percentile bands, we can see a much clearer picture of how languages group by tiers. \n There is actually a large amount of agreement between the two rankings when we view the quartiles as popularity tiers. For example, the purple cluster is almost exclusively in the 100th percentile, or top tier. Save for R, which Github ranks in the 80th percentile! Likewise, the teal cluster is primarily in the next grey band, or second tier \n There is also a clear bottom tier for the least popular languages, represented here by the orange cluster. \n For some languages this method of ranking is completely useless. In this case, those languages are in the fuchsia cluster. Many of them are very highly ranked in Github, but have zero questions tagged on StackOverflow. \n \n To further simplify the chart, and again to get closer to answering the question of what makes a programming language popular, I redrew the chart given the observations mentioned above: \n  \n You\u2019ll notice that for the languages in the green cluster I simply pull them out of the vertical ordering and call them \u201cHigh Variance.\u201d Unlike the fuchsia cluster, the rank comparison between Github and StackOverflow is meaningful, but much weaker. These languages fall anywhere between the 20th and 100th percentiles, but from the slope chart we can see that that for many of them the two data sets do not rank them wildly differently. \n Given the data, and the observation made in the slopegraph, we can make qualified statements about programming language popularity; such as, \u201cThe most popular programming languages are those that fall in the 20th percentile of rank between Github projects and StackOverflow tags.\u201d Statements about the second most, and least popular languages are equally valid, though \u2014 of course \u2014 not definitive. Finally, for completeness, here are the group membership of each language for the clusters: \n  \n Of course, all of this is merely an exercise in descriptive analysis. The real value would be in understanding why languages fall into these tiers. Hopefully the folks at Monktoberfest can help me with that one! \n Oh, and here are the slides from my presentation at Monktoberfest"], "link": "http://www.drewconway.com/zia/?p=2892", "bloglinks": {}, "links": {"http://www.drewconway.com/": 4, "http://www.edwardtufte.com/": 1, "http://www.dataists.com/": 2, "http://www.johnmyleswhite.com/": 1, "http://monktoberfest.com/": 1, "http://redmonk.com/": 1}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["The New York City data community is a very jovial and tight-knit community. We go to Meetups together, visit each other at work to talk about projects, and even occasionally take over bars together. This is partly because all of us are crammed on this tiny island with everyone else; but it\u2019s mostly due to the effort of a group of extraordinary people from very different backgrounds committed to making NYC a great place to be doing data science. \n One of the threads that has consistently come out of conversations I have had with members of the community is the desire to highlight the strength of NYC\u2019s data community vis a vis this diversity of backgrounds and interests. For example, while NYC has become a strong geographic complement to Silicon Valley as a hub for technology startups , the data community stretches far beyond the startups. \n Many of our traditional stalwart industries; such as finance, entertainment, and media, that have been building sophisticated analytics products and teams for some time, but have only recently started speaking more publicly about these efforts. At the same time, industries with less obvious connections to the data community; such as design, fashion, and non-profits, have thrust themselves into the NYC data community. \n All this is to say, we believe that there is something special about New York that makes it a great place to be doing data science. We think and do things differently, and we have a very diverse and unique set of constituencies in the city that have cultivated a special community and culture. So, we want to show everyone just how awesome this community is. \n Enter DataGotham : http://datagotham.com \n This event is the first of its kind. Rather than focus on the tools and techniques people are using, DataGotham will bring together professionals from across the NYC data community for intense discussion, networking, and sharing of wisdom. The goal is to tell stories about what problems people are solving, and the highs and lows of that process. DataGotham is being organized by Hilary Mason , Mike Dewar , John Myles White , and myself; and will take place September 13th-14th at NYU Stern. \n You can check out our great, and ever expanding, speaker list here: http://www.datagotham.com/speakers/ . We also have four tutorials running on the afternoon of the 13th , followed by cocktails and The Great Data Extravaganza Show at the Tribeca Rooftop that evening. \n Tickets are on sale now, and can be purchased at http://datagotham.eventbrite.com . Also, as a reader of this blog, I would like to offer you a special discount of 25% off registration by using the promo code \u201c dataGothamist \u201d when you register. \n We are very excited about the opportunity to provide a platform for this great community, and we would love to see you there!"], "link": "http://www.drewconway.com/zia/?p=2884", "bloglinks": {}, "links": {"http://www.datagotham.com/": 2, "https://twitter.com/": 1, "http://www.johnmyleswhite.com/": 1, "http://www.hilarymason.com/": 1, "http://datagotham.eventbrite.com": 1, "http://www.nytimes.com/": 1, "http://datagotham.com": 2}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["A couple of days ago someone posted a link to a data set of all TIME Magazine covers, from March, 1923 to March, 2012 . Of course, I downloaded it and began thumbing through the images. As is often the case when presented with a new data set I was left wondering, \u201cWhat can I ask of the data?\u201d \n After thinking it over, and with the help of Trey Causey , I came up with, \u201cHave the faces of those on the cover become more diverse over time?\u201d To address this questions I chose to answer something more specific: Has the color values of skin tones in faces on the covers changed over time? \n I developed a data visualization tool, I\u2019m calling the Shades of TIME , to explore the answer to that question. \n The process for generating the Shades of TIME required the following steps: \n \n Using OpenCV to detect and extract the faces appearing in the magazine covers \n Using the Python Image Library to implement the Peer, at al. (2003) skin tone classifier to find the dominant skin tone in each face \n Designing a data visualization and exploration tool using d3.js \n \n The code and data are all available at my Github . Instructions for how to use the tool to explore the data are available at the tool page itself . It is worth checking out just as a fun way to explore the TIME Magazine covers. \n I have two primary observations from exploring the data. First, it does appear that the variance in skin tones have changed over time, and in fact the tones are getting darker. Most of the first quarter of the data are hard to interpret because TIME was still largely using black and white images, and when they did use color it was often artist\u2019s renderings of portraits. The interpretation of skin tone in drawings is difficult. Around the mid-1970\u2032s, however, there appears to be an explosion of skin tone diversity. Of course, there can be many reasons for this, not the least of which may be improvement in photo and magazine printing technologies. \n Second, and much more certainly, is TIME has steadily increased the number of faces that appear on their covers over time. As you scroll through the visualization you will quickly notice the number of faces per cover increase from one, to a few, to many in the 1990\u2032s through 2010\u2032s. Whether this is the result of a desire to show a more diverse set of faces, or increase their marketing appeal on newsstands, or both; is completely unknown. \n But, as with most data projects of this nature the resulting tool generates more observations than questions. Perhaps the most important is how brittle the out-of-the-box face detection algorithms were. As you click through the tone cells you will notice that many of them do not correspond to a face at all. As such, it may be difficult to interpret any of this as relevant to the motivational question. That said, in aggregate there are many more faces than there are false-positives, so the exercise still seems useful."], "link": "http://www.drewconway.com/zia/?p=2874", "bloglinks": {}, "links": {"http://labs.drewconway.com/": 3, "http://www.reddit.com/": 1, "https://twitter.com/": 1, "https://github.com/": 1, "http://mbostock.github.com/": 1, "http://opencv.willowgarage.com/": 1, "http://www.uni-lj.si/": 1, "http://www.pythonware.com/": 1}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["With the release of the eBook version of Machine Learning for Hackers this week, many people have been asking for the code. With good reason\u2014as it turns out\u2014because O\u2019Reilly still (at the time of this writing) has not updated the book page to include a link to the code. \n For those interested, my co-author John Myles White is hosting the code at his Github, which can be accessed at: \n https://github.com/johnmyleswhite/ML_for_Hackers \n Please feel free to clone, fork, and hack the repository as much as you like. As we mention in the README, some of the code will not appear exactly as it does in the text. This happens for two reasons; first, because some minor formatting changes had to be made to fit the code into the book; and second, some of the code has been updated or edited to remove typos and minor errors. \n We hope you find the code a useful supplement to the text!"], "link": "http://www.drewconway.com/zia/?p=2868", "bloglinks": {}, "links": {"http://shop.oreilly.com/": 1, "https://github.com/": 1}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["If you missed the scuttlebutt on Twitter yesterday , I announced that John Myles White and my book, Machine Learning for Hackers was sent to the printers! This means that hard copies will be available very soon, and presumedly an eBook copy will be available even sooner. \n We are thrilled by the community\u2019s interest and enthusiasm for the book, and want to thank everyone who told us that they have already pre-ordered copies. Many people have been asking for a table of contents, which O\u2019Reilly has not yet posted. To give people a preview I have posted the TOC below. Hopefully this will pique your interest even more! \n TOC ML4Hackers"], "link": "http://www.drewconway.com/zia/?p=2864", "bloglinks": {}, "links": {"http://shop.oreilly.com/": 1, "https://twitter.com/": 1, "http://www.scribd.com/": 1}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["As many of you know, this week thousands of people mobilized to protest two laws being considered in Congress: the Stop Online Piracy Act (SOPA) and it\u2019s Senate version the PROTECT IP Act (PIPA). Several Internet mainstays, such as Wikipedia, Reddit andy O\u2019Reilly blacked out their sites to protest the bill . For some information on why this legislation is so dangerous check out this excellent video by The Guardian . \n The mobilization against SOPA/PIPA also included many grassroots efforts to contact Congress and demand the bill be stopped. Given the attention the bill was getting, I was curious if there was any surge in discussion of the bill by members of Congress on Twitter. \n So, I created a visualization that is a cumulative timeline of tweets by members of the U.S. Congress for \u201cSOPA\u201d or \u201cPIPA. \u201d To see if there was any surge, check out the visualization for yourself."], "link": "http://www.drewconway.com/zia/?p=2861", "bloglinks": {}, "links": {"http://labs.drewconway.com/": 2, "http://www.youtube.com/": 1, "http://www.co.uk/": 1, "http://en.wikipedia.org/": 2}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["Last night Mike Dewar presented a wonderful talk to the New York Open Statistical Programming Meetup titled, \u201c First steps in data visualisation using d3.js .\u201d Mike took the audience through an excellent review of d3.js fundamentals, as well as showed off some of the features of working with Chrome Web Developer Tools. This is one of the best talks we have ever had, and if you have had any interest in exploring d3.js , but were intimidated by the design concepts or syntax, this is exactly the talk for you. \n Also, Mike\u2019s slides were all designed using d3.js and are available for download on his Github account: https://github.com/mikedewar/d3talk ."], "link": "http://www.drewconway.com/zia/?p=2857", "bloglinks": {}, "links": {"http://mbostock.github.com/": 1, "http://mikedewar.org/": 1, "https://github.com/": 1, "http://www.meetup.com/": 2}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["Many months ago I blogged about the research that John Myles White and I are conducting on using Twitter data to estimate an individual\u2019s political ideology. As I mentioned then, we are using the Twitter activity of members of the U.S. Congress to build a training data set for our model. A large part of the effort for this project has gone into designing a system to systematically collect the Twitter data on the members of the U.S. Congress. \n Today I am pleased to announce that we have worked out most of the bugs, and now have a reliable data set upon which to build. Better still, we are ready to share . Unlike our old system, the data now lives on a live CouchDB database, and can be queried for specific research tasks. We have combined all of the data available from Twitter\u2019s search API with the information on each member from Sunlight Foundation\u2019s Congressional API . \n To show the power of the database, I decided to use my newly acquired d3.js (sick of it yet?) skills to put together a tool that allows you to compare the monthly Twitter activity of all members of the U.S. Congress on Twitter for 2011. \n Simply choose a politician from the drop-down menu (alphabetical by surname) and the graph will update with their activity data. If you want to reset the graph, just click the \u201cClear selections\u201d button. \n Feel free to add as many members as you like, but the dimensions of the visualization max out around 9. I have been playing around with for awhile, it\u2019s fun! Oh, and if you choose a member and nothing happens it is most likely because that person didn\u2019t tweet anything in 2011. I could have built-in error-catching or some warning. Also, to clear things you need to re-load the page. I\u2019ll leave real UX to the professional web designers. \n \n Back to the data. Unfortunately, the database is sitting on a server that cannot process many requests (read, web-scale) at a time. In fact, this blog post may bring it down! As such, if you are interested in getting access to the database please contact me directly. But be forewarned , working with this system and CouchDB requires a mature understanding of several tools and languages; including but not restricted to; curl , map/reduce , Javascript, and JSON. And that\u2019s before you have even done any analysis. \n Many people have asked me about working with Congressional Twitter data, so I hope this data can be useful. Please feel free to reach out if you have any questions ."], "link": "http://www.drewconway.com/zia/?p=2846", "bloglinks": {}, "links": {"http://curl.haxx.se/": 1, "http://www.drewconway.com/": 1, "https://dev.twitter.com/": 1, "http://mbostock.github.com/": 1, "http://drewconway.com/": 1, "http://services.sunlightlabs.com/": 1, "http://couchdb.apache.org/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["As the United States gears up for what appears to be a long and grueling 2012 presidential campaign , China will also undergo its decennial turnover in presidential power in 2012. Unlike the United States, however, this shift will not involve any campaigning or voting\u2014at least not with the people of China. Instead, this shift is one that is formalized within he Chinese Communist party; but that doesn\u2019t mean that there won\u2019t be interesting shifts and reallocations of power. \n This leads naturally to many questions; perhaps most importantly that of this post\u2019s title: Who are the most central members of the China\u2019s leadership as we enter 2012? \n Recently, I had the opportunity to work with Recorded Future , a startup out of Boston that specializes in longitudinal entity extraction from the massive amount of open-source data generated daily. For example, they have used their data to predict future patent issues for Apple based on issues raised by their competitors. This analysis includes many entities: Apple, HTC, Samsung, etc.; as well as the patents and law suits. \n For our analysis we focused on the China\u2019s leadership, as defined by the CIA World Factbook , and extracted all of named entities in their data for 2011 (over 4 billion events) for which any of the 33 official Chinese leaders appear. The result is a dataset with over 150,000 entities; including people, organizations, and places. To answer our questions, however, I used the co-occurrence of these entities in sentence fragments to build a large network of these entities. \n Here I define an edge between two entities as the co-occurrence of two entities in a sentence fragment, which is provided by Recorded Future. Then, by extracting only the entities that are defined as people in the data, I generated a graph with 5,435 nodes and 34,413 edges. Big, but not unreasonable for analysis. Next, I computed some basic network statistics on that graph. As I have mentioned many times before , these measures are often most interesting if compared together. To highlight key actors, I generated a scatter plot of two metrics: Eigenvector centrality and betweenness centrality. \n \n \n Eigenvector centrality measures the overall centrality of person in the network. It accounts for not only the number of connections a person has, but also the number of connections that person has to others with many connections. People with high Eigenvector centrality will be the most prominent and well-connected actors in a network. Alternatively, betweenness centrality measures that number of paths that go through an actor as a function of the total number of paths in the entire network. People with high betweenness will be those that act as critical bridges or cut-points between two densely connected parts of a network. \n When we compare these metrics, as I have above, we can easily identify key actors as those that do not follow the relatively linear relationship between to two measures. Those with high betweenness but relatively low Eigenvector are central bridges within the network. What makes this comparison important, however, is that these bridges do it with few connections\u2014hence the lower EIgenvector. Likewise, those with relatively high Eigenvector but low betweenness are network insiders. They sit inside some central region of the network, but have very few connections outside that region. To further highlight these key actors, I have shaded the data points based on how much they diverge from this linear trend. The network bridges are dark the red, while insiders are dark blue. \n The above plot was designed using d3.js and is interactive. When you roll over the data points the \u201cLeader Information\u201d section is populated and identifies who in the Chinese leadership the point represents. If you click on the point, or the photo of the leader, you are brought to their full biography page provided by China Vitae . \n What I love most about visualizing data in this way is it leads to many questions. What I love more about creating interactive graphics is it allows for that first layer of questions to be immediately answered, which in turn leads to an even richer investigation. \n Many things jump out of the above plot right away: \n \n We see the obvious placement of Hu Jintao and Wen Jiabao in the upper-right of the graph. This is useful because it confirms that nothing odd is happening in our data: the most powerful men in China on paper are also the most central in our graph. \n Popular press is reporting that current Vice-President Xi Jining will lead the transitional government, so it is interesting to see him clustered closely with Xie Xuren and Zhao Xiaochuan . What will their role be in the new government? \n Why is Yin Weimin , a man with an ostensibly minor role in government, such an important bridge in the network? \n Liang Guangile , the Chinese Minister of Defense, is a key insider. This seems makes sense given the prominence of the military in the Chinese government, but why is he isolated from the rest of the network? \n \n What\u2019s more important than these questions, however, are the non-obvious ones this plot raises. What I need is help from those with a better understanding of Chinese politics. Does the placement of some of China\u2019s leaders seem way off, or does this plot essentially reinforce well-held beliefs about the balance of power? I am very interested in getting other people\u2019s perspectives \n Finally, I want to give a special thanks to Christopher Ahlberg , CEO of Record Future, for working with me to on this data and allowing me to publish these findings."], "link": "http://www.drewconway.com/zia/?p=2825", "bloglinks": {}, "links": {"https://www.cia.gov/": 1, "http://online.wsj.com/": 1, "https://www.recordedfuture.com/": 2, "http://www.chinavitae.com/": 8, "http://mbostock.github.com/": 1, "http://en.wikipedia.org/": 1, "https://twitter.com/": 1, "http://www.nytimes.com/": 1, "http://vimeo.com/": 1}, "blogtitle": "Zero Intelligence Agents"}, {"content": ["First, from looking at the date of my last substantive post I owe everyone an apology. I have essentially let Zero Intelligence Agents wither on the vie, and that is terrible. Not so much because I think people are desperate to read it, but because I am desperate to get feedback from people on my projects and ideas. \n One such project I have been working on recently is looking at the newly released data on Federal Reserve borrowing of 407 banks and companies during the 2007-2009 financial crisis . I have been looking for data sets to tell stories with because one of the tools I am eager to learn in 2012 is Michael Bostock\u2019s d3.js , a Javascript library for data-driven design (d3, get it?). It is an incredibly powerful tool, albeit very verbose and cumbersome for a total Javascript newbie such as myself \n I decided to teach myself some d3 through this Federal Reserver data, and came up with this visualization in the labs section of drewconway.com . The image below is just a snapshot of the visualization, please click through to see the full interactive chart. \n  \n \n Because the data contained so many companies, I decided to focus on only those that were the most aggressive borrowers during the crisis. I defined this as an institution that borrowed more than 500% of its market capitalization in a single day, i.e., 5x its value. This left me with 16 companies. I then also excluded Lehman Brothers, because on a single day it borrowed over 40x its value, which was too extreme an outlier for this visualization. \n What\u2019s left are 15 companies that tell a fascinating tale of the turmoil in the financial markets from 2007-2009. What struck me the most about the visualization was how many foreign banks were among the most aggressive. From the snapshot above, you can see that Dexia SA has the largest spike in its trend line. I must admit, I had never heard of this bank, but as it turns out it is a Belgian-French bank, which also happens to also be currently under investigation by the EU . \n I would love to get feedback, both in terms of the data as well as the design of the visualization. If anyone has some insight as to what was going on with these banks during this time that might explain their trends, please let me know. Also, as I am very new to d3, if you have ideas on how to make the visualization better I welcome those as well. \n UPDATE II : Updated the visualization with a legend and company filter, as suggested by many. I think it is better . \n UPDATE : Thanks to @ deepfoo for pointing me to this backgrounder from Bloomberg."], "link": "http://www.drewconway.com/zia/?p=2816", "bloglinks": {}, "links": {"http://bitly.com/": 2, "http://www.bloomberg.com/": 2, "http://mbostock.github.com/": 1, "http://news.businessweek.com/": 1, "http://en.wikipedia.org/": 1, "http://twitter.com/": 1, "http://bit.ly/": 1}, "blogtitle": "Zero Intelligence Agents"}]