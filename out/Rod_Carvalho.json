[{"blogurl": "http://stochastix.wordpress.com\n", "blogroll": [], "title": "Rod Carvalho"}, {"content": ["[ source ] \n Freeman Dyson on Ludwig Wittgenstein : \n Wittgenstein, unlike Heidegger, did not establish an ism. He wrote very little, and everything that he wrote was simple and clear. The only book that he published during his lifetime was Tractatus Logico-Philosophicus , written in Vienna in 1918 and published in England with a long introduction by Bertrand Russell in 1922. It fills less than two hundred small pages, even though the original German and the English translation are printed side by side. I was lucky to be given a copy of the Tractatus as a prize when I was in high school. I read it through in one night, in an ecstasy of adolescent enthusiasm. Most of it is about mathematical logic. Only the last five pages deal with human problems. The text is divided into numbered sections, each consisting of one or two sentences. For example, section 6.521 says: \u201cThe solution of the problem of life is seen in the vanishing of this problem. Is not this the reason why men, to whom after long doubting the sense of life became clear, could not then say wherein this sense consisted?\u201d The most famous sentence in the book is the final section 7: \u201cWherof one cannot speak, thereof one must be silent.\u201d \n I found the book enlightening and liberating. It said that philosophy is simple and has limited scope. Philosophy is concerned with logic and the correct use of language. All speculations outside this limited area are mysticism. Section 6.522 says: \u201cThere is indeed the inexpressible. This shows itself. It is the mystical.\u201d Since the mystical is inexpressible, there is nothing more to be said. Holt summarizes the difference between Heidegger and Wittgenstein in nine words: \u201cWittgenstein was brave and ascetic, Heidegger treacherous and vain.\u201d These words apply equally to their characters as human beings and to their intellectual output. \n Wittgenstein\u2019s intellectual asceticism had a great influence on the philosophers of the English-speaking world. It narrowed the scope of philosophy by excluding ethics and aesthetics. At the same time, his personal asceticism enhanced his credibility. During World War II, he wanted to serve his adopted country in a practical way. Being too old for military service, he took a leave of absence from his academic position in Cambridge and served in a menial job, as a hospital orderly taking care of patients. When I arrived at Cambridge University in 1946, Wittgenstein had just returned from his six years of duty at the hospital. I held him in the highest respect and was delighted to find him living in a room above mine on the same staircase. I frequently met him walking up or down the stairs, but I was too shy to start a conversation. Several times I heard him muttering to himself: \u201cI get stupider and stupider every day.\u201d \n Finally, toward the end of my time in Cambridge, I ventured to speak to him. I told him I had enjoyed reading the Tractatus, and I asked him whether he still held the same views that he had expressed twenty-eight years earlier. He remained silent for a long time and then said, \u201cWhich newspaper do you represent?\u201d I told him I was a student and not a journalist, but he never answered my question. \n Wittgenstein\u2019s response to me was humiliating, and his response to female students who tried to attend his lectures was even worse. If a woman appeared in the audience, he would remain standing silent until she left the room. I decided that he was a charlatan using outrageous behavior to attract attention. I hated him for his rudeness. Fifty years later, walking through a churchyard on the outskirts of Cambridge on a sunny morning in winter, I came by chance upon his tombstone, a massive block of stone lightly covered with fresh snow. On the stone was written the single word, \u201cWITTGENSTEIN.\u201d To my surprise, I found that the old hatred was gone, replaced by a deeper understanding. He was at peace, and I was at peace too, in the white silence. He was no longer an ill-tempered charlatan. He was a tortured soul, the last survivor of a family with a tragic history, living a lonely life among strangers, trying until the end to express the inexpressible. \n __________ \n Source: \n Freeman Dyson, What can you really know? , The New York Review of Books, November 8, 2012. \n Filed under: Thoughts Tagged: Freeman Dyson , Ludwig Wittgenstein , Martin Heidegger , Philosophers , Philosophy , Tractatus Logico-Philosophicus"], "link": "http://stochastix.wordpress.com/2012/10/21/freeman-dyson-on-wittgenstein/", "bloglinks": {}, "links": {"http://stochastix.wordpress.com/": 8, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 4, "http://www.nybooks.com/": 1}, "blogtitle": "Rod Carvalho"}, {"content": ["Consider the deterministic finite automaton [1] illustrated below \n  \n [ state diagram courtesy of Michael Sipser [1] ] \n Henceforth, we will call this automaton . Note that has three states , labeled , , and . The start state is (note the arrow coming from nowhere) and the accept state is (note the double circle). There are two possible inputs, labeled and , and, depending on the input, the automaton will jump from one state to another. In the diagram above these state transitions are depicted using arrows connecting two states. \n Suppose that the automaton receives an input string such as (which the automaton reads from left to right). Since the start state is and the first input symbol is , the automaton will jump to state . The second input symbol is , so the automaton will remain at . The automaton reads the third input symbol, which is , and then jumps from state to state . The last input symbol is and thus the automaton jumps from state to state . Hence, the state sequence corresponding to the input string is the following \n \n After reading the last symbol in the input string, produces an output. Since the final state is the accept state, we have that the automaton produces the output \u201caccept\u201d. Were the final state not , the automaton would produce the output \u201creject\u201d. We conclude that this automaton accepts the input string . What other input strings does accept? Michael Sipser answers this question in [1]: \n Experimenting with this machine on a variety of input strings reveals that it accepts the strings , , , and . In fact, accepts any string that ends with a , as it goes to its accept state whenever it reads the symbol . In addition, it accepts strings , , , and , and any string that ends with an even number of s following the last . It rejects other strings, such as , , . \n A set of strings is called a language [1]. The set of all input strings accepted by the deterministic finite automaton is a language which we denote by . \n __________ \n Formal definition \n A deterministic finite automaton (DFA) consists of a finite set of states , a finite input alphabet that tells us what the allowed input symbols are, a transition function\u00a0 that tells us how to jump from one state to another, a start state , and a set of accept states . A deterministic finite automaton (DFA) is thus a -tuple of the form \n \n The deterministic finite automaton which we discussed earlier in this post is defined formally as follows \n \n where the transition function is defined enumeratively as \n ,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n ,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ,\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n Alternatively, we could view each state transition as an ordered triple , which might be easier to implement in some programming languages. \n __________ \n Computing state sequences \n Given an input string over a given alphabet , how do we obtain the corresponding state sequence? I solved that problem last January. I will not repeat myself here, but the crux of the matter is that the final state can be obtained using a left- fold \n \n whereas the entire state sequence can be computed using a left-scan \n \n Lacking a better name, I called this procedure the \u201c scanl trick \u201c, as I used the Haskell function scanl to implement it. Please let me know if you find a more sophisticated name for this \u201ctrick\u201d. \n __________ \n Haskell implementation of the DFA \n \n Without further ado, here is a Haskell script: \n data State = Q1 | Q2 | Q3 deriving (Read, Show, Eq)\n\ntype Input = Integer\n\n-- define state-transition function\ndelta :: State -> Input -> State\ndelta Q1 0 = Q1\ndelta Q1 1 = Q2\ndelta Q2 0 = Q3\ndelta Q2 1 = Q2\ndelta Q3 0 = Q2\ndelta Q3 1 = Q2\ndelta _ _ = error \"Invalid input!\"\n\n-- define initial state\ninitialstate :: State\ninitialstate = Q1\n\n-- create list of accept states\nacceptstates :: [State]\nacceptstates = [Q2]\n\n-- create infinite list of input sequences\ninputss :: [[Input]]\ninputss = concat $ iterate g [[]]\n   where g = concat . map (\\xs -> [ xs ++ [s] | s <- [0,1]])\n\n-- create accept predicate\nisAccepted :: [Input] -> Bool\nisAccepted inputs = finalstate `elem` acceptstates\n     where finalstate = foldl delta initialstate inputs\n\n-- compute language recognized by the DFA\nlanguage :: [[Input]]\nlanguage = filter isAccepted inputss \n Some remarks about this script are in order. Please note that: \n \n Sets are represented by lists. Strings are represented by lists, too. The latter is more natural than the former. Sets of strings become lists of lists. \n \n \n A new data type is created to represent the states, which we denote by Q1 , Q2 , and Q3 . The input symbols are integers. \n \n \n Note that Input is a type synonym , inputs is a list of inputs symbols (i.e., an input string), and inputss is a list of lists of inputs (i.e., a list of input strings). Yes, it is a bit confusing. \n \n \n Note also that inputss is generated using the procedure I blogged about yesterday where the alphabet is . \n \n \n The final state is computed using the higher-order function foldl (left-fold). We check if the final state is an accept state using\u00a0\u00a0the list membership predicate elem . \n \n If you have any objections to this script, please do let me know. Let us now test it! We load it into GHCi and voil\u00e0 : \n *Main> -- check list of input strings\n*Main> take 31 $ inputss\n[[],[0],[1],\n [0,0],[0,1],[1,0],[1,1],\n [0,0,0],[0,0,1],[0,1,0],[0,1,1],\n [1,0,0],[1,0,1],[1,1,0],[1,1,1],\n [0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],\n [0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],\n [1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],\n [1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]]\n*Main> -- compute the language of the automaton\n*Main> -- (let us extract some 40 input strings only)\n*Main> take 40 language\n[[1],[0,1],[1,1],\n [0,0,1],[0,1,1],[1,0,0],[1,0,1],[1,1,1],\n [0,0,0,1],[0,0,1,1],[0,1,0,0],[0,1,0,1],\n [0,1,1,1],[1,0,0,1],[1,0,1,1],[1,1,0,0],\n [1,1,0,1],[1,1,1,1],[0,0,0,0,1],[0,0,0,1,1],\n [0,0,1,0,0],[0,0,1,0,1],[0,0,1,1,1],[0,1,0,0,1],\n [0,1,0,1,1],[0,1,1,0,0],[0,1,1,0,1],[0,1,1,1,1],\n [1,0,0,0,0],[1,0,0,0,1],[1,0,0,1,1],[1,0,1,0,0],\n [1,0,1,0,1],[1,0,1,1,1],[1,1,0,0,1],[1,1,0,1,1],\n [1,1,1,0,0],[1,1,1,0,1],[1,1,1,1,1],[0,0,0,0,0,1]] \n The list of input strings is exactly the same I posted yesterday. The input strings in the language , or, to put it more precisely, the input strings in that are displayed above either end with a , or end \u201cwith an even number of s following the last \u201c, as mentioned by Sipser in [1]. Why is that? The last in the input string puts in state , and an even number of s after that lead to transitions from to and back to , e.g., \n \n or \n \n or \n \n It would also be interesting to take a look at the state sequences corresponding to the input strings in . We compute these state sequences using the infamous \u201c scanl trick \u201c: \n *Main> -- create list of sequences of states\n*Main> let statess = map (\\xs -> scanl delta initialstate xs) language\n*Main> -- take the \"first\" 20 state sequences\n*Main> take 20 statess\n[[Q1,Q2],[Q1,Q1,Q2],[Q1,Q2,Q2],\n [Q1,Q1,Q1,Q2],[Q1,Q1,Q2,Q2],[Q1,Q2,Q3,Q2],\n [Q1,Q2,Q3,Q2],[Q1,Q2,Q2,Q2],[Q1,Q1,Q1,Q1,Q2],\n [Q1,Q1,Q1,Q2,Q2],[Q1,Q1,Q2,Q3,Q2],[Q1,Q1,Q2,Q3,Q2],\n [Q1,Q1,Q2,Q2,Q2],[Q1,Q2,Q3,Q2,Q2],[Q1,Q2,Q3,Q2,Q2],\n [Q1,Q2,Q2,Q3,Q2],[Q1,Q2,Q2,Q3,Q2],[Q1,Q2,Q2,Q2,Q2],\n [Q1,Q1,Q1,Q1,Q1,Q2],[Q1,Q1,Q1,Q1,Q2,Q2]] \n Note that all state trajectories end in state Q2 , as we expected. \n __________ \n References \n [1] Michael Sipser, Introduction to the Theory of Computation (2nd edition), Thomson Course Technology, 2006. \n Filed under: Automata Theory , Haskell Tagged: Automata Theory , Deterministic Finite Automata , Finite Automata , Haskell , Models of Computation"], "link": "http://stochastix.wordpress.com/2012/10/02/deterministic-finite-automata-in-haskell/", "bloglinks": {}, "links": {"http://www-math.mit.edu/": 1, "http://feeds.wordpress.com/": 1, "http://stochastix.wordpress.com/": 11, "http://en.wikipedia.org/": 6, "https://stochastix.wordpress.com/": 2, "http://www.haskell.org/": 4, "http://en.wikibooks.org/": 1}, "blogtitle": "Rod Carvalho"}, {"content": ["An alphabet is a finite set of symbols . A word of length over an alphabet is a sequence of symbols from , i.e., \n \n Alternatively, we can view a word of length over the alphabet as an - tuple (i.e., an ordered list with elements) over\u00a0 , i.e., . The set of all finite words over , including the empty word , is denoted by , where is the Kleene star . \n We thus encounter the following problem: \n Problem: Given an alphabet , how do we generate all the words over ? In other words, given , how do we generate the Kleene closure ? \n The following Haskell script solves this problem: \n g :: [a] -> [[a]] -> [[a]]\ng alphabet = concat . map (\\xs -> [ xs ++ [s] | s <- alphabet])\n\nallwords :: [a] -> [[a]]\nallwords alphabet = concat $ iterate (g alphabet) [[]] \n where alphabet must be a finite list, otherwise the execution of the script won\u2019t ever terminate. The script above, although short, uses a lot of machinery: list comprehension , higher-order functions map and iterate , concatenation of lists of lists, and partial function application . Let us test this script. First, we load it into GHCi . Then we can run a GHCi session: \n *Main> -- define alphabet\n*Main> let alphabet = [0,1]\n*Main> -- define function f\n*Main> let f = g alphabet\n*Main> -- check type\n*Main> :t f\nf :: [[Integer]] -> [[Integer]]\n*Main> -- apply f to several lists of lists\n*Main> f [[]]\n[[0],[1]]\n*Main> (f . f) [[]]\n[[0,0],[0,1],[1,0],[1,1]]\n*Main> (f . f . f) [[]]\n[[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0],[1,0,1],[1,1,0],[1,1,1]] \n So far, so good. Suppose that we would like to find all words over alphabet whose length is less than or equal to . How many such words are there? There are words of length . Hence, there are \n \n binary words of length less than or equal to . Hence, if we make , then we obtain . Continuing our GHCi session: \n *Main> take 31 $ allwords alphabet\n[[],[0],[1],\n[0,0],[0,1],[1,0],[1,1],\n[0,0,0],[0,0,1],[0,1,0],[0,1,1],\n[1,0,0],[1,0,1],[1,1,0],[1,1,1],\n[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],\n[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],\n[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],\n[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]] \n Very nice! We could use this in Coding Theory ! For example, suppose that we would like to find all binary words of length whose Hamming weight is equal to . There are \n \n such words. We can find them as follows: \n *Main> -- take all words of length less than or equal to 8\n*Main> let words = take 511 $ allwords alphabet\n*Main> -- filter out binary words of length 8\n*Main> let wordsL8 = filter (\\xs -> length xs == 8) words\n*Main> length wordsL8\n256\n*Main> -- filter out binary words of length 8\n*Main> -- and also of Hamming weight equal to 5\n*Main> let wordsL8H5 = filter (\\xs -> sum xs == 5) wordsL8\n*Main> length wordsL8H5\n56\n*Main> wordsL8H5\n[[0,0,0,1,1,1,1,1],[0,0,1,0,1,1,1,1],[0,0,1,1,0,1,1,1],\n [0,0,1,1,1,0,1,1],[0,0,1,1,1,1,0,1],[0,0,1,1,1,1,1,0],\n [0,1,0,0,1,1,1,1],[0,1,0,1,0,1,1,1],[0,1,0,1,1,0,1,1],\n [0,1,0,1,1,1,0,1],[0,1,0,1,1,1,1,0],[0,1,1,0,0,1,1,1],\n [0,1,1,0,1,0,1,1],[0,1,1,0,1,1,0,1],[0,1,1,0,1,1,1,0],\n [0,1,1,1,0,0,1,1],[0,1,1,1,0,1,0,1],[0,1,1,1,0,1,1,0],\n [0,1,1,1,1,0,0,1],[0,1,1,1,1,0,1,0],[0,1,1,1,1,1,0,0],\n [1,0,0,0,1,1,1,1],[1,0,0,1,0,1,1,1],[1,0,0,1,1,0,1,1],\n [1,0,0,1,1,1,0,1],[1,0,0,1,1,1,1,0],[1,0,1,0,0,1,1,1],\n [1,0,1,0,1,0,1,1],[1,0,1,0,1,1,0,1],[1,0,1,0,1,1,1,0],\n [1,0,1,1,0,0,1,1],[1,0,1,1,0,1,0,1],[1,0,1,1,0,1,1,0],\n [1,0,1,1,1,0,0,1],[1,0,1,1,1,0,1,0],[1,0,1,1,1,1,0,0],\n [1,1,0,0,0,1,1,1],[1,1,0,0,1,0,1,1],[1,1,0,0,1,1,0,1],\n [1,1,0,0,1,1,1,0],[1,1,0,1,0,0,1,1],[1,1,0,1,0,1,0,1],\n [1,1,0,1,0,1,1,0],[1,1,0,1,1,0,0,1],[1,1,0,1,1,0,1,0],\n [1,1,0,1,1,1,0,0],[1,1,1,0,0,0,1,1],[1,1,1,0,0,1,0,1],\n [1,1,1,0,0,1,1,0],[1,1,1,0,1,0,0,1],[1,1,1,0,1,0,1,0],\n [1,1,1,0,1,1,0,0],[1,1,1,1,0,0,0,1],[1,1,1,1,0,0,1,0],\n [1,1,1,1,0,1,0,0],[1,1,1,1,1,0,0,0]] \n where we used function take (again) and higher-order function filter . The two predicates were built using anonymous functions . \n I am happy with this script. Please let me know in case you are not. \n __________ \n Related: \n \n Distance between two words (2009) \n \n Filed under: Computer Science , Haskell Tagged: Haskell , Higher-Order Functions , List Processing , Stringology"], "link": "http://stochastix.wordpress.com/2012/10/01/generating-all-words-over-an-alphabet/", "bloglinks": {}, "links": {"http://www.haskell.org/": 10, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 9, "http://stochastix.wordpress.com/": 7}, "blogtitle": "Rod Carvalho"}, {"content": ["Murray Gell-Mann on why being in awe inhibits learning: \n I said I\u2019d rather be poor or die than be an engineer because I would be no good at it. If I designed something it would fall down. When I was admitted to Yale, I took an aptitude test, and when the counselor gave me the results of the exam, he said: \u201cYou could be lots of different things. But don\u2019t be an engineer.\u201d \n After my father gave up on engineering, he said, \u2018How about we compromise and go with physics? General relativity, quantum mechanics, you will love it.\u2019 I thought I would give my father\u2019s advice a try. I don\u2019t know why. I never took his advice on anything else. He told me how beautiful physics would be if I stuck with it, and that notion of beauty impressed me. My father studied those things. He was a great admirer of Einstein. He would lock himself in his room and study general relativity. He never really understood it. My opinion is that you have to despise something like that to get good at it. \n If you admire it sufficiently, you\u2019ll be in awe of it, so you\u2019ll never learn it. My father thought it must be very hard, and it will take years to understand it, and only a few people understand it, and so on. But I had a wonderful teacher at Yale, Henry Margenau, who took the opposite attitude. He thought relativity was for everybody. Just learn the math. He\u2019d say, \u201cWe\u2019ll prepare the math on Tuesday and Thursday, and we\u2019ll cover general relativity on Saturday and next Tuesday.\u201d And he was right. It isn\u2019t that bad. \n __________ \n Source: \n Susan Kruglinski, The Man Who Found Quarks and Made Sense of the Universe , DISCOVER Magazine, April 2009. \n Filed under: Thoughts Tagged: Learning , Murray Gell-Mann , Wisdom"], "link": "http://stochastix.wordpress.com/2012/09/27/gell-mann-on-learning/", "bloglinks": {}, "links": {"http://discovermagazine.com/": 1, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 1, "http://stochastix.wordpress.com/": 4}, "blogtitle": "Rod Carvalho"}, {"content": ["In Haskell we can easily create a list and then access its elements using the (!!) function, which is defined in the Prelude . Here is a very brief GHCi session: \n Prelude> let xs = [7,8,9]\nPrelude> xs !! 0\n7\nPrelude> xs !! 1\n8\nPrelude> xs !! 2\n9 \n So far, so good. What if the index is negative or equals / exceeds the list\u2019s length? Let\u2019s see what happens in those cases: \n Prelude> xs !! (-1)\n*** Exception: Prelude.(!!): negative index\n\nPrelude> xs !! 3\n*** Exception: Prelude.(!!): index too large \n As expected, we get error messages. What if we used the Maybe data type to avoid exceptions? This is exercise 4 in chapter 3 of O\u2019Donnell & Hall & Page [1], which is phrased as follows: \n Write (!!) , a function that takes a natural number  a nd a list and selects the th element of the list. List elements are indexed from , not , and since the type of the incoming number does not prevent it from being out of range, the result should be a Maybe type . \n The aforementioned authors propose the following implementation: \n import Prelude hiding ((!!))\n\n(!!) :: Int -> [a] -> Maybe a\n(!!) n [] = Nothing\n(!!) 0 (x:xs) = Just x\n(!!) n (x:xs) = (!!) (n-1) xs \n where I added the import line to hide the standard (!!) function that is defined in the Prelude . My first thought was that the authors switched the function arguments, which makes the function look silly. Let\u2019s give it a try. Here\u2019s another GHCi session: \n *Main> let xs = [7,8,9]\n*Main> 0 !! xs\nJust 7\n*Main> 1 !! xs\nJust 8\n*Main> 2 !! xs\nJust 9\n*Main> (-1) !! xs\nNothing\n*Main> 3 !! xs\nNothing \n It appears to be working, but specifying the index before the list looks rather ugly. Wait, what if the index is negative? For example, why does (-3) !! xs return Nothing ? Let\u2019s use equational reasoning to find out: \n (-3) !! [7,8,9] = (-4) !! [8,9] = \n    = (-5) !! [9] =\n    = (-6) !! [] = \n    = Nothing \n This reveals a fatal flaw in the authors\u2019 implementation: if the list is infinite, then the recursion will never terminate. For example, (-1) !! [0..] will never terminate, because when the initial index is negative, decrementing the index will never get us to the zero index. \n Therefore, I propose the following implementation: \n import Prelude hiding ((!!))\n\n(!!) :: [a] -> Integer -> Maybe a\n(!!)  [] n = Nothing\n(!!) (x:xs) n | n > 0 = (!!) xs (n-1)\n    | n == 0 = Just x\n    | n < 0 = Nothing \n where the first argument is now a list, and the second argument an integer. Note that I used indices of type Integer (\u201cmathematical integers\u201d), instead of type Int (\u201ccomputer integers\u201d). Let\u2019s see if this implementation works: \n *Main> let xs = [7,8,9]\n*Main> xs !! 0\nJust 7\n*Main> xs !! 1\nJust 8\n*Main> xs !! 2\nJust 9\n*Main> xs !! (-1)\nNothing\n*Main> xs !! 3\nNothing \n It appears to be working. No errors. No exceptions. If you, dear reader, happen to be acquainted with\u00a0 Haskell you will almost certainly be shocked (!!!), for this function is trivial! Well, that is true, but I allow myself to be intrigued by trivialities. Moreover, this function is simple enough to allow us to use equational reasoning . For example, let\u2019s compute xs !! 2 using equational reasoning: \n [7,8,9] !! 2 = [8,9] !! 1 = [9] !! 0 = Just 9 \n What if the index is too large? Let\u2019s compute xs !! 4 then: \n [7,8,9] !! 4 = [8,9] !! 3 = [9] !! 2 = [] !! 1 = Nothing \n Step by step, by successively removing the head of the list, we get where we want to. Unfortunately, this suggests that accessing an arbitrary element of the list will not be . \n __________ \n References \n [1] John O\u2019Donnell, Cordelia Hall, Rex Page, Discrete Mathematics using a Computer (2nd edition), Springer, 2006. \n Filed under: Haskell Tagged: Equational Reasoning , Haskell , List Processing"], "link": "http://stochastix.wordpress.com/2012/09/20/my-implementation-of/", "bloglinks": {}, "links": {"http://www.haskell.org/": 6, "http://feeds.wordpress.com/": 1, "http://stochastix.wordpress.com/": 4, "http://en.wikipedia.org/": 2, "http://www.springer.com/": 1}, "blogtitle": "Rod Carvalho"}, {"content": ["An amusing Japanese cartoon on combinatorial explosion : \n \n Tastefully done, I would say. The target audience consists not of this blog\u2019s readers, but rather of their children. \n Hat tip: Michael Lugo \n Filed under: Art , Combinatorics , Computer Science , Fun Tagged: Combinatorial Explosion , Combinatorics , Computational Complexity , Kindergarten Mathematics , Questions kids ask"], "link": "http://stochastix.wordpress.com/2012/09/16/combinatorial-explosion/", "bloglinks": {}, "links": {"http://gottwurfelt.wordpress.com/": 1, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 1, "http://stochastix.wordpress.com/": 9}, "blogtitle": "Rod Carvalho"}, {"content": ["Alice and Bob are fruit-pickers at an orange orchard. \n Alice can pick 6 baskets of oranges in one hour. In contrast, Bob can pick 7 baskets of oranges in the same period of time. However, if Alice and Bob work together, then they can pick a total of 15 baskets in one hour. As part of a team: \n \n Alice is now picking 7.5 baskets per hour, a most impressive increase in productivity of 25%. \n \n \n Bob is now picking 7.5 baskets per hour as well, a modest increase in productivity of approximately 7%. \n \n Both Alice and Bob benefit if they work together (though Alice benefits more). We thus have an example of synergy . To use a clich\u00e9: \u201cthe whole is greater than the sum of its parts\u201d. If Alice and Bob work alone, they can only pick 13 baskets per hour in total, but if they work together they can pick 15 baskets per hour. Everyone is happy. \n Let , let denote the power set of , and let denote the set of nonnegative real numbers. We introduce a productivity function , enumeratively defined as follows \n \n \n \n \n \n \n \n \n \n \n \n \n where because the productivity of the \u201cempty team\u201d is zero. Since we have that \n \n we conclude that we have synergy . Note that the existence of a synergistic or synergetic effect is a property of the productivity function . We could attempt to study such property in a more general setting. \n __________ \n Superadditive measures \n We now introduce a definition \n Definition : Given a finite set and a function , if the following conditions are satisfied \n \n \n \n \n for all sets such that \n \n we say that is a\u00a0 superadditive measure [1]. \n Using the superadditivity property recursively, one can conclude that \n \n for every . For example, if , then we have that the measure of\u00a0 is \n \n Frankly, I have (accidentally) opened a can of worms. I started writing this post thinking about synergy and productivity, and I am now drowning in Measure Theory! As it turns out, the union of all my knowledge of Measure Theory is a set of measure zero ;-) Hence, I will abruptly finish this post with a passage from Wang & Klir [1]: \n Observe that superadditive measures are capable of expressing a cooperative action or synergy between sets in terms of the measured property, while subadditive measures are capable of expressing inhibitory effects or incompatibility between sets in terms of the measured property. Additive measures, on the other hands, are not able to express either of these interactive effects. They are applicable only to situations in which there is no interaction between sets as far as the measured property is concerned. \n I may return to this topic if I happen to have any interesting ideas. \n __________ \n References \n [1] Zhenyuan Wang, George J. Klir, Generalized Measure Theory , Springer, 2009. \n Filed under: Mathematics Tagged: Functionology , Measure Theory , Superadditivity , Synergy"], "link": "http://stochastix.wordpress.com/2012/09/14/a-measure-theoretic-definition-of-synergy/", "bloglinks": {}, "links": {"http://stochastix.wordpress.com/": 5, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 7, "http://books.google.com/": 1}, "blogtitle": "Rod Carvalho"}, {"content": ["[ source ] \n Neil Armstrong once portrayed himself as follows [1, 2]: \n I am, and ever will be, a white-socks, pocket-protector, nerdy engineer\u2014born under the second law of thermodynamics, steeped in the steam tables, in love with free-body diagrams, transformed by Laplace, and propelled by compressible flow. As an engineer, I take a substantial amount of pride in the accomplishments of my profession. \n Neil \u201cslipped the surly bonds of earth\u201d and \u201ctrod the high untrespassed sanctity of space\u201d [3].\u00a0He died yesterday [4, 5]. \n __________ \n Sources \n [1] Neil A. Armstrong, Greatest Engineering Achievements of the 20th Century (transcript of speech), National Press Club, February 22, 2000. \n [2] Neil A. Armstrong, Greatest Engineering Achievements of the 20th Century (audio of speech \u2013 Neil\u2019s address starts at approximately 5:30), National Press Club, February 22, 2000. \n [3] John Gillespie Magee, Jr., High Flight , England, 1941. \n [4] John Noble Wilford, Neil Armstrong, first man on Moon, dies at 82 , The New York Times, August 25, 2012. \n [5] Craig Nelson, Neil Armstrong, hero with a slide rule , The Wall Street Journal, August 25, 2012. \n Filed under: Thoughts Tagged: Astronauts , Engineering Professors , Engineers , Fighter Pilots , Heroes , Naval Aviators , Neil Armstrong , Test Pilots"], "link": "http://stochastix.wordpress.com/2012/08/26/born-under-the-second-law-of-thermodynamics/", "bloglinks": {}, "links": {"http://www.nasa.gov/": 1, "http://www.npr.org/": 1, "http://www.greatachievements.org/": 1, "http://www.af.mil/": 1, "http://stochastix.wordpress.com/": 10, "http://blogs.wsj.com/": 1, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 1, "http://www.nytimes.com/": 1}, "blogtitle": "Rod Carvalho"}, {"content": ["Terence Tao on classical deduction and Bayesian probability: \n In classical logic, one can represent one\u2019s information about a system as a set of possible states that the system could be in, based on the information at hand. With each new measurement of the system, some possibilities could be eliminated, leading to an updated posterior set of information that is an improvement over the prior set of information. A good example of this type of updating occurs when solving a Sudoku puzzle; each new cell value that one learns about constrains the possible values of the remaining cells. Other examples can be found in the classic detective stories of Arthur Conan Doyle featuring Sherlock Holmes. Proof by contradiction can also be viewed as an instance of this type of deduction . \n A modern refinement of classical deduction is that of Bayesian probability . Here, one\u2019s information about a system is not merely represented as a set of possible states, but by a probability distribution on the space of all states, indicating one\u2019s current beliefs on the likelihood of each particular state actually being the true state. Each new measurement of the system then updates a prior probability distribution to a posterior probability distribution , using Bayes\u2019 formula \n . \n Bayesian probability is widely used in statistics, in machine learning, and in the sciences. \n To relate Bayesian probability to classical deduction, recall that every probability distribution has a support , which (in the case when the space of states is discrete) is the set of all states that occur with non-zero probability. When performing a Bayesian update on a discrete space, any state which is inconsistent with the new piece of information will have its posterior probability set to zero, and thus be removed from the support. Thus we see that whilst the probability distribution evolves by Bayesian updating, the support evolves by classical deductive logic . Thus one can view classical logic as the qualitative projection of Bayesian probability , or equivalently, one can view Bayesian probability as a quantitative refinement of classical logic. \n Alternatively, one can view Bayesian probability as a special case of classical logic by taking a frequentist interpretation. In this interpretation, one views the actual universe (or at least the actual system) as just one of a large number of possible universes (or systems). In each of these universes, the system is in one of the possible states; the probability assigned to each state is then the proportion of the possible universes in which that state is attained. Each new measurement eliminates some fraction of the universes in a given state, depending on how likely or unlikely that state was to actually produce that measurement; the surviving universes then have a new posterior probability distribution, which is related to the prior distribution by Bayes\u2019 formula. \n It is instructive to interpret Sherlock Holmes \u2018 famous quote, \u201cWhen you have eliminated all which is impossible, then whatever remains, however improbable, must be the truth,\u201d from a Bayesian viewpoint. The statement is technically correct; however, when performing this type of elimination to an (a priori) improbable conclusion, the denominator in Bayes\u2019 formula is extremely small, and so the deduction is unstable if it later turns out that some of the possibilities thought to have been completely eliminated, were in fact only incompletely eliminated. (See also the mantra \u201cextraordinary claims require extraordinary evidence\u201d, which can be viewed as the Bayesian counterpoint to Holmes\u2019 classical remark.) \n Another interesting place where one can contrast classical deduction with Bayesian deduction is with regard to taking converses . In classical logic, if one knows that  implies , one cannot then deduce that\u00a0 implies . However, in Bayesian probability, if one knows that the presence of  elevates the likelihood that  is true, then an observation of  will conversely elevate the prior probability that  is true, thanks to Bayes\u2019 formula: if , then . This may help explain why taking converses is an intuitive operation to those who have not yet been thoroughly exposed to classical logic. (It is also instructive to understand why this disparity between the two types of deduction is not in conflict with the previously mentioned links between the two. This disparity is roughly analogous to the disparity between worst-case analysis and average-case analysis.) \n Bayesian probability can be generalised further; for instance, quantum mechanics (with the Copenhagen interpretation ) can be viewed as a noncommutative generalisation of Bayesian probability , though the connection to classical logic is then lost when one is dealing with observables that do not commute. But this is another story\u2026 \n __________ \n Please do note that Terence Tao\u2019s original post contains neither links nor boldface highlighting. I took the liberty of adding those for convenience and emphasis. To improve legibility I also wrote the mathematical expressions in . \n __________ \n Source: \n Terence Tao, \u201cA modern refinement of classical deduction is that of Bayesian probability\u201d , Google Buzz, April 4, 2010. \n Filed under: Logic , Probability Theory , Thoughts Tagged: Bayesian Deduction , Bayesianism , Classical Deduction , Frequentism , Logic , Philosophy of Probability , Terence Tao"], "link": "http://stochastix.wordpress.com/2012/08/15/from-classical-deduction-to-bayesian-probability/", "bloglinks": {}, "links": {"http://stochastix.wordpress.com/": 10, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 10, "https://profiles.google.com/": 1}, "blogtitle": "Rod Carvalho"}, {"content": ["Today we will \u201cconstruct\u201d the standard (ternary) Cantor set [1]. We start with the closed unit interval , and then remove its open middle third to obtain . We then remove the open middle thirds of each of these two intervals to obtain \n \n and, continuing this process ad infinitum , we obtain a nested sequence of compact sets . Note that is the union of intervals, each of length . We refer to the following set \n \n as the standard (ternary) Cantor set . This set is most interesting, indeed, since it is uncountable and has Lebesgue measure zero [1]. \n __________ \n In Haskell \n In Haskell , a closed interval can be represented by an ordered pair . Each set can be represented by a list of ordered pairs, where each pair represents a closed interval. We create a function that takes and returns . We will work with arbitrary-precision rational numbers , not floating-point numbers. \n The following\u00a0 Haskell script lazily generates the sequence of sets: \n import Data.Ratio\n\ntype Interval = (Rational, Rational)\n\n-- remove the middle third of an interval\nremoveMiddleThird :: Interval -> [Interval]\nremoveMiddleThird (a,b) = [(a,b'),(a',b)] \n      \t where b' = (2%3)*a + (1%3)*b\n        a' = (1%3)*a + (2%3)*b\n\n-- define function f\nf :: [Interval] -> [Interval]\nf intervals = concat $ map removeMiddleThird intervals\n\n-- create list of sets\nsets :: [[Interval]]\nsets = iterate f [(0,1)]\n\n-- define Lebesgue measure\nmeasure :: Interval -> Rational\nmeasure (a,b) = b - a \n Note that we used function iterate to generate the sequence of sets. Here is a GHCi session: \n *Main> -- take first 4 sets\n*Main> take 4 sets\n[[(0 % 1,1 % 1)],[(0 % 1,1 % 3),(2 % 3,1 % 1)],\n[(0 % 1,1 % 9),(2 % 9,1 % 3),(2 % 3,7 % 9),(8 % 9,1 % 1)],\n[(0 % 1,1 % 27),(2 % 27,1 % 9),(2 % 9,7 % 27),(8 % 27,1 % 3),\n(2 % 3,19 % 27),(20 % 27,7 % 9),(8 % 9,25 % 27),(26 % 27,1 % 1)]]\n*Main> -- compute measure of C_0\n*Main> sum $ map measure (sets !! 0) \n1 % 1\n*Main> -- compute measure of C_1\n*Main> sum $ map measure (sets !! 1) \n2 % 3\n*Main> -- compute measure of C_2\n*Main> sum $ map measure (sets !! 2) \n4 % 9\n*Main> -- compute measure of C_3\n*Main> sum $ map measure (sets !! 3) \n8 % 27 \n This is arguably the most useless Haskell script ever written. \n __________ \n References \n [1] Charles Chapman Pugh, Real Mathematical Analysis , Springer-Verlag, New York, 2002. \n Filed under: Haskell , Mathematics Tagged: Cantor Set , Data.Ratio , Haskell"], "link": "http://stochastix.wordpress.com/2012/08/03/towards-the-cantor-set/", "bloglinks": {}, "links": {"http://www.haskell.org/": 1, "http://feeds.wordpress.com/": 1, "http://stochastix.wordpress.com/": 5, "http://en.wikipedia.org/": 8, "http://books.google.com/": 1}, "blogtitle": "Rod Carvalho"}]