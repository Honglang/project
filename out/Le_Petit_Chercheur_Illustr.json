[{"blogurl": "http://yetaspblog.wordpress.com\n", "blogroll": [], "title": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["I have just found this \u201cnew\u201d (well 150 years old actually) tomographical method\u2026 for measuring the magnetic field of our own galaxy \n \u201c New all-sky map shows the magnetic fields of the Milky Way with the highest precision \u201c by Niels Oppermann et al. (arxiv work available here ) \n Selected excerpt: \n \u201c\u2026 One way to measure cosmic magnetic fields, which has been known for over 150 years, makes use of an effect known as Faraday rotation. When polarized light passes through a magnetized medium, the plane of polarization rotates. The amount of rotation depends, among other things, on the strength and direction of the magnetic field. Therefore, observing such rotation allows one to investigate the properties of the intervening magnetic fields.\u201d \n Mmmm\u2026 very interesting, at least for my personal knowledge of the wonderful tomographical problem zoo (amongst gravitational lensing, interferometry, MRI, deflectometry). \n \u00a0 \n P.S. Wow\u2026 16 months without any post here. I\u2019m really bad."], "link": "http://yetaspblog.wordpress.com/2011/12/07/tomography-of-the-magnetic-fields-of-the-milky-way/", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://www.mpg.de/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["Recently, a friend of mine asked me few questions about Noiselets for Compressed Sensing applications, i.e., in order to create efficient sensing matrices incoherent with signal which are sparse in the Haar/Daubechies wavelet basis. It seems that some of the answers are difficult to find on the web (but I\u2019m sure they are well known to specialists) and I have therefore decided to share the ones I got. \n Context: \n I wrote in 2008 a tiny Matlab toolbox (see here ) to convince myself that the Noiselet followed a Cooley-Tukey implementation already followed by the Walsh-Hadamard transform. It should have been optimized in C but I lacked of time to write this.Since this first code, I realized that Justin Romberg wrote already in 2006 with Peter Stobbe a fast code (also O(N log N) but much faster than mine) available here:  \n http://users.ece.gatech.edu/~justin/spmag \n \n People could be interested in using Justin\u2019s code since, as it will be clarified from my answers given below, it is already adapted to real valued signals, i.e., it produces real valued noiselets coefficients. \n \n Q1. Do we need to use both the real and imaginary parts of noiselets to design sensing matrices (i.e., building the matrix)?\u00a0 Can we use just the real part or just the imaginary part)?\u00a0 Any reason why you\u2019d do one thing or another? \n \n \n As for the the Random Fourier Ensemble sensing, what I personally do when I use noiselet sensing is to pick uniformly at random complex values in half the noiselet-frequency domain, and concatenate the real and the imaginary part into a real vector of length . The adjoint (transposed) operation \u2014 often needed in most of Compressed Sensing solvers \u2014 must of course sum the previously split real and imaginary parts into complex values before to pad the complementary measured domain with zeros and run the inverse noiselet transform. \n \n To understand the special treatment of the real and the imaginary parts (and not simply by considering it similar to what is done for Random Fourier Ensemble), let us consider the origin, that is, Coifman et al. Noiselets paper . \n Recall that in this paper, two kinds of noiselets are defined. The first basis, the common Noiselets basis on the interval , is defined thanks to the recursive formulas: \n  \n The second basis, or Dragon Noiselets , is slightly different. Its elements are symmetric under the change of coordinates . Their recursive definition is \n  \n To be more precise, the two sets \n , \n , \n are orthonormal bases for piecewise constant functions at resolution , that is, for functions of \n \n In Coifman et al. paper, the recursive definition of Eq. (2) (and also Eq (4) for Dragon Noiselets), which connects the noiselet functions between the noiselet index and indices or , is simply a common butterfly diagram that sustains the Cooley-Tukey implementation of the Noiselet transform. \n The coefficients involved in Eqs (2) and (4) are simply , which are of course complex conjugate of each other. \n Therefore, in the Noiselet transform of a real vector of length (in one to one correspondance with the piecewise constant functions of ) involving the noiselets of indices , the resulting decomposition diagram is fully symmetric (with a complex conjugation) under a flip of indices , for . \n This shows that \n , \n with the complex conjugation, if is real, and allows us to define \u201cReal Random Noiselet Ensemble\u201d by picking uniformly at random complex values in the half domain , that is independent real values in total, as obtained by concatenating the real and the imaginary parts (see above). \n Therefore, for real valued signals, as for Fourier, the two halves of the noiselet spectrum are not independent, and therefore, only one half is necessary to perform useful CS measurements. \n Justin\u2019s code is close to this interpretation by using a real valued version of the symmetric Dragon Noiselets described in the initial Coifman et al. paper. \n Q2. Are noiselets always binary?\u00a0 or do they take +1, -1, 0 values like Haar wavelets? \n Actually, a noiselet of index take the complex values , never .This can be easily seen from the recursive formula of Eq. (2). \n They fill also the whole interval . \n Q3. Walsh functions have the property that they are binary and zero mean, so that one half of the values are 1 and the other half are -1.\u00a0 Is it the same case with the real and/or imag parts of the noiselet transform? \n To be correct, Walsh-Hadamard functions have mean equal to 1 if their index is a power of 2 and 0 else, starting with the [0,1] indicator function of index 1. \n For Noiselets, they are all of unit average, meaning that the imaginary part has the zero average property. This can be proved easily (by induction) from their recursive definition in Coifman et al. paper (Eqts (2) and (4)). Interestingly, their unit average, that is their projection on the unit constant function, shows directly that a constant function is not sparse at all in the noiselet basis since its \u201cnoiselet spectrum\u201d is just flat. \n In fact, it is explained in Coifman paper that any Haar-Walsh wavelet packets, that is, elementary functions of formula \n \n with the Walsh functions (including the Haar functions), have a flat noiselet spectrum (all coefficients of unit amplitude), leading to the well known good (in)coherence results (that is, low coherence). To recall, the coherence is given by for the Haar wvaelet basis, and it corresponds to slightly higher values for the Daubechies wavelets D4 and D8 respectively (see, e.g., E.J. Cand\u00e8s and M.B. Wakin, \u201cAn introduction to compressive sampling\u201d , IEEE Sig. Proc. Mag., 25(2):21\u201330, 2008.) \n Q4. How come noiselets require O(N logN) computations rather than O(N) like the haar transform? \n This is a verrry common confusion. The difference comes from the locality of the Haar basis elements. \n For the Haar transform, you can use the well known pyramidal algorithm running in computations. You start from the approximation coefficients computed at the finest scale, using then the wavelet scaling relations to compute the detail and approximation coefficients at the second scale, and so on. Because of the sub-sampling occuring at each scale, the complexity is proportional to the number of coefficients, that is, it is . \n For the 3 bases Hadamard-Walsh, Noiselets and Fourier, their non-locality (i.e., their support is the whole segment [0, 1]) you cannot run a similar alorithm. However, you can use the Cooley-Tukey algorithm arising from the Butterfly diagrams linked to the corresponding recursive definitions (Eqs (2) and (4) above). \n This one is in , since the final diagram has levels, each involving multiplication-additions. \n \u2014 \n Feel free to comment this post and ask other questions. It will provide perhaps eventually a general Noiselet FAQ/HOWTO"], "link": "http://yetaspblog.wordpress.com/2010/08/21/some-comments-on-noiselets/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.ac.be/": 1, "http://yetaspblog.wordpress.com/": 2, "http://www.laurent-duval.eu/": 1, "http://users.gatech.edu/": 4, "http://dsp.rice.edu/": 1, "http://en.wikipedia.org/": 3, "http://www.ucla.edu/": 1}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["Wouaw, almost one year and half without any post here\u2026. Shame on me! I\u2019ll try to be more productive with shorter posts now \n I just found this interesting paper about concentration properties of submodular function (very common in \u201cGraph Cut\u201d methods for instance) on arxiv: \n \n \n A note on concentration of submodular functions. (arXiv:1005.2791v1 [cs.DM]) \n Jan Vondrak, May 18, 2010 \n We survey a few concentration inequalities for submodular and fractionally subadditive functions of independent random variables, implied by the entropy method for self-bounding functions. The power of these concentration bounds is that they are dimension-free, in particular implying standard deviation O(\\sqrt{\\E[f]}) rather than O(\\sqrt{n}) which can be obtained for any 1-Lipschitz function of n variables. \n In particular, the author shows some interesting concentration results in his corollary 3.2. \n \n  \n Without having performed any developments, I\u2019m wondering if this result could serve to define a new class of matrices (or non-linear operators) satisfying either the Johnson-Lindenstrauss Lemma or the Restricted Isometry Property. \n For instance, by starting from Bernoulli vectors , i.e. , the rows of a sensing matrix, and defining some specific submodular (or self-bounding) functions (e.g. for some sparse vector and a \u201ckind\u201d function ), I\u2019m wondering if the concentration results above are better than those coming from the classical concentration inequalities (based on the Lipschitz properties of or . See e.g., the books of Ledoux and Talagrand)? \n Ok, all this is perhaps just due to too early thoughts \u2026. before my mug of black coffee"], "link": "http://yetaspblog.wordpress.com/2010/05/18/new-class-of-rip-matrices/", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://feeds.wordpress.com/": 1, "http://yetaspblog.wordpress.com/": 1}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["As detected by Igor Carron , this blog has reached its 1000th visit ! Well, perhaps it\u2019s 1000th robot visit \n Yesterday I found some very funny (math) jokes on Bj\u00f8rn\u2019s maths blog about \u201cHow to catch a lion in the Sahara desert\u201d with some \u2026 mathematical tools. \n Bj\u00f8rn collected there many ways to realize this task from many places on the web. There are really tons of examples. To give you an idea, here is the Schrodinger\u2019s method: \n \u201cAt any given moment there is a positive probability that there is a lion in the cage. Sit down and wait.\u201d \n or this one : \n \u201cThe method of inverse geometry: We place a spherical cage in the desert and enter it. We then perform an inverse operation with respect to the cage. The lion is then inside the cage and we are outside.\u201d \n So, let\u2019s try something about Compressed Sensing. (Note: if you have something better than my infamous suggestion, I would be very happy to read it as a comment to this post.) \n \u201cHow to catch a lion in the Sahara desert\u201d \n The compressed sensing way: First you consider that only one lion in a big desert is definitely a very sparse situation by comparing lion\u2019s size and the desert area. No need for a cage, just project randomly the whole desert into a dune of just 5 times the lion\u2019s weight ! Since the lion obviously died in this shrinking operation, you use the RIP (!) .. and relaxed , you eventually reconstruct its best tame approximation. \n Image: Wikipedia"], "link": "http://yetaspblog.wordpress.com/2008/11/23/1000th-visit-and-some-compressed-sensing-humour/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://igorcarron.googlepages.com/": 1, "http://bjornsmaths.blogspot.com/": 2}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["Following the writing of my previous post, which obtained various interesting comments (many thanks to Gabriel Peyr\u00e9 , Igor Carron and Pierre Vandergheynst ), I sent a mail to Michael P. Friedlander and Ewout van den Berg to point them this article and possibly obtain their point of views. \n Nicely, they sent me interesting answers (many thanks to them). Here they are (using the notations of the previous post ) : \n Michael\u2019s answer is about the need of a TV Lasso solver : \n \u201cIt\u2019s an intriguing project that you describe.\u00a0 I suppose in principle the theory behind spgl1 should readily extend to TV (though I haven\u2019t thought how a semi-norm might change things).\u00a0 But I\u2019m not sure how easy it\u2019ll be to solve the \u201cTV-Lasso\u201d subproblems.\u00a0 Would be great if you can see a way to do it efficiently. \u201c \n Ewout on his side explained this : \n \u201cThe idea you suggest may very well be feasible, as the approach taken in SPGL1 can be extended to other norms (i.e., not just the one-norm), as long as the dual norm is known and there is way to orthogonally project onto the ball induced by the primal norm. In fact, the newly released version of SPGL1 takes advantage of this and now supports two new formulations. \n I heard (I haven\u2019t had time to read the paper) that Chambolle has described the dual to the \nTV-norm. Since the derivate of on the appropriate interval is given by the dual norm on , that part should be fine (for the one norm this gives the infinity norm). \n In SPGL1 we solve the Lasso problem using a spectrally projected gradient method, which \nmeans we need to have an orthogonal projector for the one-norm ball of radius . It is not immediately obvious how to (efficiently) solve the related problem (for a given ): \n minimize subject to . \n However, the general approach taken in SPGL1 does not really care about how the Lasso \nsubproblem is solved, so if there is any efficient way to solve \n minimize subject to , \n then that would be equally good. Unfortunately it seems the complexification trick ( see the previous post ) works only from the image to the differences; when working with the differences themselves, additional constraints would be needed to ensure consistency in the image; i.e., that \nsumming up the difference of going right first and then down, be equal to the sum of \ngoing down first and then right.\u201d \n In a second mail, Ewout added an explanation on this last remark : \n \n \u201cI was thinking that perhaps, instead of minimizing over the signal it would be possible to minimize over the differences (expressed in complex numbers in the two-dimensional setting). The problem with that is that most complex vectors do not represent difference vectors (i.e., the differences would not add up properly). For such an approach to work, this consistency would have to be enforced by adding some constraints.\u201d \n Actually, I saw similar considerations in A. Chambolle \u2018s paper: \u201c\ufeff\ufeff\ufeff\ufeff An Algorithm for Total Variation Minimization and Applications\u201d . It is even more clear in the paper he wrote with J.-F. Aujol , \u201cDual Norms and Image Decomposition Models\u201d . They develop there the notions of TV (semi) norm for different exponent (i.e. in the norm used on the norm of the gradient components) and in particular they answer to the problem of finding and computing the corresponding dual norms. For the usual TV norm, this leads to the G-norm : \n \n where, as for the continuous setting, is the discrete divergence operator defined as the adjoint of the finite difference gradient operator used to defined the TV norm. In other words, , where and . \n Unfortunately, the G norm computation seems not so obvious that the one of its dual counterpart and an optimization method must be used. I don\u2019t know if this could lead to an efficient implementation of a TV spgl1."], "link": "http://yetaspblog.wordpress.com/2008/09/02/spgl1-and-tv-answers-from-spgl1-authors/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://igorcarron.googlepages.com/": 1, "http://yetaspblog.wordpress.com/": 3, "http://www.ens-cachan.fr/": 1, "ftp://ftp.inria.fr/INRIA/publication/publi-pdf/RR/RR-5130.pdf": 1, "http://www.polytechnique.fr/": 1, "http://www.ubc.ca/": 5, "http://ltspc89.epfl.ch/": 1, "http://www.dauphine.fr/": 3}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["Recently, I was using the SPGL1 toolbox to recover some \u201ccompressed sensed\u201d images. As a reminder, SPGL1 implements the method described in \u201c Probing the Pareto Frontier for basis pursuit solutions \u201d of Michael P. Friedlander and Ewout van den Berg . It solves the Basis Pursuit DeNoise (or ) problem with a error power \n \n where is the usual measurement matrix for a measurement vector , and and are the and the norm of the vector respectively. In short, as shown by E. Cand\u00e8s , J. Romberg and T. Tao , if is well behaved, i.e. if it satisfies the so-called Restricted Isometry Property for any sparse signals, then the solution of approximates (with a controlled error) a sparse (or compressible) signal such that , where is an additive noise vector with power . \n The reason of this post is the following : I\u2019m wondering if SPGL1 could be \u201ceasily\u201d transformed into a solver of the Basis Pursuit with the Total Variation (TV) norm. That is, the minimization problem \n \n where with is the th component of the complex finite difference operator applied on the vectorized image of pixels (in a set of coordinates and ).\u00a0 I have used here a \u201ccomplexification\u201d trick putting the finite differences and according to the directions and in the real part and the imaginary part respectively of the complex operator . The TV norm of is then really the norm of . \n This problem is particularly well designed for the reconstruction of compressed sensed images since most of them are very sparse in the \u201cgradient basis\u201d (see for instance some references about Compressed Sensing for MRI ). Minimizing the TV norm, since performed in the spatial domain, is also sometimes more efficient than minimizing the norm is a particular sparsity basis (e.g. 2-D wavelets, curvelets, \u2026). \n Therefore, I would say that, as for the initial SPGL1 theoretical framework, it could be interesting to study the Pareto frontier related to , even if the TV norm is now a quasi-norm , i.e.\u00a0 does not imply but for a certain constant . \n To explain better that point, let me first summarize the paper of Friedlander and van den Berg quoted above. They proposed to solve by solving a LASSO problem (or ) regulated by a parameter , \n \n If I\u2019m right, the key idea is that there exists a such that is equivalent to . The problem is thus to assess this point. SPGL1 finds iteratively using the fact that all the problems define a smooth and decreasing curve of (the Pareto curve ) from the norm of the residual , where is the solution of . More precisely, the function \n \n is decreasing from to a value such that \n \n Interestingly, the derivative exists on and it is simply equal to with . \n As explained, on the point , the problem provides the solution to . But since both and are known, a Newton method on this Pareto curve can then iteratively estimate from the implicit equation . Practically, this is done by solving of an approximate at each (and the convergence of the Newton method is still linear). \n At the end, the whole approach is very efficient for solving high dimensional BPDN problems (such that BPDN for images) and the final computation cost is mainly due to the cost of the forward and transposed multiplication of the matrix/operator with vectors. \n So what happens now if the norm is replaced by the TV norm in this process ? If we switch from to ? Is there a \u201cSPGL1 way\u201d to solve that ? \n The function resulting from such a context would have now the initial point (with the constant vector) since a zero TV norm means a constant (the value of arises just from the minimization on ). Notice that if is for instance a Gaussian measurement matrix, will be very close to since the expectation value of the average of any row is zero. \n For the rest, I\u2019m unfortunately not sufficiently familiar with convex optimization theory to deduce what is for the TV framework (hum. I should definitely study that). \n However, for the case, (i.e. ) is computed approximately for each . This approximation, which is also iterative, uses a special projection operator to guarantee that the current candidate solution in the iteration remains feasible, i.e. remains in the ball . As usual, this projection is accomplished through a Soft Thresholding procedure, i.e. as a solution of the problem \n , \n where is the point to project, and where is set so that the projection is inside the ball above. \n For the TV minimization case, the TV ball defining the feasible set of the approximate LASSO procedure would possibly generate a projection operator equivalent to the one solving the problem \n . \n This is somehow related to one of the lessons provided in the TwIST paper ( \u201cA new twIst: two-step iterative shrinkage/thresholding algorithms for image restoration\u201d ) of J. Bioucas-Dias and M. Figueiredo about the so-called Moreau function : There is a deep link between some iterative resolutions of a regularized BP problem using a given sparsity metric, e.g. the or the TV norm, and the canonic denoising method of this metric, i.e. when the measurement is the identity operator, giving Soft Thresholding or TV denoising respectively. \n Thanks to the implementation of Antonin Chambolle (used also by TwIST), this last canonic TV minimization can be computed very quickly. Therefore, if needed, the required projection on the TV ball above can be also inserted in a potential \u201cSPGL1 for TV sparsity problem\u201d. \n OK\u2026 I agree that all that is just a very rough intuition. There is a lot of points to clarify and to develop. However, if you know something about all that (or if you detect that I\u2019m totally wrong), or if you just want to comment this idea, feel free to use the comment box below \u2026"], "link": "http://yetaspblog.wordpress.com/2008/08/17/spgl1-and-tv-minimization/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://yetaspblog.wordpress.com/": 2, "http://www.it.pt/": 2, "http://www.caltech.edu/": 1, "http://users.gatech.edu/": 1, "http://en.wikipedia.org/": 1, "http://www.polytechnique.fr/": 1, "http://www.rice.edu/": 1, "http://www.ubc.ca/": 7, "http://terrytao.wordpress.com/": 1}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["Recently, I have found some interesting references about techniques designed around 1938 and that, in my opinion, could be qualified of (variant of) Matching Pursuit. Perhaps this is something known by a lot of researchers in the right scientific field, but here is however what I recently discovered. \n \n From what I knew until yesterday, when S. Mallat and Z. Zhang [1] defined their greedy or iterative algorithm named \u201cMatching Pursuit\u201d to decompose a signal into a linear combination of atoms taken in a dictionary of elements, the previous work to which they were referring to was the \u201c Progression Pursuit \u201d of J. Friedman and W. Stuetzle [2] in the field of statistical regression methods. \n In short, MP is very simple. It reads (using matrix notations) \n \n with at each step \n \n The quantities and are the residual and the approximation of respectively at the th MP step (so also an approximation in terms of ). A quite direct modification of MP is the Orthogonal Matching Pursuit [8] where only the index (or parameters) of the best atom (i.e. maximizing its correlation with ) at each iteration is recorded, and the approximation computed by a least square minimization on the set of the previously selected atoms. \n It is proved in [1] that MP converges always to \u2026 something, since the energy of the residual decreases steadily towards 0 with .\u00a0 Under certain extra assumptions on the dictionary (e.g. with small coherence, or cumulative coherence, that roughly measure its closeness to an orthogonal basis) it is also proved that, if is described by a linear combination of few elements of the dictionary (for sparse or compressible signal), i.e. with for having few non-zero (or large) components, then OMP recovers in the set of coefficients computed at each iteration [9] . For instance, in the trivial case of an orthonormal basis (i.e. with vanishing coherence) (O)MP finds iteratively . \n Dozens (or hundreds ?) of variations of these initial greedy methods have been introduced since their first formulations in the signal processing community. These variations have improved for instance the initial MP rate of convergence through the iterations, or the ability to solve the sparse approximation problem (i.e. finding expressed above), or are MP techniques adapted to some specific problem like the emerging Compressed Sensing . Let\u2019s quote for instance the gradient Pursuit, stagewise OMP, CoSaMP, regularized MP,\u00a0 subspace pursuit, \u2026 (see here and here for more informations on these). \n Another variation of (O)MP explained by K. Schnass and P. Vandergheynst in [3] , splits the sensing part from the reconstruction part in the initial MP algorithm above (adding also the possibility to select more than only one atom per iteration). Indeed, the selection of the best atom is performed there by a Sensing dictionary while the reconstruction stage building the residuals and approximations is still assigned to . In short, this variation is also proved to solve the sparse problem if the two dictionaries satisfy a small cross (cumulative) coherence criterion , which is easier to fulfill than asking for a small (cumulative) coherence of only one dictionary in the initial (O)MP. \n I introduced more precisely this last (O)MP variation above since it is under this form that I discovered it in the separated works of R.V. Southwell [4] and G. Temple [5] (the last being more readable in my opinion) in 1935 and in 1938 (!!), i.e. before the building of the first (electronic) computers in the 40\u2032s. \n The context of the method in the initial Southwell\u2019s work was the determination of stresses in structures. Let\u2019s summarize his problem : the structure was modelized by connected springs. If represents any motion vector of the springs extremities, then, at the new equilibrium state reached when some external forces are applied to the structure, the internal forces provided by the springs follows of course the Hooke law , i.e for a certain symmetric matrix containing the spring constants, and finally Newton\u2019s first law implies : \n . \n The global problem of Southwell was thus : given a linear system of equations , with and positive definite, how can you recover practically from and \u00a0 ? As explained by Temple, a solution to this problem is of course also \u201capplicable to any problem which is reducible to the solution of a system of non-homogeneous, linear, simultaneous algebraic equations in a finite number of unknown variables\u201d . \n \n Nowadays, the numerical solution seems trivial : take the inverse of and apply it to , and if is really big (or even small since I\u2019m lazy) compute for instance with Matlab and run \u201c>> inv(D)*s\u201d (or do something clever with the \u201c/\u201d Matlab operator). \n However, imagine the same problem in the 30\u2032s ! And assume you have to inverse a ridiculously small matrix of size 13\u00d713. It can be really long to solve it analytically and worthless since you are interested in finding . That\u2019s why some persons were interested at that time in computing , or an approximation to it, without to have this painful inverse computation. \n The technique found by R.V. Southwell and generalized later by Temple [4,5] , was dubbed of \u201cSuccessive Relaxation\u201d inside a more general context named \u201cSuccessive Approximations\u201d. Mathematically, rewriting that work under notations similar to these of modern Matching Pursuit methods, Successive Relaxation algorithm reads : \n \n where , is the th column of , is the th component of , is the vector such that (canonical basis vector), and with, at each step , the selection (sensing) \n \n i.e. the component of the th residual with the highest amplitude. \n In this framework, since is positive definite and thus non-singular, it is proved in [5] that the vectors tend to the true answer . The parameter controls the importance of what you removed or add in the residual and in the approximation respectively. You can prove easily that the decreasing of the residual energy is of course maximum when . \n In other words, in the concepts introduced in [3] , they designed a Matching Pursuit where they selected for the reconstruction dictionary the orthonormal basis and for the sensing dictionary the identity (the canonical basis) of . Amazing, No ? \n An interesting learning of the algorithm above is the presence of the factor . In the more general context of (modern) MP with non-orthonormal dictionary, such a factor could be useful to minimize the \u201cdecoherence\u201d effect observed experimentally in the decomposition of a signal when this one is not exactly fitted by elements of the dictionary (e.g., in image processing, arbitrarily oriented edges to be described by horizontal and vertical atoms). \n G. Temple in [5] extended also the method to infinite dimensional Hilbert spaces for a different updating step of . This is nothing else but the foundation of the (continuous) MP studied by authors like R. DeVore and Temlyakov some years ago (on that topic you can read also [6] , i.e. a paper I wrote with C. De Vleeschouwer for a geometric description of this continuous formalism). \n By googling a bit on \u201cmatching pursuit\u201d and Southwell , I found this presentation of Peter Buhlmann who makes a more general connection between Southwell\u2019s work, Matching Pursuit, and greedy algorithms (around slide 15) in the context of \u201c Iterated Regularization for High-Dimensional Data \u201c. \n In conclusion of all that, who is this person who explained that we do nothing but always reinventing the wheel ? \n If you want to complete this kind of \u201carcheology of Matching Pursuit\u201d please feel free to add some comments below. I\u2019ll be happy to read them and improve my general knowledge of the topic. \n Laurent \n References : \n [1] : S. G. Mallat and Z. Zhang, Matching Pursuits with Time-Frequency Dictionaries , IEEE Transactions on Signal Processing, December 1993, pp. 3397-3415. \n [2] : J. H. Friedman and J. W. Tukey (Sept. 1974). \u201c A Projection Pursuit Algorithm for Exploratory Data Analysis \u201c. IEEE Transactions on Computers C-23 (9): 881\u2013890. doi:10.1109/T-C.1974.224051. ISSN 0018-9340. \n [3] : K. Schnass and P. Vandergheynst, Dictionary preconditioning for greedy algorithms , IEEE Transactions on Signal Processing, Vol. 56, Nr. 5, pp. 1994-2002, 2008. \n [4] : R. V. Southwell, \u201c Stress-Calculation in Frameworks by the Method of \u201cSystematic Relaxation of Constraints \u201c. I and II. Proc Roy. Soc. Series A, Mathematical and Physical Sciences, Vol. 151, No. 872 (Aug. 1, 1935), pp. 56-95 \n [5] : G. Temple, The General Theory of Relaxation Methods Applied to Linear Systems , Proc. Roy. Soc. Series A, Mathematical and Physical Sciences, Vol. 169, No. 939 (Mar. 7, 1939), pp. 476-500. \n [6] : L. Jacques and C. De Vleeschouwer, \u201c A Geometrical Study of Matching Pursuit Parametrization \u201c, To appear in IEEE Transactions on Signal Processing (2007). \n [7] : R.A. DeVore and V.N. Temlyakov. \u201cSome remarks on greedy algorithms.\u201d Adv. Comput. Math., 5:173\u2013187, 1996. \n [8] : Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, \u201c Orthogonal projection pursuit: Recursive function approximation with applications to wavelet decomposition, \u201d in Proceedings of Twenty-Seventh Asilomar Conference on Signals, Systems and Computers, vol. 1, (Pacific Grove, CA), pp. 40- \n44, NOV. 1-3, 1993. \n [9] : Tropp, J, \u201c Greed is good: algorithmic results for sparse approximation \u201c, IEEE T. Inform. Theory., 2004, 50, 2231-2242 \n Image credit : \n EDSAC was one of the first computers to implement the stored program ( von Neumann ) architecture. Wikipedia."], "link": "http://yetaspblog.wordpress.com/2008/06/12/matching-pursuit-before-computer-science/", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://www.jstor.org/": 2, "http://www.ac.be/": 1, "http://igorcarron.googlepages.com/": 1, "http://yetaspblog.wordpress.com/": 12, "http://ieeexplore.ieee.org/": 1, "http://lts2www.epfl.ch/": 2, "http://en.wikipedia.org/": 7, "http://www.ens.fr/": 1, "http://citeseer.psu.edu/": 1, "http://www.compressedsensing.com/": 1, "http://www.polytechnique.fr/": 2, "http://feeds.wordpress.com/": 1, "http://stat.ethz.ch/": 1, "http://ltspc89.epfl.ch/": 1, "http://www.google.com/": 1, "http://nuit-blanche.blogspot.com/": 1}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}, {"content": ["So, as many researchers in the world, I have just opened my own Science 2.0. blog : this \u201cLe petit chercheur illustr\u00e9\u201d. An approximative English translation would be \u201cThe Illustrated (report of a) Small Researcher\u201d. As you can see, I decided to use WordPress since this site supports LaTeX writing and I foresee to use it of course to display some useful notations and equations. \n I hope I will introduce here some interesting elements about the scientific interests of an humble researcher in applied mathematics. As you will understand, my English is far to be perfect but I\u2019ll try to do my best to express myself correctly (not as a native English however). \n To give you an idea, my fields of research are rather various. One of them is \u201csignal representation\u201d, a subtopic of \u201csignal processing\u201d. The term signal has to be understood in its wide sense, I mean, signals living in 1-D (like the record of a piece of music), 2-D or n-D (e.g. images, videos, or multi-modal signals), or on more esoteric spaces like the sphere (imagine the measure of the temperature field all over the world). Signal can also be described as data provided on a given manifold , e.g. like the electric potential on a molecular surface, or this manifold itself like the high dimensional space of all the images produced by a moving camera \u2026 \n I work also on how to obtain or tune a signal \u201crepresentation\u201d using concepts, methods or algorithms like *-lets basis, dictionaries, signal sparsity, signal compressibility, Basis Pursuit, *-Matching Pursuits, compressed sensing , \u2026 I\u2019m interested also in applications like plenoptic imaging, light field rendering, computational photography , \u2026 i.e. new ways to record visible information of the world. So, most of the news I\u2019ll publish here will concern more or less directly one of these elements. I do not plan to write here final reflection or results so be aware that I will write sometimes erroneous explanations ( C\u2019est la vie ). But I\u2019m sure you\u2019ll help me to improve them by inserting comment s. \n So, in one word : welcome ! \n Laurent \n Image Credit : Wikipedia."], "link": "http://yetaspblog.wordpress.com/2008/05/18/first-news/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.laurent-duval.eu/": 1, "http://images.google.ch/": 1, "http://wordpress.com": 1, "http://en.wikipedia.org/": 3, "http://www.rice.edu/": 1}, "blogtitle": "Le Petit Chercheur Illustr\u00e9"}]