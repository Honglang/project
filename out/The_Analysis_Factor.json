[{"blogurl": "http://www.theanalysisfactor.com\n", "blogroll": [], "title": "The Analysis Factor"}, {"content": ["by David Lillis, Phd. \n In Part 1 we installed R and used it to create a variable and summarize it using a few simple commands. Today let\u2019s re-create that variable and also create a second variable, and see what we can do with them. \n As before, we take height to be a variable that describes the heights (in cm) of ten people. Copy and paste the following code to the R command line to create this variable. \n height = c(176, 154, 138, 196, 132, 176, 181, 169, 150, 175) \n Now let\u2019s take weight to be a variable that describes the weights (in kg) of the same ten people. Copy and paste the following code to the R command line to create the weight variable. \n weight = c(82, 49, 53, 112, 47, 69, 77, 71, 62, 78) \n Both variables are now stored in the R workspace. To view them, enter: \n height \n weight \n We can now create a simple plot of the two variables as follows: \n plot(weight, height) \n However, this is a rather simple plot and we can embellish it a little. Copy and paste the following code into the R workspace: \n plot(weight, height, pch = 16, cex = 1.3, col = \u201cred\u201d, main = \u201cMY FIRST PLOT USING R\u201d, xlab = \u201cWEIGHT (kg)\u201d, ylab = \u201cHEIGHT (cm)\u201d) \n In the above code, the syntax pch = 16 creates solid dots, while cex = 1.3 creates dots that are 1.3 times bigger than the default (where cex = 1). More about these commands later. \n Now let\u2019s perform a linear regression on the two variables by adding the following text at the command line: \n lm(height, weight) \n We see that the intercept is 98.0054 and the slope is 0.9528. By the way \u2013 lm stands for \u201clinear model\u201d. \n Finally, we can add a best fit line to our plot by adding the following text at the command line: \n abline(98.0054, 0.9528) \n None of this was so difficult! \n In Part 3 we will look again at regression and create more sophisticated plots. \n About the Author: David Lillis has taught R to many researchers and statisticians. His company, Sigma Statistics and Research Limited , provides both on-line instruction and face-to-face workshops on R, and coding services in R. David holds a doctorate in applied statistics. \n \n \n \n Like this post? \nEnter your email address to have posts delivered: \n \n \n Related Posts The 3 Stages of Mastering Statistical Analysis \n SPSS, SAS, R, Stata, JMP? Choosing a Statistical Software Package or Two. \n R is Not So Hard! A Tutorial, Part 1 \n R version 2.9.1 released \n Quick-R: A guide for SPSS, SAS, and Stata Users"], "link": "http://feedproxy.google.com/~r/statchat/~3/qWHZe4tnC7A/", "bloglinks": {}, "links": {"http://www.co.nz/": 1, "http://addthis.com/": 1, "http://www.theanalysisfactor.com/": 6}, "blogtitle": "The Analysis Factor"}, {"content": ["Standardized regression coefficients remove the unit of measurement of predictor and outcome variables.\u00a0 They are sometimes called betas , but I don\u2019t like to use that term because there are too many other, and too many related, concepts that are also called beta. \n There are many good reasons to report them: \n \n They serve as standardized effect size statistics. \n They allow you to compare the relative effects of predictors measured on different scales. \n They make journal editors and committee members happy in fields where they are commonly reported. \n \n If you use a regression procedure in most software, standardized regression coefficients are reported by default. Or at least an easy option. \n But there are times you need to use some procedure that won\u2019t compute standardized coefficients for you. \n Often it makes more sense to use a general linear model procedure to run regressions.\u00a0 But GLM in SAS and SPSS don\u2019t give standardized coefficients. \n Likewise, you won\u2019t get standardized regression coefficients reported after combining results from multiple imputation. \n Luckily, there\u2019s a way to get around it. \n A standardized coefficient is the same as an unstandardized coefficient between two standardized variables. We often learn to standardize the coefficient itself because that\u2019s the shortcut.\u00a0 But implicitly, it\u2019s the equivalence to the coefficient between standardized variables that gives a standardized coefficient meaning. \n So all you have to do to get standardized coefficients is standardize your predictors and your outcome. \n How? \n The Steps \n Remember all those Z-scores you had to calculate in Intro Stats?\u00a0 It wasn\u2019t the useless exercise you thought it was at the time. \n Converting a variable to a Z-score is standardizing. \n In other words, do these steps for Y, your outcome variable, and every X, your predictors: \n 1. Calculate the mean and standard deviation. \n 2. Create a new standardized version of each variable.\u00a0 To get it, create a new variable in which you subtract the mean from the original value, then divide that by the standard error. \n 3. Use those standardized versions in the regression. \n \n Could this take a while?\u00a0 Yup. \n But if that\u2019s what the journal requires you report, just do it. \n \n A nice advantage, is you can apply it, at least partially, even in regression models that can\u2019t usually accommodate standardized regression coefficients. \n For example, in a logistic regression it doesn\u2019t make sense to standardize Y because it\u2019s categorical.\u00a0 But you can standardize all your Xs to get rid of their units. \n You can then interpret your odds ratios in terms of one standard deviation increases in each X, rather than one-unit increases. \n \n \n \n Want to learn more about making regression coefficients make sense?\u00a0 Join us in our Interpreting (Even Tricky) Regression Coefficients workshop. \n For more details, click here . \n \n Related Posts Confusing Statistical Terms #2: Alpha and Beta"], "link": "http://feedproxy.google.com/~r/statchat/~3/jcEcra5WChk/", "bloglinks": {}, "links": {"http://theanalysisinstitute.com/": 2, "http://addthis.com/": 1, "http://www.theanalysisfactor.com/": 3}, "blogtitle": "The Analysis Factor"}, {"content": ["I received an e-mail from a researcher in Canada that asked about communicating logistic regression results to non-researchers. It was an important question, and there are a number of parts to it. \n With the asker\u2019s permission, I am going to address it here. \n To give you the full context, she explained in a followup email that she is communicating to a clinical audience who will be using the results to make clinical decisions. They need to understand the size of an effect that an intervention will provide.\u00a0 She refers to an output I presented in my webinar on Probability, Odds, and Odds Ratios, which you can download free here . \n Question: \n I just went through the two lectures re: logistic regression and prob/odds/odds ratios. I completely understand everything and I have recently run some logistic and multinomial regressions. I have read many papers etc. but still struggle with the meaning of the ratio and putting it into language that nonstats folks will understand. \n In your example, you had gender 0=fem, 1=male for failing with EXP(b) at 5.18. I understand males have 5.2xs odds of failing compared to odds of females failing, and that males odds are 418% higher than females odds of failing\u2026.but how do I make sense of that with a clear example. Can I say for every one female who fails, X number of males will fail? Or for every 1% of females who fail, X% of males will fail? \n So could you provide me a way of explaining what 418% higher odds actually means \u2013 I think I am also trying to figure out how I would decide whether it is clinically meaningful. For example, if males have 5% higher odds for Y compared to females, how do I know 5% higher odds really matters? \n Answer: \n First of all, you\u2019re absolutely correct in the way you\u2019re interpreting the odds ratio. You\u2019re also correct in that for people who are not familiar with odds ratios, they\u2019re going to need an intuitive way of understanding the results. \n This is especially true when your audience is a clinical one who needs to make decisions based on your results. So you\u2019re also absolutely correct that presenting a table full of odds ratios is not the way to go here. \n To answer your first question, no. You cannot say for every one female who fails, X number of males will fail. \n A Concrete Expression of Odds \n You can, however, convey the odds ratios in a concrete way through an example. \n So for example, you could say if the odds of a female failing is 1 to 2, the odds of a male failing is about five times as big, or about 5 to 2. \n In other words, for every 10 females who fail, 20 pass all their classes. \n But for every 10 males who fail, only 4 pass all their classes. \n Most people can understand odds and odds ratios in those terms. \n This works extremely well for both categorical predictor variables, which interventions usually are (eg. control vs. intervention), or continuous variables, like SAT Math score. \n Predicted Probabilities \n Another way to do it is the way you suggest second, which is to convert predicted odds to predicted probabilities of a female or male failing. \n The one thing you have to be careful of, though, is the effect of a predictor on a probability of failing is not constant across all values of the predictor. \n That is not a big deal for the categorical predictors, but it can be misleading for continuous ones. \n For example, you can figure out the probability of failing at different SAT math scores, which had an odds ratio of .97.\u00a0 So the 3% lower odds of failing is the same whether the SAT Math score is low (at 250), medium (at 500), or high (at 750).\u00a0 It\u2019s because it works like a rate\u2013how much the odds differs depends on the starting point. \n The thing to keep in mind though, is most people will interpret those predicted probabilities as means, and think immediately in terms of the differences in those means.\u00a0 The differences in those probabilites is not the same at low, medium, and high values of SAT Math.\u00a0 It\u2019s bigger in the middle than at the ends because the relationship between SAT Math and the probability isn\u2019t linear, it\u2019s sigmoidal. \n It\u2019s easy to say that last fact isn\u2019t important, but it\u2019s why we\u2019re running logistic regression in the first place. \n So at the very least, show what the predicted probabilities are at many values of SAT math, and point out that increasing an SAT math score by 20 points has a very small effect for people whose scores are very low or very high, and a much larger effect for people whose scores are in the middle. \n Graph It \n I recently come across this great article from Decision Science News that gets at this exact question within the context of medical risk. \n It describes some graphical ways to show risk by making the frequencies that make up the probabilities explicit: Some ideas on communicating risks to the general public. \u00a0 While logistic regression results aren\u2019t necessarily about risk, risk is inherently about likelihoods that some outcome will happen, so it applies quite well. \n Clinically Meaningful Effects \n Now what\u2019s clinically meaningful is a whole different story. That can be difficult with any regression parameter in any regression model. \n The odds ratio is an effect size you can use to choose a clinically meaningful cutoff, but you\u2019re going to have to use your substantive knowledge of your variables and your field to decide how much of an effect makes a clinical difference in people\u2019s lives. \n \n \n \n If you\u2019d like to learn more about how to run, interpret, and evaluate logistic regression models, check out our 8-hour online workshop Logistic Regression for Binary, Ordinal, and Multinomial Outcomes Workshop . \n \n Related Posts Mixed Models for Logistic Regression in SPSS \n Chi-square test vs. Logistic Regression: Is a fancier test better? \n Why Logistic Regression for Binary Response? \n Confusing Statistical Terms #1: The Many Names of Independent Variables \n Introduction to Logistic Regression"], "link": "http://feedproxy.google.com/~r/statchat/~3/r0S65MGAV3k/", "bloglinks": {}, "links": {"http://theanalysisinstitute.com/": 1, "http://addthis.com/": 1, "http://www.decisionsciencenews.com/": 1, "http://www.theanalysisfactor.com/": 6}, "blogtitle": "The Analysis Factor"}, {"content": ["Every once in a while, I work with a client who is stuck between a particular statistical rock and hard place. \n It happens when they\u2019re trying to run an analysis of covariance ( ANCOVA ) model because they have a categorical independent variable and a continuous covariate. \n The problem arises when a coauthor, committee member, or reviewer insists that ANCOVA is inappropriate in this situation because one of the following ANCOVA assumptions are not met: \n 1. The independent variable and the covariate are independent of each other. \n 2. There is no interaction between independent variable and the covariate. \n If you look them up in any design of experiments textbook, which is usually where you\u2019ll find information about ANOVA and ANCOVA, you will indeed find these assumptions.\u00a0 So the critic has nice references. \n However, this is a case where it\u2019s important to stop and think about whether the assumptions apply to your situation, and how dealing with the assumption will affect the analysis and the conclusions you can draw. \n An Example \n A very simple example of this might be a study that examines the difference in heights of kids who do and do not have a parasite.\u00a0 Since a large contributor to children\u2019s height is age, this is an important control variable. \n In this graph, you see the relationship between age X1, on the x-axis and height on the y-axis at two different values of X2, parasite status.\u00a0 X2=0 indicates group of children who have the parasite and X2=1 is the group of children who do not. \n  \n Younger children tend to be afflicted with the parasite more often. That is, the mean age (mean of X1) of the blue dots is clearly lower than the mean age of the black stars.\u00a0 In other words, the ages of kids with the parasite are lower than those without. \n So the independence between the independent variable (parasite status) and the covariate (age) is clearly violated. \n How to Deal with Violation of the Assumptions \n These are your options: \n 1. Drop the covariate from the model so that you\u2019re not violating the assumptions of ANCOVA and run a one-way ANOVA. This seems to be the popular option among most critics. \n 2. Retain both the covariate and the independent variable in the model anyway. \n 3. Categorize the covariate into low and high ages, then run a 2\u00d72 ANOVA. \n Option #3 is often advocated, but I hope you will soon see why it\u2019s unnecessary, at best.\u00a0 Arbitrarily splitting a numerical variable into categories is just throwing away good information. \n Let\u2019s examine option #1. \n The problem with it is shown in the graph\u2013it doesn\u2019t accurately reflect the data or the relationships among the variables. \n With the covariate in the model, the difference in the mean height for kids with and without the parasite is estimated for\u00a0 children at the same age (the height of the red line). \n If you drop the covariate, the difference in mean height is estimated at the overall mean for each group (the purple line). \n  \n In other words, any effect of age will be added to the effect of parasite status, and you\u2019ll overstate the effect of the parasite on the mean difference in children\u2019s heights. \n Why is it an assumption, then? \n You are probably asking yourself \u201cwhy on earth would this be an assumption of ANCOVA if removing the covariate leads us to overstate relationships?\u201d \n To understand why, we need to investigate the problem this assumptions is addressing. \n In the analysis of covariance section of Geoffrey Keppel\u2019s excellent book, Design and Analysis: A Researcher\u2019s Handbook, he states: \n It [ANCOVA] is used to accomplish two important adjustments: (1) to refine estimates of experimental error and (2) to adjust treatment effects for any differences between the treatment groups that existed before the experimental treatments were administered. Because subjects were randomly assigned to the treatment conditions [emphasis mine], we would expect to find relatively small differences among the treatments on the covariate and considerably larger differences on the covariate among the subjects within the different treatment conditions. Thus the analysis of covariance is expected to achieve its greatest benefits by reducing the size of the error term [emphasis Keppel's]; any correction for pre-existing differences produced a random assignment will be small by comparison. \n A few pages later he states, \n The main criterion for a covariate is a substantial linear correlation with the dependent variable, Y. In most cases, the scores on the covariate are obtained before the initiation of the experimental treatment\u2026. Occasionally the scores are gathered after the experiment is completed. Such a procedure is defensible only when it is certain that the experimental treatment did not influence the covariate\u2026.The analysis of covariance is predicated on the assumption that the covariate is independent of the experimental treatments. \n In other words, it\u2019s about not tainting the results that can be drawn by experimentally manipulated treatments.\u00a0 If a covariate was related to the treatment, it would indicate a problem with random assignment, or it would indicate that the treatments themselves caused the covariate values.\u00a0 These are very important considerations in experiments . \n If however, as in our parasite example, the main categorical independent variable is observed and not manipulated, the independence assumption between the covariate and the independent variable is irrelevant. \n It\u2019s a design assumption. It\u2019s not a model assumption. \n The only effect of the assumption of the independent variable and the covariate being independent is in how you interpret the results . \n So what is the appropriate solution? \n The appropriate response is #2\u2013keep the covariate in the analysis, and don\u2019t interpret results from an observational study as if they were from an experiment. \n Doing so will lead to a more accurate estimate of the real relationship between the independent variable and the outcome. Just make sure you\u2019re saying that this is the mean difference at any given value of the covariate. \n The last issue then becomes: If your critic has banned the word ANCOVA because you don\u2019t have an experiment, what do you call it? \n Now it\u2019s down to semantics. It is accurate to call it a general linear model , a multiple regression, or (in my option), an ANCOVA (I have never seen anyone balk at calling an analysis an ANOVA when the two categorical IVs were related). \n The critics who get hung up on this assumption are usually the ones who want a specific name.\u00a0\u00a0 General Linear Model is too ambiguous for them. I\u2019ve had clients who had to call it a multiple regression, even though the main independent variable was the categorical one. \n One option is use \u201ccategorical predictor variable\u201d instead of \u201cindependent variable\u201d when describing the variable in the ANCOVA.\u00a0 The latter implies manipulation; the former does not. \n This is a case where it\u2019s worth fighting for your analysis , but not the name.\u00a0 The point of all this is communicating results accurately. \n \n \n \n \n To learn more about how to interpret results of linear models, including covariates, check out our 6-hour workshop \u201c Interpreting (Even Tricky) Regression Coefficients .\u201d \n The workshop goes over what the results of linear models mean even in really tricky contexts\u2014models with interactions, quadratic terms, dummy coding, and correlated predictor variables. You will also learn how strategies like centering predictors and standardizing coefficients can help you make sense of your results. \n Related Posts SPSS GLM: Choosing Fixed Factors and Covariates \n The General Linear Model, Analysis of Covariance, and How ANOVA and Linear Regression Really are the Same Model Wearing Different Clothes \n 6 Types of Dependent Variables that will Never Meet the GLM Normality Assumption \n Checking the Normality Assumption for an ANOVA Model \n Five Extensions of the General Linear Model"], "link": "http://feedproxy.google.com/~r/statchat/~3/AcmiSABflq0/", "bloglinks": {}, "links": {"http://theanalysisinstitute.com/": 1, "http://addthis.com/": 1, "http://www.theanalysisfactor.com/": 13}, "blogtitle": "The Analysis Factor"}, {"content": ["When the response variable for a regression model is categorical, linear models don\u2019t work.\u00a0 Logistic regression is one type of model that does, and it\u2019s relatively straightforward for binary responses . \n When the response variable is not just categorical, but ordered categories, the model needs to be able to handle the multiple categories, and ideally, account for the ordering. \n An easy-to-understand and common example is level of educational attainment.\u00a0 Depending on the population being studied, some response categories may include: \n 1 Less than high school \n2 Some high school, but no degree \n3 Attain GED \n4 High school graduate \n You can see how there are qualitative differences in these categories that wouldn\u2019t be captured by years of education.\u00a0 You can also see that the number of years isn\u2019t equal from one category to another, but there is a definite ordering that we wouldn\u2019t want to ignore. \n Let\u2019s say the model is interested in seeing how participation in a middle-school community program to help kids stay in school affected their later educational attainment, controlling for gender, race, size of high school, and middle school test scores. \n One popular logistic regression model for ordered responses is called a proportional odds model . \n The Proportional Odds Model \n It\u2019s a type of logistic regression in which you\u2019re modeling the relationship between predictor variables and the propensity to be in each higher ordered category. \n For example, the model would report how each predictor variable uniquely affects the odds of being in category 2 or higher compared to category 1; being in category 3 or higher compared to being in category 2 or 1; up to being in category 4 compared to being in categories 1, 2, or 3. \n Each comparison has its own intercept, but the same set of regression coefficient estimates.\u00a0 The intercepts reflect the fact that some categories, like high school graduate, are just more likely, regardless of the predictors. \n The regression coefficients represent the relationship of each predictor, each X, to the odds that an individual would be in each category or above compared to all lower categories. \n (Note: different stat software procedures use different defaults on the ordering\u2014some model being in a higher category, some model being in a lower category.\u00a0 Make sure you know which direction your software is using). \n One nice thing about this model is it is relatively simple to interpret and report\u2014there is just a single coefficient for each predictor. \n But it also creates one extremely important assumption\u2013that the relationship of predictors to the odds of a response being in the next higher order category is the same regardless of which categories you\u2019re comparing. \n So if the program doubled the odds of being a high school graduate, compared to all lower categories, it would have to also double the odds of attaining a GED or high school diploma, compared to none or some high school. \n This is called the proportional odds assumptions or the parallel regression assumption. Unfortunately this assumption is hard to meet in real data. \n In fact, it seems a middle-school program would have a much bigger effect on some of the lower categories\u2014maybe getting kids to continue into high school\u2013than it would on later categories. \n The Generalized Ordered Logistic Regression Model \n Luckily, there are alternatives.\u00a0 Here I focus on one, the generalized ordered logistic regression.\u00a0 It\u2019s a more complicated model, because it has a unique set of regression coefficients for each comparison. \n It does this by fitting a separate set of regression coefficients for each comparison. \u00a0The comparisons are the same\u2014we\u2019re still measuring, for example, the odds of being in category 2 or higher compared to category 1 or the odds of being in category 4 compared to 3 and below. \n The result is usually a much better-fitting, but complicated model.\u00a0 The result in this case would be 3 sets of regression coefficients.\u00a0 Having more response categories means having more sets of regression coefficients. \n This can be hard to interpret\u2014there is no single number for the effect of the program on the odds of attaining more education. \n But if there really is no single effect, interesting patterns may emerge.\u00a0 For example, the program may have positive impacts on getting kids into high school (being in categories 2, 3, or 4 compared to 1) but no impact on attaining some sort of degree or GED (being in category 3 or 4 compared to 1 or 2). \n These differences in impact could be the most interesting results. \n \n \n To learn more about logistic regression models for binary, ordered, and unordered categories, check out the recording of our webinar: Binary, Ordinal, and Multinomial Logistic Regression for Categorical Outcomes . It\u2019s free. \n Related Posts Logistic Regression Models for Multinomial and Ordinal Variables \n What Happened to R squared?: Assessing Model Fit for Logistic, Multilevel, and Other Models that use Maximum Likelihood Webinar \n Interpreting Regression Coefficients in Models other than Ordinary Linear Regression \n How to Combine Complicated Models with Tricky Effects \n Why use Odds Ratios in Logistic Regression"], "link": "http://feedproxy.google.com/~r/statchat/~3/5uIJhfQ5xoM/", "bloglinks": {}, "links": {"http://addthis.com/": 1, "http://www.theanalysisfactor.com/": 10}, "blogtitle": "The Analysis Factor"}, {"content": ["All statistical modeling\u2013whether ANOVA, Multiple Regression, Poisson Regression, Multilevel Model\u2013is about understanding the relationship between independent and dependent variables. The content differs, but as a data analyst, you need to follow the same 13 steps to complete your modeling. \n This webinar will give you an overview of these 13 steps: \n \n what they are \n why each one is important \n the general order in which to do them \n on which steps the different types of modeling differ and where they\u2019re the same \n \n Having a road map for the steps to take will make your modeling more efficient and keep you on track. \n Date: December 5, 2012 \n Time: 3pm Eastern Time UTC -4 (2pm Central, 1pm Mountain, 12pm Pacific) \n Where: \u00a0Anywhere you have a fast internet connection \n Length of Program: \u00a0An Hour \n Cost: \u00a0Always FREE \n Space is limited. \n[upcoming-webinar-optin]"], "link": "http://feedproxy.google.com/~r/statchat/~3/KWqLTSViEbI/", "bloglinks": {}, "links": {}, "blogtitle": "The Analysis Factor"}, {"content": ["by Maike Rahn, PhD \n Rotations \n An important feature of factor analysis is that the axes of the factors can be rotated within the multidimensional variable space. What does that mean? \n Here is, in simple terms, what a factor analysis program does while determining the best fit between the variables and the latent factors: Imagine you have 10 variables that go into a factor analysis. \n The program looks first for the strongest correlations between variables and the latent factor , and makes that Factor 1. Visually, one can think of it as an axis (Axis 1). \n The factor analysis program then looks for the second set of correlations and calls it Factor 2, and so on. \n Sometimes, the initial solution results in strong correlations of a variable with several factors or in a variable that has no strong correlations with any of the factors. \n In order to make the location of the axes fit the actual data points better, the program can rotate the axes. Ideally, the rotation will make the factors more easily interpretable. \n Here is a visual of what happens during a rotation when you only have two dimensions (x- and y-axis): \n  \n The original x- and y-axes are in black. During the rotation, the axes move to a position that encompasses the actual data points better overall. \n Programs offer many different types of rotations. An important difference between them is that they can create factors that are correlated or uncorrelated with each other. \n Rotations that allow for correlation are called oblique rotations ; rotations that assume the factors are not correlated are called orthogonal rotations . Our graph shows an orthogonal rotation. \n Once again, let\u2019s explore indicators of wealth. \n Let\u2019s imagine the orthogonal rotation did not work out as well as previously shown. Instead, we get this result: \n \n \n \n Variables \n Factor 1 \n Factor 2 \n \n \n Income \n 0.63 \n 0.14 \n \n \n Education \n 0.47 \n 0.24 \n \n \n Occupation \n 0.45 \n 0.22 \n \n \n House value \n 0.39 \n 0.25 \n \n \n Number of public parks in neighborhood \n 0.12 \n 0.20 \n \n \n Number of violent crimes per year \n 0.21 \n 0.18 \n \n \n \n Clearly, no variable is loading highly onto Factor 2. What happened? \n Since our first attempt was an orthogonal rotation, we specified that Factor 1 and 2 are not correlated. \n But it makes sense to assume that a person with a high \u201cIndividual socioeconomic status\u201d (Factor 1) lives also in an area that has a high \u201cNeighborhood socioeconomic status\u201d (Factor 2). That means the factors should be correlated. \n Consequently, the two axes of the two factors are probably closer together than an orthogonal rotation can make them. Here is a display of the oblique rotation of the axes for our new example, in which the factors are correlated with each other: \n  \n Clearly, the angle between the two factors is now smaller than 90 degrees, meaning the factors are now correlated. In this example, an oblique rotation accommodates the data better than an orthogonal rotation. \n About the Author: Maike Rahn is a health scientist with a strong background in data analysis.\u00a0\u00a0 She provides data analysis and research services to researchers in health fields as part of The Analysis Factor\u2019s partner network.\u00a0 Maike has a Ph.D. in Nutrition from Cornell University. \n \n \n Like this post? \nEnter your email address to have posts delivered:"], "link": "http://feedproxy.google.com/~r/statchat/~3/uqsdik36mUo/", "bloglinks": {}, "links": {"http://addthis.com/": 1, "http://www.theanalysisfactor.com/": 5}, "blogtitle": "The Analysis Factor"}, {"content": ["Two methods for dealing with missing data,vast improvements over traditional approaches, have become available in mainstream statistical software in the last few years. \n Both of the methods discussed here require that the data are missing at random\u2013not related to the missing values. If this assumption holds, resulting estimates (i.e., regression coefficients and standard errors) will be unbiased with no loss of power. \n The first method is Multiple Imputation (MI). Just like the old-fashioned imputation methods, Multiple Imputation fills in estimates for the missing data.\u00a0 But to capture the uncertainty in those estimates, MI estimates the values multiple times. Because it uses an imputation method with error built in, the multiple estimates should be similar, but not identical. \n The result is multiple data sets with identical values for all of the non-missing values and slightly different values for the imputed values in each data set. The statistical analysis of interest, such as ANOVA or logistic regression, is performed separately on each data set, and the results are then combined. Because of the variation in the imputed values, there should also be variation in the parameter estimates, leading to appropriate estimates of standard errors and appropriate p-values. \n Multiple Imputation is available in SAS, S-Plus, R, and now SPSS 17.0 (but you need the Missing Values Analysis add-on module). \n The second method is to analyze the full, incomplete data set using maximum likelihood estimation. This method does not impute any data, but rather uses each cases available data to compute maximum likelihood estimates. The maximum likelihood estimate of a parameter is the value of the parameter that is most likely to have resulted in the observed data. \n When data are missing, we can factor the likelihood function. The likelihood is computed separately for those cases with complete data on some variables and those with complete data on all variables. These two likelihoods are then maximized together to find the estimates. Like multiple imputation, this method gives unbiased parameter estimates and standard errors. One advantage is that it does not require the careful selection of variables used to impute values that Multiple Imputation requires. It is, however, limited to linear models. \n Analysis of the full, incomplete data set using maximum likelihood estimation is available in AMOS. AMOS is a structural equation modeling package, but it can run multiple linear regression models.\u00a0 AMOS is easy to use and is now integrated into SPSS, but it will not produce residual plots, influence statistics, and other typical output from regression packages. \n References: \nSchafer, J. Software for Multiple Imputation \nHox, J.J. (1999) A Review of Current Software for Handling Missing Data , Kwantitatieve Methoden, 62, 123-138. \nAllison, P. (2000). Multiple Imputation for Missing Data: A Cautionary Tale, Sociological Methods and Research, 28, 301-309. \n \u2014\u2014\u2013 \n Ten years ago, I got stuck with many Missing Data issues.\u00a0 They\u2019re the reason I extensively studied new approaches to missing data and developed the Effectively Dealing with Missing Data Without Biasing Your Results online workshop.\u00a0 By the end of the workshop, you\u2019ll know when and how to impute well, how and when to use maximum likelihood techniques, and when simple, traditional techniques like listwise deletion work just fine. \n Get the details and register here . \n Related Posts When Unequal Sample Sizes Are and Are NOT a Problem in ANOVA \n A Sneak Peak at SPSS 19 \n Recoding Variables in SPSS Menus and Syntax \n Five Advantages of Running Repeated Measures ANOVA as a Mixed Model \n Variable Labels and Value Labels in SPSS"], "link": "http://feedproxy.google.com/~r/statchat/~3/86Zk0PKqeYs/", "bloglinks": {}, "links": {"http://theanalysisinstitute.com/": 2, "http://www.uu.nl/": 1, "http://www.theanalysisfactor.com/": 5, "http://www.psu.edu/": 1}, "blogtitle": "The Analysis Factor"}, {"content": ["by Maike Rahn, PhD \n Why use factor analysis? \n Factor analysis is a useful tool for investigating variable relationships for complex concepts such as socioeconomic status, dietary patterns, or psychological scales. \n It allows researchers to investigate concepts that are not easily measured directly by collapsing a large number of variables into a few interpretable underlying factors. \n What is a factor? \n The key concept of factor analysis is that multiple observed variables have similar patterns of responses because of their association with an underlying latent variable, the factor, which cannot easily be measured. \n For example, people may respond similarly to questions about income, education, and occupation, which are all associated with the latent variable socioeconomic status. \n In every factor analysis, there are the same number of factors as there are variables.\u00a0 Each factor captures a certain amount of the overall variance in the observed variables, and the factors are always listed in order of how much variation they explain. \n The eigenvalue is a measure of how much of the variance of the observed variables a factor explains.\u00a0 Any factor with an eigenvalue \u22651 explains more variance than a single observed variable. \n So if the factor for socioeconomic status had an eigenvalue of 2.3 it would explain as much variance as 2.3 of the three variables.\u00a0 This factor, which captures most of the variance in those three variables, could then be used in other analyses. \n The factors that explain the least amount of variance are generally discarded.\u00a0 Deciding how many factors are useful to retain will be the subject of another post. \n What are factor loadings? \n The relationship of each variable to the underlying factor is expressed by the so-called factor loading. Here is an example of the output of a simple factor analysis looking at indicators of wealth, with just six variables and two resulting factors. \n \n \n \n Variables \n Factor 1 \n Factor 2 \n \n \n Income \n 0.65 \n 0.11 \n \n \n Education \n 0.59 \n 0.25 \n \n \n Occupation \n 0.48 \n 0.19 \n \n \n House value \n 0.38 \n 0.60 \n \n \n Number of public parks in neighborhood \n 0.13 \n 0.57 \n \n \n Number of violent crimes per year in neighborhood \n 0.23 \n 0.55 \n \n \n \n \n The variable with the strongest association to the underlying latent variable. Factor 1, is income, with a factor loading of 0.65. \n Since factor loadings can be interpreted like standardized regression coefficients , one could also say that the variable income has a correlation of 0.65 with Factor 1. This would be considered a strong association for a factor analysis in most research fields. \n Two other variables, education and occupation, are also associated with Factor 1. Based on the variables loading highly onto Factor 1, we could call it \u201cIndividual socioeconomic status.\u201d \n House value, number of public parks, and number of violent crimes per year, however, have high factor loadings on the other factor, Factor 2. They seem to indicate the overall wealth within the neighborhood, so we may want to call Factor 2 \u201cNeighborhood socioeconomic status.\u201d \n Notice that the variable house value also is marginally important in Factor 1 (loading = 0.38). This makes sense, since the value of a person\u2019s house should be associated with his or her income. \n About the Author: Maike Rahn is a health scientist with a strong background in data analysis.\u00a0\u00a0 She provides data analysis and research services to researchers in health fields as part of The Analysis Factor\u2019s partner network.\u00a0 Maike has a Ph.D. in Nutrition from Cornell University. \n \n \n Like this post? \nEnter your email address to have posts delivered: \n \n \n Related Posts Can Likert Scale Data ever be Continuous? \n Free Webinar today: Principal Component Analysis \n Confusing Statistical Term #6: Factor"], "link": "http://feedproxy.google.com/~r/statchat/~3/2vQRcZebsuQ/", "bloglinks": {}, "links": {"http://addthis.com/": 1, "http://www.theanalysisfactor.com/": 6}, "blogtitle": "The Analysis Factor"}, {"content": ["Like some of the other terms in our list\u2013 level and\u00a0 beta \u2013GLM has two different meanings. \n It\u2019s a little different than the others, though, because it\u2019s an abbreviation for two different terms: \n General Linear Model and Generalized Linear Model . \n It\u2019s extra confusing because their names are so similar on top of having the same abbreviation. \n And, oh yeah, Generalized Linear Models are an extension of General Linear Models . \n And neither should be confused with Generalized Linear Mixed Models , abbreviated GLMM. \n Naturally. \n So what\u2019s the difference?\u00a0 And does it really matter? \n General Linear Models \n You\u2019re probably familiar with General Linear Models, though possibly through the names linear regression, OLS regression, least-squares regression, ordinary regression, ANOVA, ANCOVA. \n In all of these models, there are two defining features: \n 1. The residuals (aka errors) are normally distributed. \n 2. The model parameters\u2013regression coefficients, means, residual variance\u2013are estimated using a technique called Ordinary Least Squares. \n This latter feature is important, because many of the nice statistics we get from these models\u2013R-squared, MSE, Eta-Squared\u2013come directly from OLS methods. \n And this is why you can run regressions and ANOVAs in the same General Linear Model software procedure . \n Generalized Linear Models \n But it turns out that not all dependent variables can result in residuals that are normally distributed. \n Count variables and categorical variables are both good examples.\u00a0 But it turns out that as long as the errors follow a distribution within a certain family of distributions, we can still fit a model. \n You\u2019re probably familiar with these through one of its common examples\u2013 logistic regression , Poisson regression , probit regression, negative binomial regression. \n In all of these models, there are a few more defining features: \n 1. The residuals come from a distribution in the exponential family.\u00a0 (And yes, you need to specify which one). \n 2. The mean of y has a linear form with model parameters only through a link function. \n 3. The model parameters are estimated using Maximum Likelihood Estimation.\u00a0 OLS doesn\u2019t work. \n Just like you can run a linear regression using either a linear regression or a General Linear Model procedure, you can run a logistic regression through either a logistic regression or a Generalized Linear Model procedure. \n The logistic procedure is just making some default assumptions about the model\u2013for example, that the link function is a logit.\u00a0 In the Generalized Linear Models procedure, you\u2019d have to specify that. \n \n \n \nIf you\u2019d like to learn more about some generalized linear models, download a recording of the webinar Poisson and Negative Binomial Regression for Count Data\u00a0 or Binary, Ordinal, and Multinomial Logistic Regression for Categorical Outcomes . They\u2019re both free. \n Related Posts Interpreting Interactions: When the F test and the Simple Effects disagree. \n Regression models without intercepts \n Checking Assumptions in ANOVA and Linear Regression Models: The Distribution of Dependent Variables \n A Comparison of Effect Size Statistics \n 7 Practical Guidelines for Accurate Statistical Model Building"], "link": "http://feedproxy.google.com/~r/statchat/~3/2_x0DMFLpIk/", "bloglinks": {}, "links": {"http://addthis.com/": 1, "http://www.theanalysisfactor.com/": 13}, "blogtitle": "The Analysis Factor"}]