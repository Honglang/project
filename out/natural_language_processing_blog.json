[{"blogurl": "http://nlpers.blogspot.com\n", "blogroll": [], "title": "natural language processing blog"}, {"content": ["I'll warn in advance that this is probably one of the more controversial posts I've written, but realize that my goal is really to raise questions, not necessarily give answers. It's just more fun to write strong rhetoric :). Let me write down a simple Markov Chain: Download some data from the web Call part of that data the input and part of it the label Train a classifier on bag of words and get 84% accuracy Submit a paper to *ACL Go to 1 Such papers exist in the vision community, too, where you replace \"bag of words\" with \"SIFT features\" and \"*ACL\" with \"CVPR/ICCV.\" In that community (according to my one informant :P), such papers are called \"data porn.\" Turns out this is actually a term from journalism, in which one definition is \" where journalists look for big, attention grabbing numbers or produce visualisations of data that add no value to the story .\" There's a related paper that looks at this issue in one specific setting: predicting political outcomes. On Arxiv back at the end of April, we got this wonderful, and wonderfully titled paper: \"I Wanted to Predict Elections with Twitter and all I got was this Lousy Paper\" -- A Balanced Survey on Election Prediction using Twitter Data by Daniel Gayo-Avello The thing I especially like about this paper is that it's not a complaint (like this blog post!) but rather a thoughtful critique of how one could do this sort of research right. This includes actually looking at what has been done before (political scientists have been studying this issue for a long time and perhaps we should see what they have to say; what can we do to make our results more reproducible; etc). For me, personally, this goes back to my main reviewing criteria: \"what did I learn from this paper?\" The problem is that in the extreme, cartoon version of a data porn paper (my 1-4 list above), the answer is that I learned that machine learning works pretty well, even when asked to do arbitrary tasks. Well, actually I already knew that. So I didn't really learn anything. Now, of course, many data porn-esque papers aren't actually that bad. There are many things one can do (and people often do do) that make these results interesting: Picking a real problem -- i.e., one that someone else might actually care about. There's a temptation (that I suffer from, too) of saying \"well, people are interested in X, and X' is kind of like X, so let's try to predict X' and call it a day.\" For example, in the context of looking at scientific articles, it's a joy in many communities to predict future citation counts because we think that might be indicative of something else. I've certainly been guilty of this. But where this work can get interesting is if you're able to say \"yes, I can collect data for X' and train a model there, but I'll actually evaluate it in terms of X, which is the thing that is actually interesting.\" Once you pick a real problem, there's an additional challenge: other people (perhaps social scientists, political scientists, humanities researchers, etc.) have probably looked at this in lots of different lights before. That's great! Teach me what they've learned! How, qualitatively, do your results compare to their hypotheses? If they agree, then great. If they disagree, then explain to me why this would happen: is there something your model can see that they cannot? What's going on? On the other hand, once you pick a real problem, there's a huge advantage: other people have looked at this and can help you design your model! Whether you're doing something straightforward like linear classification/regression (with feature engineering) or something more in vogue, like building some complex Bayesian model, you need information sources (preferably beyond bag of words!) and all this past work can give you insights here. Teach me how to think about the relationship between the input and the output, not just the fact that one exists. In some sense, these things are obvious. And of course I'm not saying that it's not okay to define new problems: that's part of what makes the world fun. But I think it's prudent to be careful. One attitude is \"eh, such papers will die a natural death after people realize what's going on, they won't garner citations, no harm done.\" I don't think this is all together wrong. Yes, maybe they push out better papers, but there's always going to be that effect, and it's very hard to evaluate \"better.\" The thing I'm more worried about is the impression that such work gives from our community to others. For instance, I'm sure we've all seen papers published in other venues that do NLP-ish things poorly (Joshua Goodman has his famous example in physics, but there's tons more). The thing I worry is that we're doing ourselves a disservice as a community to try to claim that we're doing something interesting in other people's spaces, without trying to understand and acknowledge what they're doing. NLP obviously has a lot of potential impact on the world, especially in the social and humanities space, but really anywhere that we want to deal with text. I'd like to see ourselves set up to succeed there, by working on real problems and making actual scientific contributions, in terms of new knowledge gathered and related to what was previously known."], "link": "http://nlpers.blogspot.com/feeds/1437557202305249633/comments/default", "bloglinks": {}, "links": {"http://drivenbydata.wordpress.com/": 1, "http://arxiv.org/": 2, "http://research.microsoft.com/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["I don't know how it happened or when it happened, but at some point NIPS workshops were posted and papers are due about a week from now and I completely missed it! The list of workshops is here:  http://nips.cc/Conferences/2012/Program/schedule.php?Session=Workshops Since my job as a blogger is to express my opinion about things you don't want to hear my opinion about, I wish they'd select fewer workshops. I've always felt that NIPS workshops are significantly better than *ACL workshops because they tend to be workshops and not mini-conferences (where \"mini\" is a euphemism for non-selective :P). At NIPS workshops people go, really talk about problems and it's really the best people and the best work in the area. And while, yes, it's nice to be supportive of lots of areas, but what ends up happening is that people jump between workshops because there are too many that interest them, and then you lose this community feeling. This is especially troubling when workshops are already competing with skiing :). Anyway, with that behind me, there are a number that NLP folks might find interesting: Algorithmic and Statistical Approaches for Large Social Networks : Social networks aren't that big of a topic in NLP, but I think it's potentially a space we'll move in to and overlap with, especially as more and more text becomes available on such networks. Discrete Optimization in Machine Learning: Structure and Scalability : One of the things that's perhaps relevant to our field but approached from a different perspective. Social network and social media analysis: Methods, models and applications : Another social network thing, perhaps focused more on media than the networks Spectral Learning Workshop : Spectral methods have made a bit of a splash in ACL land and there's definitely lots of interesting work to do here. Cross-Lingual Technologies : This is probably the closest to our hearts. Anything multilingual!!! Big Learning: New Perspectives, Implmentations and Challenges : Who doesn't like big learning. I'm still not convinced that the ML notion of \"big\" is as big as our notion of \"big\" but it's definitely getting there! Personalizing Education With Machine Learning : I don't think there's much in the way of NLP in personalizing education, but I see this as a potential big avenue for large breakthroughs in NLP in the coming 5 years, especially as we start talking about automated grading in MOOCs and whatnot. ETS folks, I hear you! With the deadlines so close I don't imagine anyone's going to be submitting stuff that they just started, but if you have things that already exist, NIPS is fun and it would be fun to see more NLPers there!"], "link": "http://nlpers.blogspot.com/feeds/4481844101429475529/comments/default", "bloglinks": {}, "links": {"http://discml.cc/": 1, "http://www.cmu.edu/": 1, "http://km.kit.edu/": 1, "http://biglearn.org/": 1, "http://www.uci.edu/": 1, "http://nips.cc/": 1, "http://www.colorado.edu/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["Like many people, I spent last week in lovely Montreal (at least lovely for the second half) at NAACL. Despite the somewhat unfortunate submission date, I thought the program this year was quite good. Of course I didn't see every talk and haven't read many of the papers yet, but I figured I'd point out what I saw that I liked and other people can comment likewise. Identifying High-Level Organizational Elements in Argumentative Discourse (Madnani, Heilman, Tetreault, Chodorow). This is maybe one of the first discourse papers I've seen where I actually believe that they have a chance of solving the problem that they've set out. Here, the problem is separating the meat (content of an essay) from the shell (the bits of discourse that hold the meat together). It's a cool problem and their solution seems to work well. Very nice paper. (And Nitin's talk was great.) Risk Training of Approximate CRF-Based NLP Systems  (Stoyanov, Eisner). This paper is basically about training approximate models based on some given loss function. Reminds me a lot of the Ross et al. CVPR 2011 paper on Message-Passing . It's a cool idea, and there's software available. Being me, the thing I wonder the most about is whether you can achieve something similar being completely greedy, and then whether you need to do all this work to get a good decision function. But that's me -- maybe other people like CRFs :). MSR SPLAT, a language analysis toolkit (Quirk, Choudhury, Gao, Suzuki, Toutanova, Gamon, Yih, Cherry, Vanderwende). This is a demo of a system where you send them a sentence and they tell you everything you want to know about it. Never run your own parser/NER/etc. again. And, having see it in action at MSR, it's fast and high quality. Parsing Time: Learning to Interpret Time Expressions (Angeli, Manning, Jurafsky). This was a great paper about semantic interpretation via compositional semantics (something sort of like lambda calculus) for time expressions. I cannot find myself getting super jazzed up about time, but it's a nice constrained problem and their solution is clean. I'm actually thinking of using something like this (or a subset thereof) as a course project for the intro CL course in the Fall, since I'm always starved for something to do with compositional semantics. Getting More from Morphology in Multilingual Dependency Parsing (Hosensee, Bender). If you have morphology, you can do better parsing by modeling things like agreement (really? people hadn't done this before??). Caveat: they use gold standard morphology. But cool idea still. Unsupervised Translation Sense Clustering (Bansal, DeNero, Lin). If you want to build a bilingual dictionary from parallel text, you need to cluster translations into senses. Here's a way to do it. Nice results improving using bilingual contexts, which was nice to see. I also feel like I should raise my glass to the organizers of NLP Idol and congrats to Ray for winning with Robert Wilensky's paper \"PAM.\" (If anyone can find an online version, please comment!) Though I would actually encourage everyone to read all three papers if you haven't already. They all changed how I was thinking about problems. Here are the others: Burstiness (Church), Context (Akman), Suppertagging (Bangalore, Joshi)."], "link": "http://nlpers.blogspot.com/feeds/5980174981959828134/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 1, "http://acl.upenn.edu/": 1, "http://braque.cc/": 6, "http://aclweb.org/": 1, "http://www.edu.tr/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["Wikipedia's category hierarchy forms a graph. It's definitely cyclic (Category:Ethology belongs to Category:Behavior, which in turn belongs to Category:Ethology). At any rate, did you know that \"Chicago Stags coaches\" are a subcategory of \"Natural sciences\"? If you don't believe me, go to the Wikipedia entry for the Natural sciences category , and expand the following list of subcategories: Biology Zoology Subfields of zoology Ethology Behavior Human behavior Recreation Games Ball games Basketball Basketball teams Defunct basketball teams Defunct National Basketball Association teams Chicago Stags Chicago Stags coaches I guess it kind of makes sense. There are some other fun ones, like \"Rhaeto-Romance languages\", \"American World War I flying aces\" and \"1911 films\". Of course, these are all quite deep in the \"hierarchy\" (all of those are at depth 15 or higher). So if you're trying to actually find pages about Natural sciences , maybe it's enough to limit the depth of your breadth first search down the graph. This is sort of reasonable, and things up to and including depth four are quite reasonable, including topics like \"Neurochemistry\", \"Planktology\" and \"Chemical elements\". There are a few outliers, like \"Earth observation satellites of Israel\" which you could certainly make a case might not be natural science. At depth five, things become much more mixed. On the one hand, you get categories you might like to include, like \"Statins\", \"Hematology\", \"Lagoons\" and \"Satellites\" (interesting that Satellites is actually deeper than the Isreal thing). But you also get a roughly equal amount of weird things, like \"Animals in popular culture\" and \"Human body positions\". It's still not 50/50, but it's getting murky. At depth six, based on my quick perusal, it's about 50/50. And although I haven't tried it, I suspect that if you use a starting point other than Natural sciences, the depth at which things get weird is going to be very different. So I guess the question is how do deal with this. One thought is to \"hope\" that editors of Wikipedia pages will list the categories of pages roughly in order of importance, so that you can assume that the first category listed for a page is \"the\" category for that page. This would render the structure to be a tree. For the above example, this would cut the list at \"Subfields of zoology\" because the first listed category for the Ethology category is \"Behavioral sciences\", not \"Subfields of zoology.\" Doing this seems to make life somewhat better; you cut out the stags coaches, but you still get the \"Chicago Stags draft picks\" (at depth 17). The path, if you care, is (Natural sciences -> Physical sciences -> Physics -> Fundamental physics concepts -> Matter -> Structure -> Difference -> Competition -> Competitions -> Sports competitions -> Sports leagues -> Sports leagues by country -> Sports leagues in the United States -> Basketball leagues in the United States -> National Basketball Association -> National Basketball Association draft picks) . Still doesn't feel like Natural sciences to me. In fairness, at depth 6, life is much better. You still get \"Heating, ventilating, and air conditioning\" but many of the weird entries have gone away. Another idea is the following. Despite not being a tree or DAG, there is a root to the Wikipedia hierarchy (called Category:Contents). For each page/category you can compute it's minimum depth from that Contents page. Now, when you consider subpages of Natural sciences, you can limit yourself to pages whose shortest path goes through Natural sciences. Basically trying to encode the idea that if the shallowest way to reach Biology is through Natural sciences, it's probably a natural science. This also fails. For instance, the depth of \"Natural sciences\" (=5) is the same as the depth of \"Natural sciences good articles\", so if you start from Natural sciences, you'll actually exclude all the good articles! Moreover, even if you insist that a shortest path go through Natural sciences, you'll notice that many editors have depth 5, so any page they've edited will be allowed. Maybe this is a fluke, but \"Biology lists\" has depth of only 4, which means that anything that can be reached through \"Biology lists\" would be excluded, something we certainly wouldn't want to do. There's also the issue that the hierarchy might be much bushier for some high-level topics than others, which makes comparing depths very difficult. So, that leaves me not really knowing what to do. Yes, I could compute unigram distributions over the pages in topics and cut when those distributions get too dissimilar, but (a) that's annoying and very computationally expensive, (b) requires you to look at the text of the pages which seems silly, (c) you now just have more hyperparameters to tune. You could annotate it by hand (\"is this a natural science\") but that doesn't scale. You could compute the graph Laplacian and look at flow and use \"average path length\" rather than shortest paths, but this is a pretty big graph that we're talking about. Has anyone else tried and succeed at using the Wikipedia category structure?"], "link": "http://nlpers.blogspot.com/feeds/5459803772463123951/comments/default", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["I received the following (slightly edited) question from my colleague Jon Katz a few days ago: I was thinking about the problem of authorship attribution... Have people thought about the flip side of this problem? Namely, \"anonymizing\" text so that it would be hard to attribute it to any author? This is something I've actually wondered about in the context of blogging for a while. I noticed at some point that my \"blogger voice\" is very similar to my \"reviewer voice\" and started worrying that I might be too identifiable as a reviewer. This might either be due to lexical choice (\"bajillion\" or \"awesome\") or due to some more subtle stylistic choices. There is quite a bit of work on authorship attribution. I think the first time I heard a talk on this topic was on March 24, 2004, when Shlomo Argamon gave a talk at ISI (no, I don't have an amazing memory, I cheated ) on \"On Writing, Our Selves: Explorations in Stylistic Text Categorization.\" The basic hypothesis of the talk, at least as I remember it, was that if you're trying to do authorship attribution, you should throw out content words and focus on things like POS tag sequences, parse tree structures, and things like that. There's been a lot of subsequent work in this, and related areas. One very related area is on things like trying to predict demographic information (age, gender, socio-economic status, education level, and, yes, astrological sign) from tweets, blog posts or emails (or other forms). One of the key distinctions that I think is important in all of this work is whether the original author is intentionally trying to hide information about him or herself. For instance, someone trying to impersonate Shakespeare, or a child predator pretending to be a different age or gender, or a job applicant trying to sound more educate than is true. This latter is a much harder problem because the stupid topically stereotypical features that pop out as being indicative (like men talking about \"wifes\" and \"football\" and women talking about \"husbands\" and \"yoga\") and the silly features that don't really tell us anything interesting (on twitter, apparently men tend to put \"http://\" before URLs more than women -- who knew?) because these \"pretenders\" are going to intentionally try to hide that information (now that everyone knows to hide \"http://\" to trick gender recognizers!). It also means that falling back on topic as a surrogate for demography should not work as well. This seems to be a very different problem from trying to identify whether a blog post is written by me or by Jon, which should be 99.9% do-able by just looking at content words. The reason I bring this all up is because we don't want to anonymize by changing the topic. The topic needs to stay the same: we just need to cut out additional identifying information. So, getting back to Jon's question, the most relevant work that I know of is on text steganography (by Ching-Yun Chang and Stephen Clark), where they use the ability to do paraphrasing to encode messages in text. Aside from the challenge of making the output actually somewhat grammatical, the basic idea is that when you have two ways of saying the same thing (via paraphases), you can choose the first one to encode a \"0\" and the second to encode a \"1\" and then use this to encode a message in seemingly-natural text. I also remember having a conversation a while ago while a (different) colleague about trying to build a chat system where you could pretend that you're chatting with someone famous (like Obama or Harry Potter or Scooby Doo). A similar problem is trying to paraphrase my own writing to sound like someone else, but zoinks, that seems hard! A basic approach would be to build a Scooby Doo language model (SDLM) and then run my blog posts through a paraphrase engine that uses the SDLM for producing the output. My vague sense is that this would work pretty poorly, primarily because the subtleness in phrase structure selection would be lost on a highly-lexicalized language model. I imagine you'd get some funny stuff out and it might be amusing to do, but I don't have time to try. As far as pure anonymization goes, it seems like doing something similar to the steganography approach would work. Here, what you could do is generate a random sequence of bits, and then \"encode\" that random sequence using the steganography system. This would at least remove some identifying information. But the goal of the steganography isn't to change every phrase, but just to change enough phrases that you can encode your message. It also wouldn't solve the problem that perhaps you can identifying a bit about an author by the lengths of their sentences. Or their oscillation between long and short sentences. This also wouldn't be hidden. An alternative, human-in-the-loop approach might be simply to have an authorship recognition system running in your word processor, and then any time you type something that enables it to identify you, it could highlight it and you could be tasked with changing it. I suspect this would be a frustrating, but fairly interesting experience (at least the first time). p.s., I'm now officially tweeting on @haldaume3 ."], "link": "http://nlpers.blogspot.com/feeds/870593167537446247/comments/default", "bloglinks": {}, "links": {"http://nlg.isi.edu/": 1, "http://twitter.com/": 1, "http://aclweb.org/": 1, "http://www.iit.edu/": 1, "https://jonkatz.wordpress.com/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["By which I mean NIPS, and the incumbent exhaustion of 14 hour days. (P.s., if you're at NIPS, see the meta-comment I have at the end of this post, even if you skip the main body :P.) Today I went to two tutorials: one by Yee Whye Teh and Peter Orbanz (who is starting shortly at Columbia) on non-parametric Bayesian stuff, and one by Naftali Tishby on information theory in learning and control. They were streamed online; I'm sure the videos will show up at some point on some web page, but I can't find them right now (incidentally, and shamelessly, I think NAACL should have video tutorials , too -- actually my dear friend Chris wants that too, and since Kevin Knight has already promised that ACL approves all my blog posts, I suppose I can only additionally promise that I will do everything I can to keep at least a handful of MT papers appearing in each NAACL despite the fact that no one really works on it anymore :P). Then there were spotlights followed by posters, passed (as well as sat down) hors d'oeuvres, free wine/sangria/beer/etc, and friends and colleagues. The first half of the Teh/Orbanz tutorial is roughly what I would categorize as \"NP Bayes 101\" -- stuff that everyone should know, with the addition of some pointers to results about consistency, rates of convergence of the posterior, etc. The second half included a lot of stuff that's recently become interesting, in particular topics like completely random measures, coagulation/fragmentation processes, and the connection between gamma processes (an example of a completely random measure) and Dirichlet processes (which we all know and love/hate). One of the more interesting things toward the end was what I was roughly characterized as variants of the DeFinetti theorem on exchangable objects. What follows is from memory, so please forgive errors: you can look it up in the tutorial. DeFinetti's theorem states that if p(X1, X2, ..., Xn, ...) is exchangeable, then p has a representation as a mixture model, with (perhaps) infinite dimensional mixing coefficients. This is a fairly well-known result, and was apparently part of the initial reason Bayesians started looking into non-parametrics. The generalizations (due to people like Kingman, Pitman, Aldous, etc...) are basically what happens for other types of data (i.e., other than just exchangeable). For instance, if a sequence of data is block-exchangeable (think of a time-series, which is obviously not exchangeable, but for which you could conceivably cut it into a bunch of contiguous pieces and these pieces would be exchangeable) then it has a representation as a mixture of Markov chains. For graph-structured data, if the nodes are exchangeable (i.e., all that matters is the pattern of edges, not precisely which nodes they happen to connect), then this also has a mixture parameterization, though I've forgotten the details. The Tishby tutorial started off with some very interesting connections between information theory, statistics, and machine learning, essentially from the point of view of hypothesis testing. The first half of the tutorial centered around information bottleneck, which is a very beautiful idea. You should all go read about it if you don't know it already. What actually really struck me was a comment that Tishby made somewhat off-hand, and I'm wondering if anyone can help me out with a reference. The statement has to do with the question \"why KL?\" His answer had two parts. For the first part, consider mutual information (which is closely related to KL). MI has the property that if \"X -> Y -> Z\" is a Markov chain, then the amount of information that Y gives you about Z is at most the amount of information that X gives you about Z. In other words, if you think if Y as a \"processed\" version of X, then this processing cannot give you more information. This property is more general than just MI, and I believe anything that obeys it is a Csiszar divergence. The second part is the part that I'm not so sure of. It originated with the observation that if you have a product, take a log, you now get an additive term. This is really nice because you can apply results like the central limit theorem to this additive term. (Many of the results in the first half of his tutorial hinged on this additivity.) The claim was something like: the only divergences that have this additivity are Bregman divergences. (This is not obvious to me, and actually not entirely obvious what the right definition of additivity is, so if someone wants to help out, please do so!) But the connection is that MI and KL are the \"intersection\" of Bregman divergences and Csiszar divergences. In other words, if you want the decreasing information property and you want the additivity property, then you MUST use information theoretic measures. I confess that roughly the middle third of the talk went above my head, but I did learn about an interesting connection between Gaussian information bottleneck and CCA: basically they're the same, up to a trimming of the eigenvalues. This is in a 2005 JMLR paper by Amir Globerson and others. In the context of this, actually, Tishby made a very offhand comment that I couldn't quite parse as whether it was a theorem or a hope. Basically the context was that when working with Gaussian distributed random variables, you can do information bottleneck \"easily,\" but that it's hard for other distributions. So what do we do? We do a kernel mapping into a high dimension space (they use an RBF kernel) where the data will look \"more Gaussian.\" As I said, I couldn't quite parse whether this is \"where the data will provably look more Gaussian\" or \"where we hope that maybe by dumb luck the data will look more Gaussian\" or something in between. If anyone knows the answer, again, I'd love to know. And if you're here at NIPS and can answer either of these two questions to my satisfaction, I'll buy you a glass of wine (or beer, but why would you want beer? :P). Anyway, that's my report for day one of NIPS! p.s. I made the silly decision of taking a flight from Granada to Madrid at 7a on Monday 19 Dec. This is way too early to take a bus, and I really don't want to take a bus Sunday night. Therefore, I will probably take a cab. I think it will be about 90 euros. If you also were silly and booked early morning travel on Monday and would like to share said cab, please email me (me AT hal3 DOT name)."], "link": "http://nlpers.blogspot.com/feeds/6489777617753556390/comments/default", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www.naacl.org/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["If you're an NLP or ML person and graduating in the next six months or so and are looking for a postdoc position with very very open goals, read on. The position would be at UMD College Park, in the greater DC area, with lots of awesome people around (as well as JHU and other universities a short drive/train away). This position could start as early as January 2012, probably more likely around June 2012 and could be as late as September 2012 for the right person. Even if you're not graduating until the one-year-from-now time frame, please contact me now! I'm looking more for a brilliant, hard-working, creative person than anyone with any particular skills. That said, you probably know what sort of problems I tend to work on, so it would be good if you're at least interested in things roughly in that space (regardless of whether you've worked on them before or not). The position would be for one year, with an extension to two if things are working out well for both of us (not subject to funding). If you're interested, please email me at postdoc@hal3.name with the following information: Inline in the email: Your PhD institution and advisor, thesis title, and expected graduation date. Links to the two or three most awesome papers you have, together with titles and venue. Link to your homepage. A list of three references (names, positions and email addresses).  Attached to the email: A copy of your CV, in PDF format. A brief (one page) research statement that focuses mostly on what problem(s) you'd most like to work on in a postdoc position with me. Also in PDF format. I need this information by November 1st so please reply quickly!!!"], "link": "http://nlpers.blogspot.com/feeds/860379158736563644/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "natural language processing blog"}, {"content": ["As Daniel Hsu and John Langford pointed out recently , there has been a lot of recent progress in active learning. This is to the point where I might actually be tempted to suggest some of these algorithms to people to use in practice, for instance the one John has that learns faster than supervised learning because it's very careful about what work it performs. That is, in particular, I might suggest that people try it out instead of the usual query-by-uncertainty (QBU) or query-by-committee (QBC). This post is a brief overview of what I understand of the state of the art in active learning (paragraphs 2 and 3) and then a discussion of why I think (a) researchers don't tend to make much use of active learning and (b) why the problem is far from solved. (a will lead to b.) For those who know what QBU and QBC are, skip this paragraph. The idea with QBU is exactly what you think: when choosing the next point to as for the label of, choose the one on which your current model is maximally uncertain. If you're using a probabilistic model, this means something like \"probability is closest to 0.5,\" or, in the non-binary case, something like \"highest entropy of p(y|x).\" If you're using something like an SVM, perhaps margin (aka distance to the hyperplane) is a reasonable measure of uncertainty. In QBC, the idea is still to query on uncertain points, but the uncertainty is computed by the amount of agreement among a committee of classifiers, for instance, classifiers trained in a boostrap manner on whatever data you have previously labeled. One of the issues with QBU and QBC and really a lot of the classic methods for active learning is that you end up with a biased set of training data. This makes it really hard to prove anything about how well your algorithm is going to do on future test examples, because you've intentially selected examples that are not random (and probably not representative). One of the \"obvious in retrospect\" ideas that's broken this barrier is to always train your classifier on all examples: the label for those that you've queried on is given by the human, and the label for those that you haven't queried on is given by your model from the previous iteration. Thus, you are always training on an iid sample from the distribution you care about (at least from a p(x) perspective). This observation, plus a lot of other work, leads to some of the breakthroughs that John mentions. An easy empirical observation is that not many people (in my sphere) actually use active learning. In fact, the only case that I know of was back in 2004 where IBM annotated extra coreference data for the Automatic Content Extraction (ACE) challenge using active learning. Of course people use it to write papers about active learning, but that hardly counts. (Note that the way that previously learned taggers, for instance the Brill Tagger, were used in the construction of the Penn Treebank does not fall under the auspices of active learning, at least as I'm thinking about it here.) It is worth thinking about why this is. I think that the main issue is that you end up with a biased training set. If you use QBC or QBU, this is very obvious. If you use one of the new approaches that self-label the rest of the data to ensure that you don't have a biased training set, then of course p(x) is unbiased, but p(y|x) is very biased by whatever classifier you are using. I think the disconnect is the following. The predominant view of active learning is that the goal is a classifier . That data that is labeled is a byproduct that will be thrown away, once the classifier exists. The problem is that this view flies in the face of the whole point of active learning: that labeling is expensive. If labeling is so expensive, we should be able to reuse this data so that the cost is amortized. That is, yes, of course we care about a classifier. But just as much, we care about having a data set (or \"corpus\" in the case of NLP). Consider, for instance, the Penn Treebank. The sorts of techniques that are good at parsing now were just flat-out not available (and perhaps not even conceivable) back in the late 1990s when the Treebank was being annotated. If we had done active learning for the Treebank under a non-lexicalized, non-parent-annoted PCFG that gets 83% accuracy, maybe worse because we didn't know how to smooth well, then how well would this data set work for modern day state splitting grammars with all sorts of crazy Markovization and whatnot going on? The answer is: I have no idea. I have never seen an experiment that looks at this issue. And it would be so easy! Run your standard active learning algorithm with one type of classifier. Plot your usual active-versus-passive learning curves. Now, using the same sequence of data , train another classifier. Plot that learning curve. Does it still beat passive selection? By how much? And then, of course, can we say anything formal about how well this will work? There are tons of ways that this problem can arise. For instance, when I don't have much data I might use a generative model and then when I have lots of data I might use a discriminative model. Or, as I get more data, I add more features. Or someone finds awesome features 5 years later for my problem. Or new machine learning techniques are developed. Or anything. I don't want my data to become obselete when this happens. I am happy to acknowledge that this is a very hard problem. In fact, I suspect that there's some sort of no-free-lunch theorem lurking in here. Essentially, if the inductive biases of the classifier that you use to the active learning and the classifier you train at the end are too different, then you could do (arbitrarily?) badly. But in the real world, our hypothesis classes aren't all that different, or perhaps you can assume you're using a universal function approximator or a universal kernel or something. Assume what you want to start, but I think it's an interesting question. And then, while we're on the topic of active learning, I'd also like to see whether an active learning algorithm's performance asymptotes before all your training data is exhausted. That is, the usual model in active learning experiments is that you have 1000 training examples because that's what someone labeled. You then do active learning up to 1000 examples, and of course at that point, everything has been labeled, so active learning performance coincides precisely with passive learning performance. But this is a poor reflection of many problems in the world, where new inputs are almost always free. I want the Banko and Brill paper for active learning... perhaps it's out there, and if you've seen it, I'd love a pointer. I ran a couple experiments along these lines (nothing concrete), but it actually seemed that active learning from a smaller pool was better, perhaps because you have fewer outliers (I was using QBU). But these results are by no means concrete, so don't cite me as saying this :) . At any rate, I agree that active learning has come a long way. I would humbly suggest that the goal of simply building a classifier is not in the real spirit of trying to save money. If you wanted to save money, you would save your data and share it (modulo lawyers). In the long run, passive learning currently seems much less expensive than active learning to me."], "link": "http://nlpers.blogspot.com/feeds/5091600234929283373/comments/default", "bloglinks": {}, "links": {"http://dl.acm.org/": 1, "http://hunch.net/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["Originally in the context of Braque and now in the context of FUSE , I've thought a bit about understanding the role of techniques and tasks in scientific papers (admittedly, mostly NLP and ML, which I realize are odd and biased). I worked with Sandeep Pokkunuri , a MS student at Utah, looking at the following problem: given a paper (title, abstract, fulltext), determine what task is being solved and what technique is being used to solve it. For instance, a paper like \"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data\" the task would be \"segmenting and labeling sequence data\" and the technique would be \"conditional random fields.\" You can actually go a long way just looking for simple patterns in paper titles, like \"TECH for TASK\" or \"TASK by TECH\" and a few things like that (after doing some NP chunking and clean-up). From there you can get a good list of seed tasks and techniques, and could conceivably bootstrap your way from there. We never got a solid result out of these, and sadly I moved and Sandeep graduated and it never went anywhere. What we wanted to do was automatically generate tables of \"for this TASK, here are all the TECHs that have been applied (and maybe here are some results) oh and by the way maybe applying these other TECHs would make sense.\" Or visa-verse: this TECH has been applied to blah blah blah tasks. You might even be able to tell what TECHs are better for what types of tasks, but that's quite a bit more challenging. At any rate, a sort of \"obvious in retrospect\" thing that we noticed was that what I might consider a technique, you might consider a task. And you can construct a chain, typically all the way back to math . For instance, I might consider movie recommendations a task. To solve recommendations, I apply the technique of sparse matrix factorization. But then to you, sparse matrix factorization is a task and to solve it, you apply the technique of compressive sensing. But to Scott Tanner, compressive sensing is a task, and he applies the technique of smoothed analysis (okay this is now false, but you get the idea). But to Daniel Spielman, smoothed analysis is the task, and he applies the technique of some other sort of crazy math. And then eventually you get to set theory (or some might claim you get to category theory, but they're weirdos :P). (Note: I suspect the same thing happens in other fields, like bio, chem, physics, etc., but I cannot offer such an example because I don't know those areas. Although not so obvious, I do think it holds in math: I use the proof technique of Shelah35 to prove blah -- there, both theorems and proof techniques are objects.) At first, this was an annoying observation. It meant that our ontology of the world into tasks and techniques was broken. But it did imply something of a richer structure than this simple ontology. For instance, one might posit as a theory of science and technologies studies (STS, a subfield of social science concerned with related things) that the most basic thing that matters is that you have objects (things of study) and an appliedTo relationship. So recommender systems, matrix factorization, compressive sensing, smoothed analysis, set theory, etc., are all objects, and they are linked by appliedTo s. You can then start thinking about what sort of properties appliedTo might have. It's certainly not a function (many things can be applied to any X, and any Y can be applied to many things). I'm pretty sure it should be antireflexive (you cannot apply X to solve X). It should probably also be antisymmetric (if X is applied to Y, probably Y cannot be applied to X). Transitivity is not so obvious, but I think you could argue that it might hold: if I apply gradient descent to an optimization problem, and my particular implementation of gradient descent uses line search, then I kind of am applying line search to my problem, though perhaps not directly. (I'd certainly be interested to hear of counter-examples if any come to mind!) If this is true, then what we're really talking about is something like a directed acyclic graph, which at least at a first cut seems like a reasonable model for this world. Probably you can find exceptions to almost everything I've said, but that's why you need statistical models or other things that can deal with \"noise\" (aka model misspecification). Actually something more like a directed acyclic hypergraph might make sense, since often you simultaneously apply several techniques in tandem to solve a problem. For instance, I apply subgradient descent and L1 regularization to my binary classification problem -- the fact that these two are being applied together rather than separately seems important somehow. Not that we've gone anywhere with modeling the world like this, but I definitely thing there are some interesting questions buried in this problem."], "link": "http://nlpers.blogspot.com/feeds/5169776816311940991/comments/default", "bloglinks": {}, "links": {"http://xkcd.com/": 1, "http://braque.cc/": 1, "http://www.utah.edu/": 1, "http://www.iarpa.gov/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["As you've noticed, I haven't posted in a while. I've also not been reading blogs. My unread number of posts is now 462. Clearly I'm not going to go back and read all 462 posts that I missed. I will claim that this was an experiment to see what a (nearly) blog-free world is like. I actually found that I missed both the reading and the writing, so now (especially that I've switch over to public transportation and so have about an hour to kill in transportation time) I'm going to go back to reading while being transported and blogging when I have time. I figured I'd return to blogging by saying a bit about a recent experience. Less than a month ago I had the honor of serving on Jurgen Van Gael's Ph.D. examination committee. Jurgen did an excellent job and, as perhaps expected, passed. But what I want to talk about is how the UK model (or at least the Cambridge model) is different from the US model. In the UK, the examination is done by two faculty members, one internal (this was Stephen Clark) and one external (that was me). It does not involve the advisor/supervisor, though this person can sit in the room without speaking :). There is no public presentation and the process we followed was basically to go through the dissertation chapter-by-chapter, ask clarification questions, perhaps some things to get Jurgen to think on his toes, and so on. This took about two hours. Contrast this to the (prototypical) US model, where a committee consists of 5 people, perhaps one external (either external to CS or to the university, depending on how your institution sets it up), and includes the advisor. The defense is typically a 45 minute public presentation followed by questions from the committee in a closed-room environment with the student. Having been involved, now, in both types, I have to say they each have their pros and cons. I think the lack of a public presentation in the UK model is a bit of a shame, though of course students could decide to do these anyway. But it's nice to have something official for parents or spouses to come to if they'd like. However, in the US, the public presentation, plus the larger committee, probably leads to situation that students often joke about that not even their committee reads their dissertation. You can always fall back on the presentation, much like students skip class reading when they know that the lecture will cover it all. When it was just me, Stephen and Jurgen, there's really no hiding in the background :). I also like how in the UK model, you can skip over the easy stuff and really spend time talking with the student about the deep material. I found myself much more impressed with how well Jurgen knows his stuff after the examination than before, and this is not a feeling I usually get with US students because their defense it typically quite high-level. And after 45 minutes of a presentation, plus 15 minutes of audience questions, the last thing anyone wants to do is sit around for another two hours examining the details of the defense chapter-by-chapter. Regarding the issue of having the advisor there or not, I don't have a strong preference. The one thing I will say is that by having the advisor missing removes the potential for weird politics. For instance, I have seen one or two defenses in which an advisor tends to answer questions for the student, without the student first attempting an answer. If I were on these committees, with a relatively senior advisor, it might be politically awkward to ask them not to do this. Luckily this issue hasn't come up for me, but I could imagine it happening. Obviously I don't really expect anyone's policies to change, and I'm not even sure that they should, but I like thinking about things that I've grown used to taking for granted. Plus, after having gone through the UK model, I think I will grill students a bit more during the Q/A time. And if this means that fewer students ask me to be on their committees, then there's more time to blog :)."], "link": "http://nlpers.blogspot.com/feeds/3111213233602280741/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "natural language processing blog"}, {"content": ["(Shameless plug/advertisement follows.) Want to be informed of new interesting papers that show up online? Tired of trolling conference proceedings to find that one gem?  Want to make sure interested parties hear about your newest results?  Want to know when a new paper comes out that cites you?  Braque ( http://braque.cc ) can help.   Braque is a news service for research papers (currently focusing primarily on NLP and ML, though it needn't be that way). You can create  channels  that provide email or RSS feeds for topics you care about. You can add your own publications page as a resource to Braque so it knows to crawl your papers and send them out to interested parties. Braque is something I built ages ago with Percy Liang , but it's finally more or less set up after my move. Feel free to email me questions and comments or (preferably) use the online comment system. As a bit of warning: Braque is neither a paper search engine nor a paper archive. And please be a bit forgiving if you go there immediately after this post shows up and it's a bit slow.... we only have one server :). ps., yes, Braque is sort of like WhatToSee on crack."], "link": "http://nlpers.blogspot.com/feeds/1346998247683034686/comments/default", "bloglinks": {}, "links": {"http://braque.cc/": 2, "http://www.berkeley.edu/": 1, "http://www.utah.edu/": 1, "http://www.artchive.com/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["I'm using ACL/ICML as an excuse to jumpstart my resumed, hopefully regular, posting. The usual \"I didn't see/read everything\" applies to all of this. My general feeling about ACL (which was echoed by several other participants) was that the program was quite strong, but there weren't many papers that really stood out as especially great. Here are some papers I liked and some attached thoughts, from ACL: P11-1002 [ bib ]: Sujith Ravi; Kevin Knight Deciphering Foreign Language This paper is about building MT systems without parallel data. There's been a bunch of work in this area. The idea here is that if I have English text, I can build an English LM. If you give me some French text and I hallucinate a F2E MT system, then it's output had better score high on the English LM. P11-1020 [ bib ] [ dataset ]: David Chen; William Dolan Collecting Highly Parallel Data for Paraphrase Evaluation Although this paper is about paraphrasing, the fun part is the YouTube stuff they did. Read it and see :). P11-1060 [ bib ]: Percy Liang; Michael Jordan; Dan Klein Learning Dependency-Based Compositional Semantics This paper is along the lines of semantic parsing stuff that various people (Ray Mooney, Luke Zettlemoyer/Mike Collins, etc.) have been doing. It's a nice compositional model that is learned online. P11-1099 [ bib ]: Vanessa Wei Feng; Graeme Hirst Classifying arguments by scheme This paper is about argumentation (in the \"debate\" sense) and identifying different argumentation types. There are some nice correlations with discourse theory, but in a different context. P11-2037 [ bib ]: Shu Cai; David Chiang; Yoav Goldberg Language-Independent Parsing with Empty Elements I'm really glad to see that people are starting to take this problem seriously again. This falls under the category of \"if you've ever actually tried to use a parser to do something then you need this.\" Okay so that's not that many papers, but I did \"accidentally\" skip some sections. So you're on your own for the rest. For ICML, I actually felt it was more of a mixed bag. Here are some things that stood out as cool: Minimum Probability Flow Learning  Jascha Sohl-Dickstein; Peter Battaglino; Michael DeWeese This is one that I need to actually go read, because it seems too good to be true. If computing a partition function ever made you squirm, read this paper. Tree-Structured Infinite Sparse Factor Model  XianXing Zhang; David Dunson; Lawrence Carin This is trying to do factor analysis with tree factors; they use a \"multiplicative gamma process\" to accomplish it. This is something we tried to do a while ago, but could never really figure out how to do it. Sparse Additive Generative Models of Text  Jacob Eisenstein; Amr Ahmed; Eric Xing The idea here is that if you're learning a model of text, don't re-learn the same \"general background\" distribution over and over again. Then learn class- or topic-specific stuff as a sparse amendment to that background. OptiML: An Implicitly Parallel Domain-Specific Language for Machine Learning  Arvind Sujeeth; HyoukJoong Lee; Kevin Brown; Tiark Rompf; Hassan Chafi; Michael Wu; Anand Atreya; Martin Odersky; Kunle Olukotun Two words: MATLAB KILLER. Six more words: Most authors ever on ICML paper.  Generalized Boosting Algorithms for Convex Optimization  Alexander Grubb; Drew Bagnell Suppose you want to boost something that's non-smooth? Now you can do it. Has nice applications in imitation learning, which is I suppose why I like it. Learning from Multiple Outlooks  Maayan Harel; Shie Mannor This is a nice approach based on distribution mapping to the problem of multiview learning when you don't have data with parallel views. (I'm not sure that we need a new name for this task, but I still like the paper.) Parsing Natural Scenes and Natural Language with Recursive Neural Networks Richard Socher; Cliff Chiung-Yu Lin; Andrew Ng; Chris Manning This is basically about learning compositional semantics for vector space models of text, something that I think is really interesting and understudied (Mirella Lapata has done some stuff). The basic idea is that if \"red\" is embedded at position x, and \"sparrow\" is embedded at y, then the embedding of the phrase \"red sparrow\" should be at f([x y]) where f is some neural network. Trained to get good representations for parsing.  Please reply in comments if you had other papers you liked!!!"], "link": "http://nlpers.blogspot.com/feeds/9151843719420526696/comments/default", "bloglinks": {}, "links": {"http://www.icml-2011.org/": 7, "http://aclweb.org/": 11}, "blogtitle": "natural language processing blog"}, {"content": ["Are you graduating and interested in doing a fun postdoc in your area of choosing on your project of choosing? Apply to be a NSF CI Fellow !"], "link": "http://nlpers.blogspot.com/feeds/3943224356416646889/comments/default", "bloglinks": {}, "links": {"http://cifellows.org/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["Someone (can't remember who anymore, even though it was just a couple days ago!) pointed out to me that Google scholar seems to be doing weird things with citations. In particular, it seems to think that the citation relation is symmetric (at least in some cases). Here's an easy example. Look up Khalid El-Arini's 2009 paper \" Turning down the noise in the blogosphere \" paper on Google scholar (or just follow that link). Apparently it's been cited by 24 papers. Let's look at who cites them . Apparently in 2003, in addition to inventing LDA, also invented a time machine so that he could cite Khalid's paper! The weird thing is that this doesn't seem to be a systematic error! It only happens some times. Oh well, I won't complain -- it just makes my H index look better :)"], "link": "http://nlpers.blogspot.com/feeds/4693793438588242974/comments/default", "bloglinks": {}, "links": {"http://scholar.google.com/": 2}, "blogtitle": "natural language processing blog"}, {"content": ["My past master's student Adam Teichert (now at JHU) did some work on inducing part of speech taggers using typological information. We wanted to compare the usefulness of using small amounts of linguistic information with small amounts of lexical information in the form of seeds. (Other papers give seeds different names, like initial dictionaries or prototypes or whatever... it's all the same basic idea.) The basic result was that if you don't use seeds, then typological information can help a lot. If you do you seeds, then your baseline performance jumps from like 5% to about 40% and then using typological information on top of this isn't really that beneficial. This was a bit frustrating, and led us to think more about the problem. The way we got seeds was to look at the wikipedia page about Portuguese (for instance) and use their example list of words for each tag. An alternative popular way is to use labeled data and extract the few most frequent words for each part of speech type. They're not identical, but there is definitely quite a bit of overlap between the words that Wikipedia lists as examples of determiners and the most frequent determiners (this correlation is especially strong for closed-class words). In terms of end performance, there are two reasons seeds can help. The first, which is the interesting case, is that knowing that \"the\" is a determiner helps you find other determiners (like \"a\") and perhaps also nouns (for instance, knowing the determiners often precede nouns in Portuguese). The second, which is the uninteresting case, is that now every time you see one of your seeds, you pretty much always get it right. In other words, just by specifying seeds, especially by frequency (or approximately by frequency ala Wikipedia), you're basically ensuring that you get 90% accuracy (due to ambiguity) on some large fraction of the corpus (again, especially for closed-class words which have short tails). This phenomena is mentioned in the text (but not the tables :P), for instance, in Haghighi & Klein's 2006 NAACL paper on prototype-driven POS tagging, wherein they say: \"Adding prototypes ... gave an accuracy of 68.8% on all tokens, but only 47.7% on non-prototype occurrences, which is only a marginal improvement over [a baseline system with no prototypes.\" Their improved system remedies this and achieves better accuracy on non-prototypes as well as prototypes (aka seeds). This is very similar to the idea of transductive learning in machine learning land. Transduction is an alternative to semi-supervised learning. The setting is that you get a bunch of data, some of which is labeled and some of which is unlabeled. Your goal is to simply label the unlabeled data. You need not \"induce\" the labeling function (though many approach do, in passing). The interesting thing is that learning with seeds is very similar to transductive learning, though perhaps with a bit stronger assumption of noise on the \"labeled\" part. The irony is that in machine learning land, you would never report \"combined training and test accuracy\" -- this would be ridiculous. Yet this is what we seem to like to do in NLP land. This is itself related to an old idea in machine learning wherein you rate yourself only on test example that you didn't see at training time. This is your out-of-sample error, and is obviously much harder than your standard generalization error. (The famous no-free-lunch theorems are from an out-of-sample analysis.) The funny thing out of sample error is that sometimes you prefer not to get more training examples, because you then know you won't be tested on it! If you were getting it right already, this just hurts you. (Perhaps you should be allowed to see x and say \"no I don't want to see y \"?) I think the key question is: what are we trying to do. If we're trying to build good taggers (i.e., we're engineers) then overall accuracy is what we care about and including \"seed\" performance in our evaluations make sense. But when we're talking about 45% tagging accuracy (like Adam and I were), then this is a pretty pathetic claim. In the case that we're trying to understand learning algorithms and study their performance on real data (i.e., we're scientists) then accuracy on non-seeds is perhaps more interesting. (Please don't jump on me for the engineer/scientist distinction: it's obviously much more subtle than this.) This also reminds me of something Eric Brill said to me when I was working with him as a summer intern in MLAS at Microsoft (back when MLAS existed and back when Eric was in MLAS....). We were working on web search stuff. His comment was that he really didn't care about doing well on the 1000 most frequent queries. Microsoft could always hire a couple annotators to manually do a good job on these queries. And in fact, this is what is often done. What we care about is the heavy tail, where there are too many somewhat common things to have humans annotate them all. This is precisely the same situation here. I can easily get 1000 seeds for a new language. Do I actually care how well I do on those, or do I care how well I do on the other 20000+ things?"], "link": "http://nlpers.blogspot.com/feeds/50543944029625227/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "natural language processing blog"}, {"content": ["Okay, now is why I take serious unfair advantage of having this blog. We have a postdoc opening. See the official ad below for details: A postdoc position is available in the Computational Linguistics and Information Processing (CLIP) Laboratory in the Institute for Advanced Computer Studies at University of Maryland. We are seeking a talented researcher in natural language processing, with strong interests in the processing of scientific literature. A successful candidate should have a strong NLP background with a track record of top-tier research publications. A Ph.D. in computer science and strong organizational and coordination skills are a must. In addition to pursuing original research in scientific literature processing, the ideal candidate will coordinate the efforts of the other members of that project. While not necessary, experience in one or more of the following areas is highly advantageous: summarization, NLP or data mining for scientific literature, machine learning, and the use of linguistic knowledge in computational systems. Additionally, experience with large-data NLP and system building will be considered favorably. The successful candidate will work closely with current CLIP faculty, especially Bonnie Dorr, Hal Daume III and Ken Fleischmann, while interacting with a large team involving NLP researchers across several other prominent institutions. The duration of the position is one year, starting Summer or Fall 2011, and is potentially extendible. CLIP is a a dynamic interdisciplinary computational linguistics program with faculty from across the university, and major research efforts in machine translation, information retrieval, semantic analysis, generation, and development of large-scale statistical language processing tools. Please send a CV and names and contact information of 3 referees, preferably by e-mail, to:  Jessica Touchard  jessica AT cs DOT umd DOT edu  Department of Computer Science  A.V. Williams Building, Room 1103  University of Maryland  College Park, MD 20742 Specific questions about the position may be addressed to Hal Daume III at hal AT umiacs DOT umd DOT edu."], "link": "http://nlpers.blogspot.com/feeds/3369394059086205008/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "natural language processing blog"}, {"content": ["Having the option of authors submitting supplementary materials is becoming popular in NLP/ML land. NIPS was one of the first conferences I submit to that has allowed this; I think ACL allowed it this past year, at least for specific types of materials (code, data), and EMNLP is thinking of allowing it at some point in the near future. Here is a snippet of the NIPS call for papers (see section 5) that describes the role of supplementary materials: In addition to the submitted PDF paper, authors can additionally submit supplementary material for their paper... Such extra material may include long technical proofs that do not fit into the paper, image, audio or video sample outputs from your algorithm, animations that describe your algorithm, details of experimental results, or even source code for running experiments. Note that the reviewers and the program committee reserve the right to judge the paper solely on the basis of the 8 pages, 9 pages including citations, of the paper; looking at any extra material is up to the discretion of the reviewers and is not required. (Emphasis mine.) Now, before everyone goes misinterpreting what I'm about to say, let me make it clear that in general I like the idea of supplementary materials, given our current publishing model. You can think of the emphasized part of the call as a form of reviewer protection. It basically says: look, we know that reviewers are overloaded; if your paper isn't very interesting, the reviewers aren't required to read the supplement. (As an aside, I feel the same thing happens with pages 2-8 given page 1 in a lot of cases :P.) I think it's good to have such a form a reviewer protection. What I wonder is whether it also makes sense to add a form of author protection. In other words, the current policy -- which seems only explicitly stated in the case of NIPS, but seems to be generally understood elsewhere, too -- is that reviewers are protected from overzealous authors. I think we need to have additional clauses that protect authors from overzealous reviewers. Why? Already I get annoyed with reviewers who seem to think that extra experiments, discussion, proofs or whatever can somehow magically fit in an already crammed 8 page page. A general suggestion to reviewers is that if you're suggesting things to add, you should also suggest things to cut. This situation is exacerbated infinity-fold with the \"option\" of supplementary material. There now is no length-limit reason why an author couldn't include everything under the sun. And it's too easy for a reviewer just to say that XYZ should have been included because, well, it could just have gone in the supplementary material! So what I'm proposing is that supplementary material clauses should have two forms of protection. The first being the existing one, protecting reviewers from overzealous authors. The second being the reverse, something like: Authors are not obligated to include supplementary materials. The paper should stand on its own, excluding any supplement. Reviewers must take into account the strict 8 page limit when evaluating papers. Or something like that: the wording isn't quite right. But without this, I fear that supplementary materials will, in the limit, simply turn into an arms race."], "link": "http://nlpers.blogspot.com/feeds/5080250926854229169/comments/default", "bloglinks": {}, "links": {"http://nips.cc/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["You may recall a while ago I ran a survey on where people applied to grad school . Obviously I've been sitting on these results for a while now, but I figured since it's that time of year when people are choosing grad schools, that I would say how things turned out. Here's a summary of things that people thought were most important (deciding factor), and moderately important (contributing factor, in parens): Academic Program  Specialty degree programs in my research area, 48% (Availability of interesting courses, 16%) (Time to completion, 4%) Application Process Nothing  Faculty Member(s)  Read research papers by faculty member, 44%  Geographic Area (Outside interests/personal preference, 15%) Recommendations from People   Professors in technical area, 45% (Teachers/academic advisors, 32%) (Technical colleagues, 20%) Reputation  ... of research group, 61% ... of department/college, 50% (Ranking of university, 35%) (Reputation of university, 34%) Research Group  Research group works on interesting problems, 55%  Many faculty in a specialty area (eg., ML), 44% (Many faculty/students in general area (eg., AI), 33%) (Research group publishes a lot, 26%) Web Presence (Learned about group via web search, 37%) (Learned about dept/univ via web search, 24%) General  Funding availability, 49% (High likelihood of being accepted, 12%) (Size of dept/university, 5%)  Overall these seem pretty reasonable. And of course they all point to the fact that everyone should come to Maryland :P. Except for the fact that we don't have specialty degree programs, but that's the one thing on the list that I actually think is a bit silly: it might make sense for MS, but I don't really think it should be an important consideration for Ph.D.s. You can get the full results if you want to read them and the comments: they're pretty interesting, IMO."], "link": "http://nlpers.blogspot.com/feeds/806876833341250075/comments/default", "bloglinks": {}, "links": {"http://hal3.name/": 1, "http://nlpers.blogspot.com/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["I'll tell you what they should be : attending the Symposium on Machine Learning in Speech and Language Processing , jointly sponsored by IMLS, ICML and ISCA, that I'm co-organizing with Dan Roth, Geoff Zweig and Joseph Keshet (the exact date is June 27, in the same venue as ICML in Bellevue, Washington). So far we've got a great list of invited speakers from all of these areas, including Mark Steedman, Stan Chen, Yoshua Bengio, Lawrence Saul, Sanjoy Dasgupta and more. (See the web page for more details.) We'll also be organizing some sort of day trips (together with the local organizers of ICML) for people who want to join! You should also consider submitting papers (deadline is April 15). I know I said a month ago that I would blog more. I guess that turned out to be a lie. The problem is that I only have so much patience for writing and I've been spending a lot of time writing non-blog things recently. I decided to use my time-off-teaching doing something far more time consuming than teaching . This has been a wonderously useful exercise for me and I hope that, perhaps starting in 2012, other people can take advantage of this work."], "link": "http://nlpers.blogspot.com/feeds/6982589199507253162/comments/default", "bloglinks": {}, "links": {"http://hal3.name/": 1, "http://www.ttic.edu/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["I remember when I took my first \"real\" Syntax class, where by \"real\" I mean \"Chomskyan.\" It was at USC in Fall 2001, taught by Roumyana Pancheva . It was hard as hell but I loved it. However, as a computationally minded guy, I remember snickering to myself the whole time we were talking about movements that get you from deep structure to surface structure . This stuff was all computationally ridiculous. But why was it computationally ridiculous? It was ridiculous because my mindset, and I think the mindset of most computational folks at the time, was that of n^3 CKY or Earley style parsing. Namely exact parsing in a context free manner. This whole idea of transformations would kill anything like that in a very bad way. However, there's been a recent shift in attitudes. Sure, people still do their n^3 parsing, but of course none of it is exact anyway (due to pruning). But more than that, things like linear time parsing algorithms as popularized by people like Joakim Nivre and Kenji Sagae and Brian Roark and Joseph Turian, have proved very useful. They work well, are incredibly efficient, and are easy to implement. They're also a bit more psychologically plausible (as Eugene Charniak said recently \"we don't know what people are doing, but they're definitely not doing CKY.\"). So I'm led to wonder: could we actually do parsing in a transformational grammar using all the new stuff we know about (for instance) left-to-right parsing? One thing that stands in our way, of course, is the stupid Penn Treebank, which was annotated only with very simple transformations (mostly noun phrase movements) and not really \"deep\" transformations as most Chomskyan linguists would recognize them. But I think you could still do it. It would end up as being partially unsupervised, but at least from a minimum description length perspective, I can either spend weights learning more special cases, or I can learn general transformational rules. It would take some thought and effort to write it out and figure out how to actually optimize such a thing, but I bet it could be done in a semester. So then the question is: aside from smaller models (potentially), is there any other reason to do it? I can think of at least one: parsing non-declarative sentences. Since almost all sentences in the Treebank are declarative, parsers do pretty crappy when tested on other things. Slav Petrov had a paper at EMNLP 2010 on parsing questions . Here is the abstract, which says pretty much everything: ... We show that dependency parsers have more difficulty parsing questions than constituency parsers. In particular, deterministic shift-reduce dependency parsers ... drop to 60% labeled accuracy on a question test set. We propose an uptraining procedure in which a deterministic parser is trained on the output of a more accurate, but slower, latent variable constituency parser (converted to dependencies). Uptraining with 100K unlabeled questions achieves results comparable to having 2K labeled questions for training. With 100K unlabeled and 2K labeled questions, uptraining is able to improve parsing accuracy to 84%, closing the gap between in-domain and out-of-domain performance. Now, at least in principle, if you can parse declarative sentences, you should be able to parse questions. At least if you know about some basic syntactic transformations in English. (As an aside, the \"uptraining\" idea is almost exactly the same as the structure compilation idea that Percy, Dan and I had at ICML 2008, though Slav and colleagues apply it to a domain adaptation problem, while we just did simple semi-supervised learning.) We have observed similar effects in the parsing of commands, such as \"Put your head in a noose\" where parsers -- even constituency ones -- really really want \"Put\" to be a noun! Again, if you know simple transformations -- like subject dropping -- you should be able to parse commands if you can already parse declarations. As with any generalization, the hope is that by realizing the generalization, you don't need to store so many specific cases. So if you can learn that commands and questions are simple transformation on declarative sentences, and you can learn to parse declaratives, you should be able to handle the other case. (Anticipating comments: yes, I know you could try to pre-transform your data, like they do in MT, but that's quite inelegant. And yes, I know you could probably take the treebank and turn a lot of the sentences into commands or questions to create a new data set. But that's kind of missing the point: I don't want to just handle commands or questions... I want to handle anything, even things that I might not have anticipated.)"], "link": "http://nlpers.blogspot.com/feeds/8527687550339950613/comments/default", "bloglinks": {}, "links": {"http://www-bcf.usc.edu/": 1, "http://en.wikipedia.org/": 1, "http://www.petrovi.de/": 1, "http://hal3.name/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["Happy New Year and I know I've been silent but I've been busy. But no teaching this semester (YAY!) so maybe you'll see more posts. At any rate, I'm really late to the table, but here are my comments about this past year's NIPS. Before we get to that, I hope that everyone knows that this coming NIPS will be in Granada, and then for (at least) the next five years will be in Tahoe. Now that I'm not in ski-land, it's nice to have a yearly ski vacation ... erm I mean scientific conference. But since this was the last year of NIPS in Vancouver, I thought I'd share a conversation that occurred this year at NIPS, with participants anonymized. (I hope everyone knows to take this in good humor: I'm perfectly happy to poke fun at people from the States, too...). The context is that one person in a large group, which was going to find lunch, had a cell phone with a data plan that worked in Canada: A: Wow, that map is really taking a long time to load. B: I know. It's probably some socialized Canadian WiFi service. C: No, it's probably just slow because every third bit has to be a Canadian bit? D: No no, it's because every bit has to be sent in both English and French! Okay it's not that funny, but it was funny at the time. (And really \"B\" is as much a joke about the US as it was about Canada :P.) But I'm sure you are here to hear about papers, not stupid Canada jokes. So here's my take. The tutorial on optimization by Stephen Wright was awesome. I hope this shows up on videolectures soon. (Update: it has !) I will make it required reading / watching for students. There's just too much great stuff in it to go in to, but how about this: momentum is the same as CG! Really?!?! There's tons of stuff that I want to look more deeply into, such as robust mirror descent, some work by Candes about SVD when we don't care about near-zero SVs, regularized stochastic gradient (Xiao) and sparse eigenvector work. Lots of awesome stuff. My favorite part of NIPS. Some papers I saw that I really liked:  A Theory of Multiclass Boosting (Indraneel Mukherjee, Robert Schapire): Formalizes boosting in a multiclass setting. The crux is a clever generalization of the \"weak learning\" notion from binary. The idea is that a weak binary classifier is one that has a small advantage over random guessing (which, in the binary case, gives 50/50). Generalize this and it works. Structured sparsity-inducing norms through submodular functions (Francis Bach): I need to read this. This was one of those talks where I understood the first half and then got lost. But the idea is that you can go back-and-forth between submodular functions and sparsity-inducing norms. Construction of Dependent Dirichlet Processes based on Poisson Processes (Dahua Lin, Eric Grimson, John Fisher): The title says it all! It's an alternative construction to the Polya urn scheme and also to the stick-breaking scheme. A Reduction from Apprenticeship Learning to Classification (Umar Syed, Robert Schapire): Right up my alley, some surprising results about apprenticeship learning (aka Hal's version of structured prediction) and classification. Similar to a recent paper by Stephane Ross and Drew Bagnell on Efficient Reductions for Imitation Learning . Variational Inference over Combinatorial Spaces (Alexandre Bouchard-Cote, Michael Jordan): When you have complex combinatorial spaces (think traveling salesman), how can you construct generic variational inference algorithms? Implicit Differentiation by Perturbation (Justin Domke): This is a great example of a paper that I never would have read, looked at, seen, visited the poster of, known about etc., were it not for serendipity at conferences (basically Justin was the only person at his poster when I showed up early for the session, so I got to see this poster). The idea is if you have a graphical model, and some loss function L(.) which is defined over the marginals mu(theta), where theta are the parameters of the model, and you want to optimize L(mu(theta)) as a function of theta. Without making any serious assumptions about the form of L, you can actually do gradient descent, where each gradient computation costs two runs of belief propagation. I think this is amazing. Probabilistic Deterministic Infinite Automata (David Pfau, Nicholas Bartlett, Frank Wood): Another one where the title says it all. DP-style construction of infinite automata. Graph-Valued Regression (Han Liu, Xi Chen, John Lafferty, Larry Wasserman): The idea here is to define a regression function over a graph. It should be regularized in a sensible way. Very LASSO-esque model, as you might expect given the author list :). Other papers I saw that I liked but not enough to write mini summaries of: Word Features for Latent Dirichlet Allocation (James Petterson, Alexander Smola, Tiberio Caetano, Wray Buntine, Shravan Narayanamurthy) Tree-Structured Stick Breaking for Hierarchical Data (Ryan Adams, Zoubin Ghahramani, Michael Jordan) Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations (Danial Lashkari, Ramesh Sridharan, Polina Golland) Trading off Mistakes and Don't-Know Predictions (Amin Sayedi, Morteza Zadimoghaddam, Avrim Blum) Joint Analysis of Time-Evolving Binary Matrices and Associated Documents (Eric Wang, Dehong Liu, Jorge Silva, David Dunson, Lawrence Carin) Learning Efficient Markov Networks (Vibhav Gogate, William Webb, Pedro Domingos) Tree-Structured Stick Breaking for Hierarchical Data (Ryan Adams, Zoubin Ghahramani, Michael Jordan) Construction of Dependent Dirichlet Processes based on Poisson Processes (Dahua Lin, Eric Grimson, John Fisher) Supervised Clustering (Pranjal Awasthi, Reza Bosagh Zadeh) Two students who work with me (though one isn't actually mine :P), who went to NIPS also shared their favorite papers. The first is a list from Avishek Saha : A Theory of Multiclass Boosting (Indraneel Mukherjee, Robert Schapire) Repeated Games against Budgeted Adversaries (Jacob Abernethy, Manfred Warmuth) Non-Stochastic Bandit Slate Problems (Satyen Kale, Lev Reyzin, Robert Schapire) Trading off Mistakes and Don't-Know Predictions (Amin Sayedi, Morteza Zadimoghaddam, Avrim Blum) Learning Bounds for Importance Weighting (Corinna Cortes, Yishay Mansour, Mehryar Mohri) Supervised Clustering (Pranjal Awasthi, Reza Bosagh Zadeh) The second list is from Piyush Rai , who apparently aimed for recall (though not with a lack of precision) :P: Online Learning: Random Averages, Combinatorial Parameters, and Learnability (Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari): defines several complexity measures for online learning akin to what we have for the batch setting (e.g., radamacher averages, covering numbers etc). Online Learning in The Manifold of Low-Rank Matrices (Uri Shalit, Daphna Weinshall, Gal Chechik): nice general framework applicable in a number of online learning settings. could also be used for online multitask learning. Fast global convergence rates of gradient methods for high-dimensional statistical recovery (Alekh Agarwal, Sahand Negahban, Martin Wainwright): shows that the properties of sparse estimation problems that lead to statistical efficiency also lead to computational efficiency which explains the faster practical convergence of gradient methods than what the theory guarantees. Copula Processes (Andrew Wilson, Zoubin Ghahramani): how do you determine the relationship between random variables which could have different marginal distributions (say one has gamma and the other has gaussian distribution)? copula process gives an answer to this. Graph-Valued Regression (Han Liu, Xi Chen, John Lafferty, Larry Wasserman): usually undirected graph structure learning involves a set of random variables y drawn from a distribution p(y). but what if y depends on another variable x? this paper is about learning the graph structure of the distribution p(y|x=x). Structured sparsity-inducing norms through submodular functions (Francis Bach): standard sparse recovery uses l1 norm as a convex proxy for the l0 norm (which constrains the number of nonzero coefficients to be small). this paper proposes several more general set functions and their corresponding convex proxies, and links them to known norms. Trading off Mistakes and Don't-Know Predictions (Amin Sayedi, Morteza Zadimoghaddam, Avrim Blum): an interesting paper -- what if in an online learning setting you could abstain from making a prediction on some of the training examples and just say \"i don't know\"? on others, you may or may not make the correct prediction. lies somewhere in the middle of always predicting right or wrong (i.e., standard mistake driven online learning) versus the recent work on only predicting correctly or otherwise saying \"i don't know\". Variational Inference over Combinatorial Spaces (Alexandre Bouchard-Cote, Michael Jordan): cool paper. applicable to lots of settings. A Theory of Multiclass Boosting (Indraneel Mukherjee, Robert Schapire): we know that boosting in binary case requires \"slightly better than random\" weak learners. this paper characterizes conditions on the weak learners for the multi-class case, and also gives a boosting algorithm. Multitask Learning without Label Correspondences (Novi Quadrianto, Alexander Smola, Tiberio Caetano, S.V.N. Vishwanathan, James Petterson): usually mtl assumes that the output space is the same for all the tasks but in many cases this may not be true. for instance, we may have two related prediction problems on two datasets but the output spaces for both may be different and may have some complex (e.g., hierarchical, and potentially time varying) output spaces. the paper uses a mutual information criteria to learn the correspondence between the output spaces. Learning Multiple Tasks with a Sparse Matrix-Normal Penalty (Yi Zhang, Jeff Schneider): presents a general multitask learning framework and many recently proposed mtl models turn out to be special cases. models both feature covariance and task covariance matrices. Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions (Achintya Kundu, vikram Tankasali, Chiranjib Bhattacharyya, Aharon Ben-Tal): the title says it all. :) multiple kernel learning is usually applied in classification setting but due to the applicability of the proposed method for a wide variety of loss functions, one can possibly also use it for unsupervised learning problems as well (e.g., spectral clustering, kernel pca, etc). Getting lost in space: Large sample analysis of the resistance distance (Ulrike von Luxburg, Agnes Radl, Matthias Hein): large sample analysis of the commute distance: shows a rather surprising result that commute distance between two vertices in the graph if the graph is \"large\" and nodes represent high dimensional variables is meaningless. the paper proposes a correction and calls it \"amplified commute distance\". A Bayesian Approach to Concept Drift (Stephen Bach, Mark Maloof): gives a bayesian approach for segmenting a sequence of observations such that each \"block\" of observations has the same underlying concept. MAP Estimation for Graphical Models by Likelihood Maximization (Akshat Kumar, Shlomo Zilberstein): they show that you can think of an mrf as a mixture of bayes nets and then the map problem on the mrf corresponds to solving a form of the maximum likelihood problem on the bayes net. em can be used to solve this in a pretty fast manner. they say that you can use this methods with the max-product lp algorithms to yield even better solutions, with a quicker convergence. Energy Disaggregation via Discriminative Sparse Coding (J. Zico Kolter, Siddharth Batra, Andrew Ng): about how sparse coding could be used to save energy. :) Semi-Supervised Learning with Adversarially Missing Label Information (Umar Syed, Ben Taskar): standard ssl assumes that labels for the unlabeled data are missing at random but in many practical settings this isn't actually true.this paper gives an algorithm to deal with the case when the labels could be adversarially missing. Multi-View Active Learning in the Non-Realizable Case (Wei Wang, Zhi-Hua Zhou): shows that (under certain assumptions) exponential improvements in the sample complexity of active learning are still possible if you have a multiview learning setting. Self-Paced Learning for Latent Variable Models (M. Pawan Kumar, Benjamin Packer, Daphne Koller): an interesting paper, somewhat similar in spirit to curriculum learning. basically, the paper suggests that in learning a latent variable model, it helps if you provide the algorithm easy examples first. More data means less inference: A pseudo-max approach to structured learning (David Sontag, Ofer Meshi, Tommi Jaakkola, Amir Globerson): a pseudo-max approach to structured learning: this is somewhat along the lines of the paper on svm's inverse dependence on training size from icml a couple of years back. :) Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning (Prateek Jain, Sudheendra Vijayanarasimhan, Kristen Grauman): selecting the most uncertain example in a pool based active learning can be expensive if the number of candidate examples is very large. this paper suggests some hashing tricks to expedite the search. Active Instance Sampling via Matrix Partition (Yuhong Guo): frames batch mode active learning as a matrix partitioning problems and proposes local optimization technique for the matrix partitioning problem. A Discriminative Latent Model of Image Region and Object Tag Correspondence (Yang Wang, Greg Mori): it's kind of doing correspondence lda on image+captions but they additionally infer the correspondences between tags and objects in the images, and show that this gives improvements over corr-lda. Factorized Latent Spaces with Structured Sparsity (Yangqing Jia, Mathieu Salzmann, Trevor Darrell): a multiview learning algorithm that uses sparse coding to learn shared as well as private features of different views of the data. Word Features for Latent Dirichlet Allocation (James Petterson, Alexander Smola, Tiberio Caetano, Wray Buntine, Shravan Narayanamurthy): extends lda for the case when you have access to features for each word in the vocabulary"], "link": "http://nlpers.blogspot.com/feeds/3555243437241244556/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 1, "http://www.blogger.com/": 2, "http://books.nips.cc/": 48, "http://www.wisc.edu/": 1, "http://videolectures.net/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["Everyone at conferences (with multiple tracks) always complains that there are time slots with nothing interesting, and other time slots with too many interesting papers. People have suggested crowdsourcing this, enabling parcipants to say -- well ahead of the conference -- which papers they'd go to... then let an algorithm schedule. I think there are various issues with this model, but don't want to talk about it. What I do want to talk about is applying the same ideas to workshop acceptance decisions. This comes up because I'm one of the two workshop chairs for ACL this year, and because John Langford just pointed to the ICML call for tutorials. (I think what I have to say applies equally to tutorials as to workshops.) I feel like a workshop (or tutorial) is successful if it is well attended. This applies both from a monetary perspective, as well as a scientific perspective. (Note, though, that I think that small workshops can also be successful, especially if they are either fostering a small community, bring people in, or serving other purposes. That is to say, size is not all that matters. But it is a big part of what matters.) We have 30-odd workshop proposals for three of us to sort through (John Carroll and I are the two workshop chairs for ACL, and Marie Candito is the workshop chair for EMNLP; workshops are being reviewed jointly -- which actually makes the allocation process more difficult). The idea would be that I could create a poll, like the following: Are you going to ACL? Yes, maybe, no Are you going to EMNLP? Yes, maybe, no If workshop A were offered at a conference you were going to, would you go to workshop A? If workshop B... And so on This gives you two forms of information. First it can help estimate expected attendance (though we ask proposers to estimate that, too, and I think they do a reasonable job if you skew their estimates down by about 10%). But more importantly, it gives correlations between workshops . This lets you be sure that you're not scheduling things on top of each other that people might want to go to. Some of these are obvious (for instance, if we got 10 MT workshop proposals... which didn't actually happen but is moderately conceivable :P), but some are not. For instance, maybe people who care about annotation also care about ML, but maybe not? I actually have no idea. Of course we're not going to do this this year. It's too late already, and it would be unfair to publicise all the proposals, given that we didn't tell proposers in advance that we would do so. And of course I don't think this should exclusively be a popularity contest . But I do beleive that popularity should be a factor. And it should probably be a reasonably big factor. Workshop chairs could then use the output of an optimization algorithm as a starting point, and use this as additional data for making decisions. Especially since two or three people are being asked to make decisions that cover--essentially--all areas of NLP, this actually seems like a good idea to me. I actually think something like this is more likely to actually happen at a conference like ICML than ACL, since ICML seems (much?) more willing to try new things than ACL (for better or for worse). But I do think it would be interesting to try to see what sort of response you get. Of course, just polling on this blog wouldn't be sufficient: you'd want to spam, perhaps all of last year's attendees. But this isn't particularly difficult. Is there anything I'm not thinking of that would make this obviously not work? I could imagine someone saying that maybe people won't propose workshops/tutorials if the proposals will be made public? I find that a bit hard to swallow. Perhaps there's a small embarassment factor if you're public and then don't get accepted. But I wouldn't advocate making the voting results public -- they would be private to the organizers / workshop chairs. I guess -- I feel like I'm channeling Fernando here? -- that another possible issue is that you might not be able to decide which workshops you'd go to without seeing what papers are there and who is presenting . This is probably true. But this is also the same problem that the workshop chairs face anyway: we have to guess that good enough papers/people will be there to make it worthwhile. I doubt I'm any better at guessing this than any other random NLP person... So what am I forgetting?"], "link": "http://nlpers.blogspot.com/feeds/7142612846170713721/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "natural language processing blog"}, {"content": ["Every time a major conference deadline (ACL, NIPS, EMNLP, ICML, etc...) comes around, we usually have a slew of papers (>=3, typically) that are getting prepared. I would say on average 1 doesn't make it, but that's par for the course. For AI-Stats, whose deadline just passed, I circulated student paper drafts to all of my folks to solicit comments at any level that they desired. Anywhere from not understanding the problem/motivation to typos or errors in equations. My experience was that it was useful, both from the perspective of distributing some of my workload and getting an alternative perspective, to keeping everyone abreast of what everyone else is working on. In fact, it was so successful that two students suggested to me that I require more-or-less complete drafts of papers at least one week in advance so that this can take place. How you require something like this is another issue, but the suggestion they came up with was that I'll only cover conference travel if this occurs. It's actually not a bad idea, but I don't know if I'm enough of a hard-ass (or perceived as enough of a hard-ass) to really pull it off. Maybe I'll try it though. The bigger question is how to manage such a thing. I was thinking of installing some conference management software locally (eg., HotCRP , which I really like) and giving students \"reviewer\" access. Then, they could upload their drafts, perhaps with an email circulated when a new draft is available, and other students (and me!) could \"review\" them. (Again, perhaps with an email circulated -- I'm a big fan of \"push\" technology: I don't have time to \"pull\" anymore!) The only concern I have is that it would be really nice to be able to track updates, or to have the ability for authors to \"check off\" things that reviewers suggested. Or to allow discussion. Or something like that. I'm curious if anyone has ever tried anything like this and whether it was successful or not. It seems like if you can get a culture of this established, it could actually be quite useful."], "link": "http://nlpers.blogspot.com/feeds/8682169235051598883/comments/default", "bloglinks": {}, "links": {"http://www.ucla.edu/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["This is something that's bothered me for quite a while, and I don't know of a good answer. I used to think it was something that theory people didn't worry about, but then this exact issue was brought up by a reviewer of a theory-heavy paper that we have at NIPS this year (with Avishek Saha and Abhishek Kumar ). There are (at least?) two issues with comparing bounds, the first is the obvious \"these are both upper bounds, what does it mean to compare them?\" The second is the slightly less obvious \"but your empirical losses may be totally different\" issue. It's actually the second one that I want to talk about, but I have much less of a good internal feel about it. Let's say that I'm considering two learning approaches. Say it's SVMs versus logistic regression. Both regularized. Or something. Doesn't really matter. At the end of the day, I'll have a bound that looks roughly like:   expected test error <= empirical training error + f( complexity / N) Here, f is often \"sqrt\", but could really be any function. And N is the number of data points. Between two algorithms, both \"f\" and \"complexity\" can vary. For instance, one might have a linear dependence on the dimensionality of the data (i.e., complexity looks like O(D), where D is dimensionality) and the other might have a superlinear dependence (eg., O(D log D)). Or one might have a square root. Who knows. Sometimes there's an inf or sup hiding in there, too, for instance in a lot of the margin bounds. At the end of the day, we of course want to say \"my algorithm is better than your algorithm.\" (What else is there in life?) The standard way to say this is that \"my f(complexity / N) looks better than your f'(complexity' / N).\" Here's where two issues crop up. The first is that our bound is just an upper bound. For instance, Alice could come up to me and say \"I'm thinking of a number between 1 and 10\" and Bob could say \"I'm thinking of a number between 1 and 100.\" Even though the bound is lower for Alice, it doesn't mean that Alice is actually thinking of a smaller number -- maybe Alice is thinking of 9 and Bob of 5. In this way, the bounds can be misleading. My general approach with this issue is to squint, as I do for experimental results. I don't actually care about constant factors: I just care about things like \"what does the dependence on D look like.\" Since D is usually huge for problems I care about, a linear or sublinear dependence on D looks really good to me. Beyond that I don't really care. I especially don't care if the proof techniques are quite similar. For instance, if they both use Rademacher complexities, then I'm more willing to compare them than if one uses Rademacher complexities and the other uses covering numbers. They somehow feel more comparable: I'm less likely to believe that the differences are due to the method of analysis. (You can also get around this issue with some techniques, like Rademacher complexities, which give you both upper and lower bounds, but I don't think anyone really does that...) The other issue I don't have as good a feeling for. The issue is that we're entirely ignoring the \"empirical training error\" question. In fact, this is often measured differently between different algorithms! For instance, for SVMs, the formal statement is more like \"expected 0/1 loss on test <= empirical hinge loss on training + ...\" Whereas for logistic regression, you might be comparing expected 0/1 loss with empirical log loss. Now I really don't know what to do. We ran into this issue because we were trying to compare some bounds between EasyAdapt and a simple model trained just on source data. The problem is that the source training error might be totally incomparable to the (source + target) training error. But the issue is for sure more general. For instance, what if your training error is measured in squared error? Now this can be huge when hinge loss is still rather small. In fact, your squared error could be quadratically large in your hinge loss. Actually it could be arbitrarily larger, since hinge goes to zero for any sufficiently correct classification, but squared error does not. (Neither does log loss.) This worries me greatly, much more than the issue of comparing upper bounds. Does this bother everyone, or is it just me? Is there a good way to think about this that gets your out of this conundrum?"], "link": "http://nlpers.blogspot.com/feeds/6296520072559484676/comments/default", "bloglinks": {}, "links": {"http://www.umd.edu/": 1}, "blogtitle": "natural language processing blog"}, {"content": ["I try to be a good reviewer, but like everything, reviewing is a learning process. About five years ago, I was reviewing a journal paper and made an error. I don't want to give up anonymity in this post, so I'm going to be vague in places that don't matter. I was reviewing a paper, which I thought was overall pretty strong. I thought there was an interesting connection to some paper from Alice Smith (not the author's real name) in the past few years and mentioned this in my review. Not a connection that made the current paper irrelevant, but something the authors should probably talk about. In the revision response, the authors said that they had looked to try to find Smith's paper, but could figure out which one I was talking about, and asked for a pointer. I spend the next five hours looking for the reference and couldn't find it myself. It turns out that actually I was thinking of a paper by Bob Jones, so I provided that citation. But the Jones paper wasn't even as relevant as it seemed at the time I wrote the review, so I apologized and told the authors they didn't really need to cover it that closely. Now, you might be thinking to yourself: aha, now I know that Hal was the reviewer of my paper! I remember that happening to me! But, sadly, this is not true. I get reviews like this all the time, and I feel it's one of the most irresponsible things reviewers can do. In fact, I don't think a single reviewing cycle has passed where I don't get a review like this. The problem with such reviews is that it enables a reviewer to make whatever claim they want, without any expectation that they have to back it up. And the claims are usually wrong. They're not necessarily being mean (I wasn't trying to be mean), but sometimes they are. Here are some of the most ridiculous cases I've seen. I mention these just to show how often this problem occurs. These are all on papers of mine. One reviewer wrote \"This idea is so obvious this must have been done before.\" This is probably the most humorous example I've seen, but the reviewer was clearly serious. And no, this was not in a review for one of the the \"frustratingly easy\" papers. In a NSF grant review for an educational proposal, we were informed by 4 of 7 reviewers (who each wrote about a paragraph) that our ideas had been done in SIGCSE several times. Before submitting, we had skimmed/read the past 8 years of SIGCSE and could find nothing. (Maybe it's true and we just were looking in the wrong place, but that still isn't helpful.) It turned out to strongly seem that this was basically their way of saying \"you are not one of us.\" In a paper on technique X for task A, we were told hands down that it's well known that technique Y works better, with no citations. The paper was rejected, we went and implemented Y, and found that it worked worse on task A. We later found one paper saying that Y works better than X on task B, for B fairly different from A. In another paper, we were told that what we were doing had been done before and in this case a citation was provided. The citation was to one of our own papers, and it was quite different by any reasonable metric. At least a citation was provided, but it was clear that the reviewer hadn't bothered reading it. We were told that we missed an enormous amount of related work that could be found by a simple web search. I've written such things in reviews, often saying something like \"search for 'non-parametric Bayesian'\" or something like that. But here, no keywords were provided. It's entirely possible (especially when someone moves into a new domain) that you can miss a large body of related work because you don't know how to find in: that's fine -- just tell me how to find it if you don't want to actually provide citations. There are other examples I could cite from my own experience, but I think you get the idea. I'm posting this not to gripe (though it's always fun to gripe about reviewing), but to try to draw attention to this problem. It's really just an issue of laziness. If I had bothered trying to look up a reference for Alice Smith's paper, I would have immediately realized I was wrong. But I was lazy. Luckily this didn't really adversely affect the outcome of the acceptance of this paper (journals are useful in that way -- authors can push back -- and yes, I know you can do this in author responses too, but you really need two rounds to make it work in this case). I've really really tried ever since my experience above to not ever do this again. And I would encourage future reviewers to try to avoid the temptation to do this: you may find your memory isn't as good as you think. I would also encourage area chairs and co-reviewers to push their colleagues to actually provide citations for otherwise unsubstantiated claims."], "link": "http://nlpers.blogspot.com/feeds/4790628870864598174/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "natural language processing blog"}]