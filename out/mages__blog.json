[{"blogurl": "http://lamages.blogspot.com\n", "blogroll": [], "title": "mages' blog"}, {"content": ["After last week's kerfuffle I hope the roll out of googleVis version 0.3.2 will be smooth. To test the water I release this version into the wild here and if it doesn't get shot down in the next days, then I shall try to upload it to CRAN . I am mindful of the CRAN policy , so please get in touch or add comments below if you find any show stoppers. \n \n So what's new in googleVis 0.3.2? The default behaviour of the functions print.gvis and plot.gvis can be set via options() . \n \nNow this doesn't sound too exciting but it can be tremendously helpful when you write Markdown files for knitr . Here is why: \n \nThe default googleVis plot method opens a browser window to display gvis-objects. That's great when you work with R and googleVis in an interactive and explorative way, but if you like to include the plots into a knitr Markdown file then you would have to change those statements to print(x, tag=\"chart\") , as explained in an earlier post . \n \n Including googleVis output in knitr with plot statement With version 0.3.2 of googleVis plot.gvis gained the argument 'tag' , which works similar to the argument of the same name in print.gvis . By default the tag argument is NULL and plot.gvis has the same behaviour as in the previous versions of googleVis. Change the tag to 'chart' and plot.gvis will produce the same output as print.gvis . That means instead of opening a browser window plot.gvis will return the HTML code of the googleVis chart. \n \nAnd here is the real trick, if tag is not set explicitly in plot.gvis then it will use the value set in options(gvis.print.tag) . Thus, if I set the gvis.plot.tag value to 'chart' in options() then all following plot statements will return the HTML code of the chart when the file is parsed with knitr . Set it back to NULL via options(gvis.plot.tag=NULL) and the old behaviour of plot.gvis has been restored. \n \nHere is the example of the updated help file to plot.gvis and package vignette . The Markdown and R code is available below. I hope this illustrates the concept clearly. \nLoading... \n \n R Markdown file \n \n R Code to knit HTML output The code below shows how the Markdown file can be converted into a HTML file and displayed in a browser. If you use RStudio then most of it is happening in the background when you hit the \"knit HTML\" button. \n \n \n \nThe following few lines replicate the whole example, sourcing the above gists. \n URL"], "link": "http://lamages.blogspot.com/feeds/8583206693754661177/comments/default", "bloglinks": {}, "links": {"https://dl.dropbox.com/": 1, "http://feedads.doubleclick.net/": 2, "http://yihui.name/": 1, "http://www.rstudio.com/": 1, "http://www.r-project.org": 1, "http://en.wikipedia.org/": 1, "http://2.blogspot.com/": 1, "http://cran.r-project.org/": 2, "http://lamages.co.uk/": 2, "http://code.google.com/": 2}, "blogtitle": "mages' blog"}, {"content": ["Version 0.3.0 of the googleVis package for R has been released on CRAN on 20 October 2012. With this version we have been able to speed up the code considerably. The transformation of R data frames into JSON works significantly faster. The execution of the gvisMotionChart function in the World Bank demo is over 35 times faster. Thanks to ideas by Wei Luo and in particular to Sebastian Kranz for providing the code. \n \n Further, the plot function plot.gvis has gained a new argument 'browser' . This argument is passed on to the function browseURL . The 'browser' argument is by default set to the output of getOption(\"browser\") in an interactive session, otherwise to 'false' . This prevents R CMD CHECK trying to open browser windows during the package checking process. Unfortunately this caused an error message under RStudio and RGui on Windows. The argument has been removed again and plot.gvis handles the check if R is running interactively internally. The bug has been fixed in googleVis-0.3.1, not yet available on CRAN, but on our project download page . Thanks to Henrik Bengtsson and Sebastian Kranz for their comments, suggestions and quick response. \n \nShould you find any further issues or bugs with the new version, then please drop me a line or add them to our issues list . \n \nSebastian also suggested to add arguments to gvisMotionChart allowing the user to set the defaults for the various visible dimensions, namely: xvar, yvar, colorvar and sizevar . Here is a simple example using the new optional arguments: \n \n \n  \n Screen shot of a motion chart \n \nThe defaults of those new arguments are set to \"\", which means gvisMotionChart will assume the columns in order of your data frame. This is the same behaviour as in the previous versions of googleVis . \n \nThanks to feedback from Erik B\u00fclow the mechanism to load the Google API is now handeled via a secure https-connection rather than http. \n \nFinally, we included a new example in the help file of gvisMap , demonstrating how to include html code into tooltips: \n \n \n Loading ..."], "link": "http://lamages.blogspot.com/feeds/5982646569283737059/comments/default", "bloglinks": {}, "links": {"http://theweiluo.wordpress.com/": 1, "http://lamages.co.uk/": 1, "http://www.uni-ulm.de/": 1, "http://www.r-project.org": 1, "http://code.google.com/": 3, "http://2.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.braju.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["Source: Wikipedia , License: CC0 \n  There is a wonderful cartoon by Loriot , a German humorist (1923 - 2011), about a couple sitting at a breakfast table, arguing about how to boil a four-and-a-half minute egg . The answer appears simple, but husband and wife argue about how to measure the time using experience, feelings and expert judgment (wife) or a clock (husband). \n \nThe whole sketch is hilarious and is often regarded as a fine observation of miss-communication. \n \nYet, I think it really points out two different approaches in decision making: You can trust your guts or use data/measurements to support your decision. \n \nA lot of industries are going through this transitions at the moment, and this transformation is not without frictions and power shifts, as I observe it in the insurance industry. \n \nWithout a stop watch or an egg-timer you need someone with good intuitive time keeping skills to cook a four-and-half minute egg. Experience and a good track record build credibility. Those people are scarce and expensive. With an egg-timer everyone can cook an egg. The magic is gone and with that the high salary for the old egg master. All you need is an instrument that can measure the observations on which your decision is based. Or, so it seems. \n \nHence, as more data is becoming available I believe we will see a power shift between the 'old guys', who rely largely on their experience, relationships (that means gossip) and guts to convince their management boards and the 'new guys', who use data to validate and support their story, who can compensate some of their lack of experience with analytics and come to a conclusion in a more objective way. \n \nThose 'new guys' will make mistakes, but the smarter ones will work with the 'old guy', learn that there is a lot more context to know to truly understand the data. And they will work with the 'old guy' and the data to test lots of hypothesis /gossip and learn quickly. \n \nThus, if you are young and sharp, trained in the scientific method , know how to analyse and present data and like to engage with people, then you are well positioned to accelerate your career. Just make sure the clock is actually working! Otherwise it is garbage in, garbage out again. Or, as Seth Godin pointed out, having a clock which is randomly wrong is the worst of all clocks and you would be better off without one."], "link": "http://lamages.blogspot.com/feeds/668825225394822754/comments/default", "bloglinks": {}, "links": {"http://creativecommons.org/": 1, "http://feedads.doubleclick.net/": 2, "http://de.wikipedia.org/": 1, "http://sethgodin.typepad.com/": 1, "http://en.wikipedia.org/": 4, "http://upload.wikimedia.org/": 1, "http://www.co.uk/": 1}, "blogtitle": "mages' blog"}, {"content": ["The third Cologne R user meeting took place last Friday, 5 October 2012 , at the Institute of Sociology. \n \nThe evening was sponsored by Revolution Analytics , who provided funding which went towards the K\u00f6lner R user group Meetup page. We had a good turn-out with 18 participants showing up and three talks by Dominik Liebl, Jonas Stein and Sarah Westrop. \n \n   \n   \n  \n  Photos: G\u00fcnter Faes \n \n \n Dominik Liebl presented ideas of his forthcoming paper/vignette with Oualid Bada on Panel Data Analysis with Heterogeneous Time Trends . Dominik and Qualid are also working on an R package to make their ideas more easily accessible. Their package phtt is currently only available via R-forge . However, Dominik's live demos suggest that the package is useable already. \n \n Jonas Stein presented his sitools package to the group. Jonas is a physicist, researching the behaviour of chilled crystals (~ 4 Kelvin) under the influence of high voltages. His work may lead to new data storage technology in the future. The data he collects in his experiments is measured in many different units and magnitudes and to help with the handling of such data he developed the sitools package, which does just that. Jonas mentioned also two functions outside his package he has found particularly useful: \n paste0 for concatenating strings without a separator \n ggsave to save plots. \n \n Sarah Westrop gave an introduction to the knitr package with RStudio and web2py . She showed us the basic concepts of Markdown . Sarah gave a live demo of RStudio and knitr, and demonstrated the value of naming code chunks: it is not only easier to return back to a specific chunk, but also graphics will be saved automatically under the code chunk name, rather then Chunk-1, etc. Using a web framework like web2py allows her to publish her output quickly on the web, including login and comments. \n \nFollowing the hard work we deserved a beer or two. The next meeting will take place on 6 February 2012. That's one day before carnival starts in Cologne. Please get in touch if you would like to present or attend the meeting. \n \n  \nPhoto: G\u00fcnter Faes"], "link": "http://lamages.blogspot.com/feeds/5596861691455263073/comments/default", "bloglinks": {}, "links": {"http://www.web2py.com": 1, "http://rstudio.org": 1, "http://cran.r-project.org/": 1, "https://r-forge.r-project.org/": 1, "http://docs.ggplot2.org/": 1, "http://delphi.uni-bonn.de/": 1, "http://stat.ethz.ch/": 1, "http://r-forge.r-project.org/": 1, "http://r-statistik.de/": 2, "http://revolutionanalytics.com": 1, "http://www.uni-koeln.de/": 1, "http://panel-htt.r-project.org/": 1, "http://photos3.meetupstatic.com/": 3, "http://lamages.co.uk/": 1, "http://yihui.name/": 1, "http://en.wikipedia.org/": 1, "https://plus.google.com/": 1, "http://jonasstein.de": 1, "http://feedads.doubleclick.net/": 2, "http://photos1.meetupstatic.com/": 1, "http://www.meetup.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["If connecting data to the real world is the next sexy job , then how do I do this? And how do I connect the real world to R? \n \nIt can be done as Matt Shottwell showed with his home made ECG and a patched version of R at useR! 2011 . However, there are other options as well and here I will use an Arduino . The Arduino is an open-source electronics prototyping platform. It has been around for a number of years and is very popular with hardware hackers. So, I had to have a go at the Arduino as well. \n \n \n  \n My Arduino starter kit from oomlout \n \nThe example I will present here is silly - it doesn't do anything meaningful and yet I believe it shows the core building blocks for future projects: Read an analog signal into the computer via the Arduino, transform it with R through Rserve and display it graphically in real time. The video below demonstrates the final result. As I turn the potentiometer random points are displayed on the screen, with the standard deviation set by the analog output (A0) of the Arduino and fed into the rnorm function in R, while at the same time the LED brightness changes. \n \n \n \nI don't claim to be an expert in any of this (far from it), but at the end my experiment worked. I am happy to share my experience, as I have benefited from reading various blog posts as well. I can't guarantee that any of this will not harm your hard- or software. Hence, I used an old computer. Of course feedback will be much appreciated if gently delivered. \n \nHere is a list of hardware and software I used and 10 steps to recreate the experiment: \n \n Hardware Arduino (Uno) \n Computer ( 2003 iBook G4 , MacOSX 10.4.11) \n USB cable \n Potentiometer (10k Ohm) \n Resistor (560 Ohm) \n LED \n Wire \n Bread-board \n \n Software  Arduino (1.0.1) \n Processing (1.5.1) \n Arduino library for Processing \n R (2.15.0) \n Rserve (0.6.8) \n REngine.jar \n RServeEngine.jar \n \n Step 1 Install Arduino , Processing and R on your computer. I have used my old iBook G4, but the software is available for most platforms, including Windows and Linux. \n \n Step 2 Assemble the hardware with the potentiometer, resistor and LED on a bread-board next to the Arduino. This setup is basically example 8 of my oomlout starter kit, with the difference that I connected the LED to Pin 9 instead of Pin 13. You find a detailed description, including video and code on their site . \n \n \n  \n Image of my setup created with Fritzing \n \n Step 3 Connect the Arduino with your computer via USB and start the Arduino software. You can reset the Arduino with a little sketch, if you want to: \n void setup(){}\nvoid loop(){} Copy and paste the code below into a new Sketch and hit the upload button. The LED should light up and as you turn the potentiometer the brightness should change. While you upload the file to the Arduino notice the connection path of your device in the bottom of the console window, mine said: Arduino Uno on /dev/cu.usbmodem3B11 . \n \nOpen the Serial Monitor in the Arduino software (Tools > Serial Monitor) and observe how the displayed numbers change from 0 to 1023 as you turn the potentiometer. \n \n \n Step 4 Close the Arduino software and start Processing. You should get a similar window and IDE as with the Arduino software. Open the preference settings and find out where Processing stores the Sketches by default; mine are saved here: /Users/Markus/Documents/Processing . \nTo make Processing talk to the Arduino it needs the Arduino library for Processing . Download the zip-file and extract it. Expanding the archive should result in a folder called arduino . If it doesn't exist already, create the folder libraries in the default Processing Sketch folder, and place in there the arduino folder. Restart Processing, click on File > Examples ... and a little window should list the new arduino library at the bottom. \n \n Step 5 Check the port number of the Arduino in Processing: The following little Processing sketch will list the serial ports in the console. Match the device paths listed against the one you noted in Step 3 and keep a record of the port number of the Arduino device path. \n \n \n Step 6 Test that you can read and write to the Arduino from Processing with the Sektch below. Change mySerialPort to the device number you identified in the previous step. Run the Processing code and you should see the potentiometer values displayed in the console window below your code. If you get any error messages regarding the RXTX library (most likely on a Mac) then check out the post by Ellen Sundh . \n \n \n \n Step 7 Open R and install the Rserve package, e.g. via the command install.packages(\"Rserve\") . Start R, load the Rserve package and start the daemon, e.g. \n library(Rserve)\nRserve() or better via the command line: R CMD Rserve \n Step 9 Download the files REngine.jar and RServeEngine.jar from the Rserve page into a temporary folder. Those files are needed to connect Processing to R. More information around how to call R from Processing has the following post on the GuruBlog . \n \n Step 10 Back to Processing. Create a new sketch in Processing, insert the code below and save the file. Add the two jar-files you downloaded in the previous step to your new sketch in Processing via Sketch > Add to File ... \n \nFingers crossed. Hit the run bottom and a little window should come up, plotting random points from left to right and as you turn the potentiometer the vertical range of the plotted points should change as well."], "link": "http://www.rforge.net/Rserve/files/REngine.jar", "bloglinks": {}, "links": {"http://www.arduino.cc/": 3, "http://fritzing.org/": 1, "http://arduino.cc/": 3, "http://feedads.doubleclick.net/": 2, "http://www.r-project.org/": 2, "http://www.rforge.net/": 7, "http://biostatmatt.com/": 2, "http://ardx.org/": 1, "http://processing.org/": 2, "http://www.local-guru.net/blog": 1, "http://2.blogspot.com/": 2, "http://www.co.uk/": 1, "http://www.sundh.com/blog": 2, "http://lamages.co.uk/": 2, "http://www.oumlout.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["The next Cologne R user group meeting is scheduled for 5 October 2012. All details and the agenda are available on the K\u00f6lnRUG Meetup site . Please sign up if you would like to come along. Notes from the last Cologne R user group meeting are available here . \n Thanks also to Revolution Analytics , who are sponsoring the Cologne R user group as part of their vector programme . \n   \n View Larger Map"], "link": "http://lamages.blogspot.com/feeds/9181487977281575119/comments/default", "bloglinks": {}, "links": {"http://maps.co.uk/": 1, "http://feedads.doubleclick.net/": 2, "http://www.revolutionanalytics.com": 1, "http://files.meetup.com/": 1, "http://www.meetup.com/": 1, "http://lamages.co.uk/": 1, "http://www.revolutionanalytics.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["Every year the UK\u2019s general insurance actuarial community organises a big conference, which they call GIRO , short for General Insurance Research Organising committee . \n \nThis year's conference is in Brussels from 18 - 21 September 2012 . Despite the fact that Brussels is actually in Belgium the UK actuaries will travel all the way to enjoy good beer and great talks. \n \nOn Wednesday morning I will run a session on Using R in insurance . It would be great to see some of you there. \n \n  \nI prepared the slides with R , RStudio , knitr , pandoc and slidy again . My title page shows a word cloud about the GIRO conference . It uses the wordcloud package and was inspired by Ian Fellows' post on FellStats . \n \nThe last slide shows the output of sessionInfo(). I am sure it will become helpful one day, when I have to remind myself how I actually created the slides and which packages and versions I used."], "link": "http://lamages.blogspot.com/feeds/866429619189312817/comments/default", "bloglinks": {}, "links": {"http://www.w3.org/": 1, "http://johnmacfarlane.net/": 1, "http://freespace.virgin.net/": 3, "http://feedads.doubleclick.net/": 2, "http://yihui.name/": 1, "http://www.org.uk/": 3, "http://blog.fellstat.com/": 1, "http://www.r-project.org": 1, "http://www.fellstat.com": 1, "http://www.rstudio.org": 1, "http://lamages.co.uk/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "mages' blog"}, {"content": ["At last week's Royal Statistical Society (RSS) conference Hal Varian , Chief Economist at Google, gave a panel talk about 'Statistics at Google'. Could he get a better audience than the RSS? \n Hal talked about his career in academia and at Google. He reminded us of the days when Google was still a small start up with no real idea about how they could actually generate revenue. At that time Eric Schmidt asked him to 'take a look' at advertising because 'it might make us a little money'. Thus, Hal got involved in Google's ad auctions . \n \n  \n Hal Varian at the Royal Statistical Society conference 2012 \n \n Another projects Hal talked about was predicting the present . Predicting the present, or 'nowcasting', is about finding correlations between events. The idea is to forecast economic behaviour, which in return can help to answer when to run certain ads. He gave the example of comparing the search requests for 'vodka' (peaking Saturdays) with 'hangover' (peaking Sundays) using Google Insight . \n \n A newer idea is to use consumer surveys as revenue stream for publishers and Google. As publishers are struggling to get paid for their content, surveys are one way of engaging with the reader. Instead of getting money directly from them you ask them for their views/data on a topic that someone else is willing to pay for. \n So, the team of statisticians grew bigger and bigger over the years, as more and more colleagues sought their advice, and the little money turned into over $36.5bn of total advertising revenue in 2011 . This may help to put Hal's famous 2009 quote \"the sexy job in the next ten years will be statisticians\" into perspective. \n Robotics - the next sexy job However, it was a question from someone in the audience at the end of Hal's talk which resonated with me most. The person refered to Hal's comment on statistician being the sexy job for the next decade. Three years have already passed since Hal's statement, the gentleman said, and hence he wanted to know if Hal would be willing to share what he thinks the next sexy job will be. \"Robotics\", Hal said without hesitation. \n Hal argued that it becomes possible to connect data to the real world. As an example he mentioned the driverless car project at Google. Only a few years ago a driverless car had to learn everything about its environment using its sensors, while today driverless cars know what to expect thanks to information from Google Maps and Street View. So the car, equipped with GPS, anticipates in advanced when to expect a crossing, traffic light, etc. \n Sebastian Thrun , a professor at Stanford and Google Fellow, gave an insightful TED talk on that topic last year. It is amazing to see what driverless cars can do already. \n  I wouldn't be surprise if Hal had some ideas up his sleeves how to generate 'some little money' with robotics as well."], "link": "http://lamages.blogspot.com/feeds/293299816966810425/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://googleblog.co.uk/": 1, "http://feedads.doubleclick.net/": 2, "http://sec.gov/": 1, "http://robots.stanford.edu": 1, "http://www.org.uk/": 1, "http://en.wikipedia.org/": 1, "http://googleresearch.co.uk/": 1, "http://www.google.com/": 2}, "blogtitle": "mages' blog"}, {"content": ["Today I feel very lucky, as I have been invited to the Royal Statistical Society conference to give a tutorial on interactive web graphs with R and googleVis . \n   I prepared my slides with RStudio , knitr , pandoc and slidy , similar to my Cambridge R talk . You can access the RSS slides online here and you find the original R-Markdown file on github . You will notice some HTML code in the file, which I had to use to overcome my knowledge gaps of Markdown or its limitations. However, my output format will always be HTML, so that should be ok. To convert the Rmd-file into a HTML slidy presentation execute the following statements on the command line: \n Rscript -e \"library(knitr); knit('googleVis_at_RSS_2012.Rmd')\"\npandoc -s -S -i -t slidy --mathjax googleVis_at_RSS_2012.md \n -o googleVis_at_RSS_2012.html \n Maintaining R code formatting While working on my slides I came across the tidy option in knitr. Setting the option tidy to false will maintain the original formatting of the code. This is actually quite helpful when I have longer code chunks. Here is a little example where by default (tidy=TRUE) the tidied code will not fit into one line of my presentation slide: \n df"], "link": "http://lamages.blogspot.com/feeds/2762233131120290165/comments/default", "bloglinks": {}, "links": {"http://www.org.uk": 1, "http://www.w3.org/": 1, "http://johnmacfarlane.net/": 1, "http://dl.dropbox.com/": 3, "http://feedads.doubleclick.net/": 2, "http://yihui.name/": 1, "https://gist.github.com/": 1, "http://3.blogspot.com/": 1, "http://www.rstudio.org": 1, "http://lamages.co.uk/": 1}, "blogtitle": "mages' blog"}, {"content": ["The German news magazine Der Spiegel published a series of articles [ 1 , 2 ] around career developments. The stories suggest that career aspirations of young professionals today are somewhat different to those of previous generations in Germany. \n Apparently money and people management responsibility are less desirable for new starters compared to being able to participate in interesting projects and to maintain a healthy work life balance. Hierarchies are seen as a mean to an end, and should be more flexible, depending on requirements and skills sets. Similar to how they evolve in online communities and projects. \n I wonder if graduates in the developed world are just starting from a different level within the hierarchy of needs pyramid ? \n The RSA (Royal Society for the encouragement of Arts, Manufactures and Commerce) published an insightful video on the topic of motivation, arguing that money becomes less important if your work is more of a creative nature: \n \n Many thanks to Michael Bach for pointing out this animation. \n Understanding motivation is answering the 'Why?' question. And this question gets a different meaning the further you are up on the hierarchy of needs pyramid; or in other words, the more idle time you have. \n Simon Sinek gave an inspiring TED talk on that topic. He is coming from the marketing angle. Why are people buying your product? Think about it, a job is a product of a company 'bought' with the limited life time of its employees. Make sure, you are spending your life time well!"], "link": "http://lamages.blogspot.com/feeds/8267259455853713622/comments/default", "bloglinks": {}, "links": {"https://plus.google.com/": 1, "http://www.thrsa.org": 1, "http://www.spiegel.de/": 2, "http://en.wikipedia.org/": 1, "http://www.spiegel.de": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "mages' blog"}, {"content": ["Michael Bach , who is a professor and vision scientist at the University of Freiburg, maintains a fascinating site about visual illusions . One visual illusion really surprised me: the sigma motion . \n The sigma motion displays a flickering figure of black and white columns. Actually it is just a chart, as displayed below, with the columns changing backwards and forwards from black to white at a rate of about 30 transitions per second. \n   It can be a bit irritating to watch the movement, hence I use a button here to display and hide the animation. \n  \n Show/hide sigma motion \n Now, put your finger on the animated chart and move the finger to the left or right, so that you cover the chart in about 2 seconds and you will notice that the flickering stops and instead it appears that the columns move in the same direction as your finger! It works particular well on a touch screen device. \n You find more information, details and another implementation on Michael's page . Below is the little R script which produced the charts/GIFs used in this post. \n R code to replicate plots library(animation)\nfps"], "link": "http://lamages.blogspot.com/feeds/4974772017607657779/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://michaelbach.de/": 3, "http://feedads.doubleclick.net/": 2, "http://www.uniklinik-freiburg.de/": 1}, "blogtitle": "mages' blog"}, {"content": ["The next version of the googleVis package has been released on the project site and CRAN . \n This version provides updates to the package vignette and a new example for the gvisMerge function. The new sections of the vignette have been featured on this blog in more detail earlier: \n Using googleVis with knitr ( Link to post ) \n \n Using Rook with googleVis ( Link to post ) \n \n Using Reduce with gvisMerge to display several charts on a page ( Link to post ) \n \n Additionally two little bugs were fixed: \n Data frames with one row only were not displayed in a chart. Thanks to Oliver Jay and Wai Tung Ho for reporting this issue. \n \n The example in the gvisGeoChart help file, which scraps data from http://www.iris.edu/seismon/last30.html to display earth quakes of the last 30 days with magnitude \u2265 4.0, didn't display the magnitude correctly. The variable Mag was read as a factor rather than numeric. Thanks to Jason Law, who highlighted this point. Here is the now working example. \n \n Loading \n The size of the bubbles reflect the magnitude and the colour shows the depth of the earthquakes in km . \n ## Plot world wide earth quakes of the last 30 days with magnitude >= 4.0 \nlibrary(XML)\nlibrary(googleVis)\n## Get earthquake data of the last 30 days\neq \n \n Setting the region argument to 'IR' highlights the recent (11 August 2012) earth quake in north-west Iran near Tabriz and Ahar, which caused the death of hundreds and injured thousands.\n Loading\n IR  sessionInfo()\n## R version 2.15.1 (2012-06-22)\n## Platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit)\n## \n## locale:\n## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\n## \n## attached base packages:\n## [1] stats  graphics grDevices utils  datasets methods base  \n## \n## other attached packages:\n## [1] googleVis_0.2.17 RJSONIO_0.98-1 XML_3.9-4  \n## \n## loaded via a namespace (and not attached):\n## [1] tools_2.15.1"], "link": "http://lamages.blogspot.com/feeds/1325013472320553252/comments/default", "bloglinks": {}, "links": {"http://www.iris.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://yihui.name/": 1, "http://code.google.com/": 2, "http://lamages.co.uk/": 3, "http://cran.r-project.org/": 3}, "blogtitle": "mages' blog"}, {"content": ["The 100m mean's sprint finals of the 2012 London Olympics are over and Usain Bolt won the gold medal again with a winning time of 9.63s. Time to compare the result with my forecast of 9.68s, posted on 22 July . \n \n  \nMy simple log-linear model predicted a winning time of 9.68s with a prediction interval from 9.39s to 9.97s. Well, that is of course a big interval of more than half a second, or \u00b13%. Yet, the winning time was only 0.05s away from my prediction. That is less than 1% difference. Not bad for such a simple model. \n \nComments on my earlier post suggested to look into other parameters as well, such as track, weather and wind, or the times of the other medal winners. Others thought to focus on the recent past performance of the participants, rather than historical times over the last 100 years. \n \nInterestingly enough the Economist published an article on the same subject ( Faster, higher, no longer ) in its current print edition (following my example :-?). The article uses data back to 1912 as well and considers wind and altitude as critical parameters. \n \nThe Olympic games are not over yet, and there are more opportunities to forecast results. Rob J. Hyndman lists further interesting examples of Olympic models and predictions on his blog , and my colleague Matt Malin presents ideas to model the 100m butterfly men's swimming final on his site. \n \n R code used in this post ## 100m men's sprint historical winning times\n## Sourced from:\n## http://www.databaseolympics.com/sport/sportevent.htm?enum=110&sp=ATH\n\ngolddata =1900 & Year"], "link": "http://lamages.blogspot.com/feeds/1020042383325674721/comments/default", "bloglinks": {}, "links": {"http://www.economist.com/": 1, "http://www.co.uk": 1, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1, "http://robjhyndman.com/": 1, "http://robjhyndman.com": 1, "http://www.co.uk/": 1, "http://lamages.co.uk/": 1}, "blogtitle": "mages' blog"}, {"content": ["What is Rook? Rook is a web server interface for R , written by Jeffrey Horner , the author of rApache and brew . But unlike other web frameworks for R, such as brew, R.rsp (which I have used in the past 1 ), Rserve , gWidgetWWWW or sumo (which I haven't used yet) Rook appears incredible lightweight. \n Rook doesn't need any configuration. It is an R package, which works out of the box with the R HTTP server (R \u2265 2.13.0 required). That means no configuration files are needed. No files have to be placed in particular folders, and I don't have to worry about access permissions. Instead, I can run web applications on my local desktop. \n \n  \n Screen shot of a Rook app running in a browser \n Web applications have the big advantage that they run in a browser and hence are somewhat independent of the operating systems and installed software. All I need is R and a web browser. But here is the catch: I have to learn a little bit about the HTTP protocol to develop Rook apps. \n Rooks feels like chatting to the HTTP server in its native tongue, while the web frameworks of brew and R.rsp feel more like sending emails in a knitr / Sweave like format to the server, mixing HTML with R code, to which it replies in pure HTML. \n Fortunately Rook comes with some great examples to get you started. You find them in the following folder, after you installed Rook: system.file(\"exampleApps\", package=\"Rook\") . \n Additionally, the slides Building R Web Applications with Rook presented by Jeffrey Horner at the 2012 useR! conference and the talk about Accelerating Model Deployment with Rook given by Jean-Robert Avettand-Fenoel at LondonR in September 2011 provide some great insight and examples. \n Using googleVis with Rook Here is my first Rook app with googleVis , see code below. It displays a little R data frame in a googleVis table by default. The user can change the visualisation by clicking on the Edit me! button and upload her/his own CSV-file, see screen shot above. And as the output of a googleVis function is HTML code already, it is easy to create a web app with it. \n Imagine that I would add R code to validate the CSV-file, or even allow Excel-files, or carry out a statistical analysis before I display the data, and suddenly we are talking about a serious web app, with all R code hidden. Even the R-phobics would be happy to use it. \n Rook rocks with googleVis ! \n 1. For more information about using googleVis with brew and R.rsp, see the googleVis package vignette . \u21a9"], "link": "http://lamages.blogspot.com/feeds/3032394349773213386/comments/default", "bloglinks": {}, "links": {"http://feeds.feedburner.com/blog": 2, "http://www.londonr.org/": 1, "http://www.jstatsoft.org/": 1, "http://rapache.net/": 1, "http://code.google.com/": 2, "https://docs.google.com/": 1, "http://yihui.name/": 1, "http://www.rforge.net/": 1, "http://en.wikipedia.org/": 2, "http://www.r-project.org": 1, "https://github.com/": 1, "http://journal.r-project.org/": 1, "http://jeffreyhorner.tumblr.com/": 1, "http://4.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://cran.r-project.org/": 4}, "blogtitle": "mages' blog"}, {"content": ["It is less than a week before the 2012 Olympic games will start in London. No surprise therefore that the papers are all over it, including a lot of data and statistis around the games. \n The Economist investigated the potential financial impact on sponsors (some benefits), tax payers (no benefits) and the athletes (if they are lucky) in its recent issue and video. \n  The Guardian has a whole series around the Olympics , including the data of all Summer Olympic Medallists since 1896 .  100m men final The biggest event of the Olympics will be one of the shortest: the 100 metres men final. It will be all over in less than 10 seconds. In 1968 Jim Hines was the first gold medal winner, who achieved a sub-ten-seconds time and since 1984 all gold medal winners have run faster than 10 seconds. The historical run times of the past Olympics going back to 1896 are available from databasesport.com . Looking at the data it appears that a simple log-linear model will give a reasonable forecast for the 2012 Olympic's result (ignoring the 1896 time). Of course such a model doesn't make sense forever, as it would suggest that future run-times will continue to shrink. Hence, some kind of logistics model might be a better approach, but I have no idea what would be a sensible floor for it. Others have used ideas from extreme value theory to investigate the 100m sprint, see the paper by Einmahl and Smeets, which would suggest a floor greater than 9 seconds.  \n \n  \n Historical winning times for the 100m mean final. \nRed line: log-linear regression, black line: logistic regression. \n My simple log-linear model forecasts a winning time of 9.68 seconds, which is 1/100 of a second faster than Usain Bolt's winning time in Beijing in 2008, but still 1/10 of a second slower than his 2009 World Record (9.58s) in Berlin. Never-mind, I shall stick to my forecast. The 100m final will be held on 5 August 2012. Now even I get excited about the Olympics, and be it for less than 10 seconds. R code Here is the R code used in this the post: library(XML)\nlibrary(drc) \nurl =1900), fct = L.4())\nlog.linear =1900)) \nyears Update 5 August 2012 You find a comparison of my forecast to the final outcome of Usain Bolt's winning time of 9.63s on my follow-up post ."], "link": "http://lamages.blogspot.com/feeds/4735546855586079589/comments/default", "bloglinks": {}, "links": {"http://www.economist.com/": 1, "http://arno.uvt.nl/": 1, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1, "http://www.databasesports.com/": 1, "https://docs.google.com/": 1, "http://www.co.uk/": 1, "http://lamages.co.uk/": 1}, "blogtitle": "mages' blog"}, {"content": ["The other day I saw a fantastic exhibition of work by Bridget Riley . Karsten Schubert , who is Riley's main agent , has a some of her most famous and influential artwork from 1960 - 1966 on display, including the seminal Moving Squares from 1961. \n \n  \n Photo of Moving Squares by Bridget Riley, 1961 \nEmulsion on board, 123.2 x 121.3cm \n In the 1960s Bridget Riley created some great black and white artwork, which at a first glance may look simple and deterministic or sometimes random, but has fascinated me since I saw some of her work for the first time about 9 years ago at the Tate Modern . \n Her work prompted a very simple question to me: When does a pattern appear random? As human beings most of our life is focused on pattern recognition. It is about making sense of the world around us, being able to understand what people are saying; seeing lots of different things and yet knowing when something is a table and when it is not. No surprise, I suppose, that pattern recognition is such a big topic in statistics and machine learning. \n Neil Dodgson published some interesting research papers related to the work by Bridget Riley. In a paper from 2008 he concluded that even limited randomness can produce a great deal of complexity, such as in Riley's Fragment 6/9 , and hence might be so appealing to the pattern detection systems of our brains. I wonder, if it may also help to explain why some people can spend hours looking at stock market charts. \n Of course I couldn't resist trying to reproduce the Moving Squares in R. Here it is: \n ## Inspired by Birdget Riley's Moving Squares\nx \n \n However, what may look similar on screen is quite different when you see the actual painting. Thus, if you are in London and have time, make your way to the gallery in Soho. I recommend it!"], "link": "http://lamages.blogspot.com/feeds/6949435232724902090/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.karstenschubert.com/": 3, "http://3.blogspot.com/": 1, "http://www.ac.uk/": 3, "http://www.org.uk/": 3}, "blogtitle": "mages' blog"}, {"content": ["The second Cologne R user meeting took place last Friday, 6 July 2012, at the Institute of Sociology . Thanks to Bernd Wei\u00df , who provided the meeting room, we didn't have to worry about the infrastructure, like we did at our first gathering . \n Again, we had an interesting mix of people turning up, with a very diverse background from chemistry to geo-science, energy, finance, sociology, pharma, physics, psychology, mathematics, statistics, computer science, telco, etc. Yet, the gender mix was still somewhat biased, with only one female attendee. \n We had two fantastic talks by Bernd Wei\u00df and Stephan Sprenger . Bernd talked about Emacs' Org-Mode and R. He highlighted the differences between literate programming and reproducible research and if you follow his slides, you get the impression that Emacs with Org-mode is the all-singing-all-dancing editor, or for those who speak German: eine eierlegende Wollmilchsau . \n  In the second talk Stephan presented the Deducer package . Deducer is an intuitive, cross-platform graphical data analysis system and works best with JGR . Stephan focused on Deducer as a graphical user interface to ggplot2 . I believe most of us were in awe to see how easy it was with Deducer to generate ggplot2 charts and the underlying R statements. Stephan was brave enough to give live demos and to show that some of Deducer's power lies in its drag and drop user interface design. For more details see also his blog entry . \n  At the end there was still time for a drink or two, and even some food (I know, some say: \"Eating is cheating.\"). Luckily the weather was on our side, and we could enjoy our drinks in the beer-garden. \n   The next meeting is scheduled in three month time at 5 October 2012. Watch out our Meetup site for further announcements."], "link": "http://lamages.blogspot.com/feeds/145739989312737280/comments/default", "bloglinks": {}, "links": {"https://dl.dropbox.com/": 1, "https://github.com/": 1, "http://www.dw.de/": 1, "http://www.jstatsoft.org/": 1, "http://photos2.meetupstatic.com/": 1, "http://feedads.doubleclick.net/": 2, "http://berndweiss.net/": 1, "http://berndweiss.net": 1, "http://fibosworld.wordpress.com/": 2, "http://cran.r-project.org/": 3, "http://www.openstreetmap.org/": 1, "http://lamages.co.uk/": 1, "http://www.meetup.com/": 2}, "blogtitle": "mages' blog"}, {"content": ["At the R in Finance conference Paul Teetor gave a fantastic talk about Fast(er) R Code . Paul mentioned the common higher-order function Reduce , which I hadn't used before. \n Reduce allows me to apply a function successively over a vector. \n What does that mean? Well, if I would like to add up the figures 1 to 5, I could say: add or Reduce(add, 1:5) Now this might not sound exciting, but Reduce can be powerful. Here is an example with googleVis . To merge two charts I use the function gvisMerge , which takes two charts and wraps them up in an HTML table. Hence, to create a page with a line-, column- area- and bar chart I could use: \n \n pp \n \nor use Reduce instead: \n \n pr \n Loading ... \n library(googleVis)\ndata(OpenClose)\nops"], "link": "http://lamages.blogspot.com/feeds/2003386335327091774/comments/default", "bloglinks": {}, "links": {"http://www.rinfinance.com/": 2, "http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1, "http://quanttrader.info/": 1, "http://stat.ethz.ch/": 1}, "blogtitle": "mages' blog"}, {"content": ["This post is a quick reminder that the next Cologne R user group meeting is only one week away. We will meet on 6 July 2012. The meeting will kick off at 18:00 with three short talks at the Institute of Sociology and will continue, even more informal, from 20:00 in a pub ( LUX ) nearby. All details are available on the K\u00f6lnRUG Meetup site . Please sign up if you would like to come along. Notes from the first Cologne R user group meeting are available here . \n   \n View Larger Map"], "link": "http://lamages.blogspot.com/feeds/6146017198927795730/comments/default", "bloglinks": {}, "links": {"http://www.restaurant-lux.com": 1, "http://feedads.doubleclick.net/": 2, "http://files.meetup.com/": 1, "http://maps.co.uk/": 1, "http://www.openstreetmap.org/": 1, "http://lamages.co.uk/": 1, "http://www.meetup.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["One of the great research papers of the 20th century celebrates its 60th anniversary in a few weeks time: A quantitative description of membrane current and its application to conduction and excitation in nerve by Alan Hodgkin and Andrew Huxley . Only a few weeks after Andrew Huxley died, 30th May 2012 , aged 94. \n In 1952 Hodgkin and Huxley published a series of papers , describing the basic processes underlying the nervous mechanisms of control and the communication between nerve cells, for wich they received the Nobel prize in physiology and medicine, together with John Eccles in 1963. \n Their research was based on electrophysiological experiments carried out in the late 1940s and early 1950 on a giant squid axon to understand how action potentials in neurons are initiated and propagated. \n \n From their experiments they derived a mathematical model which approximates the electrical characteristics of excitable cells such as neurons. Their key idea was to regard the cell membran and the various ion currents as an electrical circuit made up of capacitors, resistors and batteries. \n \nMembrane Circuit by Nrets, Source: Wikipedia , License: CC-BY-SA \n The capacitive current across the cell membrane can be described as the sum of changes in the membrane voltage \\(V_m\\) and ion currents caused primarily by sodium (Na) and potassium (K) and other leakages (L), mainly chloride ions. The ion currents are defined by their conductances (\\(g\\), with the Na and K conductances being voltage depended), their equilibrium potentials (\\(E\\)) and how the channel gates open and close (\\(m, n, h\\)): \n\\[ \\begin{aligned} \nI_{ext} &= C_m \\frac{dV_m}{dt} + I_{ion} \\\\ \n& = C_m \\frac{dV_m}{dt} + g_{Na} m^3 h(V-E_{Na}) + g _K n^4 (V - E_K) + g_L (V - E_L) \n\\end{aligned} \n\\] \nThe standard Hodgkin-Huxley model expands into a set of four differential equations: \n \\[ \\begin{aligned} \nC \\frac{dV}{dt} & = I - g_{Na} m^3 h(V-E_{Na}) - g _K n^4 (V - E_K) -g_L (V - E_L)\\\\ \n\\frac{dm}{dt} & = a_m(V) (1 - m) - b_m(V)m\\\\ \n\\frac{dh}{dt} & = a_h(V)(1 - h) - b_h(V)h\\\\ \n\\frac{dn}{dt} & = a_n(V) (1 - n) - b_n(V)n\\\\ \n\\\\ \na_m(V) & = 0.1(V+40)/(1 - \\exp(-(V+40)/10)) \\\\ \nb_m(V) & = 4 \\exp(-(V + 65)/18) \\\\ \na_h(V) & = 0.07 \\exp(-(V+65)/20) \\\\ \nb_h(V) & = 1/(1 + \\exp(- (V + 35)/10))\\\\ \na_n(V) & = 0.01 (V + 55)/(1 - \\exp(-(V+55)/10))\\\\ \nb_n(V) & = 0.125 \\exp(-(V + 65)/80) \n\\end{aligned} \n\\] \n To model and simulate those differential equations in R I use the simecol package. The package comes with a great vignette which gives a good introduction, and you may also find my recent LondonR talk helpful. library(simecol)\n## Hodkin-Huxley model\nHH \n \n Let's run and plot the model: \n HH \n \n \n From the initial state I observe a tiny stimulus which reverts quickly back to the resting state. So let's increase the external stimulus in steps to observe its impact on the membrane voltage:\n times(HH)[\"to\"] \n \n  \n Now this is exciting, as I increase the external stimulus an action potential is generated. Increasing the stimulus further results in a constant firing of the neuron, or in other words, the model is going through a\n Hopf-bifurcation .\n I think it is absolutely remarkable what Hodgkin and Huxley achieved 60 years ago. They were able to demonstrate that numerical integration of their model could reproduce all the key biophysical properties of the action potential. And what seems so easy on a computer with R or other software, such as XPP/XPPAUT today, must have been a lot of work in the late 40s, early 50s of the 20th century."], "link": "http://lamages.blogspot.com/feeds/8791671551766823832/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://simecol.r-project.org/": 1, "http://www.cnsorg.org/": 1, "http://www.pitt.edu/": 1, "http://www.sfn.org/": 1, "http://feedads.doubleclick.net/": 2, "http://creativecommons.org/": 1, "http://www.scholarpedia.org/": 1, "http://www.nobelprize.org/": 1, "http://lamages.blogspot.com/": 1, "http://www.nih.gov/": 1, "http://en.wikipedia.org/": 2, "http://nelson.illinois.edu/": 1, "http://upload.wikimedia.org/": 1, "http://2.blogspot.com/": 1, "http://www.co.uk/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "mages' blog"}, {"content": ["This evening I will talk about Dynamical systems in R with simecol  at the LondonR meeting. \n Thanks to the work by Thomas Petzoldt, Karsten Rinke, Karline Soetaert and R. Woodrow Setzer it is really straight forward to model and analyse dynamical systems in R with their deSolve and simecol packages. \n I will give a brief overview of the functionality using a predator-prey model as an example. \n \n This is of course a repeat of my presentation given at the K\u00f6ln R user group meeting in March . \n For a further example of a dynamical system with simecol see my post about the Hodgkin-Huxley model , which describes the action potential of a giant squid axon. \n I shouldn't forget to mention the other talks tonight as well: Writing R for Dummies - Andrie De Vries \n News from data.table 1.6, 1.7 and 1.8 - Matthew Dowle \n Converting S Plus Applications into R - Andy Nicholls (postponed to 18 September 2012) \n \n For more information about venue and timing see the LondonR web site ."], "link": "http://lamages.blogspot.com/feeds/3556668677577680638/comments/default", "bloglinks": {}, "links": {"http://desolve.r-project.org/": 1, "http://www.londonr.org/": 3, "http://feedads.doubleclick.net/": 2, "http://simecol.r-project.org/": 2, "http://lamages.blogspot.com/": 1, "http://londonr.org": 1, "http://lamages.co.uk/": 1, "http://www.meetup.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["Transforming data sets with R is usually the starting point of my data analysis work. Here is a scenario which comes up from time to time: transform subsets of a data frame, based on context given in one or a combination of columns. \n As an example I use a data set which shows sales figures by product for a number of years: df \n \n I am interested in absolute and relative sales developments by product over time. Hence, I would like to add a column to my data frame that shows the sales figures divided by the total sum of sales in each year, so I can create a chart which looks like this:\n \n There are lots of ways of doing this transformation in R. Here are three approaches using: \n base R with by , \n ddply of the plyr package, \n data.table of the package with the same name. \n by The idea here is to use by to split the data for each year and to apply the transform function to each subset to calculate the share of sales for each product with the following function: fn \n \nHaving defined the function fn I can apply it in a by statement, and as its output will be a list, I wrap it into a do.call command to row-bind ( rbind ) the list elements: R1 \n \n ddply Hadely's plyr package provides an elegant wrapper for this job with the ddply function. Again I use the transform function with my self defined fn function: library(plyr)\nR2 \n \n data.table With data.table I have to do a little bit more legwork, in particular I have to think about the indices I need to use. Yet, it is still straight forward: library(data.table)\n## Convert df into a data.table\ndt \n Although data.table may look cumbersome compared to ddply and by , I will show below that it is actually a lot faster than the two other approaches. Plotting the results With any of the three outputs I can create the chart from above with latticeExtra : library(latticeExtra)\nasTheEconomist(\n xyplot(Sales + Share ~ Year, groups=Product, \n data=R3, t=\"b\", \n scales=list(relation=\"free\",x=list(rot=45)), \n auto.key=list(space=\"top\", column=3),\n main=\"Product information\")\n) \n Comparing performance of by, ddply and data.table Let me move on to a more real life example with 100 companies, each with 20 products and a 10 year history: set.seed(1)\ndf \n I use the same three approaches to calculate the share of sales by product for each year and company, but this time I will measure the execution time on my old iBook G4, running R-2.15.0 : r1 \n \nAnd here are the results:\n r1 # by\n## user system elapsed \n## 13.690 4.178 42.118 \nr2 # ddply \n## user system elapsed \n## 18.215 6.873 53.061\nr3 # data.table \n## user system elapsed \n## 0.171 0.036 0.442 \n It is quite astonishing to see the speed of data.table in comparison to by and ddply , but maybe it shouldn't be surprise that the elegance of ddply comes with a price as well. \n Addition (13 June 2012): See also Matt's comments below. I completely missed ave from base R, which is rather simple and quick as well. Additionally his link to a stackoverflow discussion provides further examples and benchmarks.\n Finally my session info: > sessionInfo() # iBook G4 800 MHZ, 640 MB RAM\nR version 2.15.0 Patched (2012-06-03 r59505)\nPlatform: powerpc-apple-darwin8.11.0 (32-bit)\n\nlocale:\n[1] C\n\nattached base packages:\n[1] stats  graphics grDevices utils  datasets methods base  \n\nother attached packages:\n[1] latticeExtra_0.6-19 lattice_0.20-6  RColorBrewer_1.0-5 \n[4] data.table_1.8.0 plyr_1.7.1   \n\nloaded via a namespace (and not attached):\n[1] grid_2.15.0"], "link": "http://lamages.blogspot.com/feeds/5025958134120367418/comments/default", "bloglinks": {}, "links": {"http://datatable.r-project.org/": 2, "http://latticeextra.r-project.org/": 1, "http://stackoverflow.com/": 1, "http://feedads.doubleclick.net/": 2, "http://plyr.co.nz/": 2, "http://lamages.blogspot.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["A new version of googleVis has been released on CRAN and the project site . Version 0.2.16 adds the functionality to plot quarterly and monthly data as a motion chart. \n To illustrate the new feature I looked for a quarterly data set and stumbled across the quarterly UK house price data published by Nationwide , a building society. The data is available in a spread sheet format and presents the average house prices and indexed to 100 in Q1 1993 by region in the UK from Q4 1973 to Q1 2012. Unfortunately the data is formated for human eyes rather than for computers, see the screen shot below. \n \n  \n Screen shot of Nationwide's UK house price data in Excel \n Never-mind, the XLConnect package by Mirai Solutions does a fabulous job in reading Excel files into R. An advantage of XLConnect, compared to other packages, is that it also works on a Mac, although I had to install the package from source ( install.packages(\"XLConnect\", type=\"source\") ). \n To visualise the data in a motion chart it needs a certain structure: The time dimension has to be in one column, the regions in another and all measurements in further columns. And as I use quarterly data, I must ensure that the time dimension is given in a format understood by the Google API , which means YYYYQq, e.g. '2012Q1', rather than 'Q1 2012' as in the Nationwide data. Once I have achieved this, see the R code for those transformations below, I can create the motion charts easily. \n &lt;p&gt;Loading ...&lt;/p&gt; \nThe motion chart of price index vs. actual amounts shows some interesting patterns, in particular how the relativity between the regions has widened, the impact of the financial crisis in 2008 and its severity on Northern Ireland. \n I can investigate the data further by changing the view of the motion chart into a line chart. \n &lt;p&gt;Loading ...&lt;/p&gt; \nYorkshire & Humberside seems to have deteriorate the most over the past 39 years. The same region was still in line with the UK average 20 years ago, but since then it has dropped to 81% in Q1 2012. Of course living in London has always been more expensive than in other regions, in some cases it is twice as expensive to buy a property in London than elsewhere today. Anna Powell-Smith developed a nice application analysing the correlation between house prices and train times to London in more detail. \n Switching the chart to an index basis amplifies the housing bubble in Northern Ireland. The index reached 652 in Q2 2007 and has since crashed to 317 and unlike in other regions its downward trend hasn't stopped yet. \n&lt;p&gt;Loading ...&lt;/p&gt; \n The last downturn in the UK lasted from Q4 1989 to Q1 1993 and if history would repeat itself, it would suggest that prices might raise again soon. However, Robert Gardner, chief economist at Nationwide, was quoted in the FT a few weeks ago saying that \"The challenging economic backdrop suggests that a significant acceleration in prices or activity is unlikely near term\". \n Last but not least, a traditional lattice plot of the house price indices for the readers on iOS ;-) \n \n As usual, here is the R code of this post."], "link": "http://lamages.blogspot.com/feeds/2222202337146987011/comments/default", "bloglinks": {}, "links": {"http://darkgreener.com/": 1, "http://www.blogger.com/": 1, "http://annapowellsmith.com/": 1, "http://feedads.doubleclick.net/": 2, "https://developers.google.com/": 1, "http://cran.r-project.org/": 1, "http://www.ft.com/": 1, "http://www.mirai-solutions.com/": 1, "http://2.blogspot.com/": 1, "http://www.co.uk/": 3, "http://code.google.com/": 1}, "blogtitle": "mages' blog"}, {"content": ["Tonight I will give a talk at the Cambridge R user group about googleVis . Following my good experience with knitr and RStudio to create interactive reports, I thought that I should try to create the slides in the same way as well. \n Christopher Gandrud's recent post reminded me of deck.js , a JavaScript library for interactive html slides, which I have used in the past , but as Christopher experienced, it is currently not that straightforward to use with R and knitr . \n Thus, I decided to try slidy in combination with knitr and pandoc . And it worked nicely. \n I used RStudio again to edit my Rmd-file and knitr to generate the Markdown md-file output. Following this I run pandoc on the command line to convert the md-file into a single slidy html-file: \n pandoc -s -S -i -t slidy --mathjax Cambridge_R_googleVis_with_knitr_and_RStudio_May_2012.md -o Cambridge_R_googleVis_with_knitr_and_RStudio_May_2012.html Et voli\u00e0, here is the result : \n   Addition (2 June 2012) \n Oh boy, knitr and Markdown are hitting a nail. With slidify by Ramnath Vaidyanathan another project sprung up to ease the creation of web presentations."], "link": "http://lamages.blogspot.com/feeds/3739326214560078230/comments/default", "bloglinks": {}, "links": {"http://www.w3.org/": 1, "http://rstudio.org": 1, "http://christophergandrud.co.uk/": 1, "http://dl.dropbox.com/": 2, "https://github.com/": 1, "http://johnmacfarlane.net/": 1, "http://feedads.doubleclick.net/": 2, "http://www.org.uk/": 1, "http://ramnathv.github.com/": 1, "http://feeds.feedburner.com/blog": 1, "http://code.google.com/": 1, "http://www.rstudio.org/": 1, "http://imakewebthings.com/": 1, "https://gist.github.com/": 1, "http://lamages.co.uk/": 2}, "blogtitle": "mages' blog"}, {"content": ["John D. Cook gave a great talk about ' Why and how people use R '. The talk resonated with me and highlighted why R is such a great tool for end user computing. A topic which has become increasingly important in the European insurance industry.  John's main point on why people use R is that R gets the job done and I think he is spot on. Of course that's the trouble with R sometimes as well, or to quote Bo again: \n \"The best thing about R is that it was developed by statisticians. \n\"The worst thing about R is that it was developed by statisticians.\" \nBo Cowgill, Google \n Indeed R is frequently used by individuals who commission their own work, rather than by professional programmers who develop tools for others. Or in other words R is mainly used for end user computing. And more often than not R users don't use the software for big monster projects but to find answers to their own questions, and then one answer leads to another questions which eventually leads to insight. \n John also points out that R is often not learned like other programming languages by reading a book about the language definition, control structures, etc. but by learning statistics and using R as a teaching tool. Indeed, wasn't that one of the original motivations of Ross and Robert , the creators of R, apart from getting their own research done? Probably very few R users have ever read the R Language Definition from cover to cover. \n To some extend the same arguments are true for spread sheet software as well, in particular that it gets the job done and that most people learn it by using it. Yet there is one fundamental difference between spread sheets and R, or other languages. Programming languages like R are based on plain text files and that is quite a big deal, if you want to manage end user computing. \n I realised the power of text files when I started my first R package . Suddenly I had to think about version control , documentation and testing and as a result was forced to think a little bit like a professional programmer. Still what I was doing was end user computing, I was doing all of this to get my own work done. However I realised that I had to manage my code better. In the past I thought documentation is for people without talent. That's actually quite arrogant. I have heard others say that they don't like documenting their work because it takes away their magic. But here is the deal, over time even I will turn into a another person and then the documentation becomes incredible helpful, not even mentioning version control. \n Funnily enough the experience I gained in building packages was quite helpful for my job as well. The insurance industry is going through a huge transition in Europe. A new regulatory regime called Solvency II is being rolled out. This changes the way how insurance companies have to asses their capital requirements. A lot of work is required around data management and end user computing to ensure that certain standards and audit criteria are met. Most of this actually good practice anyhow, see Solvency II Data Audit Appendix 2, Table 5.1. \n Open source communities had to overcome those challenges in the past already: How do you organise work across multiple teams? How do you define interfaces? How do you deal with security, incident management, documentation, testing, roll out, etc.? \n The R documentation on writing R extensions answers those questions and offers a blue print and framework for end user computing. Over 3800 packages on CRAN demonstrate the success of this approach. And I believe that most of the packages are the result of end user computing. So maybe this is actually the biggest deal about R that it is build successfully on end user computing. \n Finally, here are some horror spread sheet stories , and if you need to talk to your IT department about end user computing, see Neil McBride's ideas on how you can lead the discussions ."], "link": "http://lamages.blogspot.com/feeds/2756980631368501040/comments/default", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 1, "http://betterexplained.com/": 1, "http://feedads.doubleclick.net/": 2, "http://biostat.vanderbilt.edu/": 1, "http://www.r-project.org/": 1, "http://www.ac.uk/": 2, "http://www.eusprig.org/": 1, "http://channel9.msdn.com/": 1, "http://en.wikipedia.org/": 4, "http://code.google.com/": 1, "http://www.lloyds.com/": 1, "http://cran.r-project.org/": 3}, "blogtitle": "mages' blog"}]