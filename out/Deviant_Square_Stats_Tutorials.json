[{"blogurl": "http://www.statsmakemecry.com/smmctheblog\n", "blogroll": [], "title": "Deviant Square Stats Tutorials"}, {"content": ["To start, let me say that this is not a Stats Make Me Cry original, but rather something I came across and found very interesting. If you are interested in the topic, please read the preview and follow the link below to the author's site ( The Analysis Factor ) for the full article: \n \n The intercept (often labeled the constant) is the expected mean value of Y when all X=0. \n Start with a regression equation with one predictor, X. \n If X sometimes = 0, the intercept is simply the expected mean value of Y at that value. \n If X never = 0, then the intercept has no intrinsic meaning. In scientific research, the purpose of a regression model is to understand the relationship between predictors and the response. If so, and if X never = 0, there is no interest in the intercept. It doesn\u2019t tell you anything about the relationship between X and Y. \n You do need it to calculate predicted values, though. In market research, there is usually more interest in prediction, so the intercept is more important here... \n Read the rest of Karen's article here..."], "link": "http://www.statsmakemecry.com/smmctheblog/2012/10/29/interpreting-the-intercept-in-a-regression-model-repost.html", "bloglinks": {}, "links": {"http://www.theanalysisfactor.com/": 2}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["To start, let me say that this is not a Stats Make Me Cry original, but rather something I came across and found very interesting. If you are interested in the topic, please read the preview and follow the link below to the author's site ( Carlisle Rainey ) for the full article: \n\n \n\nIt is quite common in political science for researchers to run statistical models, find that a coefficient for a variable is not statistically significant, and then claim that the variable \"has no effect.\" This is equivalent to proposing a research hypothesis, failing to reject the null, and then claiming that the null hypothesis is true (or discussing results as though the null hypothesis is true). This is a terrible idea. Even if you believe the null, you shouldn't use p > 0.05 as evidence for your claim. In this post, I illustrate why.\n \nTo demonstrate why analysts should not conclude \"no effect\" from insignificant coefficients, I return to a debate waged over blogs and Twitter about a NYT article. See Seth Masket's original take, my response, and Seth's recasting. The data come from Nate Silver's post, which adopts a more nuanced position that I think is appropriate in light of the data. \n\n Read the rest of Carlisle's article here..."], "link": "http://www.statsmakemecry.com/smmctheblog/2012/6/25/why-you-shouldnt-conclude-no-effect-from-statistically-insig.html", "bloglinks": {}, "links": {"http://blog.carlislerainey.com/": 2}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["As some of you probably noticed, life doesn't always allow me to blog as often as I'd like. However, I don't want that to stop the flow and dissemination of info to my readers, especially when I'm constantly stumbling across great content in the \"blogosphere\", on a variety of stats topics! \n With this in mind, I'm thinking of adopting the practice of occasionally posting (in a limited way) a syndicated article that I think is especially interesting. To be clear, I intend to spread good content not steal someone's effort. To make this clear, my plan is to only post the title and a paragraph or two of the article, which will be followed by a link to the full article on the original blogger's site. \n \n What do you think? Do you think a curated stream of stats content that I particularly find interesting, sprinkled in among my normal blog posts would be a good idea?"], "link": "http://www.statsmakemecry.com/smmctheblog/2012/6/23/to-syndicate-or-not-to-syndicate-that-is-the-question.html", "bloglinks": {}, "links": {}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["Please tell me what stats terms you think are the most confusing! Please order the terms you choose, according to how confusing they are (with #1 being most confusing). The results will dictate what topics are covered in future blogs!\n \nBlog entries for Confusing Stats Terms #10, #9, and #8 are already posted, so I'm only asking for terms #7 through #1. Thanks for your input!\n http://www.statsmakemecry.com/confusing-stats-terms/ \n Related Content:\n \n \n Top Ten Confusing Stats Terms Explained in \u201cPlain English\u201d (#8: Residual) \n \n Top Ten Confusing Stats Terms Explained in \u201cPlain English\u201d (#9: Multicollinearity) \n \n Top Ten Confusing Stats Terms Explained in \u201cPlain English\u201d (#10: Standard Deviation)"], "link": "http://www.statsmakemecry.com/smmctheblog/2012/4/23/please-vote-on-the-top-confusing-stats-terms.html", "bloglinks": {}, "links": {"http://www.statsmakemecry.com/": 4}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["In today's blog entry, I will walk through the basics of conducting a repeated-measures MANCOVA in SPSS. I will focus on the most basic steps of conducting this analysis (I will not address some complex side issues, such as assumptions, power\u2026etc). If you find yourself with lingering questions after walking through this blog, feel free to leave questions in the \"comments\" section, or visit the MANCOVA section of my discussion forum to find answers and/or ask questions of your own. Full disclosure: the example data used is from the SPSS sample/help files, and it can be downloaded below.\n \n RIGHT-CLICK HERE AND \"SAVE AS FILE\" FOR SAMPLE DATA  \n \n Let's get started:  Repeated-Measures MANCOVA is used to examine how a dependent variable (DV) varies over time, using multiple measurements of that variable, with each measurement separated by a given period of time. In addition to determining whether the DV itself varies, a MANCOVA can also determine wether other variables are predictive of variability in the DV over time. If that wasn't crystal clear, don't worry, just keep reading. \n Related Content: Within-Subject and Between-Subject Effects... Discussion Forum: What stat tool\u2026 Discussion Forum: On the right track? Discussion Forum: MANCOVA Help  \n Repeated-Measures MANCOVA Example: \nIn our example, your local stats store Stats \"R\" Us launched a marketing campaign, with three different strategies ( variable name: promo ; value labels: Strategy A, Strategy B, Strategy C ). Stats \"R\" Us launched campaigns in markets of three different sizes ( variable name: mktsize ; value labels: Small, Medium, and Large ), and measured the sales in each store every three months over the course of one year (4 time points; variable names: sales.1 , sales.2 , sales.3 , and sales.4 ; see data below ).\n  NOTE: Sales are scaled in \"thousands\" (e.g. 70.63 is actually $70,630). Also, your data should be in person-level (a.k.a. \"wide\") format (as opposed to person-period, a.k.a. \"long\", format), meaning each row of data is a single case (store, in our example). If it were in person-period (long) format, each case (store) would have the number of rows equal to the number of repeated measures (four, in our example), because the repeated measures ( sales.1 , sales.2 , sales.3 , and sales.4 ) would be stacked to form a single variable ( Sales ). Here is a useful resource for converting data between the two forms: CLICK HERE FOR INFO ABOUT CONVERTING DATA FORMS. To begin your analysis using the SPSS drop-down menus, click on: Analyze > General Linear Model > Repeated Measures... (1, below)  In the Repeated Measures Define Factors dialogue window, do the following:  Replace the default Within-Subject Factor Name , which is factor1 , with your own name for the concept of time. I've chosen to use the name Time ( 1, below ).  Type the number of times your DV was measured (how many DV variables you have) in the Number of Levels box ( 2, below ) and click the \"Add\" button.  Choose a name for your DV (the variable that is measured repeatedly), and type it in the Measure Name box. I chose the name Sales ( 3, below ).  Click the \"Add\" button again ( 4, below ).  Click the \"Define\" button ( 5, below ).     In the Repeated Measures dialogue window that appears next, move the four sales variables ( 1, below ) to the Within-Subjects Variables (Time) box ( 2, below ). NOTE: Be sure that they stay in the same order (Sales.1, Sales.2, Sales.3, and Sales.4).   Next, move both promo and mktsize to the Between-Subjects Factor(s): box ( 1, below ). NOTE: Both promo and mktsize were placed into the Between-Subjects Factor(s) box because they are categorical variables (discrete variables). Continuous variables (scale variables) would go into the Covariates box ( 2, below ).   Next, click on the \"Model\" button ( 3, above ). In the Repeated Measures: Model dialogue window ( 1, below ), you can specify your model. In other words, you can choose which variables have \"main effects\" on the DV (individual predictors), and which variables might interact with each other to predict the DV. The default option is the Full factorial ( 2, below ), which will examine every variable's main effect, as well as every possible interaction among all variables.  We'll stick with Full factorial for today. However, if you wanted build your own model, you can choose Custom ( 1, below ), and then use the Build Term(s) tool ( 2, below ) to specify what kind of effects/interactions you want. Again, in this example, we'll stick with Full factorial ( 2, above ). To exit this dialogue window, click the \"Continue\" button.  Next, you'll need to click on the \"Contrasts\" button ( 1, below ). In the Repeated Measures: Contrasts dialogue window that appears, you can change each factor variable's type of contrast. I recommend leaving the Time variable with its default contrast \"Polynomial\" ( 2, below ), and changing both promo and mktsize to \"Simple\" and \"First\". To change each, you must select \"Simple\" from the list, click on \"First\", and then click on the \"Change\" button ( 3, below ).  Next, click on the \"Plots\" button ( 1, below ). In the Repeated Measures: Profile Plots dialogue window that appears ( 2, below ), you can choose what graphs you'd like to see. In repeated measures models, I like to produce plots with Time on the Horizontal Axis (x-axis; 3, below ) and my factor variables as Separate Lines ( 4, below ). NOTE: The reason you don't see anywhere to specify the vertical axis (y-axis), is that the DV (i.e. Sales) is assumed to be on the y-axis in this dialogue window. As you can see, in our example I've made a Time-by-Factor plot for each of the factors in our model ( promo and mktsize ).  If you'd like to get Post Hoc comparisons of the DV (comparing between each of the factor levels, respectively), click on the \"Post Hoc\" button. Once in the dialogue window:  Move the factors from the Factor(s) box ( 1, below ) to the Post Hoc Tests box ( 2, below ). Choose the type of Post Hoc test to use, and place a check-mark in its respective box (you can choose more than one). The most commonly used is Tukey's , which I've chosen below ( 3, below ). Click the \"Continue\" button.  I also recommend clicking on the \"Save\" button ( 1, below ), and choosing Predicted Values:Unstandardized ( 2, below ) and Residuals: Unstandardized ( 3, below ) in the Repeated Measures: Save dialogue window. NOTE: By checking these two boxes, your analysis will now produce two new variables in your dataset, called PRED_1 (Predicted) and RES_1 (Residual), which can be used to produce graphs after analysis (if you choose). We will not cover this in this tutorial.  Back at the main Repeated Measures dialogue, you can either click \"OK\" ( 1, below ), to execute the analysis, or you can click \"Paste\" ( 2, below ), to paste the analysis commands into a syntax window. I recommend choose the \"Paste\" option, as that will allow you to more easily re-create the analysis later.  Below is the syntax window, with the various commands of the analysis, which you specified while going through the dialogue windows. Over time, you may learn to use syntax exclusively, bypassing the need to use the dialogue windows. Learning syntax can dramatically improve your efficiency, especially when you need to create a lot of different types and/or iterations of analyses.  To execute the commands in the syntax, simply highlight all the text you want to run, and push the green play button ( 1, above ). Alternatively, you can use the menus: Run > Selection . Interpreting Output/Results There is a lot to digest in the output file that results from an analysis, so we'll stick to the basics. Below is the Descriptive Statistics table, which simply shows the Mean ( 1, below ), Standard Deviation (Std. Deviation; 2, below ), and sample size (N; 3, below ) for each DV, broken-down by all subgroups of your factors ( promo and mktsize ).  The next table we'll examine is the Mauchly's Test of Sphericity . This test essentially determines whether the variance of the difference between each pair of repeated measure (of your DV) is approximately equal. This is a bit of an over-simplification, but it'll work here. For our purposes, we just need to be concerned with whether it is significant or not ( 1, below ). If it is NOT significant (i.e. Sig. is greater than .05), then sphericity can be assumed (more on that soon). If it IS significant (i.e. Sig. is less than .05), then sphericity can not be assumed (more about why we care, in a moment). NOTE: I know I said earlier that we wouldn't deal with assumptions today, but this is an exception, because it directly determines how we interpret the next table... In our example, we CAN NOT assume sphericity (p=.003).  The reason we want to note whether sphericity can be assumed, is that it directly determines how we interpret our next table, the Tests of Within-Subjects Effects table ( 1, below ). For each effect in our model, there are four estimates present ( 2, below ). If sphericity CAN be assumed, then we can reference the first estimate, aptly labeled Sphericity Assumed . If sphericity CAN NOT be assumed, then we'll want to reference one of the other three (the differences between them is somewhat esoteric, but I typically choose Greenhouse-Geisser ). In either case, we reference the Sig. column ( 3, below ) to determine whether our effects are significant. In our example, we see that we had no significant effects. Since we could NOT assume sphericity, the Greenhouse-Geisser test tells us that Time was not a significant predictor of Sales (i.e. there was no overall positive or negative trend in Sales in the company as a whole), F(2.743, 340.097)=.743, p=.516, \u03b7p2=.006. We also see that neither promo F(5.485, 340.097)=.660, p=.668, \u03b7p2=.011, nor mktsize F(5.485, 340.097)=1.048, p=.391, \u03b7p2=.017 interacted with Time to predict trends in Sales . Additionally, there was also no significant three-way interaction between Time , promo , and mktsize F(10.971, 340.097)=.940, p=.502, \u03b7p2=.029. Take note of how I report those statistics , as it is necessary for APA format.  The next table was produced because we chose the \"Polynomial\" contrast for Time earlier. It is very useful in case non-linear relationships exist in your data. More specifically, it determines whether there is a Linear or a non-linear relationship exists, such as Quadratic or Cubic ( 1, below ). The more nuanced differences between these effects is beyond the scope of this blog, but Notre Dame's Dr. Richard Williams explains it well in his page on Non-linear Relationships .  The Tests of Between-Subjects Effects table ( below ) shows whether the factors were associated with differences in Sales (overall, as opposed to whether there were differences in trends). Results indicate that both promo F(2, 124)=12.837, p<.001, \u03b7p2=.172 and mktsize F(2, 124)=15.085, p<.001, \u03b7p2=.196 were predictive of differences in Sales (overall), while the interaction between the two was not significant F(4, 124)=.186, p=.945, \u03b7p2=.006. These results may seem a bit confusing, because they are in direct contrast to the within-subject effects reported earlier, but it will become more clear when we examine the plots of the effects next.  The plot below shows the mean Sales at each of the four data collections, for stores using each of the three promotional Strategies (three lines). The graph demonstrates that there are distinctions between sales numbers of the three strategy groups, as ( Strategy A was highest at every time point and Strategy B was lowest at every time point). However, since the trend for each group (if you were to impose a trendline across the four points for each group) is not dramatically different (and because the interaction term was not significant), we can't clearly say that one promotional strategy is superior to the others. Since, the differences between groups at time pionts 2, 3, and 4 are largely reflective of the differencs that existed at baseline (time 1), it seems that differences that exist between groups are more likely attributed to differences in the composition of the groups, rather than differences in the promotional strategy. The graph for mktsize can be interpreted in the same way as promo .  This graph further shows how it is better to examine within subject differences when analyzing change over time, as plotting those effects makes the lack of differences in trend between promo groups more clear. Thanks for reading and please leave comments and/or questions!"], "link": "http://www.statsmakemecry.com/smmctheblog/2012/4/20/how-to-conduct-a-repeated-measures-mancova-in-spss.html", "bloglinks": {}, "links": {"http://www.facelab.org/": 1, "http://www.statsmakemecry.com/": 5, "http://nd.edu/": 1, "http://www.visi.com/": 1, "http://jeremyjaytaylor.squarespace.com/": 2}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["I have a saying that I like to tell consulting clients, which is easier said than done, but I think are words for doctoral candidates to live by: \"The only bad dissertation draft is one that isn't turned-in.\" The most common factor that unnecessarily slows progress on a dissertation proposal or defense is a propensity to strive for the perfect draft. As a graduate student, we all fantasized of turning-in our first draft and having our advisor, being so amazed at its brilliance, insist that you accept your PhD on the spot. \n Unfortunately, reality inevitably sets-in in the form of red-line-filled draft that features more recommendations than the surgeon general and more added work than you ever imagined would be forthcoming. The feeling can be both crushing and disheartening. The good news is: the pain can be avoided, or at least reduced to a minor setback. \n The key to limiting the frustration inherent in this process is: TURN YOUR DRAFT IN! Relinquish your dreams of producing a perfect draft and turn-in the draft you have. The sooner you get a draft on your advisor's desk, the sooner you can receive their valuable feedback, allowing you to spend more time integrating it and less time on details that they may (or may not) ultimately approve of. \n Please don't misinterpret what I'm saying here: I'm not suggesting that you should turn-in work that is blatantly of sub-standard quality. Certainly an advisor's time is valuable and not to be wasted. However, I am suggesting that you let go of the fantasy that your first draft will be perfect. When you feel that you've made an honest effort to move your manuscript forward, turn in a draft and allow your advisor to direct you to where things can be improved. The reality is that many details that are agonized over in early drafts are made irrelevant by recommended cuts and revisions by our advisor. \n As a warning, the concepts I'm discussing here will feel unnatural to many readers. In fact, most doctoral candidates are high-achieving by nature, so striving for anything less than perfection may feel uncomfortable, or even down-right wrong. However, you'll be happy you heeded this advise when you receive your advisor's feedback and know that you are moving closer to your degree and saved yourself a lot of agonizing and wasted energy along the way. \n Understand that major feedback is unquestionably coming and try to get your hands on that feedback sooner, rather than later. The alternative is spending day, weeks, or even months longer on a draft, only to receive a very similar amount of feedback, with a whole lot more frustration."], "link": "http://www.statsmakemecry.com/smmctheblog/2011/9/6/the-worst-mistake-made-on-a-dissertation-is.html", "bloglinks": {}, "links": {}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["I received a great question this week, as a submission to my Ask the Stats Make Me Cry Guy page, which asked: In order for a moderating relationship to exist, do the predictor IV and dependent variable need to be significantly correlated?\". This is a question that I am asked a lot, partly because of the common confusion between mediators and moderators and the commonly held belief that an IV and DV should be related for mediation to be present (see my video blog on Mediators, Moderators, and Supressors for more info on this topic). However, moderators are a completely different story. In fact, a simple correlation between two variables can be very misleading, if one relies on it as an indicator of potential moderating effects and/or as an indicator that moderating effects should be tested.\n \nImagine the circumstance where you are testing whether there is an association between \"number of carrots consumed\" and \"blood pressure\". Imagine further that you have reason to believe that the association between these two variables varies by age (for this example let's make age dichotomous, i.e. old vs. young). Perhaps you expect that the more carrots someone eats, the lower their blood pressure will be (negative association), but you think this will be more true for older people than younger (i.e. age moderates the effect of carrot consumption on blood pressure).\n \nSince you'd expect that the association will probably still be negative in both groups, but more negative in older people (if your hypothesis is accurate), you might expect to see a graph like the one below:\n \n \n Related Content:\n \n \n Video: How To Test for Mediators, Moderators, and Suppressors \n \n Video: How To Run a Multiple Regression in SPSS \n \n Discussion Forum: How to Create Dummy Variables for Multiple Regression \n \n Discussion Forum: General linear model- univariate spss write up \n \n \n \n \n \nThis graph is fairly typical of a two-way interaction, where the two groups (young vs. old) have differing slopes. Since both group's slopes are negative, it isn't supriising that the overall sample slope is also negative. However, imagine a slightly different scenario where younger people's slope was actually positive for some unknown reason. In this case, your graph would look like this: \n\n \n\nIn this scenario, you would still likely have significant moderation (probably an even strong interaction effect, since the difference in slope is even larger), however you might not see a significant association between the IV (carrots) and DV (Blood Pressure) in the sample as a whole. This example highlights the danger of relying only on correlations and failing to consider/test potential moderating effects. Thanks for the great question, Ken!"], "link": "http://www.statsmakemecry.com/smmctheblog/2011/7/13/moderating-effects-with-seemingly-uncorrelated-variables.html", "bloglinks": {}, "links": {"http://www.statsmakemecry.com/": 5}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["Preparing a dataset for analysis is an arduous process. Besides recoding and cleaning variables, a diligent data analyst also must assign variable labels and value labels , unless they choose to wait until after your output is exported to Microsoft Word. Unfortunately, that option only leaves additional opportunity for error and confusion, not to mention the inefficiency of editing tables in Microsoft Word. Who among us have not been frustrated while wrestling with Microsoft Word?\n \nWhen used in conjunction with the customizable SPSS table \"Looks\" function, formatting your variable labels and value labels can make your SPSS results tables nearly ready for publication, immediately after analysis! Fortunately, SPSS syntax offers a fairly straightforward method for assigning proper labels to both your variable labels and value labels.\n \n For those of you unsure about the distinction between the two: \n \n Variable Labels : Variable labels are composed of a few words that describe what a variable represents. If the variable labels are properly formatted in SPSS, they will show in output tables and graphs, instead of variable names.\n \n Value Labels : Value labels are labels for coded variables in our dataset. For example, \"Gender\" may be coded 0 (Males) and 1 (Females). \n Related Content:\n \n \n Video: How to make SPSS produce all tables in APA format automatically! \n \n Video: How to Define Variable Sets in SPSS \n \n Video: How To Use SPSS Syntax Without \"Knowing\" Syntax \n \n Discussion Forum: Centering Variables in SPSS \n \n \n \n \nThe screenshot below shows an example SPSS dataset I created for demonstration purposes (as you can see at the bottom of the screenshot, we are seeing the \"variable view\", as opposed to \"data view\". To review, \"data view\" is used for editing the actual data, whereas \"variable view\" is used for editing the attributes of the variables (such as number of decimal places allowed, type of variable, the variable name, variable label, and value label). In our example below, neither the variable labels (1) nor the value labels (2) have been assigned for any of our four example variables.\n \n \n \n One way to rectify this problem would be to: \n \n \n Click in the field under \"Label\" for each variable and simply type in a label. \n Click in the field under \"Value\" ( on the right side of the field, click on the button that appears when you click in the field initially; see below left ) \n Enter the current number code that is assigned with the label that you desire for each code ( clicking \"add\" between each code ) in the dialogue that appears ( see below right ). \n \n \n\n \n\nThat would work fine if you only have a couple of variables, However, what if you have 10 variables, or 20, or 100... or 1000? Obviously, this can quickly turn into a ridiculously long process. This is when syntax makes things MUCH easier!\n \n Here are the steps to assign variable labels: \n \n \n Open a new syntax window by clicking through the following menu path ( see below ): File->New->Syntax. \n Type the command \"VARIABLE LABELS\" (be careful of spelling). \n On the next line (new line not required, but recommended), first type the name of the variable you want to assign a label to (in my example, the variable is \"Example1\"; see below ). \n On the same line as the variable name, insert a space, followed by a \"single quote\" (not a double quote/quotation mark), followed by whatever text you'd like to assign as the variable label for that variable, followed by another \"single quote\", and finally a period. \n \n NOTE : if that is all you wish to do, start a new line and type EXECUTE, followed by another period; If you want to also assign value labels, as we will here, you can save the EXECUTE until the end.\n \n \n \n Here are the steps to assign value labels (in the same syntax window): \n \n \n Type the command \"VALUE LABELS\" (be careful of spelling). \n On the next line (new line not required, but recommended), type the name of the variable you want to assign a value labels to (in my example, the variable is \"Example1\"; see below ). \n On the next line (new line not required, but recommended), type the number code that is currently in your data (to which you want to assign labels; in my example, the first code is 1), followed by a space... \n On the same line as the variable name, insert a space, followed by a \"single quote\", followed by the label you wish to assign to that numeric code, followed by another \"single quote\". \n On a new line, type the command EXECUTE, followed by a period, and then \"run\" the syntax. \n \n NOTE : repeat step 2 &3 for each numeric code that exists for that variable; Insert a period after the last code is entered for that variable.\n \n NOTE : To run the syntax, highlight the portion you wish to run AND THEN click on the \"green play button\" in the tool bar OR click through the following menu path \"Run-> Selection\" OR press the keyboard shortcut (\"Command+r\" [for mac]; \"Control+r\" [for PC]) .\n \n \n \n \nWhile this may be useful, doing this with only one variable doesn't offer that much improvement over the \"point and click\" method. However, when you apply this technique to transform several variables simultaneously, the time saved really begins to accumulate ( see below ).\n \n \n \n There are a few things to pay particular attention to when working with multiple variables at once: \n \n \n Be sure to insert a period after every line in the variable label command ( as pictured above ). \n When assigning value labels, a \"variable TO variable\" format can be used ( see above ), instead of listing each variable name separately, as long as the variables are in consecutive order in the dataset (and you want to assign the same labels to all of those variables). IF THEY ARE NOT in consecutive order, you simply need to list each of the names separately. \n If you have want to assign different sets of value labels to various variables, you simply need to separate each set of value labels with a forward slash ( / ), and begin with the new variable name on the next line ( see below ). \n \n \n \n \nAfter you run your syntax, you should now see the information you chose populating the \"Label\" and \"Value\" columns of your dataset ( seen below ).\n \n \n \n NOTE : If you also want to specify what codes in your data signify that the data is missing (\"Missing\" column [see above]), add the following commands to your syntax:\n \n To assign for a single variable: \n \nMISSING VALUES\n \nExample (999).\n \nEXECUTE.\n \n To assign for multiple variables: \n \nMISSING VALUES\n \nExample1 to Example4 (999).\n \nEXECUTE.\n \n \n NOTE: I will post links below that will allow you to download sample SPSS syntax files used in this tutorial (download either or both below).\n \n \"SINGLE VARIABLE\" EXAMPLE SYNTAX \n \n \"MULTI-VARIABLE\" EXAMPLE SYNTAX  \n\n \n IF THIS WAS HELPFUL, PLEASE \"LIKE\" AND/OR \"+1\" THIS PAGE!"], "link": "http://www.statsmakemecry.com/smmctheblog/2011/6/20/using-syntax-to-assign-variable-labels-and-value-labels-in-s.html", "bloglinks": {}, "links": {"http://www.statsmakemecry.com/": 4, "http://jeremyjaytaylor.squarespace.com/": 2}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["Formatting a graph that was exported from SPSS to Microsoft Word can be an absolute pain. Since neither program is known \"user-friendliness\", the interaction between the two can be predictably tedious and frustrating. The process of converting a standard SPSS table to APA format can become overwhelming when you have an entire manuscript worth of tables. Fortunately, a few minor alterations to your SPSS settings can make SPSS do most of the heavily lifting for you, making SPSS automatically produce tables that closely resemble APA format and cutting down your formatting time by as much as 90%! \n \n \n \n Related Content:\n \n \n Using Syntax to Assign 'Variable Labels' and 'Value Labels' in SPSS \n \n Video: How to make SPSS produce all tables in APA format automatically! \n \n Video: How to Define Variable Sets in SPSS \n \n Video: How To Use SPSS Syntax Without \"Knowing\" Syntax \n \n Discussion Forum: Centering Variables in SPSS \n \n \n \n \nPictured ( above ) are examples of standard SPSS tables ( left ) and tables produced in SPSS after a few adjustments ( right ) to the settings. The table on the right more closely aligns with APA format than the table on the left in several ways: \n \n The title has been changed from center justified and bold to left justified, italics , and NOT bold ( [1] above-right; APA format). \n The table borders have been adjusted appropriately (details of specific changes to follow shortly). \n The default font type and size has been changed to Times New Roman 12pt. \n \n \nThe adjustments to SPSS that are needed to produce tables like the ones on the right are only necessary to be made once, after which the adjustments are made automatically by SPSS and you'll find all of your future tables are ready for insertion into your APA manuscript immediately after analysis. The necessary changes can be accomplished in 3 steps: \n \n Produce an initial table for alteration (using any analysis; a simple frequency table is sufficient). \n Create a custom \"Table Look Style\", by \"Editing\" the initial table's \"Look Style\" and saving the changes as a custom \"style\" (\"APA Table\" seems like a reasonable choice). \n Adjust your SPSS settings (options) so that SPSS recognizes your newly created \"Look Style\" as the default table \"Look Style\". \n \n From there, you can simply run your analyses as you typically would and your tables should be formatted in APA format. Let's get into the specifics about how to accomplish these three steps... \n 1) PRODUCE INITIAL TABLE \n The first step to make your SPSS adjustment is to produce an initial table for editing. For our purposes, a simple frequency does the trick (in the SPSS drop-down menus, navigate to: Analyze>descriptives>frequencies). Once your table is produced ( below ), right click on the table and click on \"Edit Content\" and then either \"In Viewer\" or \"In Separate Window\" (it doesn't really matter which you choose, for our purposes). \n \n \nOnce your table is in \"editing\" mode ( below ), right click again and click on \"TableLooks...\" \n  \n \nNext, the \"TableLooks\" screen ( below ) should pop-up.\n \n  \n \nUnder \"TableLooks Files:\", change the selection \"CompactAcademicTimesRoman\" ( [1] below ).\n \n \n \nWhile simply making that switch gets us a lot closer to APA format than the \"default\" SPSS table, we can improve the settings to get us much closer with a few additional changes.\n \nNOTE: \"CompactAcademicTimesRoman\" is the closest \"TableLook\" to APA on its own, but luckily we can alter its attributes and save the changes!\n \nOnce you've clicked on \"CompactAcademicTimesRoman\", click on the \"Edit Look...\" button ( [2] above ). After clicking on \"Edit Look...\", the \"Table Properties\" screen should pop-up ( below ).\n \n \n \nWithin the \"Table Properties\" screen, we are going to adjust elements of both the \"Cell Formats\" tab ( above) and the \"Borders\" tab ( [1] below ).\n \nFirst, the \"Cell Formats\" tab ( above) :\n \nOn the \"Cell Formats\" screen, you are able to adjust: the tables \"Text\" (font), the \"Alignment\" (justifications) of the text, the background color (which we will not be adjusting), and the \"Inner Margins\". We will only be changing the \"Text\" and \"Alignment\" settings. We'll deal with the \"Text\" first.\n \nThe default of all text in SPSS tables is 8 pt ( [4] above ), while the appropriate APA format font is 12 point, so the first thing we'll need to to is change all of the text in the table from 8 pt ( [4] above ) to 12 pt.\n \n \n \n Unfortunately, you are required to change each text element separately by either clicking on the element in the \"Sample\" table on the right side of the screen ( [1] above ), or by selecting different elements in the \"Area\" drop-down menu ( [2] above ) . \n \n \n For example , click on the \" Table Title \" ( [3] above ) in the \"Sample\" table to edit that element. After clicking on the element, simply adjust the attributes on the left side of th screen \n \n NOTE : to comply with APA format for table titles, change your font size from 8 pt. to 12 pt. ( [4] above ), make it italics and not bold ( [5] above) , and click on \"Left Alignment\" ( [6] above ] ) \n \nNext, switch to the \"Borders\" tab ( [1] below). \n \n \n \nOnce in the \"Borders\" tab, there are three elements that we are going to adjust:\n \n \n Top inner frame ( [2] above ) \n Bottom inner frame \n Data area top \n \nTo adjust the \"Top inner frame\", highlight it in the Border menu section ( [1] below ). Next, click on the \"Style\" drop-down menu ( [2] below ) and change the style from the double line (not APA format) to the single thin line ( [3] below ; second from the bottom; complies with APA format).\n \n \n \nNext, repeat the style adjustment for the \"Bottom inner frame\" ( [1] below ).\n \n \n \nAgain, repeat the style adjustment for the \"Data area top\" ( [1] below ).\n \n \n \nNext, click the \"Apply\" button ( [2] above ), followed by the \"OK\" button ( [3] above ).\n \n 2) CREATE CUSTOM TABLE LOOK STYLE \n \nAfter clicking the \"OK\" button, you should find yourself back at the \"TableLooks\" screen ( [1] below ). On this screen click on \"Save As\" ( [2] below ).\n \n \n \nIn the \"Save As\" dialogue screen ( below), give your newly create table \"Look\" a name, preferably something self-explanatory and easy to remember. As you can see, I choose to call it \"APA Table\" ( [1] below ).\n \n \n \n Before clicking \"Save\" , make sure you are saving the \"TableLook\" file in the correct directory:\n \n \n On a mac , the \"Looks\" directory can be found in the \"SPSS\" folder (or PASWStatistics folder; [1] below ) within \"Applications\" ( [2] below ).\n \n On a PC , the \"Looks\" director can be found at C:>Documents and Settings> Program Files>SPSS \n  \n \n \n \nOnce inside the \"Looks\" folder ( below ), you should see various other \"TableLooks\" files (the files end in \".stt\"). If you see that, you know you are in the right folder. From here, check to make sure your \"File Name\" is what you want it to be and then click \"Save\" ( [1] below ).\n \n \n \nAfter you've clicked \"Save\", you should find yourself back in the \"TableLooks\" dialogue screen ( below ). Also, you should now see a newly available \"TableLook\" in the \"TableLook Files:\" area ( [1] below ) (the one you saved above). Next, simply click on that to highlight it ( 1] below ) and then click the \"OK\" button ( [2] below ).\n \n \n \nAfter clicking \"OK\", the \"TableLooks\" screen should disappear and the initial table you created should again be visible, but its format should now reflect the changes we've made and it should more closely resemble APA format ( below )!\n \n \n \nWhile certainly you could choose to do all of those steps for every graph you produce from now until forever, that wouldn't seem to be a very efficient use of your time. Instead, let's change the default SPSS settings to automatically use our newly created \"TableLook\" for all tables that are created in the future.\n \n 3) ADJUST SPSS TABLE \"LOOK STYLE\" SETTINGS (OPTIONS) \n \nTo adjust the SPSS \"TableLook\" settings, go to \"Options\" ( [1] below ), which you'll find under the \"Edit\" menu.\n \n \n \nWith the \"Options\" dialogue screen now visible, select the \"Pivot Tables\" tab ( [1] below ). Next, select our newly created \"Table Look\" (I called mine \"APA table\"; [2] below ).\n \n On a side note , I'd also suggest changing the \"Copying wide tables to the clipboard in rich text format\" option ( [3] below ) to \"Shrink width to fit\". Making this change will prevent SPSS from wrapping tables that are too wide for your page to another row (making them appear as two tables, even though they are really just two parts of the same table). I personally find that very irritating. Instead, this will tell SPSS to adjust the width of the cells in the table so that the table can fit within the margins of the page.\n \n Finally , click on the \"Apply\" button ( [4] below ), followed by the \"OK\" button ( [5] below ). You should now be done and all future graphs should be produced in APA format (or closer to it anyway). Happy table making!\n \n \n \n RIGHT-CLICK HERE AND \"SAVE AS FILE\" TO DOWNLOAD THE STT \"LOOKS\" FILE"], "link": "http://www.statsmakemecry.com/smmctheblog/2011/2/3/how-to-make-spss-produce-all-tables-in-apa-format-automatica.html", "bloglinks": {}, "links": {"http://www.statsmakemecry.com/": 5, "http://jeremyjaytaylor.squarespace.com/": 1}, "blogtitle": "Deviant Square Stats Tutorials"}, {"content": ["When I hear the word \"residual\", the pulp left over after I drink my orange juice pops into my brain, or perhaps the film left on the car after a heavy rain. However, when my regression model spits out an estimate of my model's residual, I'm fairly confident it isn't referring to OJ or automobile gunk...right? \u00a0Not so fast, that imagery is more similar to it's statistical meaning than you might initially think.\n \nIn statistics, a residual refers to the amount of variability in a dependent variable (DV) that is \"left over\" after accounting for the variability explained by the predictors in your analysis (often a regression). Right about now you are probably thinking: \"this guy likes the word \"variability\" way too much, he should buy a thesaurus already!\"\n \nLet me try again: when you include predictors (independent variables) in a regression, you are making a guess (or prediction) that they are associated with the DV; a residual is a numeric value for how much you were wrong with that prediction. The lower the residual, the more accurate the the predictions in your regression are, indicating your IVs are related to (predictive of) the DV. \n Related Content:\n \n \n Vote on the \"Top Confusing Stats Terms\") \n \n Top Ten Confusing Stats Terms Explained in \u201cPlain English\u201d (#9: Multicollinearity) \n \n Top Ten Confusing Stats Terms Explained in \u201cPlain English\u201d (#10: Standard Deviation) \n \n \n \nKeep in mind that each person in your sample will have their own residual score. This is because a regression model provided a \"predicted value\" for every individual, which is estimated from the values of the IVs of the regression. Each person's residual score is the difference between their predicted score (determined by the values of the IV's) and the actual observed score of your DV by that individual. That \"left-over\" value is a residual.\n \nLike the imagery of the orange pulp, a statistical residual is simply what's left over from your regression model. They can be used for many things, such as estimating accuracy of your model and checking assumptions, but that is a chat for another time..."], "link": "http://www.statsmakemecry.com/smmctheblog/2010/10/25/top-ten-confusing-stats-terms-explained-in-plain-english-8-r.html", "bloglinks": {}, "links": {"http://www.statsmakemecry.com/": 3}, "blogtitle": "Deviant Square Stats Tutorials"}]