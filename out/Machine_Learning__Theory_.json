[{"blogurl": "http://hunch.net\n", "blogroll": [], "title": "Machine Learning (Theory)"}, {"content": ["The New York ML symposium was last Friday. There were 303 registrations, up a bit from last year . I particularly enjoyed talks by Bill Freeman on vision and ML, Jon Lenchner on strategy in Jeopardy, and Tara N. Sainath on deep learning for speech recognition. If anyone has suggestions or thoughts for next year, please speak up. \n I also attended Strata + Hadoop World for the first time. This is primarily a trade conference rather than an academic conference, but I found it pretty interesting as a first time attendee. This is ground zero for the Big data buzzword, and I see now why. It\u2019s about data, and the word \u201cbig\u201d is so ambiguous that everyone can lay claim to it. There were essentially zero academic talks. Instead, the focus was on war stories, product announcements, and education. The general level of education is much lower\u2014explaining Machine Learning to the SQL educated is the primary operating point. Nevertheless that\u2019s happening, and the fact that machine learning is considered a necessary technology for industry is a giant step for the field. Over time, I expect the industrial side of Machine Learning to grow, and perhaps surpass the academic side, in the same sense as has already occurred for chip design. Amongst the talks I could catch, I particularly liked the Github , Zillow , and Pandas talks. Ted Dunning also gave a particularly masterful talk, although I have doubts about the core Bayesian Bandit approach(*). The streaming k-means algorithm they implemented does look quite handy. \n (*) The doubt is the following: prior elicitation is generally hard, and Bayesian techniques are not robust to misspecification. This matters in standard supervised settings, but it may matter more in exploration settings where misspecification can imply data starvation."], "link": "http://hunch.net/?p=2599", "bloglinks": {}, "links": {"http://www.linkedin.com/": 1, "http://hunch.net/": 1, "http://books.nips.cc/": 1, "http://people.mit.edu/": 1, "http://www.ibm.com/": 1, "http://strataconf.com/": 5, "http://www.nyas.org/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["A reminder that the New York Academy of Sciences will be hosting the\u00a0 7th Annual Machine Learning\u00a0Symposium tomorrow from 9:30am. \n The main program will feature invited talks from Peter Bartlett ,\u00a0 William Freeman , and Vladimir Vapnik , along with numerous spotlight\u00a0talks and a poster session. Following the main program, hackNY and Microsoft\u00a0Research are sponsoring a networking hour with talks from machine\u00a0learning practitioners at NYC startups (specifically bit.ly , Buzzfeed , Chartbeat , and Sense Networks , Visual Revenue ). This should be of great interest to everyone considering working in machine learning."], "link": "http://hunch.net/?p=2586", "bloglinks": {}, "links": {"http://chartbeat.com": 1, "http://visualrevenue.com": 1, "http://hunch.net/": 1, "http://sensenetworks.com": 1, "http://people.mit.edu/": 1, "http://buzzfeed.com": 1, "http://en.wikipedia.org/": 1, "http://bit.ly": 1, "http://www.berkeley.edu/": 1, "http://hackny.org/": 1, "http://www.nyas.org/": 2}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["A new version of VW is out . The primary changes are: \n \n Learning Reductions : I\u2019ve wanted to get learning reductions working and we\u2019ve finally done it. Not everything is implemented yet, but VW now supports direct:\n \n Multiclass Classification \u2013oaa or \u2013ect . \n Cost Sensitive Multiclass Classification \u2013csoaa or \u2013wap . \n Contextual Bandit Classification \u2013cb . \n Sequential Structured Prediction \u2013searn or \u2013dagger \n \n In addition, it is now easy to build your own custom learning reductions for various plausible uses: feature diddling, custom structured prediction problems, or alternate learning reductions. This effort is far from done, but it is now in a generally useful state. Note that all learning reductions inherit the ability to do cluster parallel learning.\n \n Library interface : VW now has a basic library interface. The library provides most of the functionality of VW, with the limitation that it is monolithic and nonreentrant. These will be improved over time. \n Windows port : The priority of a windows port jumped way up once we moved to Microsoft . The only feature which we know doesn\u2019t work at present is automatic backgrounding when in daemon mode. \n New update rule : Stephane visited us this summer, and we fixed the default online update rule so that it is unit invariant. \n \n There are also many other small updates including some contributed utilities that aid the process of applying and using VW. \n Plans for the near future involve improving the quality of various items above, and of course better documentation: several of the reductions are not yet well documented."], "link": "http://hunch.net/?p=2578", "bloglinks": {}, "links": {"http://www.microsoft.com/": 1, "http://www.cmu.edu/": 1, "https://github.com/": 1, "http://hunch.net/": 2}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["The New York Machine Learning Symposium is October 19 with a 2 page abstract deadline due September 13 via email with subject \u201cMachine Learning Poster Submission\u201d sent to physicalscience@nyas.org. Everyone is welcome to submit. Last year\u2019s attendance was 246 and I expect more this year. \n The primary experiment for ICML 2013 is multiple paper submission deadlines with rolling review cycles. The key dates are October 1, December 15, and February 15. This is an attempt to shift ICML further towards a journal style review process and reduce peak load. The \u201cnot for proceedings\u201d experiment from this year\u2019s ICML is not continuing. \n Edit: Fixed second ICML deadline."], "link": "http://hunch.net/?p=2574", "bloglinks": {}, "links": {"http://www.nyas.org/": 1, "http://icml.cc/": 1}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["There are a handful of basic code patterns that I wish I was more aware of when I started research in machine learning. Each on its own may seem pointless, but collectively they go a long way towards making the typical research workflow more efficient. Here they are: \n \n Separate code from data. \n Separate input data, working data and output data. \n Save everything to disk frequently. \n Separate options from parameters. \n Do not use global variables. \n Record the options used to generate each run of the algorithm. \n Make it easy to sweep options. \n Make it easy to execute only portions of the code. \n Use checkpointing. \n Write demos and tests. \n \n Click here for discussion and examples for each item. Also see Charles Sutton\u2019s and HackerNews\u2019 thoughts on the same topic. \n My guess is that these patterns will not only be useful for machine learning, but also any other computational work that involves either a) processing large amounts of data, or b) algorithms that take a significant amount of time to execute. Share this list with your students and colleagues. Trust me, they\u2019ll appreciate it."], "link": "http://hunch.net/?p=2562", "bloglinks": {}, "links": {"http://news.ycombinator.com/": 1, "http://www.theexclusive.org/": 1, "http://arkitus.com/": 2}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["The workshop on the Meaningful Use of Complex Medical Data is happening again, August 9-12 in LA, near UAI on Catalina Island August 15-17. I enjoyed my visit last year, and expect this year to be interesting also. \n The first Bay Area Machine Learning Symposium is August 30 at Google . Abstracts are due July 30."], "link": "http://hunch.net/?p=2556", "bloglinks": {}, "links": {"http://www.auai.org/": 1, "http://www.baylearn.org/": 1, "http://google.com": 1, "http://mucmd.org": 1}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["Yaser points out some nicely videotaped machine learning lectures at Caltech . Yaser taught me machine learning, and I always found the lectures clear and interesting, so I expect many people can benefit from watching. Relative to Andrew Ng \u2019s ML class there are somewhat different areas of emphasis but the topic is the same, so picking and choosing the union may be helpful."], "link": "http://hunch.net/?p=2546", "bloglinks": {}, "links": {"http://work.caltech.edu/": 1, "http://www.caltech.edu/": 2, "http://ai.stanford.edu/": 1, "http://see.stanford.edu/": 1}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["Just about nothing could keep me from attending ICML , except for Dora who arrived on Monday. Consequently, I have only secondhand reports that the conference is going well. \n For those who are remote (like me) or after the conference (like everyone), Mark Reid has setup the ICML discussion site where you can comment on any paper or subscribe to papers. Authors are automatically subscribed to their own papers, so it should be possible to have a discussion significantly after the fact, as people desire. \n We also conducted a survey before the conference and have the survey results now. This can be compared with the ICML 2010 survey results . Looking at the comparable questions, we can sometimes order the answers to have scores ranging from 0 to 3 or 0 to 4 with 3 or 4 being best and 0 worst, then compute the average difference between 2012 and 2010. \n Glancing through them, I see: \n \n Most people found the papers they reviewed a good fit for their expertise (-.037 w.r.t 2010). Achieving this was one of our subgoals in the pursuit of high quality decisions. \n Most people had sufficient time for doing reviews. This was something that we worried about significantly in shifting the paper deadline and otherwise massaging the schedule. Most people also thought the review period was sufficiently long and most reviews were high quality (+.023 w.r.t. 2010) \n About 1/4 of reviewers say that author response changed their mind on a paper and 2/3 of reviewers say discussion changed their mind on a paper. The expectation of decision impact from author response is reduced from 2010 (-.135). The existence of author response is overwhelmingly preferred. \n People generally found ICML reviewing the same or better than previous ICMLs (+.35 w.r.t. 2010) and other similar conferences (+.198 w.r.t. 2010) at the cost of being somewhat more work. A substantial bump in reviewing quality was a primary goal. \n The ACs spent substantially more time (43 hours on average) than PC members (28 hours on average). This agrees with our expectation\u2014the set of ACs didn\u2019t change even after we had a 50% increase in submissions. The AC load we had this year was probably too high and will need to be reduced somewhat for next year. \n 2/3 of authors prefer the option to revise a paper during author response. \n The choice of how to deal with increased submissions is deeply undecided, with a slight preference for short talk+poster as we did. \n Most people like having two workshop days or don\u2019t care. \n There is a strong preference for COLT and UAI colocation with the next tier of preference for IJCAI, KDD, AAAI, and CVPR."], "link": "http://hunch.net/?p=2532", "bloglinks": {}, "links": {"http://icml.cc/": 2, "http://icml.cc": 1, "http://mark.reid.name/": 1, "http://www.icml2010.org/": 1, "http://hunch.net/": 1}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["Larry Wasserman has started the Normal Deviate blog which I added to the blogroll on the right. \n Manfred Warmuth points out the UCSC machine learning summer school running July 9-20 which may be of particular interest to those in silicon valley."], "link": "http://hunch.net/?p=2530", "bloglinks": {}, "links": {"http://www.cmu.edu/": 1, "http://normaldeviate.wordpress.com/": 1, "https://mlss.ucsc.edu/": 1, "http://users.ucsc.edu/": 1}, "blogtitle": "Machine Learning (Theory)"}, {"content": ["People are naturally interested in slicing the ICML acceptance statistics in various ways. Here\u2019s a rundown for the top categories. \n \n \n 18/66 = 0.27 \n in (0.18,0.36) \n Reinforcement Learning \n \n \n 10/52 = 0.19 \n in (0.17,0.37) \n Supervised Learning \n \n \n 9/51 = 0.18 \n not in (0.18, 0.37) \n Clustering \n \n \n 12/46 = 0.26 \n in (0.17, 0.37) \n Kernel Methods \n \n \n 11/40 = 0.28 \n in (0.15, 0.4) \n Optimization Algorithms \n \n \n 8/33 = 0.24 \n in (0.15, 0.39) \n Learning Theory \n \n \n 14/33 = 0.42 \n not in (0.15, 0.39) \n Graphical Models \n \n \n 10/32 = 0.31 \n in (0.15, 0.41) \n Applications (+5 invited) \n \n \n 8/29 = 0.28 \n in (0.14, 0.41]) \n Probabilistic Models \n \n \n 13/29 = 0.45 \n not in (0.14, 0.41) \n NN & Deep Learning \n \n \n 8/26 = 0.31 \n in (0.12, 0.42) \n Transfer and Multi-Task Learning \n \n \n 13/25 = 0.52 \n not in (0.12, 0.44) \n Online Learning \n \n \n 5/25 = 0.20 \n in (0.12, 0.44) \n Active Learning \n \n \n 6/22 = 0.27 \n in (0.14, 0.41) \n Semi-Supervised Learning \n \n \n 7/20 = 0.35 \n in (0.1, 0.45) \n Statistical Methods \n \n \n 4/20 = 0.20 \n in (0.1, 0.45) \n Sparsity and Compressed Sensing \n \n \n 1/19 = 0.05 \n not in (0.11, 0.42) \n Ensemble Methods \n \n \n 5/18 = 0.28 \n in (0.11, 0.44) \n Structured Output Prediction \n \n \n 4/18 = 0.22 \n in (0.11, 0.44) \n Recommendation and Matrix Factorization \n \n \n 7/18 = 0.39 \n in (0.11, 0.44) \n Latent-Variable Models and Topic Models \n \n \n 1/17 = 0.06 \n not in (0.12, 0.47) \n Graph-Based Learning Methods \n \n \n 5/16 = 0.31 \n in (0.13, 0.44) \n Nonparametric Bayesian Inference \n \n \n 3/15 = 0.20 \n in (0.7, 0.47) \n Unsupervised Learning and Outlier Detection \n \n \n 7/12 = 0.58 \n not in (0.08, 0.50) \n Gaussian Processes \n \n \n 5/11 = 0.45 \n not in (0.09, 0.45) \n Ranking and Preference Learning \n \n \n 2/11 = 0.18 \n in (0.09, 0.45) \n Large-Scale Learning \n \n \n 0/9 = 0.00 \n in [0, 0.56) \n Vision \n \n \n 3/9 = 0.33 \n in [0, 0.56) \n Social Network Analysis \n \n \n 0/9 = 0.00 \n in [0, 0.56) \n Multi-agent & Cooperative Learning \n \n \n 2/9 = 0.22 \n in [0, 0.56) \n Manifold Learning \n \n \n 4/8 = 0.50 \n not in [0, 0.5) \n Time-Series Analysis \n \n \n 2/8 = 0.25 \n in [0, 0.5] \n Large-Margin Methods \n \n \n 2/8 = 0.25 \n in [0, 0.5] \n Cost Sensitive Learning \n \n \n 2/7 = 0.29 \n in [0, 0.57) \n Recommender Systems \n \n \n 3/7 = 0.43 \n in [0, 0.57) \n Privacy, Anonymity, and Security \n \n \n 0/7 = 0.00 \n in [0, 0.57) \n Neural Networks \n \n \n 0/7 = 0.00 \n in [0, 0.57) \n Empirical Insights \n \n \n 0/7 = 0.00 \n in [0, 0.57) \n Bioinformatics \n \n \n 1/6 = 0.17 \n in [0, 0.5) \n Information Retrieval \n \n \n 2/6 = 0.33 \n in [0, 0.5) \n Evaluation Methodology \n \n \n Update: See Brendan\u2019s graph for a visualization. \n I usually find these numbers hard to interpret. At the grossest level, all areas have significant selection. At a finer level, one way to add further interpretation is to pretend that the acceptance rate of all papers is 0.27, then compute a 5% lower tail and a 5% upper tail. With 40 categories, we expect to have about 4 violations of tail inequalities. Instead, we have 9, so there is some evidence that individual areas are particularly hot or cold. In particular, the hot topics are Graphical models, Neural Networks and Deep Learning, Online Learning, Gaussian Processes, Ranking and Preference Learning, and Time Series Analysis. The cold topics are Clustering, Ensemble Methods, and Graph-Based Learning Methods. \n We also experimented with AIStats resubmits (3/4 accepted) and NFP papers (4/7 accepted) but the numbers were to small to read anything significant. \n One thing that surprised me was how uniform decisions were as a function of average score in reviews. All reviews included a decision from {Strong Reject, Weak Reject, Weak Accept, Strong Accept}. These were mapped to numbers in the range {1,2,3,4}. In essence, average review score < 2.2 meant 0% chance of acceptance, and average review score > 3.1 meant acceptance. Due to discretization in the number of reviewers and review scores there were only 3 typical uncertain outcomes: \n \n 2.33. This was either 2 Weak Rejects+Weak Accept or Strong Reject+2 Weak Accepts or (rarely) Strong Reject+Weak Reject+Strong Accept. About 8% of these paper were accepted. \n 2.67. This was either Weak Reject+Weak Accept*2 or Strong Accept+2 Weak Rejects or (rarely) Strong Reject+Weak Accept+Strong Accept. About 48% of these paper were accepted. \n 3.0. This was commonly 3 Weak Accepts or Strong Accept+Weak Accept+Weak Reject or (rarely) 2 Strong Accepts + Strong Reject. About 90% of these papers were accepted. \n \n One question I\u2019ve always wondered is: How much variance is there in the accept/reject decision? In general, correlated assignment of reviewers can greatly increase the amount of variance, so one of our goals this year was doing as independent an assignment as possible. If you accept that as independence, we essentially get 3 samples for each paper where the average standard deviation of reviewer scores before author feedback and discussion is 0.64. After author feedback and discussion the standard deviation drops to 0.51. If we pretend that papers have an intrinsic value between 1 and 4 then think of reviews as discretized gaussian measurements fed through the above decision criteria, we get the following: \n \nThere are great caveats to this picture. For example, treating the AC\u2019s decision as random conditioned on the reviewer average is a worst-case analysis. The reality is that ACs are removing noise from the few events that I monitored carefully, although it is difficult to quantify this. Similarly, treating the reviews observed after discussion as independent is clearly flawed. A reasonable way to look at it is: author feedback and discussion get us about 1/3 or 1/4 of the way to the final decision from the initial reviews. \n Conditioned on the papers, discussion, author feedback and reviews, AC\u2019s are pretty uniform in their decisions with ~30 papers where ACs disagreed on the accept/reject decision. For half of those, the ACs discussed further and agreed, leaving Joelle and I a feasible quantity of cases to look at (plus several other exceptions). \n At the outset, we promised a zero- spof reviewing process. We actually aimed higher: at least 3 people needed to make a wrong decision for the ICML 2012 reviewing process to kick out a wrong decision. I expect this happened a few times given the overall level of quality disagreement and quantities involved, but hopefully we managed to reduce the noise appreciably."], "link": "http://hunch.net/?p=2517", "bloglinks": {}, "links": {"http://brenocon.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Machine Learning (Theory)"}]