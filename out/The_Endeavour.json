[{"blogurl": "http://www.johndcook.com/blog\n", "blogroll": [], "title": "The Endeavour"}, {"content": ["John Myles White brings up an interesting question on Twitter: \n Ioannidis thinks most published biological research findings are false . Do you think >50% of tweets are false? \n I\u2019m inclined to think tweets may be more accurate than research papers, mostly because people tweet about mundane things that they understand. If someone says that there\u2019s a long line at the Apple store, I believe them. When someone says that a food increases or decreases your risk of some malady, I\u2019m more skeptical. I\u2019ll wait to see such a result replicated before I put much faith in it. A lot of tweets are jokes or opinions, but of those that are factual statements, they\u2019re often true. \n Tweets are not subject to publication pressure; few people risk losing their job if they don\u2019t tweet. There\u2019s also not a positive publication bias: people can tweet positive or negative conclusions. There is a bias toward tweeting what makes you look good, but that\u2019s not limited to Twitter. \n Errors are corrected quickly on Twitter. When I make factual errors on Twitter, I usually hear about it within minutes. As the saga of Anil Potti illustrates, errors or fraud in scientific papers can take years to retract. \n (My experience with Twitter may be atypical. I follow people with a relatively high signal to noise ratio, and among those I have a shorter list that I keep up with.) \n Related : \n My Twitter accounts \n Popular research areas produce more false results"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/wAi5bjqMCAQ/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www.johndcook.com/blog": 3, "http://www.johndcook.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["For any convex solid, V \u2013 E + F = 2 where V is the number of vertices, E the number of edges, and F the number of faces. The number 2 in this formula is a topological invariant of a sphere, called its Euler characteristic. But if you compute the Euler characteristic for a figure with a hole in it, you get a different value. For a torus (the surface of a doughnut) we get V \u2013 E + F = 0. \n You can demonstrate this with eight 6-sided dice. A single die has 8 vertices, 12 edges, and 6 faces, and so V \u2013 E + F = 2. Next join two dice together along one face. \n \n Before joining, the two dice separately have 16 vertices, 24 edges, and 12 faces. But when we join them together, we have 4 fewer vertices since 4 pairs of edges are identified together. Similarly, 4 pairs of edges are identified, and 2 faces are identified. So the joined pair now has 12 vertices, 20 edges, and 10 faces, and once again V \u2013 E + F = 2. \n We can keep on adding dice this way, and each time the Euler characteristic doesn\u2019t change. Each new die adds 4 vertices, 8 edges, and 4 faces, so V \u2013 E + F doesn\u2019t change. \n \n But when we join the dice into a circle, the Euler characteristic changes when we put the last die in place. \n \n The last die doesn\u2019t change the total number of vertices, since all its vertices are identified with previous vertices. The last die adds 4 edges. It adds a net of 2 faces: it adds 4 new faces, but it removes 2 existing faces. So the net change to the Euler characteristic is 0 \u2013 4 + 2 = -2. The last die lowers the Euler characteristic from 2 to 0. \n Exercise: Use a similar procedure to find the Euler characteristic of a two-holed torus. \n Related : \n Make your own buckyball \n If you like topology and geometry, follow @TopologyFact ."], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/mkKI2DPIKlM/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www.johndcook.com/blog": 1}, "blogtitle": "The Endeavour"}, {"content": ["Here are a few of my favorite programming-related links that I\u2019ve run across lately. \n \n Functional Programming with Python \n NoSQL is dual to SQL \n Why you would want to program at fifty (or any other age) \n The Poetry of Function Naming \n Alan Turing totally looks like Zac Efron"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/AaakXVR_MmY/", "bloglinks": {}, "links": {"http://queue.acm.org/": 1, "http://ua.pycon.org/": 1, "https://i.chzbgr.com/": 1, "http://blog.stephenwolfram.com/": 1, "http://blog.vivekhaldar.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["The product of two normal PDFs is proportional to a normal PDF. This is well known in Bayesian statistics because a normal likelihood times a normal prior gives a normal posterior. But because Bayesian applications don\u2019t usually need to know the proportionality constant, it\u2019s a little hard to find. I needed to calculate this constant, so I\u2019m recording the result here for my future reference and for anyone else who might find it useful. \n Denote the normal PDF by \n \n Then the product of two normal PDFs is given by the equation \n \n where \n \n and \n \n Note that the product of two normal random variables is not normal, but the product of their PDFs is proportional to the PDF of another normal."], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/CtDw7w0NIw0/", "bloglinks": {}, "links": {}, "blogtitle": "The Endeavour"}, {"content": ["This weekend a couple of my daughters and I put together a buckyball from a Zometool kit. The shape is named for Buckminster Fuller of geodesic dome fame. Two years after Fuller\u2019s death, scientists discovered that the shape appears naturally in the form of a C 60 molecule, named Buckminsterfullerene in his honor. In geometric lingo, the shape is a truncated icosahedron. It\u2019s also the shape of many soccer balls. \n \n I used the buckyball to introduce the Euler\u2019s formula:\u00a0 V \u2013 E + F = 2. (The number of vertices (black balls) minus the number of edges (blue sticks) plus the number of faces (in this case, pentagons and hexagons) always equals 2 for a shape that can be deformed into a sphere.) Being able to physically add and remove vertices or nodes makes the induction proof of Euler\u2019s formula really tangible. Then we looked at 6- and 12-sided dice to show that V \u2013 E + F = 2 for these shapes as well. \n Thanks to Zometool for sending me the kit. \n Update : How to show that the Euler characteristic of a torus is zero \n Related post : Platonic solids"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/JbCmqWB3YuU/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 2, "http://en.wikipedia.org/": 1, "http://zometool.com/": 2}, "blogtitle": "The Endeavour"}, {"content": ["G. H. Hardy called the following equation Ramanujan\u2019s \u201cmost beautiful identity.\u201d For | q | < 1, \n \n If I understood it, I might say it\u2019s beautiful, but for now I can only say it\u2019s mysterious. Still, I explain what I can. \n The function p on the left side is the partition function. For a positive integer argument n , p ( n ) is the number of ways one can write n as the sum of a non-decreasing sequence of positive integers. \n The right side of the equation is an example of a q -series. Strictly speaking it\u2019s a product, not a series, but it\u2019s the kind of thing that goes under the general heading of q -series. \n I hardly know anything about q -series, and they don\u2019t seem very motivated. However, I keep running into them in unexpected places. They seem to be a common thread running through several things I\u2019m vaguely familiar with and would like to understand better. \n As mysterious as Ramanujan\u2019s identity is, it\u2019s not entirely unprecedented. In the eighteenth century, Euler proved that the generating function for partition numbers is a q-product: \n \n So in discovering his most beautiful identity (and others) Ramanujan followed in Euler\u2019s footsteps. \n Reference: An Invitation to q-series"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/LAvh-LUCVLE/", "bloglinks": {}, "links": {"http://www.amazon.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["I thought this tweet from @WoodyOsher was pretty funny. \n Everything our parents said was good is bad. Sun, milk, red meat \u2026 the least-squares method. \n I wouldn\u2019t say these things are bad , but they are now viewed more critically than they were a generation ago. \n Sun exposure may be an apt example since it has alternately been seen as good or bad throughout history. The latest I\u2019ve heard is that moderate sun exposure may lower your risk of cancer, even skin cancer, presumably because of vitamin D production. And sunlight appears to reduce your risk of multiple sclerosis since MS is more prevalent at higher latitudes. But like milk, red meat, or the least squares method, you can over do it. \n More on least squares : When it works, it works really well"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/ptkeu6kCl_s/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www.johndcook.com/blog": 1}, "blogtitle": "The Endeavour"}, {"content": ["I recommend using Python for data analysis, and I recommend Wes McKinney\u2019s book Python for Data Analysis . \n  \n I prefer Python to R for mathematical computing because mathematical computing doesn\u2019t exist in a vacuum; there\u2019s always other stuff to do. I find doing mathematical programming in a general-purpose language is easier than doing general-purpose programming in a mathematical language. Also, general-purpose languages like Python have larger user bases, are better designed, have better tool support, etc. \n Python per se doesn\u2019t have everything you need for mathematical computing. You need to combine several tools and libraries, typically at least SciPy, matplotlib, and IPython. Because there are different pieces involved, it\u2019s hard to find one source to explain using them all together. Also, even with the three additional components mentioned before, there is a need for additional software for working with structured data. \n Wes McKinney developed the pandas library to give Python \u201crich data structures and functions designed to make working with structured data fast, easy, and expressive.\u201d And now he has addressed the need for unified exposition by writing a single book that describes how to use the Python mathematical computing stack. Importantly, the book covers two recent developments that make Python more competitive with other environments for data analysis: enhancements to IPython and Wes\u2019 own pandas project. \n Python for Data Analysis is available for pre-order. I don\u2019t know when the book will be available but Amazon lists the publication date as October 29. My review copy was a PDF, but at least one paper copy has been spotted in the wild: \n \n Wes McKinney holding his book at O\u2019Reilly\u2019s Strata Conference. Photo posted on Twitter yesterday."], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/ns2QlntXG1k/", "bloglinks": {}, "links": {"http://www.amazon.com/": 3, "https://twitter.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["Paul Erd\u0151s was an extraordinary mathematical collaborator. He traveled constantly, cross-pollinating the mathematical community. He wrote about 1500 papers and had around 500 coauthors. According to Ron Graham, \n He\u2019s still writing papers, actually. He\u2019s slowed down. Because many people started a paper with Erd\u0151s and have let it lay in a stack some place and didn\u2019t quite get around to it \u2026 In the last couple years he\u2019s published three or four papers. Of course he\u2019s been dead almost 15 years, so he\u2019s slowed a bit. \n For more on Erd\u0151s, listen to Samuel Hansen\u2019s excellent podcast . \n Related posts : \n Six degrees of Paul Erd\u0151s \n Anti-calculus proposition of Erd\u0151s \n An elegant proof from Erd\u0151s"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/vc7q6RAN0Fs/", "bloglinks": {}, "links": {"https://itunes.apple.com/": 1, "http://www.johndcook.com/blog": 3}, "blogtitle": "The Endeavour"}, {"content": ["Lately I\u2019ve been reading The Best Writing on Mathematics 2012 . I\u2019d like to present a alternative perspective on one of the articles. \n In his article \u201cAn Adventure in the Nth Dimension,\u201d Brian Hayes explores how in high dimensions, balls have surprisingly little volume. As the dimension n increases, the volume of a ball of radius 1 increases until n = 5. Then for larger n the volume steadily decreases. Hayes asks \n What is it about five-dimensional space that allows a unit 5-ball to spread out more expansively than any other n-ball? \n He says that it all has to do with the value of \u03c0 and that if \u03c0 were different, the unit ball would have its maximum value for a different dimension n . While that is true, it seems odd to speculate about changing the value of \u03c0. It seems much more natural to speculate about changing the radius of the balls. \n The volume of a ball of radius r in dimension n is \n \n If we fix r at 1 and let n vary, we get a curve like this: \n \n But for different values of r , the plot will have its maximum at different values of n . For example, here is the curve for balls of radius 2: \n \n Let\u2019s think of n in our volume formula as a continuous variable so we can differentiate with respect to n . It turns out to be more convenient to work with the logarithm of the volume. This makes no difference: the logarithm of a function takes on its maximum exactly where the original function does since log is an increasing function. \n \n We can tell from this equation that volume (eventually) decreases as a function of n because \u03c8 is an unbounded increasing function. The derivative has a unique zero, and we can move the location of that zero out by increasing r . So for any dimension n , we can solve for a value of r such that a ball of radius r has its maximum volume in that dimension:"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/Flj_xllvgBU/", "bloglinks": {}, "links": {"http://www.amazon.com/": 2}, "blogtitle": "The Endeavour"}, {"content": ["When I was growing up and ordinary people were becoming aware of computers, my father told me that he thought there would be good money in fixing computers when they break down. \n Looking back on this, it\u2019s obvious why he would say that: he fixed things. I could just imagine a salesman saying at the same time \u201cSon, there\u2019s going to be good money in selling computers.\u201d Maybe a policeman was telling his son that computer crime was going to be a big problem some day. And maybe a politician was telling his son that we\u2019ve got to find a way to tax computers. \n *** \n I first posted this on Google+ a few days ago."], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/abGQXY73jzU/", "bloglinks": {}, "links": {"https://plus.google.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["Randomized clinical trials essentially flip a coin to assign patients to treatment arms. Outcome-adaptive randomization \u201cbends\u201d the coin to favor what appears to be the better treatment at the time each randomized assignment is made. The method aims to treat more patients in the trial effectively, and on average it succeeds. \n However, looking only at the average number of patients assigned to each treatment arm conceals the fact that the number of patients assigned to each arm can be surprisingly variable compared to equal randomization. \n Suppose we have 100 patients to enroll in a clinical trial. If we assign each patient to a treatment arm with probability 1/2, there will be about 50 patients on each treatment. The following histogram shows the number of patients assigned to the first treatment arm in 1000 simulations. The standard deviation is about 5. \n \n Next we let the randomization probability vary. Suppose the true probability of response is 50% on one arm and 70% on the other. We model the probability of response on each arm as a beta distribution, starting from a uniform prior. We randomize to an arm with probability equal to the posterior probability that that arm has higher response. The histogram below shows the number of patients assigned to the better treatment in 1000 simulations. \n \n The standard deviation in the number of patients is now about 17. Note that while most trials assign 50 or more patients to the better treatment, some trials in this simulation put less than 20 patients on this treatment. Not only will these trials treat patients less effectively, they will also have low statistical power (as will the trials that put nearly all the patients on the better arm). \n The reason for this volatility is that the method can easily be mislead by early outcomes. With one or two early failures on an arm, the method could assign more patients to the other arm and not give the first arm a chance to redeem itself. \n Because of this dynamic, various methods have been proposed to add \u201cballast\u201d to adaptive randomization. See a comparison of three such methods here . These methods reduce the volatility in adaptive randomization, but do not eliminate it. For example, the following histogram shows the effect of adding a burn-in period to the example above, randomizing the first 20 patients equally. \n \n The standard deviation is now 13.8, less than without the burn-in period, but still large compared to a standard deviation of 5 for equal randomization. \n Another approach is to transform the randomization probability. If we use an exponential tuning parameter of 0.5, the sample standard deviation of the number of patients on the better arm is essentially the same, 13.4. If we combine a burn-in period of 20 and an exponential parameter of 0.5, the sample standard deviation is 11.7, still more than twice that of equal randomization. \n Related : \n Power and bias in adaptively randomized clinical trials \n Three ways of tuning an adaptively randomized trial"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/3OeBjDs-IaA/", "bloglinks": {}, "links": {"http://biostats.bepress.com/": 2, "http://www.johndcook.com/blog": 1, "http://www.johndcook.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["The phrase \u201cdownward nobility\u201d is a pun on \u201cupward mobility.\u201d It usually refers to taking a less lucrative but more admired position. For example, it might be used to describe a stock broker who becomes a teacher in a poor school. (I don\u2019t believe that being a teacher is necessarily more noble than being a stock broker, but many people would think so.) \n Daniel Lemire looks at a variation on downward nobility in his blog post Why you may not like your job, even though everyone envies you . He comments on Matt Welsh\u2019s decision to leave a position as a tenured professor at Harvard to develop software for Google. Welsh may not have taken a pay cut \u2014 he may well have gotten a raise \u2014 but he took a cut in prestige in order to do work that he found more fulfilling. \n The Peter Principle describes people how people take more prestigious positions as they become less competent. The kind of downward nobility Daniel describes is a sort of anti-Peter Principle, taking a step down in prestige to move deeper into your area of competence. \n Paul Graham touches on this disregard for prestige in his essay How to do what you love . \n If you admire two kinds of work equally, but one is more prestigious, you should probably choose the other. Your opinions about what\u2019s admirable are always going to be slightly influenced by prestige, so if the two seem equal to you, you probably have more genuine admiration for the less prestigious one. \n Matt Welsh now has a less prestigious position in the assessment of the general public. But in a sense he didn\u2019t give up prestige for competence. Instead, he chose a new environment in which his area competence carries more prestige. \n Related posts : \n Super-competence \n How do you know when someone\u2019s great? \n The dark side of linchpins"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/_tLaOEqAzFY/", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://www.johndcook.com/blog": 3, "http://www.paulgraham.com/": 1, "http://lemire.me/blog": 1}, "blogtitle": "The Endeavour"}, {"content": ["I recently ran across a series of articles from Carin Meier going through seven papers by the late computer science pioneer John McCarthy in seven weeks. Published so far: \n Prologue \n #1: Ascribing Mental Qualities to Machines \n #2: Towards a Mathematical Science of Computation \n Carin has announced that the next paper will be \u201cFirst Order Theories of Individual Concepts and Propositions\u201d but she hasn\u2019t posted a commentary on it yet."], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/dtSXiUZtYdg/", "bloglinks": {}, "links": {"http://gigasquidsoftware.com/": 4}, "blogtitle": "The Endeavour"}, {"content": ["One reason the normal distribution is easy to work with is that you can vary the mean and variance independently. With other distribution families, the mean and variance may be linked in some nonlinear way. \n I was looking for a faster way to compute Prob( X > Y + \u03b4) where X and Y are independent inverse gamma random variables. If \u03b4 were zero, the probability could be computed analytically. But when \u03b4 is positive, the calculation requires numerical integration. When the calculation is in the inner loop of a simulation, most of the simulation\u2019s time is spent doing the integration. \n Let Z = Y + \u03b4. If Z were another inverse gamma random variable, we could compute Prob( X > Z ) quickly and accurately without integration. Unfortunately, Z is not an inverse gamma. But it is approximately an inverse gamma, at least if Y has a moderately large shape parameter, which it always does in my applications. So let Z be inverse gamma with parameters to match the mean and variance of Y + \u03b4. Then Prob( X > Z ) is a good approximation to Prob( X > Y + \u03b4). \n For more details, see Fast approximation of inverse gamma inequalities . \n Related posts : \n Fast approximation of beta inequalities \n Gamma inequalities"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/cBsdCdShLUs/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 2, "http://www.johndcook.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["Chuck Bearden posted this quote from Steve Holmes on his blog the other day: \n Usefulness comes not from pursuing it, but from patiently gathering enough of a reservoir of material so that one has the quirky bit of knowledge \u2026 that turns out to be the key to unlocking the problem which someone offers. \n Holmes was speaking specifically of theology. I edited out some of the particulars of his quote to emphasize that his idea applies more generally. \n Obviously usefulness can come from pursuing it. But there\u2019s a special pleasure in applying some \u201cquirky bit of knowledge\u201d that you acquired for its own sake. It can feel like simply walking up to a gate and unlocking it after unsuccessful attempts to storm the gate by force."], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/qTYTP0tnhTA/", "bloglinks": {}, "links": {"http://ungleichschaltung.tumblr.com/": 1}, "blogtitle": "The Endeavour"}, {"content": ["From G. K. Chesterton\u2019s essay Conceit and Caricature : \n Before we congratulate ourselves upon the absence of certain faults from our nation or society, we ought to ask ourselves why it is that these faults are absent. Are we without the fault because we have the opposite virtue? Or are we without the fault because we have the opposite fault? It is a good thing assuredly, to be innocent of any excess; but let us be sure that we are not innocent of excess merely by being guilty of defect. \n For example, when we boast of being tolerant, are we gracious and charitable toward those with whom we fervently disagree, or are we actually apathetic?"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/9It18Pu8sUA/", "bloglinks": {}, "links": {"http://www.gutenberg.org/": 1}, "blogtitle": "The Endeavour"}, {"content": ["A beta distribution has an approximate normal shape if its parameters are large, and so you could use normal approximations to compute beta inequalities. The corresponding normal inequalities can be computed in closed form. \n This works surprisingly well. Even when the beta parameters are small and the normal approximation is a bad fit, the corresponding inequality approximation is pretty good. \n For more details, see the tech report Fast approximation of beta inequalities . \n Related post : \n Beta inequalities in R"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/2H8YZuuBIDA/", "bloglinks": {}, "links": {"http://biostats.bepress.com/": 1, "http://www.johndcook.com/blog": 1}, "blogtitle": "The Endeavour"}, {"content": ["Jeff Atwood wrote the other day that if you need a to-do list, something\u2019s wrong. \n If you can\u2019t wake up every day and, using your 100% original equipment God-given organic brain, come up with the three most important things you need to do that day \u2013 then you should seriously work on fixing that. I don\u2019t mean install another app, or read more productivity blogs and books. You have to figure out what\u2019s important to you and what motivates you; ask yourself why that stuff isn\u2019t gnawing at you enough to make you get it done. Fix that . \n I agree with him to some extent, but not entirely. \n The simplest time in my life was probably graduate school. For a couple years, this was my to-do list: \n \n Write a dissertation. \n \n I could remember that. There were a few other things I needed to do, but that was the main thing. I didn\u2019t supervise anyone, and didn\u2019t collaborate with anyone. My wife and I didn\u2019t have children yet. We lived in an apartment and so there were no repairs to be done. (Well, there were, but they weren\u2019t our responsibility.) There wasn\u2019t much to keep up with. \n My personal and professional responsibilities are more complicated now. I can\u2019t always wake up and know what I need to do that day. To-do lists and calendars help. \n But I agree with Jeff that to the extent possible, you should work on a small number of projects at once. Ideally one, maybe two. Not many people could have just one or two big things going on in their life at once, but more could have just one or two things going on within each sphere of life: one big work project, one home repair project, etc. \n Jeff also says that your big projects should be things you believe are important and you are motivated to do. Again I agree that\u2019s ideal, but most of us have some obligations that we don\u2019t think are important but that nevertheless need to be done. I try to minimize these \u2014 it drives me crazy to do something that I don\u2019t think needs to be done \u2014 but they won\u2019t go away entirely. \n I agree with the spirit of Jeff\u2019s remarks, though I don\u2019t think they apply directly to people who have more diverse responsibilities. I believe he\u2019s right that when you find it hard to keep track of everything you need to do, maybe you\u2019re doing too much, or maybe you\u2019re doing things that are a poor fit. \n Related posts : \n Losing patience with wastes of time \n Selfish minimalism \n Personal organization software"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/Q8UbQnQwU-8/", "bloglinks": {}, "links": {"http://www.codinghorror.com/blog": 1, "http://www.johndcook.com/blog": 3}, "blogtitle": "The Endeavour"}, {"content": ["It\u2019s not enough for software to be correct. It has to be defensible. \n I\u2019m not thinking of defending against malicious hackers. I\u2019m thinking about defending against sincere critics. I can\u2019t count how many times someone was absolutely convinced that software I had a hand in was wrong when it in fact it was performing as designed. \n In order to defend software, you have to understand what it does. Not just one little piece of it, but the whole system. You need to understand it better than the people who commissioned it: the presumed errors may stem from unforeseen consequences of the specification. \n Related post : The buck stops with the programmer"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/XWdL_9Kcf4k/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 1}, "blogtitle": "The Endeavour"}, {"content": ["From Seth Godin\u2019s Startup School : \n So the first thing about the duck is that there are a lot of people who spend their time getting all their ducks in a row. \u2026 If you want to be a neurosurgeon, you spend 15 years of your life getting your ducks in a row and one day somebody says \u201cNow you\u2019re a neurosurgeon.\u201d But if you\u2019re an entrepreneur, you\u2019re an entrepreneur. Immediately. \u2026 Along the way you can collect more ducks and get them in a row \u2026 You\u2019ve got to do something with the duck. \n Similar posts : \n Endless preparation \n Fairy dust on the diploma"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/WIu8YMUFxcQ/", "bloglinks": {}, "links": {"http://www.earwolf.com/": 1, "http://www.johndcook.com/blog": 2}, "blogtitle": "The Endeavour"}, {"content": ["Someone asked me yesterday for R code to compute the probability P( X > Y + \u03b4) where X and Y are independent beta random variables. I\u2019m posting the solution here in case it benefits anyone else. \n For an example of why you might want to compute this probability, see A Bayesian view of Amazon resellers . \n \n Let X be a Beta(a, b) random variable and Y be a Beta(c, d) random variable. Denote PDFs by f and CDFs by F . Then the probability we need is \n \n If you just need to compute this probability a few times, here is a desktop application to compute random inequalities. \n But if you need to do this computation repeated inside R code, you could use the following. \n beta.ineq <- function(a, b, c, d, delta)\n{\n integrand <- function(x) { dbeta(x, a, b)*pbeta(x-delta, c, d) }\n integrate(integrand, delta, 1, rel.tol=1e-4)$value\n} \n The code is as good or as bad as R\u2019s integrate function. It\u2019s probably accurate enough as long as none of the parameters a , b , c , or d are near zero. When one or more of these parameters is small, the integral is harder to compute numerically. \n There is no error checking in the code above. A more robust version would verify that all parameters are positive and that delta is less than 1. \n Here\u2019s the solution to the corresponding problem for gamma random variables, provided delta is zero: A support one-liner . \n And here is a series of blog posts on random inequalities. \n Introduction \n Analytical results \n Numerical results \n Cauchy distributions \n Beta distributions \n Gamma distributions \n Three or more random variables \n Folded normals"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/OYLZIwGqlGQ/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 10, "https://biostatistics.mdanderson.org/": 1}, "blogtitle": "The Endeavour"}, {"content": ["When I hear someone say \u201cpersonalized medicine\u201d I want to ask \u201cas opposed to what?\u201d \n All medicine is personalized. If you are in an emergency room with a broken leg and the person next to you is lapsing into a diabetic coma, the two of you will be treated differently. \n The aim of personalized medicine is to increase the degree of personalization, not to introduce personalization. In particular, there is the popular notion that it will become routine to sequence your DNA any time you receive medical attention, and that this sequence data will enable treatment uniquely customized for you. All we have to do is collect a lot of data and let computers sift through it. There are numerous reasons why this is incredibly naive. Here are three to start with. \n \n Maybe the information relevant to treating your malady is in how DNA is expressed, not in the DNA per se, in which case a sequence of your genome would be useless. Or maybe the most important information is not genetic at all. The data may not contain the answer. \n \n Maybe the information a doctor needs is not in one gene but in the interaction of 50 genes or 100 genes. Unless a small number of genes are involved, there is no way to explore the combinations by brute force. For example, the number of ways to select 5 genes out of 20,000 is 26,653,335,666,500,004,000. The number of ways to select 32 genes is over a googol, and there isn\u2019t a googol of anything in the universe . Moore\u2019s law will not get us around this impasse. \n Most clinical trials use no biomarker information at all. It is exceptional to incorporate information from one biomarker. Investigating a handful of biomarkers in a single trial is statistically dubious. Blindly exploring tens of thousands of biomarkers is out of the question, at least with current approaches. \n \n Genetic technology has the potential to incrementally increase the degree of personalization in medicine. But these discoveries will require new insight, and not simply more data and more computing power. \n Related posts : \n Predicting height from genes \n Why microarray studies are often wrong"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/mssbfoaAeRs/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 4}, "blogtitle": "The Endeavour"}, {"content": ["Erik Meijer\u2019s paper Your Mouse is a Database has an interesting illustration of \u201cThe Big Data Cube\u201d using three axes to classify databases. \n \n The volume axis is big vs. small, or perhaps better, open vs. closed. Relational databases can be large, and non-relational databases can be small. But the relational database model is closed in the sense that \u201cit assumes a closed world that is under full control by the database.\u201d \n The velocity axis is (synchronous) pull vs. (asynchronous) push. The variety axis captures whether data is stored by foreign-key/primary-key relations or key-value pairs. The first axis could be labeled \n Here are the corners identified by the paper: \n \n Traditional RDBMS (small, pull, fk/pk) \n Hadoop HBase (big, pull, fk/pk) \n Object/relational mappers (small, pull, k/v) \n LINQ to Objects (big, pull, k/v) \n Reactive Extensions (big, push, k/v) \n \n How would you fill in the three corners not listed above? \n Related links : \n Big data is not enough \n Big data and humility \n coSQL (exploring the variety axis)"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/cNT_H_56aYg/", "bloglinks": {}, "links": {"http://queue.acm.org/": 2, "http://www.johndcook.com/blog": 2}, "blogtitle": "The Endeavour"}, {"content": ["The problems with JavaScript come from premature standardization. The language\u2019s author Brendan Eich said \n I had to be done in ten days or something worse than JS would have happened. \n For a programming language designed in 10 days, he did an amazing job. Maybe he did too good a job: his first draft was good enough to use, and so he never got a chance to fix the language\u2019s flaws. \n The opposite of JavaScript may be Perl 6 . The language has been in the works for 12 years and is still in development, though there are compilers you can use today. An awful lot of thought has gone into the language\u2019s design. Importantly, some early design decisions were overturned after the community had time to think, a luxury JavaScript never had. \n Perl 6 has gotten a lot of ridicule for being so slow to come out, but it may have the last laugh. Someone learning Perl 6 in the future will not care how long the language was in development, but they will appreciate that the language was very thoughtfully designed. \n *** \n Another contrast between JavaScript and Perl 6 is their names. Netscape gave JavaScript a deliberately misleading name to imply a connection to the Java language. The Perl 6 name honestly positions the new language as a successor to Perl 5. \n Perl 6 really is a new language, compatible in spirit with earlier versions of Perl though not always in syntax. Damian Conway has suggested that perhaps Perl 6 should have been developed under a completely different name. Then after it was completed, the developers could announce, \u201cOh, and by the way, this language is the upgrade path for Perl.\u201d \n If you think of Perl 6 as a new language, your expectations are quite different than if you think of it as an upgrade. If it\u2019s a new language, it doesn\u2019t matter so much how long it was in development. Perl programmers would be pleased with how similar the new language is to their familiar one, rather than upset about the differences. And people would evaluate the new language on its merits rather than being prejudiced by previous experience with Perl. \n Related posts : \n Three-hour-a-week language \n Dialing back the cleverness"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/rN_YfowEHCM/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 2, "http://gotocon.com/": 1, "http://perl6.org/": 2}, "blogtitle": "The Endeavour"}, {"content": ["Here\u2019s a mysterious approximation to \u03c0 from Ramanujan: \n \n The approximation is correct to 18 decimal places. I have no idea what inspired it. \n Related posts : \n A Ramanujan series for calculating pi \n Ramanujan\u2019s factorial approximation"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/dOEInZP6Uy0/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 2}, "blogtitle": "The Endeavour"}, {"content": ["From Erik Meijer: \n Functional Hacker := Think like a fundamentalist, code like a pragmatist. \n Related posts: \n 85% functional language purity \n You wanted a banana but you got a gorilla holding the banana"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/CSfhu5hrOCA/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 2}, "blogtitle": "The Endeavour"}, {"content": ["Terminal A, Copenhagen Airport. \n Other architecture posts : \n Geometry of the Sydney Opera House \n Houston Deco"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/NMTYs6A2p4I/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 2}, "blogtitle": "The Endeavour"}, {"content": ["Suppose you have a large number of buckets and an equal number of balls. You randomly pick a bucket to put each ball in one at a time. When you\u2019re done, about how what proportion of buckets will be empty? \n One line of reasoning says that since you have as many balls as buckets, each bucket gets one ball on average, so nearly all the buckets get a ball. \n Another line of reasoning says that randomness is clumpier than you think. Some buckets will have several balls. Maybe most of the balls will end up buckets with more than one ball, and so nearly all the buckets will be empty. \n Is either extreme correct or is the answer in the middle? Does the answer depend on the number n of buckets and balls? (If you look at the cases n = 1 and 2, obviously the answer depends on n . But how much does it depend on n if n is large?) Hint: There is a fairly simple solution. \n What applications can you imagine for the result? \n Other quiz/puzzle posts : \n Photo quiz \n Roman numeral puzzle \n Renaissance math puzzle \n Technology history quiz \n A knight\u2019s random walk \n Algebra problem from 1798 \n Monday morning math puzzle"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/Tq1NPH5j8yY/", "bloglinks": {}, "links": {"http://www.johndcook.com/blog": 7}, "blogtitle": "The Endeavour"}, {"content": ["Walter Bright made an interesting point in his talk at GOTO Aarhus this afternoon. He says that software developers like to think of their code as pipelines. Data comes in, goes through an assembly line-like series of processing steps, and comes out the end. \n input\u00a0 -> step1 -> step 2 -> step3 -> output \n And yet code hardly ever looks like that. Most software looks more like a whirlpool than a pipeline . Data swirls around in loops before going down the drain. \n \n These loops make it hard to identify parts that can be extracted into encapsulated components that can be snapped together or swapped out. You can draw a box around a piece of an assembly line and identify it as a component. But you can\u2019t draw a box around a portion of a whirlpool and identify it as something than can be swapped out like a black box. \n It is possible to write pipe-and-filter style apps, but the vast majority of programmers find loops more natural to write. And according to Walter Bright, it takes the combination of a fair number of programming language features to pull it off well, features which his D language has. I recommend watching his talk when the conference videos are posted. ( Update : This article contains much of the material as the conference talk.) \n I\u2019ve been thinking about pipeline-style programming lately, and wondering why it\u2019s so much harder than it looks. It\u2019s easy to find examples of Unix shell scripts that solve some problem by snapping a handful of utilities together with pipes. And yet when I try to write my own software like that, especially outside the context of a shell, it\u2019s not as easy as it looks. Brian Marick has an extended example in his book that takes a problem that appears to require loops and branching logic, but that can be turned into a pipeline. I haven\u2019t grokked his example yet; I want to go back and look at it in light of today\u2019s talk. \n Related posts : \n The D programming language \n Comparing the Unix and PowerShell pipelines"], "link": "http://feedproxy.google.com/~r/TheEndeavour/~3/FN1UpJH52ng/", "bloglinks": {}, "links": {"http://www.walterbright.com/": 1, "https://leanpub.com/": 1, "http://www.johndcook.com/blog": 2, "http://www.drdobbs.com/": 1, "http://gotocon.com/": 1}, "blogtitle": "The Endeavour"}]