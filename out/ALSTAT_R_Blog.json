[{"blogurl": "http://alstatr.blogspot.com\n", "blogroll": [], "title": "ALSTAT R Blog"}, {"content": ["QUESTIONS  1. The tensile strength of Portland cement is being studied. Four different mixing techniques can be used economically. The following data have been collected:    (a) Test the hypothesis that mixing techniques affect the strength of the cement. Use $\\alpha=0.05$.  (b) Construct a graphical display as described in Section 3-5.3 to compare the mean tensile strengths for the four mixing techniques. What are your conclusions?  (c) Use the Fisher LSD method with $\\alpha=0.05$ to make comparisons between pairs of means.  (d) Construct a normal probability plot of the residuals. What conclusion would you draw about the validity of the normality assumption?  (e) Plot the residuals versus the predicted tensile strength. Comment on the plot.  (f) Prepare a scatter plot of the results to aid the interpretation of the results of this experiment    2. Rework second part of Problem (1) using Duncan\u2019s multiple range test with $\\alpha=0.05$. Does this make any difference in your conclusions?  (a.) Rework second part of Problem (1) using Tukey\u2019s test with $\\alpha=0.05$. Do you get the same conclusions from Tukey\u2019s test that you did from the graphical procedure and/or Duncan\u2019s multiple range test?      COMPUTATIONAL AND GRAPHICAL SECTION The tensile strength of Portland cement is being studied. Four different mixing techniques can be used economically. The following data have been collected:      (a) Test the hypothesis that mixing techniques affect the strength of the cement. Use $\\alpha=0.05$.   I.  Hypotheses:   H\u00ad0: $\\mu_{1}=\\mu_{2}=\\mu_{3}=\\mu_{4}$   H1: some means are different.   II.  Level of significance: $\\alpha = 0.05$   III. Test Statistics: $$F_{0}=\\frac{\\frac{SS_{Treatments}}{a-1}}{\\frac{SS_{E}}{N-a}}=\\frac{MS_{Treatments}}{MS_{E}}$$   IV. Rejection Region:   $$F_{0}>F_{\\alpha,a-1,N-a}\\\\F_{0}>F_{0.05,3,12}\\\\F_{0}>3.49$$   V. Computation: $$SS_{T}=\\sum_{i=1}^5\\sum_{j=1}^5y_{ij}^2-\\frac{y_{..}^2}{N}\\\\=(3129)^2+(3000)^2+\\dots+(2600)^2+(2765)^2-\\frac{(46909)^2}{16}\\\\=138172041-\\frac{(46909)^2}{16}=643648.4375\\\\SS_{Treatments}=\\frac{1}{n}\\sum_{i=1}^5y_{i.}^2-\\frac{y_{..}^2}{N}\\\\\\frac{1}{4}[(11884)^2+\\dots+(10665)^2]-\\frac{(46909)^2}{16}=489740.1875\\\\SS_{E}=SS_{T}-SS_{Treatments}\\\\=643648.4375-489740.1875=153908.25$$      The F-value of 12.73 implies that the model is significant, since it is greater than the tabulated value, 3.49. And the p-value of it is also less than the level of significance. Thus, will lead to the rejection of the null hypothesis and conclude that the mean techniques affect the strength of the cement significantly.   (b) Construct a graphical display as described in Section 3-5.3 to compare the mean tensile strengths for the four mixing techniques. What are your conclusions?     Dashed line in the plot by color:    Red - $\\bar{y}_{4}$ Mean of Treatment 4 (2666.25)    Pink - $\\bar{y}_{..}$ Grand Mean (2931.81)    Brown - $\\bar{y}_{3}$ Mean of Treatment 3 (2933.75)    Green - $\\bar{y}_{1}$ Mean of Treatment 1 (2971.00)    Blue - $\\bar{y}_{2}$ Mean of Treatment 2 (3156.25)   Based on the plot and from the data also, we would conclude that $\\bar{y}_{1}$ and $\\bar{y}_{3}$ are the same, refer also to plot of question 1, the sixth one. Morever, the $\\bar{y}_{4}$ differs from that of $\\bar{y}_{1}$ and $\\bar{y}_{3}$, and that $\\bar{y}_{2}$ differs from $\\bar{y}_{1}$ and $\\bar{y}_{3}$, and that $\\bar{y}_{2}$ and $\\bar{y}_{4}$ are different.   How did I do it?   First thing we need to do is to make a student t distribution with degrees of freedom $N-1=15$. After having that plot, we need to insert the four means of the treatment and locate it in the x-values. Now, since the mean values are not seen on the plot because it's too large, we then convert it first to t-values, using the following formula,$t=\\frac{\\bar{y}_{i}-\\bar{y}_{..}}{\\frac{\\sigma}{\\sqrt{n}}}$   (c) Use the Fisher LSD method with $\\alpha = 0.05$ to make comparisons between pairs of means.$$LSD=t_{\\frac{\\alpha}{2},N-a}\\sqrt{\\frac{2MS_{E}}{n}}=t_{0.025,16-4}\\sqrt{\\frac{2(12825.7)}{4}}\\\\=2.179\\sqrt{6412.85}=174.495$$ Thus, any pair of treatment averages that differ in absolute value by more than 174.495 would imply that the corresponding pair of population means are significantly different.   The differences in averages are $$\\bar{y}_{1.}-\\bar{y}_{2.}=2971.00-3156.25=-185.25>174.495*\\\\\\bar{y}_{1.}-\\bar{y}_{3.}=2971.00-2933.75=37.25<174.495\\\\\\bar{y}_{1.}-\\bar{y}_{4.}=2971.00-2933.75=304.75>174.495*\\\\\\bar{y}_{2.}-\\bar{y}_{3.}=3156.25-2933.75=222.25>174.495*\\\\\\bar{y}_{2.}-\\bar{y}_{4.}=3156.25-2666.25=490.00>174.495*\\\\\\bar{y}_{3.}-\\bar{y}_{4.}=2933.75-2666.25=267.5>174.495*$$ The starred values indicate pairs of means that are significantly different.   (d) Construct a normal probability plot of the residuals. What conclusion would you draw about the validity of the normality assumption?    Nothing is unusual in the plot. Thus, the residuals met the normality assumption since the points fluctuate within the 95 percent confidence interval.   (e) Plot the residuals versus the predicted tensile strength. Comment on the plot.    The plot exhibits a little outward-opening funnel or megaphone, though not too obvious but still affect the non-constancy of the error variance.   (f) Prepare a scatter plot of the results to aid the interpretation of the results of this experiment.     2. Rework second part of Problem (1) using Duncan\u2019s multiple range test with . Does this make any difference in your conclusions?   Ranking the treatment averages in ascending order, we have  $$\\bar{y}_{4.}=2666.25\\\\\\bar{y}_{3.}=2933.75\\\\\\bar{y}_{1.}=2971.00\\\\\\bar{y}_{2.}=3156.25$$ The standard error of each average is $S_{\\bar{y}_{i}}=\\sqrt{\\frac{12825.69}{4}}=56.625$. From the table of significant ranges for 12 degrees of freedom and $\\alpha=0.05$, we obtain $r_{0.05}(2,12)=3.081,r_{0.05}(3,12)=3.225,$ and $r_{0.05}(4,12)=3.312$. Thus, the least significant ranges are $$R_{2}=r_{0.05}(2,20)S_{\\bar{y}_{i.}}=(3.081)(56.625)=174.46\\\\R_{3}=r_{0.05}(3,12)S_{\\bar{y}_{i.}}=(3.312)(56.625)=182.62\\\\R_{4}=r_{0.05}(4,12)S_{\\bar{y}_{i.}}=(3.312)(56.625)=187.54$$ The comparison would yield $$2 vs. 4: 3156.25-2666.25=490>187.54(R_{4})\\\\2 vs. 3: 3156.25-2933.75=222.5>182.62(R_{3})\\\\2 vs. 1: 3156.25-2971.00=185.25>174.46(R_{2})\\\\1 vs. 4: 2971.00-2666.25=304.75>182.62(R_{3})\\\\1 vs. 3: 2971.00-2933.75=37.25<174.46(R_{2})\\\\3 vs. 4: 2933.75-2666.25=267.5>174.46(R_{2})$$ From the analysis we observed that there are significant differences between all pairs of means except 1 and 3. This makes no difference in the previous conclusion of LSD method, which confirms that the Duncan\u2019s multiple range test and the LSD method produce identical conclusions.  (a) Rework second part of Problem (1) using Tukey\u2019s test with $\\alpha=0.05$. Do you get the same conclusions from Tukey\u2019s test that you did from the graphical procedure and/or Duncan\u2019s multiple range test?$$T_{0.05}=q_{0.05}(4,12)\\sqrt{\\frac{MS_{E}}{n}}=4.20\\sqrt{\\frac{12825.69}{4}}\\\\=4.20(56.625)=237.825$$  Thus, any pair of treatment averages that differ in absolute value by more than 237.825 would imply that the corresponding pair of population means are significantly different. The four treatment averages are, $$\\bar{y}_{1.}=2971.00~~~~~\\bar{y}_{2.}=3156.25~~~~~\\bar{y}_{3.}=2933.75~~~~~\\bar{y}_{4.}=2666.25$$  And the differences in averages are $$\\bar{y}_{1.}-\\bar{y}_{2.}=2971.00-3156.25=-185.25\\\\\\bar{y}_{1.}-\\bar{y}_{3.}=2971.00-2933.75=37.25\\\\\\bar{y}_{1.}-\\bar{y}_{4.}=2971.00-2666.25=304.75*\\\\\\bar{y}_{2.}-\\bar{y}_{3.}=3156.25-2933.75=222.5\\\\\\bar{y}_{2.}-\\bar{y}_{4.}=3156.25-2666.25=490*\\\\\\bar{y}_{3.}-\\bar{y}_{4.}=2933.75-2666.75=267.5*$$ The starred values indicate pairs of means that are significantly different.   The conclusions are not the same. The mean of Treatment 4 is different than the mean of Treatments 1, 2, and 3 in Duncans. However, the mean of Treatment 1 and mean of Treatment 2 is not different in Tukey computation as well as the mean of Treatment 1 and mean of Treatment 3. They were found to be different using the graphical method and the Fisher LSD method.    Reference:  Design and Analysis of Experiments by Douglas C. Montgomery  R Code          #(1.a) Test the hypothesis that mixing techniques affect the strength of #the cement. Use \u03b1=0.05.  #INPUT TensileData <- read.table ( header = TRUE , text = \" Treatment Observations Predicted A 3129 2971 A 3000 2971 A 2865 2971 A 2890 2971 B 3200 3156.25 B 3300 3156.25 B 2975 3156.25 B 3150 3156.25 C 2800 2933.75 C 2900 2933.75 C 2985 2933.75 C 3050 2933.75 D 2600 2666.25 D 2700 2666.25 D 2600 2666.25 D 2765 2666.25\" )  attach ( TensileData ) Model<- aov ( Observations~Treatment , data =TensileData ) summary ( Model )  #(1.b) Construct a graphical display as described in Section 3-5.3 to #compare the mean tensile strengths for the four mixing techniques. What are #your conclusions?  #INPUT Library ( ggplot2 ) x <- seq ( - 4.5 , 4.5 , length = 90 ) xval <- c ( 2666.25 , 2933.75 , 2971 , 3156.25 ) xvaltrn <- ( xval - mean ( xval ) ) / ( sd ( xval ) /sqrt ( 3 ) ) tvalues <- dt ( x , 15 ) vlines <- data.frame ( xint = c ( xvaltrn , mean ( xvaltrn ) ) , grp = letters [ 1 : 5 ] ) attach ( vlines ) qplot ( x , tvalues ) + geom_polygon ( fill = \"purple\" , colour = \"purple\" , alpha = 0.5 ) + geom_point ( fill = \"purple\" , colour = \"purple\" , alpha = 0.2 , pch = 21 ) + geom_vline ( data = vlines , aes ( xintercept = xint , colour = grp ) , linetype = \"dashed\" , size = 1 ) + theme_bw ( ) + xlab ( bquote ( bold ( 'x values with intercept of Average Tensile Strength (lb/in' ^ '2' * ')' ) ) ) + ylab ( expression ( bold ( P ( x ) ) ) ) + opts ( title = expression ( bold ( \"Student t Distribution\" ) ) , plot.title = theme_text ( size = 20 , colour = \"darkblue\" ) , panel.border = theme_rect ( size = 2 , colour = \"red\" ) )  #(1.c) Use the Fisher LSD method with \u03b1=0.05 to make comparisons between #pairs of means.  #INPUT Library ( agricolae ) LSD.test ( Model , \"Treatment\" )  #(1.d) Construct a normal probability plot of the residuals. What #conclusion would you draw about the validity of the normality assumption?  #INPUT Residuals <- Observations \u2013 Predicted #Make sure you run the #attach(TensileData) first library ( ggplot2 ) library ( MASS ) df<- data.frame ( x= sort ( Residuals ) , y= qnorm ( ppoints ( length ( Residuals ) ) ) ) probs <- c ( 0.01 , 0.05 , seq ( 0.1 , 0.9 , by = 0.1 ) , 0.95 , 0.99 ) qprobs<- qnorm ( probs ) xl <- quantile ( Residuals , c ( 0.25 , 0.75 ) ) yl <- qnorm ( c ( 0.25 , 0.75 ) ) slope <- diff ( yl ) /diff ( xl ) int <- yl [ 1 ] - slope * xl [ 1 ] fd<- fitdistr ( Residuals , \"normal\" ) #Maximum-likelihood Fitting of Univariate #Dist from MASS xp_hat<- fd $estimate [ 1 ] +qprobs* fd $estimate [ 2 ] #estimated perc. for the fitted #normal #var. of estimated perc v_xp_hat<- fd $sd [ 1 ] ^ 2 +qprobs^ 2 * fd $sd [ 2 ] ^ 2 + 2 *qprobs* fd $vcov [ 1 , 2 ] xpl<-xp_hat + qnorm ( 0.025 ) * sqrt ( v_xp_hat ) #lower bound xpu<-xp_hat + qnorm ( 0.975 ) * sqrt ( v_xp_hat ) #upper bound df.bound<- data.frame ( xp=xp_hat , xpl=xpl , xpu = xpu , nquant=qprobs )  #The above codes was originally programmed by Julie B at stackoverflow.com, #Link to her stackoverflow profile: #http://stackoverflow.com/users/1200228/julie-b #Link to the posted question in stackoverflow: #http://stackoverflow.com/questions/3929611/recreate-minitab-normal-#probability-plot  ggplot ( data = df , aes ( x = x , y = y ) ) + geom_point ( colour = \"darkred\" , size = 3 ) + geom_abline ( intercept = int , slope = slope , colour = \"purple\" , size = 2 , alpha = 0.5 ) + scale_y_continuous ( limits= range ( qprobs ) , breaks=qprobs , labels = 100 *probs ) + geom_line ( data =df.bound , aes ( x = xpl , y = qprobs ) , colour = \"darkgreen\" , alpha = 0.5 , size = 1 ) + geom_line ( data =df.bound , aes ( x = xpu , y = qprobs ) , colour = \"darkgreen\" , alpha = 0.5 , size = 1 ) + xlab ( expression ( bold ( \"Residuals\" ) ) ) + ylab ( expression ( bold ( \"Normal % Probability\" ) ) ) + theme_bw ( ) + opts ( title = expression ( bold ( \"Normal Probabiliy Plot of Residuals\" ) ) , plot.title = theme_text ( size = 20 , colour = \"darkblue\" ) , panel.border = theme_rect ( size = 2 , colour = \"red\" ) )  #(1.e) Plot the residuals versus the predicted tensile strength. Comment on #the plot.  #INPUT library ( colorRamps )  ggplot ( data = TensileData , aes ( x = Predicted , y = Residuals ) ) + ylim ( c ( - 210 , 210 ) ) + geom_point ( aes ( size = 3 , colour = matlab.like ( 16 ) ) ) + theme_bw ( ) + xlab ( expression ( bold ( \"Predicted Values\" ) ) ) + ylab ( expression ( bold ( \"Residuals\" ) ) ) + opts ( title = expression ( bold ( \"Residuals versus Fitted\" ) ) , plot.title = theme_text ( colour = \"darkblue\" , size = 20 ) , panel.border = theme_rect ( size = 2 , colour = \"red\" ) , legend.position = \"none\" )  #(1.f) Prepare a scatter plot of the results to aid the interpretation of #the results of this experiment  #INPUT ggplot ( data = TensileData , aes ( factor ( Treatment ) , y = Observations ) ) + geom_point ( colour = \"darkred\" , size = 3 ) + labs ( y = \"Percent\" , x= \"Data\" ) + geom_boxplot ( aes ( fill = factor ( Treatment ) ) ) + xlab ( expression ( bold ( \"Mixing Technique\" ) ) ) + ylab ( expression ( bold ( \"Strength\" ) ) ) + theme_bw ( ) + opts ( title = bquote ( bold ( 'Mean of Tensile Strength (lb/in' ^ '2' * ') by Treatment' ) ) , plot.title = theme_text ( size = 20 , colour = \"darkblue\" ) , panel.border = theme_rect ( size = 2 , colour = \"red\" ) , legend.position = \"none\" )  #(2.a) Rework second part of Problem (1) using Duncan\u2019s multiple range test #with \u03b1=0.05. Does this make any difference in your conclusions?  #INPUT duncan.test ( Model , \"Treatment\" )  #(2.b) Rework second part of Problem (1) using Tukey\u2019s test with \u03b1=0.05. Do #you get the same conclusions from Tukey\u2019s test that you did from the #graphical procedure and/or Duncan\u2019s multiple range test?  #INPUT TukeyHSD ( Model )"], "link": "http://alstatr.blogspot.com/feeds/584483697780031243/comments/default", "bloglinks": {}, "links": {"http://inside-r.org/": 93, "http://www.blogger.com/blog": 7}, "blogtitle": "ALSTAT R Blog"}, {"content": ["I have been working at Provincial Statistics Office of Tawi-Tawi (Philippines) which was part of the training on my OJT (On-Job Training). One of the requirements of the training is at least 80 hours of services, so I decided to work from April 19 to May 18, 2012, making it sure to surpass the required hours.   In that office, there is this daily records of our attendance which has a Sign in and Sign Out columns, where we actually put the time we arrive and dismiss, respectively. The first time I notice this, I started to get excited and very particular on the time I input, because I know that at the end of my training I'll be collecting this and make some analysis.   Here's the first plot below, which shows the Arrival and Dismissal time.   In the plot, notice that there are two groups of arrival and dismissal points. The first arrival points are dotted in the early morning of the day, this is due to the schedule of services of the office which opens at 8:00 am and will close at 12:00 pm. And the other one are in the afternoon, at 1:00 pm and will close at 5:00 pm. And I'm home, taking my lunch between 12:00 pm and 1:00 pm.  Now, It is clearly shown in the plot that I've been late in most days, as seen on the arrival points which is dotted between 12 and 14, (12:00 pm and 2:00 pm, respectively). There are cases that I went to the office at 2:00 pm, which is very late, and this happened on May 10. Similarly, I've been late eight times in the morning, where five of it happened in the last week of my training.  The next plot below shows the number of hours I've spent in the morning (8:00 am - 12:00 pm).    The trend of the blue lines are not consistent, this is expected since I was not able to maintain my arrival time, As you've known earlier that I was late in most days. By the way, there's no services during weekends, but as you notice on April 28 which is Saturday I've spent 4.2 hours, this was actually a general cleaning in the office, and so we the trainees were ordered to report on the said date to help on cleaning also, and in change for that is nonworking day in April 30, that's Monday. But that didn't happen to me, since I was given a special task by my boss, and was told to report on Monday morning, and that's why I've spent 3.8 hours in that day. In the average, I've spent about 4 hours in the morning, black horizontal line in the plot represents it.    Here's the plot of the hours I've spent in the afternoon of my training, that's from 1:00 pm to 5:00 pm. And the blank in the plot is a missing value, this is because I only reported in the morning of the April 30.   And on the average, I spent about 3.7 hours in the afternoon of my training, as shown in the plot. Now the longest hours recorded in my entire training in the afternoon is 5 hours, this is because of the whole day general cleaning which happened on April 28. And the lowest number of hours I've spent happened on May 10, this is because I arrived at 2:00 pm in that day.   And for overall, the number of hours I've spent in a day is shown below. And on the average, I've spent about 7.4 hours a day.   There is a huge decline on April 30, that's 3.8 hours only a day. Well, that's because I only reported in the morning. Finally, the largest number of hours ever recorded on my training was on April 28, since it was a whole day cleaning.  R Codes         <        <   <                                                     <       <                                                                           <       <                                                                            <       <"], "link": "http://alstatr.blogspot.com/feeds/7623516451477154999/comments/default", "bloglinks": {}, "links": {"http://inside-r.org/": 79, "http://www.blogger.com/blog": 4}, "blogtitle": "ALSTAT R Blog"}, {"content": ["R Codes   library ( ggplot2 ) TawiTawiGrowthRate <- as.numeric ( c ( 2.6 , 3.3 , 5.9 , 1.6 , 1.8 , 5.5 , 5 ) ) CensalYear <- c ( \"1948-1960\" , \"1960-1970\" , \"1970-1980\" , \"1980-1990\" , \"1990-1995\" , \"1995-2000\" , \"2000-2007\" ) qplot ( CensalYear , TawiTawiGrowthRate , xlab = expression ( bold ( \"Censal Year\" ) ) , ylab = expression ( bold ( \"Growth Rate\" ) ) , ylim = c ( 1.3 , 6.3 ) ) + geom_path ( aes ( group = 1 ) , size = 1.3 , colour = \"green\" ) + theme_bw ( ) + geom_point ( size = 13 , colour = \"blue\" , fill = \"white\" , pch = 21 ) + opts ( title = expression ( bold ( \"Tawi-Tawi: Average Annual Population Growth Rate\" ) ) , plot.title = theme_text ( size = 18 , colour = \"darkblue\" ) , panel.border = theme_rect ( linetype = \"dashed\" , colour = \"red\" ) ) + geom_text ( aes ( label = TawiTawiGrowthRate ) , angle = - 0.13 , size = 4 , colour = \"black\" )"], "link": "http://alstatr.blogspot.com/feeds/6737188579319461798/comments/default", "bloglinks": {}, "links": {"http://inside-r.org/": 14, "http://www.blogger.com/blog": 1}, "blogtitle": "ALSTAT R Blog"}, {"content": ["R Codes  library(ggplot2)  library(grDevices)  IliganCity <- c(104493, 118778, 167358, 226568, 273004, 285061, 308046, 322821)  CensalYear <- c(\"1970\", \"1975\", \"1980\", \"1990\", \"1995\", \"2000\", \"2007\", \"2010\")  qplot(CensalYear, IliganCity, xlab = expression(bold(\"Censal Year\")),    ylab = expression(bold(\"Population\")), geom = \"bar\", colour = I(\"red\"),    fill = IliganCity, stat = \"identity\", ylim = c(0, 370000)) + theme_bw() +    opts(     title = expression(bold(\"Iligan City Population from 1970 to 2010\")),     plot.title = theme_text(size = 18, colour = \"darkblue\"),     panel.border = theme_rect(linetype = \"dashed\", colour = \"red\"),     legend.position = \"none\") + geom_text(aes(label = IliganCity, angle = 90,     hjust = -0.1), size = 4)"], "link": "http://alstatr.blogspot.com/feeds/4394856576117931535/comments/default", "bloglinks": {}, "links": {"http://www.blogger.com/blog": 1}, "blogtitle": "ALSTAT R Blog"}, {"content": ["R Codes  library(ggplot2)  library(colorRamps)  TawiTawiPop <- c(17000, 45000, 46000, 59000, 79000, 110000, 143000, 195000, 228204,      250718, 322317, 450346, 366550)  YearNames <- c(\"1903\", \"1918\", \"1939\", \"1948\", \"1960\", \"1970\", \"1975\", \"1980\", \"1990\",      \"1995\", \"2000\", \"2007\", \"2010\")  qplot(YearNames, TawiTawiPop,    xlab = expression(bold(\"Censal Year\")),    ylab = expression(bold(\"Population\")),    geom = \"bar\",    stat = \"identity\", colour = I(\"red\"),    fill = matlab.like2(13), ylim = c(0, 520000)) + theme_bw() +    opts(     title = expression(bold(\"Tawi-Tawi Population from 1903 to 2010\")),     plot.title = theme_text(size = 18, colour = \"darkblue\"),     legend.position = \"none\",     panel.border = theme_rect(linetype = \"dashed\", colour = \"red\"),     plot.title = theme_text(size = 18, colour = \"darkblue\")) +     geom_text(aes(label = TawiTawiPop, angle = 90, hjust = -0.1), size = 4)"], "link": "http://alstatr.blogspot.com/feeds/1244833942154001066/comments/default", "bloglinks": {}, "links": {"http://www.blogger.com/blog": 1}, "blogtitle": "ALSTAT R Blog"}, {"content": ["To run the Cochran Q Test in R, we need to download the package of it first, since it is not built-in in R. The name of the package is RVAideMemoire authored by Maxime Herv\u00e9. Here's how to do it.    Codes:  Here we are installing the package named RVAideMemoire. Press 'Enter/Return' key then there will be a pop-up window asking you to choose your CRAN Mirror. Select your country or any country you want, then hit 'Ok', and it will be downloaded and installed. After installation, you can now run the package, and to do that you need to type the following code,  Note: Every time you use the Cochran Q Test function make sure you call the package first. This time lets try and ask help for the function of it.   In the documentation, you'll see an example and the syntax of the test. So lets try an example.  Example: Four methods (A, B, C, and D) of treating raw fabric to make it water repellent were tested for effectiveness on six types of fabric. A satisfactory result got a 1.     Method A   Method B   Method C   Method D    1  1  0  0   1  1  0  1   1  0  0  0   1  1  1  0   1  1  0  1   1  1  0  1    Solution:   1. H0: The four treatments are equally effective.   H1: The four treatments are not equally effective.   2. Level of significance: alpha = 0.05   3. Test Statistics:    4. Rejection Region: Reject the null hypothesis if X^2 >= 7.185. Otherwise, do not reject it.   5. Computation:     Method A   Method B   Method C   Method D   Li   (Li)^2    1  1  0  0  2  4   1  1  0  1  3  9   1  0  0  0  1  1   1  1  1  0  3  9   1  1  0  1  3  9   1  1  0  1  3  9    G1   G2   G3   G4   Li   (Li)^2    6  5  1  3  15  41           6. Decision: Since 9.31579 is greater than 7.185, then we reject the null hypothesis   7. Conclusion: Therefore, we have sufficient statistical evidence to conclude that the effectiveness of at least two treatments differ.   Computations in R   Now lets try and solve the above example using Cochran Q Test in R. First thing we need to do is to input the data.   Now, its better to run the length function, so that we would know if we miss something.   This time lets make the four columns and the 6 rows, i.e., the four methods and the 6 different types of fabric, respectively.  Now, lets combine the rows and columns to make a table.  The output of this should look like this.   So, as you can see everything is the same with the given table in the example. Now, lets try and confirm the above computations.    Clearly in the result, that our test statistics Q is equal to 9.3158 and the p-value is 0.025374. And since the p-value is less than the alpha, then we reject the null hypothesis. That's why, we have a pairwise comparisons by Wilcoxon sign test.  \"Special Thanks to: Tal Galili and Maxime Herv\u00e9\"  Example used: http://frank.mtsu.edu/~dwalsh/6604/6604COCH.pdf"], "link": "http://alstatr.blogspot.com/feeds/4116612533413894532/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://frank.mtsu.edu/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "ALSTAT R Blog"}, {"content": ["The codes below was done in our regression laboratory class. Here, we run first the data in SPSS, and take the ANOVA output where we can find the computed values of SSR, SSE, and SST.  \t \t \t \t  #Al-Ahmadgaid Asaad  #As part of the yearly report of the manager of human   #resources at St. Luke\u2019s Medical Center, he is requested to  #present analysis of the salaried employees. He selects a  #random sample of 30, from over 1,000 employees, and records   #the Monthly salary, service in months, and age of the  #employee.      ANOVA b      Model  Sum of Squares  df  Mean Square  F  Sig.   1  Regression  640267.552  1  640267.552  9.822  .004 a   Residual  1825213.815  28  65186.208        Total  2465481.367  29          a. Predictors: (Constant), Length of Service          b. Dependent Variable: Monthly Salary              \t \t \t \t  #Solving for the values of SSR,SSE, and SST using matrix in R.  #Codes:  #Lets input first everything we need in the computations.  #Inputting the given for the dependent and independent value.  IndependentVar &lt;-c(1,93,1,104,1,104,1,126,1,98,1,99,1,94,1,96,  [17] 1,124,1,73,1,110,1,90,1,104,1,81,1,106,1,113,1,129,1,97,1,  [37] 101,1,91,1,100,1,123,1,88,1,117,1,107,1,105,1,86,1,131,1,  [58] 95,1,98)    DependentVar <- c(1769,1740,1941,2367,2467,1640,1756,1706,  [9] 1767,1200,1706,1985,1555,1749,2056,1729,2186,1858,1819,  [20] 1350,2030,2550,1544,1766, 1937,1691,1623,1791,2001,1874)    #Next we put them in a matrix    X <- matrix(IndependentVar, ncol = 2, byrow = T)    Y <- matrix(DependentVar, ncol = 1, byrow = T)    #This time lets solve for the X\u2019 and Y\u2019.    XPrime <- t(X) #transposing X  YPrime <- t(Y) #transposing Y    #Now, lets make an Identity matrix    n <- c(30) #here we assign our n equal to 30  I <- matrix(0, nrow = n, ncol = n) #we define an n by n         #matrix here that has an entries of 0.  I[row(I) == col(I)] <- 1 #here we assign a value 1 if the ith         #rows is equal to the jth columns.    #Since we have our Identity matrix now, it\u2019s easy for us then  #to make a J matrix that consist of entries 1.    J <- matrix(1, nrow = n, ncol = n)    #We are almost ready for computation, but before that we need  #to define first our H. To do that lets define the inverse  #first.  XXPrime <- XPrime%*%X  #here we define X\u2019X  Inverse <- solve(XXPrime) #here we have (X\u2019X)^(-1)  H <- X%*%Inverse%*%XPrime  #Ok we\u2019re done with all the variables we need in our  #computation, this time lets compute for SSR.    SSRCen <- H \u2013 J/30 #here we define (H \u2013 J/n)  SSR <- YPrime%*%SSRCen%*%Y  SSR    [,1]  [1,] 640267.6    #Next, we compute for SSE    SSECen <- I \u2013 H #here we define (I \u2013 H)  SSE <- YPrime%*%SSECen%*%Y  SSR    [,1]  [1,] 1825214    #Time to smile, we are approaching to the finish line. Lets  #make it fast and solve for SST :)  SSTCen <- I \u2013 J/30 #here we define (I \u2013 J/n)  SST <- YPrime%*%SSTCen%*%Y  SST    [,1]  [1,] 2465481  #Ok there you go, we got the values of SSR, SSE, and SST. You  #can confirm that in SPSS (we did it actually).    #Moreover, you can also confirm SSR by using SSR = SST \u2013 SSE  SSR <- SST \u2013 SSE  SSR    [,1]  [1,] 640267.6    #In addition, we can also solve for matrix beta\u2019s, i.e. the  #beta null and beta one.  Beta <- Inverse%*%XPrime%*%Y  Beta    [,1]  [1,] 761.04720  [2,] 10.48381    #There you go, everything is done, and we\u2019re ready to submit  #it. :)"], "link": "http://alstatr.blogspot.com/feeds/8934972051530901260/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "ALSTAT R Blog"}, {"content": ["The recovery time (in days) is measured for 10 patients taking a new drug and for 10 different patients taking a placebo. We wish to test the hypothesis that the mean recovery time for patients taking the drug is less than for those taking placebo. The data are:    With drug: 15,10,13,7,9,8,21,9,14,8  Placebo: 15,14,12,8,14,7,16,10,15,12  A. Perform a hypothesis testing under an assumption of normality and equal population variances.  Solution:  Codes:   Output:   Interpretation: We lack the evidence to reject the null hypothesis with p-value at 0.3002, hence the true difference in mean recovery time for patients taking drug is either equal to or greater than those taking placebo.  B. Perform a hypothesis testing under an assumption of normality and unequal population variances.  Solution:  Codes: (Using the defined data above)  Output:   Interpretation: Still, we lack the evidence to reject the null hypothesis at p = 0.3006, and so the true difference in mean recovery time for patients taking the drug is either equal to or greater than those taking a placebo."], "link": "http://alstatr.blogspot.com/feeds/7397036327193740974/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "ALSTAT R Blog"}, {"content": ["Using the stack loss dataset, test the hypothesis that the mean of the stackloss is equal to 20 versus a two-sided alternative.  Solution:  Codes:   Output:   Interpretation: With the p-value greater than the level of significance alpha at 0.05, then we lack the evidence to reject the null hypothesis. And thus, the true mean is equal to 20."], "link": "http://alstatr.blogspot.com/feeds/2797009517181209085/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "ALSTAT R Blog"}]