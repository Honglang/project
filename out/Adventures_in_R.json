[{"blogurl": "http://adventuresinr.wordpress.com\n", "blogroll": [], "title": "Adventures in R"}, {"content": ["#For use when comparing the fit of nested models with complex data, \n # (e.g. TYPE = COMPLEX is mplus) \n # The Scaled Difference Chi-square Test Statistic can be found at \n #http://preprints.stat.ucla.edu/260/chisquare.pdf \n # This function provides scaled differences tests based on chi-square \n # and loglikihood values. \n # The notation below is taken from http://www.statmodel.com/chidiff.shtml \n \n #For the chi-square difference test (which is the default in this function), \n # the notation from \n #www.statmodel.com reads: \n # \"d0 is the degrees of freedom in the nested model, \n #c0 is the scaling correction factor for the nested model, \n #d1 is the degrees of freedom in the comparison model, \n #and c1 is the scaling correction factor for the comparison model. \n #Be sure to use the correction factor given in the output for the H0 model. \n #... T0 and T1 are the MLM, MLR, \n #or WLSM chi-square values for the nested and comparison model, respectively.\" \n \n #For the difference test using logliklihood value (loglike=TRUE): \n #d0 and d1 are the number of parameters for the H0 and H1 models. \n #c0 and c1 are the scaling correction factors for the H0 and H1 models. \n # t0 and t1 are the loglikelihood values for the H0 and H1 models. \n \nscale.diff.test <- function ( d0 , d1 , c0 , c1 , t0 , t1 , loglike= FALSE ) { \n if ( loglike ) { cd.2 <- ( d0 * c0 - d1 * c1 ) / ( d0 - d1 ) \n x <- t0 - t1\n TRd.2 <- - 2 * ( x ) / cd.2\n df.2 <- abs ( d0 - d1 ) \n p.2 <- pchisq ( TRd.2 , df.2 ) \n sig.2 <- 1 - p.2\n cat ( \"scaled loglikelihood difference test for complex data:\" , \" \\n \" ) \n cat ( \"value =\" , TRd.2 ) \n cat ( \" df=\" , df.2 ) \n cat ( \" sig. =\" , sig.2 , \" \\n \" ) } \n else { cd <- ( d0 * c0 - d1 * c1 ) / ( d0 - d1 ) \n TRd <- ( t0 * c0 - t1 * c1 ) / cd\n df <- abs ( d0 - d1 ) \n p <- pchisq ( TRd , df ) \n sig <- 1 - p\n cat ( \"scaled Chi-squared difference test for complex data:\" , \" \\n \" ) \n cat ( \"value =\" , TRd ) \n cat ( \" df=\" , df ) \n cat ( \" sig. = \" , sig , \" \\n \" ) } \n } \n \nscale.diff.test ( 45 , 43 , 1.12 , 1.123 , 246.44 , 237.846 ) \nscale.diff.test ( 39 , 47 , 1.45 , 1.546 , - 2606 , - 2583 , loglike= TRUE ) \n \nscale.diff.test ( 162 , 166 , 1.088 , 1.09 , 777.043 , 791.371 ) \nscale.diff.test ( 166 , 162 , 1.09 , 1.088 , 791.371 , 777.043 ) \n \nscale.diff.test ( 77 , 72 , 1 , 1 , 250 , 231 ) \n \n \n Created by Pretty R at inside-R.org \n Examples: \nscale.diff.test (45, 43, 1.12, 1.123, 246.44, 237.846) \nscale.diff.test (39, 47, 1.45, 1.546, -2606, -2583, loglike=TRUE) \n Output: \nscaled Chi-squared difference test for complex data: \nvalue = 8.443147 df= 2 sig. = 0.01467553 \n scaled loglikelihood difference test for complex data: \nvalue = 22.84012 df= 8 sig. = 0.003575695"], "link": "http://adventuresinr.wordpress.com/2011/03/16/function-for-scaled-difference-tests-chi-square-and-logliklihood-values/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 16, "http://www.inside-r.org/": 1}, "blogtitle": "Adventures in R"}, {"content": ["library ( MASS ) \ncovar <- mvrnorm ( 250 , c ( 0 , 0 ) , matrix ( c ( 1 , 0.00 , 0.00 , 1 ) , 2 , 2 ) ) \nmydata <- data.frame ( covar ) \n names ( mydata ) <- c ( \"sat\" , \"mot\" ) \nmydata $ admin <- mydata $ sat + mydata $ mot\nmydata $ admin2 <- ifelse ( mydata $ admin >= quantile ( mydata $ admin , .85 ) , \"pass\" , \"fail\" ) \n \n library ( ggplot2 ) \nqplot ( sat , mot , data = mydata , color = admin2 ) \n \n \n Created by Pretty R at inside-R.org"], "link": "http://adventuresinr.wordpress.com/2011/02/24/collider-example/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 14, "http://adventuresinr.wordpress.com/": 1, "http://www.inside-r.org/": 1}, "blogtitle": "Adventures in R"}, {"content": ["I am sure it is out there but for my record keeping purposes and for anyone interested here is a quick function to find Stirling\u2019s approximation of all possible permutations \n \n \n Stirling <- function ( n ) ( ( 2 * pi * n ) ^ .5 ) * ( n / exp ( 1 ) ) ^ n \n \n \n Created by Pretty R at inside-R.org"], "link": "http://adventuresinr.wordpress.com/2011/02/01/stirlings-formula-for-r/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 1, "http://www.inside-r.org/": 1}, "blogtitle": "Adventures in R"}, {"content": ["R does not give a Pseudo R square value and neither does Mplus. As such, here is a real quick R function to calculate McFadden\u2019s and Adjusted McFadden\u2019s Pseudo R square value: \n \n \n \n #Pesudo R square \n #x = full model \n #y = null model \n #k = number of parameters \n \nPRsq <- function ( x , y , k ) { \na= 1 - ( x / y ) \nb= 1 - ( ( x - k ) / y ) \n cat ( \"Pesudo R Square Estimates\" , \" \\n \" ) \n cat ( \"McFadden's:   \" , a , \" \\n \" ) \n cat ( \"adjusted McFadden's: \" , b , \" \\n \" ) \n }"], "link": "http://adventuresinr.wordpress.com/2011/01/26/pseudo-r-square-values-in-r/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 4}, "blogtitle": "Adventures in R"}, {"content": ["I have reworked the graphing and summarizing functions I wrote for the matchit package to be cleaner and to provide more information. \n Currently I have functions for a sorted by size summary table of the post matching mean differences: \n \n \n meandifftable<- function ( x ) { \n\tpost<- data.frame ( x$sum.matched [ 4 ] ) \n\tmatchID <- as.vector ( row.names ( post ) ) \n\t names ( post ) [ 1 ] <- c ( \"m_mean_diff\" ) \n\tpost$absolute<- abs ( post [ 1 ] ) \n\ttotal2<-post [ order ( -post$absolute , na.last= NA ) , ] \n\tmeandiffover1<- subset ( total2 [ 1 ] , total2 [ 1 ] > .1 | total2 [ 1 ] < - .1 ) \n\tmeandiffover1\n } \n \n \n A graph function (histogram and density plot) for both pre-matched mean differences: \n \n \n all_meandiffplot <- function ( x ) { \n\tadiff<- data.frame ( x$sum.all ) \n\t names ( adiff ) [ 4 ] <- c ( \"all_mean_diff\" ) \n\tdiffplot<- ggplot ( adiff , aes ( all_mean_diff ) ) \n\tdiffplot<- diffplot+ geom_histogram ( fill= \"grey\" ) \n\tdiffplot<- diffplot+ geom_density ( colour= \"red\" ) \n\tdiffplot<-diffplot+xlim ( - .5 , .5 ) \n\tdiffplot\n\t } \n \n \n and after matching mean differences: \n \n \n matched_meandiffplot <- function ( x ) { \n\tmdiff<- data.frame ( x$sum.matched ) \n\t names ( mdiff ) [ 4 ] <- c ( \"matched_mean_diff\" ) \n\tdiffplot<- ggplot ( mdiff , aes ( matched_mean_diff ) ) \n\tdiffplot<- diffplot+ geom_histogram ( fill= \"grey\" ) \n\tdiffplot<- diffplot+ geom_density ( colour= \"red\" ) \n\tdiffplot<-diffplot+xlim ( - .5 , .5 ) \n\tdiffplot\n\t } \n \n \n Not that both plots are on a scale from -.5 to .5 so that they can easily be compared. \n Finally I have a set of simple tables which indicate how many \u201clarge\u201d (>.25), \u201cmedium\u201d (>.20) and \u201csmall\u201d (<.20) standardized mean differences there were before matching: \n \n \n all_meandiffcount <- function ( x ) { \n\tall <- data.frame ( x $ sum.all [ 4 ] ) \n\t all $ all_group [ all [ 1 ] > .25 ] <- \"Large\" \n \t all $ all_group [ all [ 1 ] < - .25 ] <- \"Large\" \n\t all $ all_group [ all [ 1 ] > .20 & all [ 1 ] < .25 ] <- \"Medium\" \n\t all $ all_group [ all [ 1 ] < - .20 & all [ 1 ] > - .25 ] <- \"Medium\" \n\t all $ all_group [ all [ 1 ] < .20 & all [ 1 ] > .00 ] <- \"Small\" \n\t all $ all_group [ all [ 1 ] > - .20 & all [ 1 ] < .00 ] <- \"Small\" \n\t table ( all $ all_group ) \n } \n \n \n and after matching: \n \n \n matched_meandiffcount <- function ( x ) { \n\tmatched <- data.frame ( x $ sum.matched [ 4 ] ) \n\tmatched $ matched_group [ matched [ 1 ] > .25 ] <- \"Large\" \n \tmatched $ matched_group [ matched [ 1 ] < - .25 ] <- \"Large\" \n\tmatched $ matched_group [ matched [ 1 ] > .20 & matched [ 1 ] < .25 ] <- \"Medium\" \n\tmatched $ matched_group [ matched [ 1 ] < - .20 & matched [ 1 ] > - .25 ] <- \"Medium\" \n\tmatched $ matched_group [ matched [ 1 ] < .20 & matched [ 1 ] > .00 ] <- \"Small\" \n\tmatched $ matched_group [ matched [ 1 ] > - .20 & matched [ 1 ] < .00 ] <- \"Small\" \n\t table ( matched $ matched_group ) \n }"], "link": "http://adventuresinr.wordpress.com/2010/11/13/additional-functions-for-the-matchit-package-part-2/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 40, "http://adventuresinr.wordpress.com/": 2}, "blogtitle": "Adventures in R"}, {"content": ["Have been attending a seminar on propensity score matching this week, when I was asked if I could write a function to reduce the complexity of the matchit package output. I produced two functions. \n First I wrote a function to produce a histogram of the matched mean differences which gives a nice summary of how well the matching procedure has achieved balance (note you need to install the package ggplot2 to use this function): \n \n meandiffplot <- function ( x ) { \n\tmdiff <- data.frame ( x $ sum.matched ) \n\t names ( mdiff ) [ 4 ] <- c ( \"m_mean_diff\" ) \n\tdiffplot <- ggplot ( mdiff , aes ( m_mean_diff ) ) \n\tdiffplot <- diffplot + geom_histogram ( fill= \"grey\" ) \n\tdiffplot <- diffplot + geom_density ( colour= \"red\" ) \n\tdiffplot \n\t } \n \n Next I produced a function the reports only those matched mean differences with a\u00a0standardized\u00a0difference\u00a0of over .1 (sorted by absolute size). \n \n meandifftable <- function ( x ) { \n\tpost <- data.frame ( x $ sum.matched [ 4 ] ) \n\tmatchID <- as.vector ( row.names ( post ) ) \n\t names ( post ) [ 1 ] <- c ( \"m_mean_diff\" ) \n\tpost $ absolute <- abs ( post [ 1 ] ) \n\ttotal2 <- post [ order ( - post $ absolute , na.last= NA ) , ] \n\tmeandiffover1 <- subset ( total2 [ 1 ] , total2 [ 1 ] > .1 | total2 [ 1 ] < - .1 ) \n\tmeandiffover1\n } \n \n If you use the matchit package I think these functions are really useful."], "link": "http://adventuresinr.wordpress.com/2010/11/02/simplified-output-for-matchit-package/", "bloglinks": {}, "links": {"http://inside-r.org/": 13, "http://feeds.wordpress.com/": 1, "http://gking.harvard.edu/": 1, "http://adventuresinr.wordpress.com/": 1}, "blogtitle": "Adventures in R"}, {"content": ["I am a fan of K-means approaches to clustering data particularly when you have a theoretical reason to expect a certain number of clusters and you have a large data set. However, I think ploting the cluster means can be misleading. Reading though Hadley Wickham\u2019s ggplot2 book he suggest the following, to which I add a few little change. \n \n \n #First we run the kmeans analysis: In brackets is the dataset used \n #(in this case I only want variables #1 through 11 hence the [1:11]) \n #and the number of clusters I want produced (in this case 4). \n \ncl <- kmeans ( mydata [ 1 : 11 ] , 4 ) \n \n #We will need to add an id variable for later use. In this case I have called it .row. \n \nclustT1WIN $ .row <- rownames ( clustT1WIN ) \n \n #At this stage I also make a new variable indicating cluster membership as below. \n # I have a good #idea of what my clusters will be called so \n #I gave them those names in the second line of the code. \n #Then I put it together and put the data in a form that is good for graphing. \n \ncluster <- cl $ cluster\n \ncl.cluster <- as.vector ( recode ( cluster , \"1='FC'; 2='FV'; 3='SO'; 4= 'OS' \" , \nas.numeric.result= FALSE ) ) \n \nclustT1WIN2 <- data.frame ( clustT1WIN [ 1 : 12 ] , cl.cluster ) \nmolten2 <- melt ( clustT1WIN2 , id = c ( \".row\" , \"cl.cluster\" ) ) \n \n #OK set up the graph background. \n #Following the ggplot book I also create a jit parameter cause it is \n #much easier to alter this and type it in than the full code over and over again. \n \npcp_cl <- ggplot ( molten2 , aes ( variable , value , group = .row , colour = cl.cluster ) )   \njit <- position_jitter ( width = .08 , height = .08 ) \n \n #Ok first graph the cluster means. \n \npcp_cl + stat_summary ( aes ( group = cl.cluster ) , fun.y = mean , geom = \"line\" ) \n \n #Then we produce a colourful but uninformative parallel coordinates \n #plot with a bit of alpha blending and jitter. \n \npcp_cl + geom_line ( position = jit , alpha = 1 / 5 ) \n \n #All code up to this point is as per Wickham but \n #I also add the cluster means graph that we \n #first produced as well as changing the angle of the x axis text so it is readable. \n \npcp_cl + geom_line ( position = jit , colour = alpha ( \"black\" , 1 / 4 ) ) \n + stat_summary ( aes ( group = cl.cluster ) , fun.y = mean , geom = \"line\" , size = 1.5 ) \n + facet_wrap ( ~ cl.cluster ) + opts ( axis.text.x=theme_text ( angle= - 45 , hjust= 0 ) ) \n \n \n Relatively simple but visually very informative. Here is the final result: \n  The final product"], "link": "http://adventuresinr.wordpress.com/2010/10/22/displaying-k-means-results/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 8, "http://adventuresinr.wordpress.com/": 4}, "blogtitle": "Adventures in R"}, {"content": ["There maybe times when you need scores corrected for measurement error and latent factor loading. In these instances it can be\u00a0beneficial\u00a0to use \u00a0composite scores derived from one factor congeneric models. While easy they are time consuming to calculate by hand and I am unaware of any program that does it for you. The following code in R will take normal Mplus output (which must include\u00a0FSCOEFFICIENT FSDETERMINACY in the output line of your mplus input syntax) and produce a vector of fit and a vector of weights. I have also included the nifty feature of calculating latent variable reliability estimates. The code is in early stages of development so only supports scales with 2 to 4 items but you will get the idea. Now this looks like a lot of work BUT in reality all you need to do is specify where your mplus output file is located in the first line of code and then simply run all the code giving you\u00a0proportional\u00a0weights for composite score calculation in about 5 seconds. Again R code is in italics (compile these with the non-italics instructions removed before running). \n Creation of proportional factor score regression: Weights for composite score creation \n \n \n #First run your model in mplus and read it into R. This is done as follows \n \nout <- readLines ( \u201c D : MPLUSFILENAME.out\u201d ) \n \n #Next we use the grep feature to tell R to find aspects of our data \n #to include in the output we will produce. This has the advantage of \n #being the same no mater how long or short your Mplus input file is \n #(that means you dont need to constantly count lines and change input). \n \n #Lets start with chi square. This will give us the value, degrees of freedom and significance. \n \n # Chi Square \n \nind1 <- grep ( \u201cChi - Square Test of Model Fit\u201d , out ) [ 1 ] \n \nchisq <- c ( as.numeric ( substring ( out [ ind1 + 2 ] , 38 ) ) ) \n \n df <- c ( as.numeric ( substring ( out [ ind1 + 3 ] , 38 ) ) ) \n \np <- c ( as.numeric ( substring ( out [ ind1 + 4 ] , 38 ) ) ) \n \n #Next we will get the fit \n \n # CFI / TLI \n \nind2 <- grep ( \u201cCFI / TLI\u201d , out ) \n \ncfi <- c ( as.numeric ( substring ( out [ ind2 + 2 ] , 38 ) ) ) \n \ntli <- c ( as.numeric ( substring ( out [ ind2 + 3 ] , 38 ) ) ) \n \n #RMSEA \n \nind3 <- grep ( \u201cRMSEA\u201d , out ) [ 1 ] \n \nrmsea <- c ( as.numeric ( substring ( out [ ind3 + 2 ] , 38 ) ) ) \n \n # SRMR \n \nind4 <- grep ( \u201cSRMR\u201d , out ) [ 1 ] \n \nsrmr <- c ( as.numeric ( substring ( out [ ind4 + 2 ] , 38 ) ) ) \n \n #Next we calculate the latent factor reliabilities. \n #First we grab the factor loadings then the residuals \n \n #Factor Loadings \n \nind6 <- grep ( \u201cSTDYX Standardization\u201d , out ) \n \nPEST <- c (  as.numeric ( substring ( out [ ind6 + 6 ] , 23 , 28 ) ) ,        \n as.numeric ( substring ( out [ ind6 + 7 ] , 23 , 28 ) ) ,        \n as.numeric ( substring ( out [ ind6 + 8 ] , 23 , 28 ) ) ,        \n as.numeric ( substring ( out [ ind6 + 9 ] , 23 , 28 ) ) \n \n ) \n \n #Residuals \n \nRES <- c (  as.numeric ( substring ( out [ ind6 + 21 ] , 23 , 28 ) ) , \n \n as.numeric ( substring ( out [ ind6 + 22 ] , 23 , 28 ) ) , \n as.numeric ( substring ( out [ ind6 + 23 ] , 23 , 28 ) ) , \n as.numeric ( substring ( out [ ind6 + 24 ] , 23 , 28 ) ) \n ) \n \n #Next we calculate the reliability rounded to 2 decimal places. \n \nrel <- round ( ( sum ( PEST ) ^ 2 ) / ( ( sum ( PEST ) ^ 2 ) + sum ( RES ) ) , digits = 2 ) \n \n #We can now wrap the fit into a list and \n #move one to calculating the proportional weights \n # for the composite scores calculations. \n \nfit <- data.frame ( chisq , df , p , cfi , tli , rmsea , srmr , fsd , rel ) \n \n #Now specific to the factor scores \n #we want we will get the factor determinates and regression weights. \n \n #Factor score determinates \n \nind5 <- grep ( \u201d   FACTOR DETERMINACIES\u201d , out ) \n \nfsd <- c ( as.numeric ( substring ( out [ ind5 + 2 ] , 22 , 27 ) ) ) \n \n #Factor score regression weights \n \nind7 <- grep ( \u201cSUMMARY OF FACTOR SCORES\u201d , out ) \n \nFSRW <- c (  as.numeric ( substring ( out [ ind7 + 9 ] , 16 , 21 ) ) , \n as.numeric ( substring ( out [ ind7 + 9 ] , 30 , 35 ) ) , \n as.numeric ( substring ( out [ ind7 + 9 ] , 44 , 49 ) ) , \n as.numeric ( substring ( out [ ind7 + 9 ] , 58 , 63 ) ) ,  \n as.numeric ( substring ( out [ ind7 + 9 ] , 72 , 77 ) ) \n \n ) \n \n #Now we have the data we just calculate the weights \n \nFSTOT <- sum ( FSRW , na.rm=T ) \n \nFSPROP <- FSRW / FSTOT\n \n #Finally we call the output we need. \n \n #First we check the proportional weights add to 1 as required \n \n sum ( FSPROP , na.rm=T ) \n \n #Next we get the vector of fit values and the latent variable relatibility \n \nfit\n \n #Now all we need is the composite score weights \n \nFSPROP"], "link": "http://adventuresinr.wordpress.com/2010/10/13/creating-composite-scores-in-r/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 71}, "blogtitle": "Adventures in R"}, {"content": ["Calculating Cohen\u2019s D is a pain particularly when you have data which is not independent (e.g. is longitudinal). So I have written a function which take away the\u00a0hassle. It is in early form but produces a normal Cohen\u2019s D and a corrected Cohen\u2019s D for the\u00a0paired\u00a0nature of the data. \n \n \n CCD <- function ( x1 , x2 ) { \ndif <- mean ( x2 , na.rm= TRUE ) - mean ( x1 , , na.rm= TRUE ) \nSD <- ( sd ( x1 , na.rm= TRUE ) + sd ( x2 , na.rm= TRUE ) ) / 2 \nCOR <- cor ( x1 , x2 , use= \"complete.obs\" ) \t\nCD <- ( dif / SD ) \t\nCCD <- CD / ( ( 2 * ( 1 - COR ) ) ^ .5 ) \t\n list ( \"Cohen's d\" =CD , \"Corrected Cohen's d\" =CCD ) \n }"], "link": "http://adventuresinr.wordpress.com/2010/10/13/corrected-cohens-d/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 7}, "blogtitle": "Adventures in R"}, {"content": ["Ok now we have done a simple regression we will move to a more general strategy. In this case we will generate a simple covariance matrix with a mean structure. In this case I am only using this approach to generate 2\u00a0continuous variables but it can be extended to include many more. Furthermore this approach can\u00a0\u00a0be used for a number of purposes only limited by the fact I have not figured out how to include categorical variables in this system (let me know if you figure it out please). Note that will will need to install the package MASS. Again R code is in italics. \n Generating a covariance structure: \n \n \n #First install MASS which allows for the creation of covariance structures. \n #Then load said package. \n \n library ( MASS ) \n \n #Now we need to specify a multivariate normal covariance matrix. \n \ncovar <- mvrnorm ( 100 , c ( 0 , 0 ) , matrix ( c ( 1 , 0.50 , 0.50 , 1 ) , 2 , 2 ) ) \n \n #The first set of numbers = number of cases (here 100). \n #Second set in c() = means (here set to 0). \n #Third set = the covariances (variances here set at 1 and covariances at .5). \n #Fourth set = nature of matrix (here its a 2 by 2). Vary all of these as you like. \n #Now lets check the matrix looks right. To check the matrix has the setup you want use the following. \n \ncheck <- matrix ( c ( 1 , 0.50 , 0.50 , 1 ) , nrow = 2 , ncol = 2 , byrow= TRUE ) \n \n #Next we wrap it up into a dataframe \n \nmydata <- data.frame ( covar ) \n \n #Finally we give the variables names of interest \n # (best to let them reflect the sort of thing you do in research commonly) \n #and attach names for use \n \n names ( mydata ) <- c ( \"x1\" , \"x2\" ) \n \n attach ( mydata )"], "link": "http://adventuresinr.wordpress.com/2010/10/12/data-generation-in-r-part-2-covariance-matirx/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://inside-r.org/": 14}, "blogtitle": "Adventures in R"}]