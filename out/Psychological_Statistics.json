[{"blogurl": "http://psychologicalstatistics.blogspot.com\n", "blogroll": [], "title": "Psychological Statistics"}, {"content": ["In a break from my usual obsessions and interests here is a guest blog post by Ian Walker . I'm posting it because I think it is rather cool and hope it will be of interest to some of my regular readers. Ian is perhaps best known (in the blogosphere) for his work on transport psychology - particularly cycling - but is also an expert on psychological statistics. \n \nSome time ago, I had some data that lent themselves to a three-dimensional surface plot. The problem was, the plot was quite asymmetrical, and finding the right viewing angle to see it effectively on a computer screen was extremely difficult. I spent ages tweaking angles and every possible view seemed to involve an unacceptable compromise. \n \nOf course, displaying fundamentally three-dimensional items in two dimensions is an ancient problem, as any cartographer will tell you. That night, as I lay thinking in bed, a solution presented itself. I had recently been reading about the work of a fellow University of Bath researcher, Adrian Bowyer, and his RepRap project , to produce an open-source three-dimensional printer. The solution was obvious: I had to find a way to print R data on one of these printers! \n \nI managed to meet up with Adrian back in May 2012, and he explained to me the structure of the STL (stereolithography) files commonly used for three-dimensional printing. These describe an object as a large series of triangles. I decided I'd have a go at writing R code to produce valid STL files. \n \nI'm normally a terrible hacker when it comes to programming; I usually storm in and try to make things work as quickly as possible then fix all the mistakes later. This time, I was much more methodical. As a little lesson to us all, the methodical approach worked: I had the core code producing valid STL files in under 3 hours. \n \nUnfortunately, it then took until September 2012 before I could get hold of somebody with a 3D printer who'd let me test my code. A few days ago the first prototype was produced, as you can see in this photograph: \n \n \n \n \n \nSo now I'd like to share the code under a Creative Commons BY-NC-SA licence, in case anybody else finds it useful. You can download the code here , in a file called r2stl.r . One day, when I learn how, I might try to make this a library, but for now you can just call this code with R's source() command. All that is in the file is the function r2stl() , and having once called the file with source() , you can then use the r2stl function to generate your STL files. The command is: \n \n r2stl(x, y, z, filename='3d-R-object.stl', object.name='r2stl-object', z.expand=FALSE, min.height=0.008, show.persp=FALSE, strict.stl=FALSE) \n \n \n \n \n \n x , y and z should be vectors of numbers, exactly as with R's normal persp() plot. x and y represent a flat grid and z represents heights above this grid \n \n filename is pretty obvious, I hope \n \n object.name The STL file format requires the object that is being described to have a name specified inside the file. It's unlikely anybody will ever see this, so there's probably no point changing it from the default \n \n z.expand By default, r2stl() normalizes each axis so it runs from 0 to 1 (this is an attempt to give you an object that is agnostic with regard to how large it will eventually be printed). Normally, the code then rescales the z axis back down so its proportions relative to x and y are what they were originally. If, for some reason, you want your 3D plot to touch all six faces of the imaginary cube that surrounds it, set this parameter to TRUE \n \n min.height Your printed model would fall apart if some parts of it had z values of zero, as this would mean zero material is laid down in those parts of the plot. This parameter therefore provides a minimum height for the printed material. The default of 0.008 ensures that, when printed, no part of your object is thinner than around 0.5 mm, assuming that it is printed inside a 60 mm x 60 mm x 60 mm cube. Recall that the z axis gets scaled from 0 to 1. If you are printing a 60mm-tall object then a z-value of 1 represents 60mm. The formula is min.height=min.mm/overall.mm, so if we want a minimum printed thickness of 0.5mm and the overall height of your object will be 60mm, 0.5/60 = 0.008, which is the default. If you want the same minimum printed thickness of 0.5mm but want to print your object to 100mm, this parameter would be set to 0.5/100 = 0.005 \n \n show.persp Do you want to see a persp() plot of this object on your screen as the STL is being generated? Default is FALSE \n \n strict.stl To make files smaller, this code cheats and simply describes the entire rectangular base of your object as two huge triangles. This seems to work fine for printing, but isn't strictly proper STL format. Set this to TRUE if you want the base of your object described as a large number of triangles and don't mind larger files \n \n \n \n \nTo view and test your STL files before you print them, you can use various programs. I have had good experiences with the free, open-source Meshlab , which even has iPhone and Android versions so you can let people interact with your data even when you're in the pub. Even if all you ever do is show people your 3D plots using Meshlab, I believe r2stl() still offers a useful service, as it makes viewing data far more interactive than static persp() plots. To actually get your hands on a printer, you might try your local school - apparently lots of schools have got rapid prototypers these days. \n \n \n Demo \n \n source('r2stl.r') # Let's do the classic persp() demo plot, as shown in the photograph above x <- seq(-10, 10, length= 100) y <- x f <- function(x,y) { r <- sqrt(x^2+y^2); 10 * sin(r)/r } z <- outer(x, y, f) z[is.na(z)] <- 1 r2stl(x, y, z, filename=\"lovelyfunction.stl\", show.persp=TRUE) # Now let's look at R's Volcano data z <- volcano x <- 1:dim(volcano)[1] y <- 1:dim(volcano)[2] r2stl(x, y, z, filename=\"volcano.stl\", show.persp=TRUE) \n \n \nI hope you might find this code useful. Any questions or suggestions, then please get in touch. \n \n \nSeptember 2012 - Ian Walker , Department of Psychology, University of Bath."], "link": "http://psychologicalstatistics.blogspot.com/feeds/8418503535344214853/comments/default", "bloglinks": {}, "links": {"http://www.drianwalker.com/": 2, "http://meshlab.sourceforge.net/": 1, "http://www.reprap.org/": 1, "http://feedads.doubleclick.net/": 2, "http://drianwalker.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["I wasn't going to post on this ... but couldn't resist. A recent QJEP paper reports suspicious patterns in p values across three psychology journals. \n \n This has been blogged elsewhere (see here and here ), so I haven't got too much to add. Although I generally like the paper and am glad it got published in a decent journal (I'm an EPS member and subscriber so I'm glad they published it), I can't say I find the main finding surprising. Everything we know about how significance testing is used in practice would predict the basic pattern of a bump just below p = .05. \n \n My main criticism of the paper would be that they didn't use a multilevel model to account for dependencies in p values from the same paper - while I agree with the authors that the outcome wouldn't change, I think it could be potentially informative to model the dependency (and a simple multilevel model shouldn't be more than a few minutes to set-up and run if the data are coded by study). An even better approach for future work might be to sample papers from multiple journals and thus estimate the stability of the pattern across journals, disciplines etc. \n \n The main reason I'm blogging is because of a few points raised at other blogs. The generally excellent BPS research digest reported: \n \n Masicampo and Lalande said their findings pointed to the need to educate researchers about the proper interpretation of null hypothesis significance testing and the value of alternative approaches, such as reporting  effect sizes  and  confidence intervals . \" ... [T]he field may benefit from practices aimed at counteracting the single-minded drive toward achieving statistical significance,\" they said. \n  \n I couldn't find the effect sizes comment in the paper (but it may have been from conversations with the authors). I'm pretty sure all the journals surveyed reported effect sizes - point estimates of the magnitude of the effect. Perhaps they meant standardized effect size, but if so the advice is in my view doubly wrong. Standardized effect size has all sort of problems (see here and here for my take). \n  \n I won't repeat myself too much over this, but point out that standardized effect size generally obscures the thing you want to measure (as well as distorting it in several ways). That is the first reason why they aren't a solution to dodgy analyses - they will tend to make it harder for people to spot the problems in a study when there are meaningful, well-understood units (such as percentage recall, response times and so forth). They can maybe, sometimes help you interpret arbitrary measures - but generally only if you don't know the range of the scale. For instance, a shift of 3 points on a 7 point scale is huge and might make people suspicious whereas an effect size estimate of d = .6 or r = .4 might not. \n  \n The second reason is that effect size in general (and standardized effect size in particular) is at least as vulnerable as p values to publication bias, optional stopping and all the other known problems with statistical significance as routinely practiced. Combine small studies with a threshold for publication (e.g., p < .05 or d > 0.5) and effect size is substantially biased upwards (the bias is lower for large studies). Standardized effect size makes this worse because small samples tend to underestimate population variance and thus inflate the effect size estimate yet further. The final straw is that standardized effect size is easy to game. For instance, Cohen's d type metrics for a paired design come in at least four flavours (removing between-subject variance or not, crossed with a pooled or non-pooled error standard deviation). Removing the between-subject effect increases the reported d value artificially relative to an independent subjects design (even though the magnitude of the effect is generally assumed not to change). A similar problem occurs with variance explained measures such as eta-squared. People generally report partial eta-squared inappropriately in preference to classical eta-squared (which is generally smaller except in very simple designs).* \n  \n So, effect size is a red herring. The point on confidence intervals (and interval estimates in general) is a good one. \n  \n The piece at The Scholarly Kitchen (TSK) annoyed me a little. The tone implied that the problem with p values a problem in psychology. It isn't. It is a problem in any field that obsesses about p values (including most quantitative work in social sciences, medicine and science). I'm prepared to bet you'd get the same patterns in medical research (and they could well be much worse given some of the pressures to publish positive results in that area). More interesting TSK notes that JEP: General fared better than the other two journals looked at. TSK speculates that this is a result of better editing and reviewing. As it is the only one of the three journals I have reviewed for, I am disposed to agree with this. However, I suspect it may simply be that JEP: General publishes papers with methods that dilute the bias. In particular, I think it publishes more papers that focus on model fit and hence look for p > .05 more often. \n \n The QJEP paper also mentions replicability statistics in passing. Another red herring, I think. Replicability can't be reliably estimated from small samples (see here ). \n  \n  \n * Actually, if you have to report eta-squared measures you should use generalized eta-squared (see Olejnik & Algina, 2003; Baguley, 2012). \n  \n References \n  \n  \n \n Baguley, T. (2012, in press). Serious stats: A guide to advanced statistics for the behavioral sciences . Basingstoke: Palgrave. \n \n Olejnik, S., & Algina, J. (2003). Generalized eta and omega squared statistics: Measures of effect size for some common research designs. Psychological Methods, 8, 434-447."], "link": "http://psychologicalstatistics.blogspot.com/feeds/8261916757342750742/comments/default", "bloglinks": {}, "links": {"http://scholarlykitchen.sspnet.org/": 2, "http://www.wfu.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://en.wikipedia.org/": 2, "http://bps-research-digest.co.uk/": 2, "http://www.co.uk/": 1, "http://nottinghamtrent.academia.edu/": 3}, "blogtitle": "Psychological Statistics"}, {"content": ["... or to be more precise, what's up with experimental social psychology? \n \n \nA number of high profile cases of suspected (in some cases admitted) fraud have been highlighted in psychology recently - my own discipline - but they've (nearly all arisen in experimental social psychology. If you are unaware, the best known cases seem to be Diederik Stapel , Dirk Smeesters and now Lawrence Sanna . Another high profile case, Marc Hauser , is in a somewhat related field (but a stretch to call it experimental social psychology). The not so recent case of Karen Ruggiero could also be included. \n \n \n \nA separate problem - set aside from the problem of deliberate fraud - are some controversies over specific studies which have apparently hard to replicate effects. The discussion here is around Bargh's priming study and Bem's ESP study. There is ample discussion of this elsewhere , but the main point is that the standard practices in experimental social psychology may encourage publication of spurious effects. \n \n \n \nFraud and other kinds of academic misconduct are rare and far from confined to psychology - see retraction watch (though the scale of Stapel's fraud may have raise psychology's profile on its own). However, the spotlight is focusing heavily on social psychology the moment. My initial view was that experimental social psychology was coming up purely by coincidence, but the recent cases have made me wonder. In the rest of this post I'm going to sketch out some thoughts on what might be going on. \n \n \n \n(1) Coincidence. There remains quite good evidence for the whole thing being coincidence. Psychology and social psychology are popular fields with lots of researchers so there will (sadly) be a few frauds. Deliberate fraud is a rare event and (to quote the late, great Robert Abelson ) \"probability is lumpy\". Discrete random events are appear to be evenly or smoothly distributed in the long run (averaging over many events) so rare events are usually clustered in a given sample of small, fixed n . So if you look a 10 or 100 fraud cases there are bound to be clusters among certain disciplines. \n \n \n \n(2) Deep discipline-specific flaws. Is there something fundamentally wrong with experimental social psychology research itself? Perhaps. The Bem and Bargh cases point to problems such as lack of replication, pressure to publish, over-emphasis on p values, intolerance of messy data and desire for surprising or counter-intuitive effects. The problem with most of these arguments is that they are not discipline-specific and they are often cited as factors leading to fraud in other disciplines. On the other hand it may be that one or more of these factors are particularly pronounced in experimental social psychology (and I'll come back to this point later). \n \n \n \n(3) Enhanced scrutiny. There are three strong reasons to suspect enhanced scrutiny contributes to the recent cases. First, the reports of fraud or other problems are not independent. A case - particularly a big one such as the Diederik Stapel case - necessarily draws further scrutiny to particular journals, groups of researchers and perhaps the whole of a field or discipline. Second, several of the cases were uncovered by the same person: Uri Simonsohn . As Simonsohn works broadly in the area of experimental social psychology, it isn't that surprising that he applied his fraud detection tools to suspicious studies in his own field. Third, findings in experimental social psychology compete for explanatory power more closely with folk psychology explanations than most other fields. To put it another way, just about everyone can assess the plausibility of a study that looks at whether exposing people to old age related material in the lab makes walk faster when they leave the lab or whether eating meat makes you more aggressive. Moreover, they often have strong opinions on these kinds of findings. \n \n \n \nAt the moment, I think the enhanced scrutiny explanation looks like a strong contender to me. I wouldn't rule out coincidence, but I think we can expect to see a few more dodgy studies unearthed. I think we can also expect to see the label of social psychology expanded to include suspect research in related areas (such as Marc Hauser's work). \n \n \n \nNevertheless, I do think there is one area in which experimental social psychology may be particularly vulnerable to fraud or questionable research practices. High status journals often seek interesting (aka surprising) effects and large effect sizes in the papers they publish. Such findings are more likely to be false (e.g., see here ). This is part of a general problem with statistical significance which acts a filter (see Andrew Gelman's blog for lots of discussion on this ). A single small experiment can usually only detect relatively 'big' effects - hence it overestimates the size of effects. When you add an implicit requirement for 'big' effects you are biasing your journal or discipline to spurious and fraudulent results. Thus far experimental psychology isn't so different from other fields where small studies are common (e.g., much of medicine, health, neuroscience, biology, and education). The problem may be that effects are inherently smaller in experimental social psychology than other areas of psychology. \n \n \n \nI've put the label 'big' in italics because what we're really talking about is the detectability or discriminability of an effect (standardized effect size) - which is its size relative to the noise or error in the data. Experiments with social stimuli are inherently noisy because there are so many variables to control for and because it is often difficult to use big manipulations (as they tend to be pretty obvious to participants). Of course many of the effects may truly be tiny. For example the age priming effect seems plausible to me but I can't believe it would be a large absolute effect in terms of walking speed (easily swamped by other factors or exaggerated by them) - thus my guess is that the original Bargh study over-estimated the effect size ( as most early studies tend to ). \n \n \n \nI think that social psychology and psychology will learn from these cases and the increased scrutiny that seems to be around. I hope we will improve our statistical work, place greater value on replication and reduce the ridiculous pressure to publish ground-breaking, surprising, counter-intuitive work with high frequency. Ground-breaking work will get published, but you can't really tell what research will have real scientific impact until years later (at least two or three years and often much longer, in my view). I hope that psychologists (particularly editors and reviewers) will be more tolerant of messy data (see here ) and not quite perfectly watertight conclusions. Many fraudulent studies are detected because of data that are far too clean (real data tend to be messy)."], "link": "http://psychologicalstatistics.blogspot.com/feeds/3440129871350770610/comments/default", "bloglinks": {}, "links": {"": 1, "http://papers.ssrn.com/": 1, "http://blogs.discovermagazine.com/": 1, "http://feedads.doubleclick.net/": 2, "http://andrewgelman.com/": 2, "http://www.nature.com/": 1, "http://www.nih.gov/": 1, "http://en.wikipedia.org/": 1, "http://neuroskeptic.co.uk/": 1, "http://amzn.to/": 1, "http://retractionwatch.wordpress.com/": 3}, "blogtitle": "Psychological Statistics"}, {"content": ["My serious stats book is officially published (in the UK at least). The US release date is next month (August 7th). I'm not sure why the release is later (possibly extra shipping time for the books). The earlier European release date is I suppose compensation for the usual pattern for most books and DVDs (where the US is usually first). \n \nMore details on the companion blog for the book . The book blog also contains additional resources related to the book and updates on topics covered in the book. Past posts include CIs for differences in correlations, alternatives to the Friedman test, graphing condition means for ANOVA designs, Upcoming plans include (among other things) posts on multilevel ordered logistic regression, p values from multilevel models, order restricted inference ... as soon as I get the time."], "link": "http://psychologicalstatistics.blogspot.com/feeds/8082734134750023512/comments/default", "bloglinks": {}, "links": {"http://amzn.to/": 1, "http://feedads.doubleclick.net/": 2, "http://seriousstats.wordpress.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["I've been thinking about p values quite a bit recently - prompted by a other bloggers and by some journal work. One interesting phenomenon in this area is the cliff effect: a supposed abrupt shift in researchers' confidence in an effect when moving from p > .05 to p < .05 (assuming that alpha is set at .05, as it usually is). \n \n The p value cliff effect is interesting for a number of reasons (e.g., as a possible cause of dichotomous thinking about effects - see Rosenthal & Gaito, 1963). Personally (e.g., Baguley, 2012a), I think that the tendency to dichotomous thinking is more subtle and complex - and not just a consequence of the cliff effect. Furthermore, there is some doubt about the prevalence of the cliff effect. Poitevineau and Lecoutre (2001) argued that there was a clear cliff effect in only a minority of participants (4 out of 18 in their sample of researchers with PhDs). The appearance of a cliff in the full sample came from averaging over a mix of patterns among the participants - with, for instance, some showing linear increases and some exponential increases in confidence as p decreased. \n \n My own musings on this are about whether there is a 'correct' pattern of confidence in an effect as p decreases. I'm not sure a sensible answer is possible, but I think it is possible to have a stab at answering if you make a few assumptions. First, assume that confidence is driven by the evidential value of a p value. This allows us to draw on the likelihood principle (and hence have some mathematical and philosophical basis for the analysis). I'm not going to explain likelihood or the likelihood principle here - though there is a brief introduction in Chapter 11 of Serious Stats (Baguley, 2012b). \n \n Proceeding on this basis, you hit a major stumbling block. The evidential value in the data is contained in the likelihood function - but the p value doesn't give us the likelihood function (at least, not straight-forwardly). Furthermore, the law of likelihood implies that you can't get the evidence for an effect from either p or the likelihood. Evidence for a specific hypothesis (e.g., a null hypothesis such as mu = 0) can only be assessed relative to a specific alternative hypothesis (e.g., mu = 10). So even if you could get at the likelihood function you'd need to know what alternative hypothesis each researcher had in mind when they evaluated p . To make the problem tractable you could assume that the assume the alternative hypothesis at the most likely value (the maximum likelihood estimate). For simple situations this boils down to the observed parameter estimate of interest (e.g., a mean or difference in means). This is helpful because you can go from a p value derived from a z statistic to an approximate maximum likelihood ratio fairly easily (with additional assumptions about the asymptotic distribution of the test statistic).* The maximum likelihood ratio is the largest possible ratio of the evidential support in favour of the alternative hypothesis relative to the null hypothesis (of zero effect) from the data at hand (ignoring support from other sources). This is the most favourable assessment of the evidence for the alternative hypothesis and against the null on the basis of the data alone (and true evidential support is almost certain to be lower) . \n \n Plotting the maximum likelihood ratio as a function of p gives the following curve: \n \n \n \n It flattens out when p > .20, so I've just shown the 'curvy' bit. Of course there is no particular reason why you'd use likelihood ratio ratio than say probability of the null hypothesis being false: \n \n \n \n \n \n This shows a more gradual curve. Yet another possibility is the logarithm of the maximum likelihood ratio (commonly used to index evidential support in statistics): \n \n \n \n \n Each of these is a plausible quantity to drive intuitions of evidential support (though you could easily arrive at different curves by making different - and perhaps more plausible - assumptions). \n \n The upshot is that neither a fairly abrupt cliff effect, nor a smooth (near linear) function nor a steep exponential-style curve would be unreasonable. It is also clear that the task itself - assessing confidence in an effect - is far from trivial. Almost any continuous monotonic function could be considered as rational given the right assumptions and there is no particular reason to expect researchers to be able to do the right computations in their heads ... \n \n On the other hand, a step function cliff effect does seem unreasonable - and it is definitely still interesting to understand the psychology of expert and non-expert statistical reasoning. However, we should be wary of assuming that people are being irrational or are bad at statistical reasoning on the basis of this kind of task. \n \n Post script . This is all predicated on a few idle speculations, several highly debatable assumptions and a quick play with some R code. It is quite possible I've overlooked something major ... \n \n * see Edwards et al. (1963) or Serious Stats Chapter section 11.4.2. \n \n References \n \n  \n Baguley, T. (2012a). Can we be confident in our statistics? The Psychologist, 25 , 128-9. \n \n Baguley, T. (2012b).  Serious stats: A guide to advanced statistics for the behavioral sciences  . Basingstoke: Palgrave. \n  \n Edwards, W., Lindman, H., & Savage, L. J. (1963). Bayesian statistical inference for psychological research. Psychological Review, 70 , 193-242. \n \n Poitevineau, J., & Lecoutre, B. (2001). Interpretation of significance levels by psychological \n researchers: The .05 cliff effect may be overstated. Psychonomic Bulletin & Review, 8 , \n 846-850. \n \n Rosenthal, R. & Gaito, J. (1963). The interpretation of levels of significance by psychological researchers. Journal of Psychology, 5 5, 33-38. \n \n Rosenthal, R. & Gaito, J. (1963). The interpreta tion of levels of significance by psychological researchers. Journal of Psychology, 55, 33\ue00338."], "link": "http://psychologicalstatistics.blogspot.com/feeds/2588197047120107212/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://nottinghamtrent.academia.edu/": 1, "http://en.wikipedia.org/": 2, "http://4.blogspot.com/": 2, "http://amzn.to/": 1, "http://2.blogspot.com/": 1, "http://seriousstats.wordpress.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["Neuroskeptic has just blogged on a new paper by Judd, Westfall and Kenny on Treating stimuli as a random factor in social psychology: A new and comprehensive solution to a pervasive but largely ignored problem . I can't access the original paper (which is supposed to be available via my University but hasn't appeared yet ...) but I know a little bit about the topic and thought I'd write a few words. \n \n  \n \n What stimulated me to write was a) a few of the comments on Neuroskeptic's blog, and b) that I've just written a book that covers the topic in some detail. (Yes - this book !). \n \n  \n \n The basic problem is that standard statistical analyses in psychology treat participants (subjects) as a random factor, but stimuli as a fixed factor. Thus our statistics assume that the goal of inference is to say something about some population that those participants are representative of (rather than just the particular people in our study). By treating stimuli as fixed it is assumed that we've exhaustively sampled the population of interest in our study. This limits statistical generalization to those particular stimuli. This is an unattractive property for psycholinguists (because they tend to be interested in, say, all concrete nouns rather than the 30 nouns used in the study). The same issue may apply to lots of other types of stimuli (faces, people, voices, pictures, logic problems and so forth). \n \n  \n \n The comments fell into several camps, but one response was that this was another case of researchers getting basic stats wrong. I consider this to be unfair because we're not talking basic stats here. The problem is quite subtle and the solutions are, in statistical terms, far from basic. Furthermore, it is not always an error. There are situations in which you don't need to worry about the problem and situations in which it is debatable what the correct approach is. \n \n  \n \n Another response was the psycholinguists have known about this problem for years (true!) and have analyzed their data correctly (false!). The problem came to prominence in a paper by Herb Clark ( The language-as-fixed-effect fallacy ), but was originally raised by Coleman (1964). Clark noted that running separate ANOVAs treating subjects as unit of analysis and items as unit of analysis did not solve the problem (by-subject and by-item analyses). Either analysis is statistically non-significant the effect fails to generalize, but if both are statistically significant the correct analysis (that combines variability across subjects and items) might still be statistically non-significant. His solution was to estimate the correct ANOVA test statistic (quasi F or F ') with a simple-to-calculate minimum value (min F '). This is known to be conservative (i.e., produces p values that are slightly too large) but not unreasonably so in practice (see Raaijmakers et al., 1999). Raaijmakers et al. (1999) show that until recently most psycholinguistic researchers still got it wrong (e.g., by reporting separate by-item and by-subject analyses). \n \n  \n \n What is the correct approach? Well, it depends. First, do you need to generalize beyond your stimuli set? This has to do with your research goals. In some applied research you might just need to understand how people respond to a particular set of stimuli. A single stimulus or stimulus set can offer a counterexample to a strong claim (e.g., that X is always the case). Alternatively, it might be reasonable to assume that the stimuli are - for the purposes of the study - very similar to others in the population (i.e., that population variability is negligible). This might be the case for certain mass-produced products (e.g., brands of chocolate bar) or precision-engineered equipment. However, a lot of the time you do want to generalize beyond your sample of stimuli ... \n \n \n \n That leaves you with the option of altering the design of the study or doing incorporating the extra variability owing to stimuli into the analysis. The design option was considered by Clark (1973) and by Raaijmakers et al. (1999). Clark pointed out that if each person had a different (ideally random) sample of items from the stimulus population then the F ratio of a conventional ANOVA would be correct. The principle here is quite simple: all relevant sources of variability need to be represented in the analysis . By varying the stimuli between participants the variability is present and ends up being incorporated into the between-subjects error term.* This is quite a neat method and can be easy to set up in some studies (e.g., if you have a very large pool of words to sample from by computer). Raaijmakers et al. (1999) also notes that you get the correct F ratios from certain other designs. This, in my view, is only partly true. Any design that restricts the population sampled from (of participants or stimuli) restricts its variability and therefore restricts its generalizability to the pool of participants or stimuli being sampled from. \n \n  \n \n Recent development in statistics and software (or at least recent awareness of them in psychology) have brought the discussion of the  language-as-fixed-effect fallacy or more properly stimuli-as-fixed-effect fallacy back to prominence. In principle it is possible to use a multilevel (or linear mixed) model to deal with the problem of multiple random effects (and this has all sorts of other advantages). However, the usual default model is a nested model that implicitly assumes that stimuli presented to each person are different. \n \n \n \n A nice point here is that a nested multilevel repeated measures model fitted with RML (restricted maximum likelihood) and a certain covariance structure (compound symmetry) is pretty much equivalent to repeated measures ANOVA and can be used to derive standard F tests etc. Thus Clark's assertion about using a design with stimuli nested within participants producing the correct F ratios is confirmed. \n \n \n \n Baayen et al. (2008) offered a critique of the standard approach and explained how to fit a multilevel model with crossed random factors (i.e., where stimuli are the same for all participants ... or equivalently participants are the same for all stimuli). These models can be fit in software such as MLwiN or R (but not SPSS**) that allows for cross-classified multilevel. The lme4 package in R is particularly useful because it fits these models fairly effortlessly. \n \n  \n \n This looks to be the solution described by Judd, Westfall and Kenny - as far as I can tell by their abstract and the solution I cover in my book (Baguley, 2012). \n \n* Note that a by-item analysis or by-subject analysis violates this principle because the each analysis uses the average response (averaged over the levels of the other random factor) and the variability around this average is unavailable to the analysis. \n \n** UPDATE: Jake Westfall kindly sent me a copy of the paper. I have not read it properly yet but looks extremely good. He points out that recent versions of SPSS can run cross-classified models (I'm still on an older version). Their paper includes SPSS, R and SAS code. I would still recommend R over SPSS. One highlight is that show how to compute the Kenwood-Roger approximation in R. Complex multilevel models make it difficult to assess the correct df for effects and the Kenwood-Roger approximation is one of the better solutions. In my book I used parametric boostrapping or HPD intervals to get round this problem, but this is potentially a very useful addition. \n \n  \n \n References \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n Baayen, R. H., Davidson, D. J., & Bates, D. M.\n(2008). Mixed-effects modeling with crossed random effects for subjects and\nitems. Journal of Memory & Language,\n59 , 390-412. \n \n \n \n \n \n \n \n Baguley, T. (2012). Serious stats: A guide to advanced statistics for the behavioral sciences . Basingstoke: Palgrave. \n \n \n \n \n \n Clark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of Verbal Learning and Verbal Behavior, 12 , 335-359.  \n \n \n \n Coleman, E. B. (1964). Generalizing to a language population. Psychological Reports, 14 , 219-226. \n \n \n \n \n \n Raaijmakers, J. G. W., Schrijnemakers, J. M. C., & Gremmen, F. (1999). How to deal with \"The language-as-fixed-effect fallacy\": Common misconceptions and alternative solutions. Journal of Memory & Language, 41 , 416-426."], "link": "http://psychologicalstatistics.blogspot.com/feeds/850886292749739104/comments/default", "bloglinks": {}, "links": {"http://amzn.to/": 2, "http://feedads.doubleclick.net/": 2, "http://neuroskeptic.co.uk/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["This blog and my other main blog (the companion blog for my book) are now syndicated via R-bloggers (posts tagged R only) and statsblogs.com . The latter is a relatively new blog aggregator but looks to have some interesting content. R-bloggers it quite well established and I was already an occasional reader. \n \nLooking at some recent content I noticed an interesting piece by Ben Bolker (author, among other things, of the excellent bbmle package in R) on dynamite plots. Until a few years ago (possibly in the early research for my book) I had not heard the term 'dynamite plot' or the negative press the attract in some research fields. In my own discipline (psychology) and in experimental psychology in particular bar plots with error bars (looking like sticks of dynamite stacked in a row) are rather popular. In fact, I was taught to use them in preference to dot plots when plotting interactions in ANOVA (their main application in experimental psychology). The main arguments against dot plots are that it is easy to manipulate them to make effects look large large by adjusting the scale (and sometimes software does this automatically). The advantage of switching to a bar plot is that these are supposed to be zero-referenced and thus effects are likely to more appropriately scaled. \n \nHere is an example of a dynamite plot adapted from chapter 3 of my book : \n \n \n \n \n(Colleagues of mine will note that the quantity displayed by the error bars is not labeled. This should always be clear from plot or figure caption - here they are 95% CIs). \n \nSome of the material on dynamite plots on the web is somewhat one-sided (e.g., see here , here , here or the comments here ). Ben Bolker bravely presents a more balanced picture. He also gets to the heart of the issue by noting that most criticisms of dynamite plots suggest box plots or plots of raw data as alternatives. This doesn't seem appropriate if your goal is inference rather than description. As Bolker notes, if you've decided to something like ANOVA you are already implicitly assuming approximate normality of the errors and so forth. Thus if the main purpose of the plot is inferential or to display key patterns among the data, box plots or raw data plots are not so useful. (Don't get me wrong I think think that plotting raw data is a good idea - but exploratory work and model checking are different from inference). So for a plot of means with error bars, the choice of dot plot or bar plot is one of aesthetics. These days my preference is for dot plots (which are more versatile and have a better information to ink ratio), but I think a well constructed dynamite plot can be appropriate in some situations. I would usually save these for a situation in which the pattern was quite simple (e.g., a 2 by 2 interaction), there was a meaningful zero or other reference point and when my audience are familiar with this style of plot and may prefer them. \n \nA further aesthetic point here is how to plot the error bars themselves. I am persuaded by Andrew Gelman's argument that the crossbars on conventional error bar plots are ugly and counterproductive. They draw your attention to the extremes of the error bar - when values closer to the statistic being estimated are more plausible. Here is the earlier dyamite plot redrawn as a conventional error bar plot and in cleaner Gelman-approved style: \n \n \n \nI find the version on the right to be much prettier. Furthermore it makes it easier to adapt them into two-tiered error bar plots. I like to use two-tier plots to convey 95% CIs for individual means (outer tier) and inferential (difference-adjusted) 95% CIs (inner tier). The inner tier approximates to a 95% CI for the difference - so that the means can be considered different by conventional criteria if the inner tier error bars don't overlap: \n \n \n \n \n \nIn my paper on within-subject CIs I used the style on the left. However, with hindsight I wish I'd included the style on the right. Varying the width of the bars avoids the ugly crossbars but may make detecting a 'statistically significant' difference trickier. I think that aesthetics win here because graphical methods aim to support informal inference - they are not supposed to be there for fine-grain, formal inference (which can be supported by formal hypothesis tests of various kinds - not just null hypothesis significance tests). \n \nUPDATE: The functions for these plots are on the book blog . More generally my functions for the book, CIs for ANOVA and a few other things are all available here . I plan to update these functions regularly to add functionality and deal with any undocumented features ."], "link": "http://psychologicalstatistics.blogspot.com/feeds/8611486187331986229/comments/default", "bloglinks": {}, "links": {"http://www2.ntupsychology.net/": 1, "http://3.blogspot.com/": 1, "http://www.blogger.com/": 1, "http://emdbolker.wikidot.com/blog": 1, "http://feedads.doubleclick.net/": 2, "http://pablomarin-garcia.co.nz/": 1, "http://biostat.vanderbilt.edu/": 1, "http://flowingdata.com/": 1, "http://andrewgelman.com/": 1, "http://seriousstats.wordpress.com/": 1, "http://www.r-bloggers.com/": 1, "http://4.blogspot.com/": 2, "http://sharpstatistics.co.uk/": 1, "http://psychologicalstatistics.co.uk/": 2, "http://statsblogs.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["The companion web site for Serious Stats is now live: \n \n http://www.palgrave.com/psychology/baguley/ \n \nThe web site includes: \n \n- a free sample chapter (Chapter 15: Contrasts) \n- data sets \n- R scripts \n- 5 online supplements (for meta-analysis, multiple imputation, replication probabilities, pseudo-R squared and loglinear models) \n \nAlso don't forget the Serious stats blog to accompany the book."], "link": "http://psychologicalstatistics.blogspot.com/feeds/7702174590325559825/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.palgrave.com/": 5, "http://seriousstats.wordpress.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["This is a quick follow up to my earlier post that discussed how to graph CIs for within-subjects (repeated measures) ANOVA designs . My forthcoming book Serious stats describes how to do this for between-subjects designs (a much simpler problem). The blog that accompanies the book now has a post summarizing the main options and explaining how to plot difference-adjusted CIs (95% CIs constructed so that non-overlapping intervals correspond to a statistically significant difference between means at p < .05). In addition, the post includes R functions to calculate and plot difference-adjusted CIs (though the calculations are not difficult to reproduce by hand). \n \nUPDATE: I've now added functions for two-tiered CIs for between-subjects designs on the book blog . More generally my functions for the book, CIs for ANOVA and a few other things are all available here . I plan to update these functions regularly to add functionality and deal with any undocumented features ."], "link": "http://psychologicalstatistics.blogspot.com/feeds/8291477012342848370/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www2.ntupsychology.net/": 1, "http://en.wikipedia.org/": 1, "http://www.co.uk/": 1, "http://psychologicalstatistics.co.uk/": 1, "http://seriousstats.wordpress.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["I finally found some time to take a closer look at p curves. I haven't had a chance to follow-up my simulations (and probably won't for a few weeks if not months), but I have had time to think through the ideas the p curve approach raises based on some of the comments I've received and a brief exchange with Uri Simonsohn (who has answered a few of my questions). \n \n \n \n First, I got a couple of things at least partly wrong. \n \n \n \n i) how p curves work \n \n ii) the potential for correlated p values \n \n \n \n How p curves work \n \n \n \n I made the (I think) reasonable assumption that p curve analysis involved focusing on a bump just under the p = .05 threshold. Other work (Wicherts et al., 2011) has shown that there is indeed some distortion around this value. My crude simulation suggested that p curves could maybe be used to detect this kind of bump - but that the method was noisy and required large N. \n \n \n \n All good so far except my assumption was completely wrong. This isn't what Simonsohn and colleagues are proposing at all. They are focusing on the whole of the distribution between p = 0 and p = .05. This is a very different kind of analysis because it uses all the available p value information about ' p hacking' (if you accept the highly plausible premise that p hacking is concentrated on statistically significant p values). \n \n \n \n Null effects will therefore produce a flat p curve (because the distribution of p under the null is uniform). Simonsohn argues that non-null effects should produce downward sloping p curves. He and his colleagues have simulated p curves under various ranges of effect size to confirm this - and there is also an analytic proof for the normal case (Hung et al., 1997).* I also (inadvertently) confirmed this in my original simulations - which show the downward sloping trend (but note that I include p values up to p = .10 in my plots). \n \n \n \n However, mixing in p hacked studies to a flat curve will produce an upward sloping curve - the feature that Simonshohn and his colleagues are focusing on. I haven't simulated this directly - but it seems sensible because p hacking is (in essence) a flavour of optional stopping (adding data or iterating analyses until you squeeze a statistically significant effect out). Certainly, an upward sloping curve would be a signal of something wierd going on. \n \n \n \n This approach uses more information than my mistaken ' p bump' approach and so should be much more stable. \n \n \n \n * It is far from unreasonable to treat the distribution of effects as approximately normal - as is common in meta-analysis (and see also Gillett, 1994), but I don't think the pattern depends strongly on this assumption. \n \n \n \n Correlated p values \n \n \n \n It is well known that p values are inherently extremely noisy 'statistics' - they jump around all over the place for identical replications. Geoff Cumming and colleagues have published some good work on this (e.g., Cumming & Fidler, 2009). Thus the same effect in different studies or different effects of similar sizes will in general not tend to have correlated p values. However, the noise that causes this jumping around will be crystalized if you use the same data to re-calculate the p value. This could cause correlated p values where data is re-used or where variables are very highly correlated. For example, this could happen if you add a covariate that is a modest predictor of Y and uncorrelated with and report p values with and without the covariate. It could also happen if you report essentially the same analysis twice with a very similar variable (e.g., X correlated with children's age or X correlated with years of schooling). \n \n \n \n There are two main solutions here: a) just filter out p values that re-use data or use highly-correlated data, or b) model the correlations in some way by accounting for within-study clustering - as you might in a multilevel model and some forms of meta-analysis (itself a form of multilevel model). \n \n \n \n In summary, I think the p curve approach looks very interesting, and I'd certainly like to see more work on it (and hope to see the full version published some time soon). \n \n \n \n References \n \n \n \n Cumming, G., & Fidler, F. (2009). Confidence Intervals. Zeitschrift f\u00fcr Psychologie / Journal of Psychology , 217 (1), 15-26. \n \n Gillett, R. (1994). Post hoc power analysis. Journal of Applied Psychology , 79 (5), 783-785. doi:10.1037//0021-9010.79.5.783 \n \n Hung, H. M., O\u2019Neill, R. T., Bauer, P., & K\u00f6hne, K. (1997). The behavior of the P-value when the alternative hypothesis is true. Biometrics , 53 (1), 11-22. \n \n Wicherts, J. M., Bakker, M., & Molenaar, D. (2011). Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results. PloS one , 6 (11), e26828. doi:10.1371/journal.pone.0026828"], "link": "http://psychologicalstatistics.blogspot.com/feeds/7898274551397259397/comments/default", "bloglinks": {}, "links": {"http://psychologicalstatistics.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Psychological Statistics"}, {"content": ["I have finally got around to posting the R code for my p curve simulation . Those familiar with R will realize how crude it is (I've been caught up with other urgent stuff and had no time to explore further). \n You are welcome to play with (and improve!) the code. Changing delta will alter the (at present) fixed effect size. It would be more realistic to vary this (and the sample sizes). A good starting point for the effect size distribution (in the population) might be a normal distribution with say a mean of zero and a variance of 1 (see Gillett, 1994). \n \n \n delta <- 0.5 \nm1 <- 10 \n sd <- 2 \nm2 <- m1 + sd *delta\nn1 <- n2 <- 25 \n \nn.sims <- 500 \np.data <- replicate ( n.sims , t.test ( rnorm ( n1 , m1 , sd ) , rnorm ( n2 , m2 , sd ) ) $p.val , simplify=T ) \n \n par ( mfrow= c ( 5 , 3 ) ) \n for ( i in 1 : 15 ) { \n p.data <- replicate ( n.sims , t.test ( rnorm ( n1 , m1 , sd ) , rnorm ( n2 , m2 , sd ) ) $p.val , simplify=T ) \n hist ( p.data , xlim= c ( 0 , 0.1 ) , breaks = 99 , col = 'gray' ) \n } \n \n \n \n \n \n \n R code html script courtesy of Pretty R at inside-R.org \n \n References \n \n \n Gillett, R. (1994). Post Hoc Power Analysis. Journal of Applied Psychology , 79 , 783-785."], "link": "http://psychologicalstatistics.blogspot.com/feeds/8018539215618229135/comments/default", "bloglinks": {}, "links": {"http://psychologicalstatistics.blogspot.com/": 1, "http://inside-r.org/": 19, "http://feedads.doubleclick.net/": 2, "http://www.inside-r.org/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["Psych your mind has an interesting blog post on using p curves to detect dodgy stats in a a volume of published work (e.g., for a researcher or journal). The idea apparently comes from Uri Simonsohn (one of the authors of a recent paper on dodgy stats ). The author (Michael W. Kraus) bravely plotted and published his own p curve - which looks reasonably 'healthy'. However, he makes an interesting point - which is that we don't know how useful these curves are in practice - which depends among other things on the variability inherent in the profile of p values. \n \nI quickly threw together a simulation to address this in R. It is pretty limited (as I don't have much time right now), but potentially interesting. It simulates independent t test p values where the samples are drawn from independent, normal distributions with equal variances but different means (and n = 25 per group). The population standardized effect size is fixed at d = 0.5 (as psychology research generally reports median effect sizes around this value). Fixing the parameters is unrealistic, but is perhaps OK for a quick simulation. \n \nI ran this several times and plotted p curves (really just histograms with bins collecting p values at relevant intervals). First I plotted for an early career researcher with just a few publications reporting 50 p values. I then repeated for more experienced researchers with n = 100 or n = 500 published p values. \n \nHere are the 15 random plots for 50 p values: \n \n \n \n \n \n \nAt least one of the plots has a suspicious spike between p = .04 and .05 (exactly where dodgy practices would tend to push the p values). \n \n \n \nWhat about 100 p values? \n \n \n \n \n \n \n \nHere the plots are still variable (but closer to the theoretical ideal plotted on Kraus' blog). \n \n \n \nYou can see this pattern even more clearly with 500 p values: \n \n \n \n \n \n \n \n \n \nSome quick conclusions ... The method is too unreliable for use with early career researchers. You need a few hundred p values to be pretty confidence of a nice flat pattern between p = .01 and p = .06. Varying the effect size and other parameters might well inject further noise (as would adding in null effects which have a uniform distribution of p values and are thus probably rather noisy). \n \n \n \nI'm also skeptical that this is useful for detecting fraud (as presumably deliberate fraud will tend to go for 'impressive' p values such as p < .0001). Also (going forward) fraudsters will be able to generate results to circumvent tools such as p curves (if they are known to be in use)."], "link": "http://psych-your-mind.blogspot.com/2012/02/friday-fun-one-researchers-p-curve.html", "bloglinks": {}, "links": {"http://4.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://people.cornell.edu/": 1, "http://1.blogspot.com/": 1, "http://psych-your-mind.blogspot.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["I'm not a big fan of the term \"nonparametric statistics\", or at least how it is used in psychology and related fields (e.g., education and health research). This is one reason why I don't make a big deal of the parametric/non-parametric distinction in my  Serious stats  book and probably partly why a recent article in APS Observer annoyed me so much. \n \n \n Why did it annoy me so much? It says, in essence, what many standard psychology text books say (and also makes several good points - which I shall largely ignore for rhetorical purposes). I like in particular the points about statistical tests used to check statistical assumptions lacking power and making statistical assumptions themselves (which are typically unchecked and they are not robust to). Indeed, that's another thing that annoys me and I have blogged/bored people about in the past (e.g., see here ). I also think the article might have been chopped about a bit by editors (as it contains at least one important reference not cited and some of my gripes are also contained in references cited by the article I'm annoyed with). So, on balance, it is (by psychology mainstream standards) not at all a bad piece. However, by buying into the standard nonparametric stats presentation it perpetuates a few myths or errors and may inadvertently gives some pretty poor advice mixed in with the good. \n \n \n Here is a quick list of my criticisms: \n \n  (1) The definition of nonparametric statistics is deeply confused. The author starts by writing: \"Nonparametric statistical analyses are used to investigate research questions in which the dependent variable is ranked or categorical rather than quantified in a true numeric sense\". This seems to suggest that non-parametric procedures are defined by having a discrete or bounded DV. Later it adds that \"Traditional parametric statistics require a number of assumptions about the characteristics (i.e., parameters) of the data.\" This is an appeal to the idea that parametric statistics assume a particular probability distribution (the parameters of which are estimated by the data). This seems like a better definition to me, although like many psychologists, the author appears to assume that the probabilty distribution assumed is always a normal distribution. Mixing the two aspects of the definition is confusing. It is easy to find statistical procedures that are parametric in the second sense, but involve ranked or categorical DVs. I would argue that a chi-square test of independence or a sign test is parametric by the second definition. Not only is the definition problematic, but it could lead to poor analytic decisions. For instance, dichotomous outcomes are often best analyzed using parametric techniques such as logistic regression (a generalized linear model with a logistic link function and a binomial random component) rather than the methods surveyed in the article. \n \n (2) The article also reads as if nonparametric tests are the same thing as rank randomization tests. Rank randomization tests are examples of nonparametric tests in the sense they are distribution free (making no assumption about a particular probability distribution for the data). However, there are many other nonparametric methods that don't involve ranks. Rank randomization tests are useful but limited in scope. In addition, if the raw data are in the form of ranks then a rank transformation is pretty pointless (you might as well jut run a regular test). \n \n The main limitation of the rank approaches is that the rank transformation is irreversible; it destroys the link between the analysis and the raw scores. This is a serious problem if you care about the raw scores - perhaps because you want to test whether effects are non-additive or because you want to get an interval estimate or a effect sizes on the original scale. There are ways to help you do this with rank transformation procedures, but they are generally pretty fiddly. There are good reasons why people don't tend to use rank transformations in multiple regression, use them to test interaction effects or use them to construct confidence intervals. \n \n Third, rank randomization (and other nonparametric) procedures are bound by pesky assumptions! They tend to make weaker assumptions than parametric procedures, but all statistical procedures make assumptions. The assertion that the \"main assumptions of nonparametric tests are that the dependent variable should be continuous and have independent random sampling, which means that nonparametric statistics do not require assumptions of homogeneity of variance and normality\" is misleading. The precise assumptions vary from test to test and with the hypothesis being tested. As a rule, rank randomization and related rank transformation tests assume that samples with similar shapes of distribution are being compared. They can therefore be undermined by heterogeneity of variance or varying degrees of skew. If the distributions have very dissimilar shapes then the tests can sometimes behave very strangely (e.g., it is possible to get outcomes where A > B, B > C and yet C > A). Continuity is also not necessarily a requirement of the DV or of the underlying construct being measured (though it may be for some hypotheses). \n \n (4) Many generalizations are made that simply don't hold up: \"When the data violates the assumptions of a parametric test, nonparametric tests are again the more powerful analytic technique (Siegel, 1957).\" This doesn't follow. Often this is true, but not always. There has been a lot of research on this topic since 1957 and parametric tests are not always inferior. This is particularly true if you don't restrict parametric techniques to t tests, linear regression and ANOVA. Some nonparametric techniques are known to have very poor power. I also got irritated by the critique of log and similar transformations that states: \"while these transformations can make variables more normally distributed, they can also diminish or alter experimental effects, which can reduce power.\" To the extent this is true, it is also true of the rank transformation (more so in some cases). Furthermore, the real problem is with arbitrary transformations. Log transformations - where appropriate - tend to aid interpretation of effects (e.g., by quantifying them as proportionate rather than additive effects). I also dislike the implication that \"experimental effects\" exist in a pristine form prior to transformation. This is simply not the case - how to quantify and scale measurement of an effect is a tricky business (e.g., memory researchers can use percentage correct, hits minus false alarms, A prime d prime etc.) and many effects come \"pre-transformed\" (e.g., measurement of loudness in decibels). Transformations (including the rank transformation) are useful tools that can increase power and aid interpretation if used carefully and appropriately. \n \n (5) The use of parametric statistics to analyze Likert-style rating scales may be one of the \"seven deadly sins of statistical analysis\", but it is rarely a big problem in practice. Least squares methods are most messed up by heavy tailed distributions, severe skew or outliers. If anything, Likert-style rating scales tend not to have these problems (or to manifest them relatively mildly). Furthermore, where there are problems with Likert-style measures, rank randomization or transformation tests s are probably not the solution. A number of parametric procedures for ordinal outcomes exist - notably ordered logistic regression (though least squares methods such as t tests, ANOVA or regression should work well when their assumptions are not badly violated). \n \n (6) The variety of nonparametric tests referred to is slightly artificial. As a general rule there are advantages to sticking to standard 'parametric' tests such as the Welch-Satterthwaite t test or one-way ANOVA rather than using named rank-transformation tests such as the Mann-Whitney U test. In some cases there may be advantages with specialized rank randomization tests where sample sizes are small (e.g., because software such as R implements exact versions). However, there are a few cases where the rank randomization tests are not robust (e.g., Mann-Whitney U test is not robust to heterogeneity of variance) or lack power (e.g., the Friedman test, Page's L test and most multiple comparison procedures available for ranks). Rank transforming the data and then running a t test with Welch-Satterthwaite correction is superior to running the Mann-Whitney directly (Zimmerman & Zumbo, 1993a). For more on the low power of the Friedman test and better alternatives see here . \n Rather than think in terms of nonparametric statistics, it is better to focus on checking assumptions (using graphical methods and simple descriptives) and checking our models against more robust procedures. If more robust methods show different results - the next step is to find out why (and definitely not report just the outcome you prefer). This should lead you to a superior model (using robust methods or perhaps a more appropriate parametric model). The consideration of robust methods is particularly important. This includes some rank transformation tests, but also includes robust regression, bootstrapping and other tools (e.g., see Wilcox & Keselman, 2003; 2004; Baguley, 2012). \n  \n \n References \n     \n \n Baguley, T. (2012, in press). Serious stats: A guide to advanced statistics for the behavioral sciences  . Basingstoke: Palgrave. \n \n \n Wilcox, R. R., & Keselman, H. J. (2003). Modern robust data analysis methods: Measures of central tendency. Psychological Methods, 8 , 254-274.  \n       Wilcox, R. R.,& Keselman, H. J. (2004). Robust regression methods: Achieving small standard errors when there is heteroscedasticity. Understanding Statistics, 3 , 349- 364. \n \n Zimmerman, D. W., & Zumbo, B. D. (1993b). Rank transformations and the power of the Student t test and Welch t ' test for non-normal populations with unequal variances. Canadian Journal of Experimental Psychology, 47 , 523-539."], "link": "http://psychologicalstatistics.blogspot.com/feeds/5330311759010315757/comments/default", "bloglinks": {}, "links": {"http://psychologicalstatistics.blogspot.com/": 1, "http://www.co.uk/": 2, "http://feedads.doubleclick.net/": 2, "http://seriousstats.wordpress.com/": 1, "http://www.psychologicalscience.org/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["I have just published R code for calculating CIs for differences between correlations on the Serious stats book blog. This covers independent correlations (taken from chapter 6 of the book) and dependent correlations (new R code written as a supplement to chapter 6). \n \nUPDATE on the update ... \n \nI have also added an Excel spreadsheet that should match the R output (though the latter is probably more accurate and reliable)."], "link": "http://seriousstats.wordpress.com/2012/02/05/comparing-correlations/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.co.uk/": 1, "http://seriousstats.wordpress.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["This is a quick update to announce my new blog Serious Stats . This is a companion to my forthcoming book of the same name: \n \n \n Baguley, T. (2012, in press). Serious stats: A guide to advanced statistics for the behavioral sciences . Basingstoke: Palgrave. \n \n \n I think is better to separate book specific content out from my regular posts (though in some cases this will be a bit fuzzy). I will also try and post short updates here when something relevant gets published on the blog for the book."], "link": "http://seriousstats.wordpress.com/", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.co.uk/": 1, "http://seriousstats.wordpress.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["A longer version of my earlier post A problem of significance just appeared in The Psychologist. \n \n \n Baguley, T. (2012). Can we be confident in our statistics? The Psychologist. 25, 128-9. \n \n \n Shortly after publication I received an email asking about statistical analysis of differences in correlations. This is more tricky than you might think. I'm working on some R code to implement one of the better approaches and plan to blog on this shortly ... \n \n \n (See update here.)"], "link": "http://psychologicalstatistics.blogspot.com/feeds/7472403449669426879/comments/default", "bloglinks": {}, "links": {"http://psychologicalstatistics.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://nottinghamtrent.academia.edu/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["Psychologists are gradually coming round to the view that it is a good idea to present interval estimates alongside point estimates of statistics. The most common statistic reported in psychology research is almost certainly the mean (strictly the arithmetic mean). Presenting an interval estimate for the mean of a single sample is usually quite simple. This is usually done as 95% confidence interval about the mean \u2013 and most researchers in psychology are able to calculate this by hand or get their statistical software to calculate and graph it for them. \n \n \n \n  \n \n Extending this to more than one mean introduces an additional layer of complexity. This is because the difference between two means is a different quantity, and its CI (although related to those of the individual means) is different in width from the CIs of the individual means. This creates a problem when plotting the CI because a researcher might be interested in the CI for an individual mean, the CI for their difference (or both). \n \n \n \n \n The complexity increases further if the aim is to plot a set of means (e.g., from an ANOVA design). In this case, plotting all the possible differences (as is commonly done) obscures patterns in the individual means (e.g., linear or quadratic trends). Last, but not least, if the means are not from independent samples, there are further difficulties. This happens in within-subjects or repeated measures designs. \n \n \n \n \n In these designs the variation around each mean is correlated with the variation around the other means. This correlation arises from individual differences. Statistical procedures such as ANOVA can capitalize on these individual differences to produce more sensitive statistical inferences (i.e., to increase statistical power or obtain narrower CIs). This is done by estimating the variation due to individual differences, and removing it from the error variance (the estimate of statistical noise in the data set). \n \n \n \n \n This is a problem for graphical presentation of means because the precision of individual means is influenced by individual differences, whereas the precision of differences between means is not (because the estimate of individual differences is common to repeated samples from the same people and thus can be removed). Further complications arise when the sphericity assumption of repeated measures ANOVA is violated. \n \n \n \n \n Several solutions to these problems have been proposed in the literature. The best known of these in psychology is that of Loftus and Masson (1994). Another well-known solution is that of Goldstein and Healy (1995), extended to correlated samples by Afshartous and Preston (2010). \n \n \n \n \n Despite a large literature on the problems of graphing a set of correlated means, many people avoid the problems altogether by not reporting (or graphing) CIs or report CIs that are misleading in some way. Researchers are often unaware of the problems or find the solutions hard to understand and implement. \n \n \n \n \n I recently reviewed the main approaches in the literature, describe how to obtain suitable intervals for individual means and differences between means and provide R code to calculate and plot the intervals. \n \n \n \n \n The main highlights are that: \n \n \n \n \n i) for inferences about individual means the standard approach works fairly well for between-subject (independent measures) designs, but there is a case to use CIs from a multilevel model for within-subject (repeated measures) designs \n \n \n \n \n ii) an approach proposed by Cousineau (2005) with a correction by Morey (2008) offers advantages over the Loftus and Masson (1994) approach for within-subject ANOVA designs. It simplifies the calculations and does not assume sphericity. The Loftus-Masson approach will however usually be superior when n is small. \n \n \n \n \n iii) if you are interested in differences between means then you should probably plot a version of the Cousineau-Morey (or Loftus-Masson) interval that is adjusted so that overlap of the CIs around two individual means corresponds to overlap of the CI for their difference. This can be done by incorporating a multiplier to the width of the individual CIs. This multiplier is equal to (2^0.5)/2. \n \n \n \n \n iv) if you are interested in both precision of individual means and their differences you can use a two-tiered error bar to display both quantities (Cleveland, 1985). \n \n \n \n \n v) the intervals (and graphical presentation of means) are useful for informal inference about a set of means. For formal inference it is better to set up precise hypotheses and test these via an a priori of contrast. This could be a traditional null hypothesis significance test, but other approaches are available. These include confidence intervals, Bayes factors, likelihood ratios and so forth (Baguley, in press; Dienes, 2008). \n \n \n \n \n \n The paper is available here , the R code here and the data sets here . \n \nUpdate: R functions now available for the simpler between-subjects (independent measures) ANOVA \ncase (at the Serious stats blog). \n \n \n \n References \n \n \n \n Afshartous D., & Preston R. A. (2010). Confidence intervals for dependent data: equating nonoverlap with statistical significance. Computational Statistics and Data \n \n Analysis. 54 , 2296-2305. \n \n \n \n \n Baguley, T. (2011, in press).  Calculating and graphing within-subject confidence intervals for ANOVA. Behavior Research Methods. DOI: 10.3758/s13428-011-0123-7 \n \n \n \n Baguley, T. (2012, in press). Serious Stats: A guide to advanced statistics for the behavioral sciences . Basingstoke: Palgrave. \n \n \n \n \n \n Cleveland. W. S. (1985). Elements Of Graphing Data . New York, NY: Chapman & Hall. \n \n \n \n Cousineau, D. (2005). Confidence intervals in within-subject designs: A simpler solution to Loftus and Masson\u2019s method. Tutorials in Quantitative Methods for Psychology, 1 , 42-45. \n \n \n \n \n Dienes, Z. (2008). Understanding Psychology as a Science: An Introduction to Scientific and Statistical Inference . Basingstoke: Palgrave Macmillan. \n \n \n \n \n Goldstein, H., & Healy, M. J. R. (1995). The graphical presentation of a collection of means. Journal of the Royal Statistical Society. Series A (Statistics in Society), 158 , 175-177. \n \n \n \n \n Loftus, G. R., & Masson, M. E. J. (1994). Using confidence intervals in within-subject designs. Psychonomic Bulletin & Review , 1 , 476-490. \n \n \n \n \n Morey, R. D. (2008). Confidence intervals from normalized data: A correction to Cousineau (2005). Tutorials in Quantitative Methods for Psychology, 4 , 61-64."], "link": "http://nottinghamtrent.academia.edu/ThomBaguley/Papers/900446/Calculating_and_graphing_within-subject_confidence_intervals_for_ANOVA", "bloglinks": {}, "links": {"http://www.blogger.com/": 1, "http://feedads.doubleclick.net/": 2, "https://docs.google.com/": 2, "http://nottinghamtrent.academia.edu/": 2, "http://www.co.uk/": 4, "http://seriousstats.wordpress.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["Several people have drawn my attention to a recent article on a common error in published statistical analyses in neuroscience. Sander Nieuwenhuis, Birte Forstmann and Eric-Jan Wagenmakers published (in Nature Neuroscience) a critique of statistical analyses in the neuroscience literature. This paper has been written about by Ben Goldacre and Andrew Gelman (who published an article on the general problem some time ago) - so I won't go into too much detail. \n \nThe point of interest for me is that the error concerns something that most psychologists should know all about (and hence should be expected not to make the error). It concerns the case of two differences, one statistically significant and one non-significant. For example, group 1 may show a significant difference between experimental condition and placebo (for a drug intervention), while group 2 do not. A naive interpretation is that the drug works for group 1 but not group 2. This is not necessarily true. The proper test of a difference in effects of the drug between groups is an interaction test. Psychologists tend to avoid this error because we have heavily trained in ANOVA as undergraduates (certainly in the UK and probably also in the US and most of Europe). Even if we fail to learn this, reviewers and editors (in psychology) tend to spot the error.* \n \nAre psychologists then entitled to feel a little bit smug? Perhaps, but only a little. First, I think the reason we are relatively good performers on this point is because we tend to view many statistical analyses through an \"ANOVA\" lens. Factorial ANOVA (in which factors are orthogonal) includes the interaction term by default. The 2 by 2 factorial ANOVA is the workhorse of experimental psychology. Our familiarity with this type of design and analysis makes this easy to spot. Second, our ANOVA lens leads to other errors - notably dichotomizing continuous variables (e.g., via median split) in order to squeeze them into an ANOVA design. This always decreases statistical power, and can - albeit infrequently - produce spuriously significant effects (see MacCallum et al., 2002). These errors are sometimes less serious than the difference of differences/interaction error (but are not harmless). \n \nThe real test then, is whether psychologists make the same (conceptual) error in a different context. The obvious context is that of association rather than difference. If males show a significant correlation between testosterone and aggression (e.g., r = .5, N = 25) and females don't (e.g., r = .3, N = 25), the correlation between testosterone and aggression is not significantly bigger for males than females. To confirm this you'd need to construct a test or (better still) confidence interval for the difference in correlations. This is hardly ever done - and, in my experience, psychologists frequently make this kind of claim without backing it up.** Methods for testing differences in correlations are a bit fiddly (e.g., depending on overlap or lack of overlap in the measurements), and rarely taught at undergraduate or even postgraduate level. The methods that are taught are also often a bit dodgy (see Zou, 2007, for some better alternatives). \n \nAlso note that (in both cases) the error can work the other way. Two correlations could be non-significantly different from zero but different from each other (e.g., r = .5 and r = -.5 with N = 10). \n \n Postscript \n \nThere is, I think, a lesson or two here. A minor lesson is that interactions are bit more complicated than psychologists (particularly those very familiar with ANOVA) often think. I could write more on this (and do a bit in my forthcoming book). A major lesson is that this concept (the difference between significant and non-significant is not necessarily also statistically significant - see Gelman & Stern, 2006) is probably quite tricky. It may be worth exploring why ... I suspect it is because of several factors. \n \nSee updates here and here . \n \n References \n \n \nGelman, A., & Stern, H. (2006). The difference between \u201csignificant\u201d and \u201cnot significant\u201d is not itself statistically significant. American Statistician, 60 , 328\u2013331. \n \nMacCallum, R. C., Zhang, S., Preacher, K. J., & Rucker, D. D. (2002). On the practice of dichotomization of quantitative variables. Psychological Methods, 7 , 19-40. \n \nNieuwenhuis, S., Forstmann, B. U., & Wagenmakers, E.-J. (2011). Erroneous analyses of interactions in neuroscience: A problem of significance. Nature Neuroscience, 14 , 1105-1107. \n \nZou, G. Y. (2007). Toward using confidence intervals to compare correlations. Psychological Methods, 12, 399-413. \n \n \n \n * Do I have any support for this position? Yes: anecodotal support (e.g., from editing or reviewing many dozens of papers) and some support from Nieuwenhuis et al. They found the error more prevalent in cellular and molecular neuroscience. ANOVA is core training in psychology and widely used in cognitive and behavioural neuroscience - and I'd argue that this reflects the influence of psychologists working in this area and of neuroscientists trained in and using similar methods. \n  \n** Do I have any support for this position? A little. It is easy to find basic psychology texts with ANOVA but without tests of differences in correlations being mentioned. It is rare to find tests of CIs of differences in published papers."], "link": "http://www.ejwagenmakers.com/2011/NieuwenhuisEtAl2011.pdf", "bloglinks": {}, "links": {"http://andrewgelman.com/": 1, "http://psychologicalstatistics.blogspot.com/": 2, "http://www.co.uk/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Psychological Statistics"}, {"content": ["Here are the slides from the Introduction to R session Danny Kaye and I ran at the BPS Mathematics, Statistics & Computing section CPS Workshop (13 December 2010, Nottingham Trent University)."], "link": "https://docs.google.com/present/edit?id=0AcYTvx5C8MjXZHA4Nmg2dF8xMDJnY2c2eDdoYw&hl=en_GB&authkey=CMHxy54B", "bloglinks": {}, "links": {"https://docs.google.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Psychological Statistics"}, {"content": ["I was recently asked to give a talk to our graduate school annual conference. I offered several titles and the one they picked was Using R in research . I'm not sure if this was a good idea or not. The graduate school covers PhD students across three areas of the university: social sciences (including psychology), law and business. In addition the students tend to specialize in either qualitative or quantitive research methods, so I was talking to an audience who might know nothing about statistics or a great deal (e.g., several students have completed MSc courses in psychological research methods here or elsewhere). \n \n \n \nMy solution was try and explain the advantages of R relative to alternatives such as SPSS (probably the most common statistic package in the University). I also focussed a lot on graphical methods and simulation. It seemed to go quite well, but I worry that quite a few members of the audience were overwhelmed by large chunks of it. \n \nI promised to put my slides on my blog - though I am not sure how useful they are to anybody who wasn't there. Without my commentary some (possibly most) of the slides won't make much sense. I spent a good deal of the time talking through exploratory plots of one data set (from Hayden, 2005). I use this example a lot in teaching and it involves a bit of class participating (guessing the origin of the data) - so I won't go into to detail here (lest I spoil it for future students), but you can google the original article if you are curious. I also spent some time on how R works (e.g., object types, assignment, basic modeling, plotting functions). My reasoning was that many of the audience have no familiarity with non-GUI interfaces in software and without explaining the basics of the interface they will not have the faintest clue how R works. For those with some familiarity (e.g., SPSS syntax) the examples were selected to show how powerful R can be for things like exploratory graphics. \n \nSeveral students ask about resources for learning R. I mentioned some in earlier blog posts, but for psychologists Li and Baron's web resources are a good place to start. The other major resource is probably Quick R , but there are hundreds of other places to look online (depending on what stuff you need most)."], "link": "http://psychologicalstatistics.blogspot.com/feeds/1960747264016613408/comments/default", "bloglinks": {}, "links": {"http://www.statmethods.net/": 1, "http://www.upenn.edu/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "Psychological Statistics"}, {"content": ["A while ago I wrote a co-wrote chapter for an introductory psychology text book Essential Psychology: A Concise Introduction . This is a book edited and written by members of the department where I work. My contribution was the chapter on human memory (cunningly titled Memory ). \nI produced several plots for the chapter (some of which got cut due to severe space restrictions). One that stayed in was a serial position curve. For this plot I used data from Postman and Phillips (1965). \n  \nI feel particular proud of this plot because I was just beginning to use and learn R at the time (as opposed to dabbling) and because I had had a really hard time getting hold of the data. I first tried google, but had no joy (for some reason I thought someone would have put the raw data online, as it is a classic study - though maybe I just missed it). Then I searched for alternative data sets (as around that period there were quite a few similar studies). I was probably being too picky, but whatever the reason I had no luck. \n \nIt would have been trivial to make up fake data, but that didn't feel right. What I eventually did (and wished I'd done straight away) was print out the original figure and measure all the points by hand. I then entered these values into a spreadsheet and tweaked and remeasured until all the summary statistics matched those in the original paper to about one decimal place. This was a lot quicker than I had thought. I cheated slightly because I only needed data from the 20 word conditions (so I could leave out the 10 and 30 word conditions). \n \n(I'm pretty sure I could have used computer software to capture the raw data from an image file, but I'd have had to find the software, learn how to use it and do all the checking anyway. For a single figure I'm reasonably sure measuring by hand would be faster.) \n \nIn re-plotting it I noticed a few things that I hadn't paid much attention to before. The main one was the authors report frequency of recalls for 18 participants with 6 lists each. This means all scores are out of 108 and I suspect lots of casual readers would (like me) assume they were percentages. For re-plotting I rescaled the data as percentages. \n \nThe plot itself just uses basic R functions. I'm writing about it because: i) I think it is a fairly clear illustration of how basic plot functions in R can produce what I think is a rather nice Figure. (The published version has been edited by the publisher, adding colour and making the style match figures in other chapters), ii) people may find it useful for teaching purposes. So please feel free to use and adapt the R code for non-commercial (e.g., teaching use). \n \nFirst load the data from this .csv file (you will need to specify the path or change the working directory if the file is saved elsewhere). \n \n pp65 <- read.csv(\" pp65.csv \") \n \n Then paste the following: \n \n plot(pp65$SP, pch=NA, ylim=c(0,80), xlab= \"Serial position\", ylab= \"Mean percentage recall\", main = \"Postman & Phillips (1965)\", sub = '(20 word conditions only)') \n \n points(pp65$C0, pch=19, col='black', cex=.7) lines(pp65$C0, lty=3) points(pp65$C15, pch=24, col='black', cex=.7) lines(pp65$C15, lty=2) points(pp65$C30, pch=22, col='black', cex=.7) lines(pp65$C30, lty=5) legend(3, 80, legend=c(\"No delay\",\"15 second delay\",\"30 second delay\"), lty=c(3,2,5) ) \n If you are new to R you can find out more about these plotting functions by using R help: ?par , ?plot , ?points and so on ... \n \n \n References  \n Baguley, T., & Edmonds, A. J. (2010). Memory. In P. Banyard, M. N. O. Davies, C. Norman, & B. Winder (Eds.) Essential Psychology: A Concise Introduction (pp. 65-82). London: Sage.  \n \n \n \n \n Postman , L. & Philips , L. W. ( 1965 ). Short-term temporal changes in free recall. Quarterly Journal of Experimental Psychology, 17 , 132-138."], "link": "http://psychologicalstatistics.blogspot.com/feeds/7227981003814024457/comments/default", "bloglinks": {}, "links": {"https://docs.google.com/": 2, "http://feedads.doubleclick.net/": 2, "http://www.co.uk/": 3, "http://1.blogspot.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["I needed to produce a few a interaction plots for my book in R and, while the interaction.plot( ) function is useful it has a couple of drawbacks. First, the default output isn't very pretty. Second, it works from the raw data, whereas I often need plots from cell means. For teaching purposes it is quite common to produce plots without raw data (for hypothetical data or from published examples). \n \nMy first attempts at the plots involved setting them up element by element. Just going over some examples I decided to turn the basic plot (for a 2 x 2 ANOVA) into a simple function. Nothing fancy, just a regular interaction plot in black and white that I think is prettier than the SPSS, Excel or R defaults. At some point I may have a go turning it into a general I x J ANOVA plot (or maybe even add CIs, but I'll probably do that from raw data if I ever get round to it). \n \n \n plot.2by2 <- function(A1B1,A1B2, A2B1, A2B2, group.names, legend = TRUE, leg.loc=NULL, factor.labels=c('Factor A', 'Factor B'), swap = FALSE, ylab= NULL, main = NULL){ \n group.means <- c(A1B1, A2B1, A1B2, A2B2) \n if(missing(ylab)) ylab <- expression(italic(DV)) \n if(swap==TRUE) { \n group.names <- list(group.names[[2]], group.names[[1]]) ; group.means <- c(A1B1, A1B2, A2B1, A2B2); factor.labels <- c(factor.labels[2], factor.labels[1]) \n } \n plot(group.means, pch=NA, ylim=c(min(group.means)*.95, max(group.means)*1.025), xlim=c(0.8,2.2), ylab=ylab, xaxt='n', xlab=factor.labels[1], main=main) \n points(group.means[1:2], pch = 21) \n points(group.means[3:4], pch = 19) \n axis(side = 1, at = c(1:2), labels = group.names[[1]]) \n lines(group.means[1:2], lwd = .6, lty = 2) \n lines(group.means[3:4], lwd = .6) \n if(missing(leg.loc)) leg.loc <- c(1,max(group.means)) \n if(legend ==TRUE) legend(leg.loc[1], leg.loc[2],legend = group.names[[2]], title = factor.labels[2], lty = c(3,1)) \n } \n \n \n Call the function by entering the four cell means in conventional order: A1B1, A1B2 and so on where A1B1 is the mean of level 1 of factor A at level 1 of factor B. You also need a two item list containing text strings of the two level names of each factor. For instance: \n lev.names <- list(c('A1', 'A2'), c('B1', 'B2')) \n plot.2by2(5,15,10,20, lev.names) \n  You can swap the axes by adding the argument swap = TRUE : \n \n plot.2by2(5,15,10,20, lev.names, swap = TRUE) \n   \n  \n \n The default factor names are 'Factor A' and 'Factor B', but these are over-ridden in the call: \n \n plot.2by2(5,15,10,20,lev.names, swap = TRUE, factor.labels= c('Factor 1','Factor 2')) \n \n \n You can also change the y- axis label with ylab or add a main title with main . The legend can be dropped ( legend = FALSE ) if you don't want one or need it to be located outside the plot. To move the legend just specify coordinates with an argument such as leg.loc = c(1,10 ) . You can also edit the source code directly. Here is an example with title and meaningful labels: \n \n \n group.names <- list(c('placebo','drug'), c('male', 'female')) \n \n    \n plot.2by2(10,10,15,20, group.names, factor.labels=c('Drug', 'Sex'), swap = FALSE) \n  \n  As this just uses basic plotting functions in R you can also manipulate the plot in other ways: adding lines with segments() , adding text with text() changing graphical parameters with par( ) and so on. Depending on your platform it is also easy to extract the plot as a .pdf or .jpg file. On a mac I save it as a .pdf file and open it in preview which allows me to save it as .png, .gif or whatever I need."], "link": "http://psychologicalstatistics.blogspot.com/feeds/6683967616011422003/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 3}, "blogtitle": "Psychological Statistics"}, {"content": ["A while ago I promised a longer post on standardized effect size. This isn't it. Instead it is a link to  my piece in the February 2010 issue of The Psychologist  . \n \n \n \n \n \n \n \n \n I had intended to write a short summary of my 2009 BJP paper, but that didn't really get off the ground. However, I had for a while wanted to write about the Society for the Suppression of the Correlation Coefficient . I had first read about this in Tukey's (1954) chapter on regression and path analysis. This is one of the earlier papers in the literature criticizing the preference for (standardized) correlation coefficients over (simple, unstandardized) regression coefficients. Read the article for an earlier example! I was reminded of its existence by Brillinger's (2001) paper. \n \n \n \n  \n \nAs soon as I tried writing about the Society for the Suppression of the Correlation Coefficient things (I think) fell into place. The result is a piece that is one part history of statistics trivia, one part mini-tutorial and one part a summary of my 2009 paper. \n \nFor non-members of the BPS the link below contains a pre-publication version of 'When correlations go bad ...'. \n \n \nI may also get around to the web site one day. \n \nP.S. I adapted data and R code from Gelman and Hill (2007) for the example. I chose it because it a nice simple example of regression and because it is also a pointer to someone who argues in favour of standardization )at least in some situations). Both my 2009 paper and the 'When correlations go bad ...' are my attempts at getting people to rethink the use of correlation and standardization. The tone is deliberately (slightly?) polemic. There are other views so don't just take my word for it ... think about what you are trying to do and decide whether a correlation coefficient (or a standardized mean d ifference) is the right option. \n \n \n  \n \n \n \n References \n \n \n \n  \n \n \n \n Baguley, T. (2009). Standardized or simple effect size: What should be reported? British Journal of Psychology. 100, 603\u2013617. \n \n  Baguley, T. (2010). When correlations go bad \u2026 The Psychologist, 23  , 122-3. \n \n  Brillinger, D.R. (2001). John Tukey and the correlation coefficient, Computing Science and Statistics, 33, 204\u2013218. \n \n Gelman, A. & Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research) . Cambridge: Cambridge University Press. \n \n Tukey, J.W. (1954). Causation, regression and path analysis. In O. Kempthorne, T.A. Bancroft, J.W. Gowen & J.L. Lush (Eds.) Statistics and mathematics in biology (pp.35\u201366). Ames, IA: Iowa State College Press."], "link": "http://psychologicalstatistics.blogspot.com/feeds/4284625626024500904/comments/default", "bloglinks": {}, "links": {"http://societytosupressthecorrelationcoefficient.wordpress.com/": 2, "http://feedads.doubleclick.net/": 2, "http://www.org.uk/": 1, "http://www.co.uk/": 1, "http://nottinghamtrent.academia.edu/": 2, "http://dx.doi.org/": 2}, "blogtitle": "Psychological Statistics"}, {"content": ["I recently gave an introduction to multilevel modeling talk at Strathclyde University for PsyPAG . I promised to make my slides available. I'm still using powerpoint (and regretting it) so some of the symbols may be garbled. (I noticed that powerpoint turned all my tilda symbols to colons during the talk). So I hope it is readable: \n \nHere is the link:"], "link": "http://psychologicalstatistics.blogspot.com/feeds/1846690160708647315/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.co.uk/": 1, "http://bps-msc.blogspot.com/": 1}, "blogtitle": "Psychological Statistics"}, {"content": ["1. Who is correct?  Professor B is correct. If the average family has 1.8 children then the average child would be expected to have more than 0.8 siblings. \n \n  2. Why?  The average child is not from the average family. The concepts of average child and average family are different. For this reason there should no expectation that the average child should be from a family with an average number of children. Although there are restricted circumstances under which this can happen, they are sufficiently implausible to be discounted in any real world application (e.g., if all families have exactly the same number of children). \n \n       \n  In one case the unit of analysis of the family and in the other it is the child. A concrete example may help. (I'll stick to defining average as arithmetic mean throughout, but the same logic extends to other averages such as the median - see the quotation from Kish below). \n \n      \n Assuming that the number of children varies between families (which it must do if the mean number of children per family is 1.8) then the average child will be from a family with a larger number of children than average. For example, imagine there are only four families: \n  \n \n  \n         \n  \n         \n  Number of siblings per child \n \n  Family \n  Number of Children \n  1  st   child \n  2  nd   child \n  3  rd   child \n  4  th   child \n \n  A \n  1 \n  0 \n  - \n  - \n  - \n \n  B \n  1 \n  0 \n  - \n  - \n  - \n \n  C \n  2 \n  1 \n  1 \n  - \n  - \n \n  D \n  4 \n  3 \n  3 \n  3 \n  3 \n \n  \n        \n There are (1+1+2+4) = 8 children in the four families, thus the mean number of children per family is: 8/4 = 2. \n   \n It follows that the mean number of siblings per child is therefore: \n \n \n (0+0+1+1+3+3+3+3)/8 = 14/8 = 1.75 \n   \n So although each family has only two children (on average) each child has 1.75 siblings (not 1 sibling). \n   \n Note is that there are  N  = 4 families and  n  = 8 children. So switching the unit of analysis changes the denominator. Also note that while the families can plausibly be considered independent of each other the children can't (all children in the same family have the same number of siblings in this example, and more generally the number of siblings will be correlated). \n   \n What about zeroes?  In the calculations above I excluded childless households as families. If you include only households without children as families the discrepancy would be larger. \n    \n  Does it matte  r? Yes. Much real world data is clustered in this way. It is important to realize that the average   is not going to be from the average  (e.g., the average worker isn't from the average firm) . \n   \n In practical terms this means that careful attention needs to be paid to sampling from families, workplaces, schools and so forth. A random sample of children will disproportionately sample children from large families. There are also social policy implications (e.g., if you are interested in reducing child poverty). Another example is that a random sample of schools will disproportionately sample small schools. \n   \n There are also other problems with analysis of clustered data. For this reason anyone working with clustered data needs to seriously consider using multilevel modeling or other methods that i) take into the clustering and ii) allow hypotheses about different levels of the model (e.g., children and families) to be explored. \n \n        \n Postscript \n   \n This is quite an old puzzle. I first came across this puzzle in the article by Jenkins and Tuten (1992). They include formulae for deriving one average from the other an d cite  Huntington (1924) and other later observations of the same phenomenon.  I made the connection to multilevel models a little later. For a good (if slightly out of date) introduction to multilevel models see Snijders and Bosker (1999). Recently I noticed that Kish (1965) discusses the same phenomenon in passing. \n   \n This quote from Kish sets out the problem quite clearly : \n Although the mean number of adults per household is only 2.02, the mean number of household members is 2.24 for the average adult. The greater size ranges of large organizations produce more striking effects. In 1960, 50 million people lived in 130 U. S. cities that had 100,000 or more population; in this population, the average city size was 0.39 million, but the size of the city in which the average person lived was 2.0 millions. Using medians does not help: the median city size was 0.19 million, but the median person lived in a city of 0.62 million. \n Kish (1965, p. 571). \n   References \n  \n        \n Jenkins, J. J., & Tuten, J. T. (1992). Why isn\u2019t the average child from the average family? \u2013 and similar puzzles.  American Journal of Psychology, 105  , 517-526. \n \n \n Kish  , L. (  1965  ). Sampling Organizations and Groups of Unequal Sizes,  American Sociological Review, 20  , 564-72 . \n \n \n Sniders, T. & Boskers, R. (1999).  Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling.   London: Sage."], "link": "http://psychologicalstatistics.blogspot.com/feeds/6707489846455399931/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.co.uk/": 1}, "blogtitle": "Psychological Statistics"}]