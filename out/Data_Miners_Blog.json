[{"blogurl": "http://blog.data-miners.com\n", "blogroll": [], "title": "Data Miners Blog"}, {"content": ["Gordon and I rarely find ourselves in the same city these days, but on November 15 we will be in Cary, North Carolina with our friends at JMP for a webcast with Anne Milley . The format will be kind of like the first presidential debate with Anne as the moderator, and kind of like the second one with questions from you , the audience. Sign up here ."], "link": "http://blog.data-miners.com/feeds/7132761509197519752/comments/default", "bloglinks": {}, "links": {"http://www.jmp.com/": 4}, "blogtitle": "Data Miners Blog"}, {"content": ["After taking a break from speaking at conferences for a while, I will be speaking at two in the next month. Both events are here in Boston. This Friday (9/14) I will be at Big Data Innovation talking about how Tripadvisor for Business models subscriber happiness and what we can do to improve a subscriber's probability of renewal. On October 1 and 2 I will be at Predictive Analytics World in Boston. This has become my favorite data mining conference. On the Monday, I will be visiting with my friends at JMP and giving a sponsored talk about how we use JMP for cannibalization analysis at Tripadvisor for Business. On Tuesday, I will go into the details of that analysis in more detail in a regular conference talk."], "link": "http://blog.data-miners.com/feeds/3471661835085855912/comments/default", "bloglinks": {}, "links": {"http://www.predictiveanalyticsworld.com/": 1, "http://analytics.theiegroup.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["One of our clients is a large media website that faced a simple question: What is the best way to find the most engaged users on the web site? The goal was to focus a marketing effort on these users. A media web site is challenging, because there is no simple definition of engagement or customer worth. The idea is that engagement can either lead to more advertising views or to longer subscriptions, depending on the business model for the site. On the other hand, for a retailing site, the question is simpler, because there is a simple method to see who the best customers are. Namely, the amount of money they spend. Engagement is a nice marketing concept, but how can it be defined in the real world? One way is to simply look at the number of page views during some period of time. Another is to look at the number of sessions (or alternatively days of activity if sessions are not available) during a specified period of time. Yet another is to measure breadth of usage of the site over a period of time: Does the user only go to one page? Is the user only coming in on referrals from Google? The first analysis used one month of data to define engagement. The top users for one month were determined based on pages and sessions. Of course, there is a lot of overlap between the two groups -- about 60% of the top deciles overlapped. Which group seems better for defining engagement, the top users by page views or by sessions? To answer this, let's borrow an idea from survival and measure how many users are still around nine months later. (Nine months is arbitrary in this case). In this case, the return rate for the top decile for sessions was 74.4% but for the top decile for pages was lower at 73.8%. Not a big difference, but one that suggests that sessions are better. Actually, the results are even more striking for visitors who are not in both top deciles. For the non-overlapping group, the session return rate is69.6% versus 67.9% for the page deciles. For defining engagement, we then extended these results to three months instead of one to find the top one million most engaged users. The three measures are: Visitors that have the most page views over three months. Visitors that have the most sessions over three months. Visitors in the top tercile of sessions (third) in each month, then take the highest terciles. Three months was chosen as a rather arbitrary length of time, because the data was available. Holding it constant also lets us understand the difference between sessions and page views. These three methods all produced about the same number of visitors -- the goal was to find the top one million most engaged users. By these measures, the top one million visitors chosen by the three methods had the following \"return\" rates, nine months later: Page views in three months: 65.4% Sessions in three months: 65.9% Sessions over three months: 66.9% The nine-month survival suggests that the sessions over three months is the better approach for measuring engagement."], "link": "http://blog.data-miners.com/feeds/2389730628569045410/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Miners Blog"}, {"content": ["When a company introduces a new product into the same market served by an existing one, it is possible that the new product will achieve success at the expense of the first. For example, when Netflix introduced movie downloading, it knew it would put a dent in DVD subscriptions. This is called cannibalization. Here at TripAdvisor, I recently did a study to determine whether this was occurring on our sites. A good methodology to use for this kind of study is matched pairs . This allows you to isolate the effects of a single variable while controlling for many others. The idea is simple: To measure the effect of a treatment, you take pairs of subjects who are similar in every way and give the treatment to one, but not the other. In medical studies, twins come in handy for this purpose.  To simplify slightly, at TripAdvisor we have two ways to generate revenue from the millions of travelers who come to one of our sites to read reviews: they can click on link A to be taken to an on-line travel agency which pays us for the referral or they can click on link B to be taken directly to the site of a hotel that has subscribed to our business listing product. So the question is \u201cDoes the presence of link B have an effect on the number of clicks received by link A?\u201d To answer this question, each property with a business listing is paired with a \u201ctwin\u201d that does not have a business listing. The result is two cohorts with extremely similar distributions of average daily rate, number of reviews, amount of traffic on review page, number of rooms, and everything else I could think of that might influence clicks on link A. Since the only consistent difference between the cohorts is the presence or absence of link B, any statistically significant difference in Link-A clicks can be attributed to the presence of the business listing.  Why not just compare a random sample of hotels with links A and B with a random sample of hotels with only link A? Such a comparison would be very flattering to link B; on average, hotels with a business listings subscription perform better than those without one on all kinds of metrics including clicks on link A. This is not surprising. Business listings do not appeal to all properties equally, nor have they been marketed with equal vigor in all markets and market segments. Such a study cannot distinguish between a difference caused by link B and one that is merely correlated with link B. For example, perhaps link B appeals more to hotels in high-traffic destinations and those same properties also attract more clicks of all kinds  Why not do a longitudinal study? The goal would be to compare the click rate before and after link B goes live on a hotel\u2019s review page. The problem with this approach is that though the change in click rate is easy to measure, it is hard to interpret. The quantity of clicks varies over time for all sorts of reasons that have nothing to do with the presence or absence of a business listing. In addition to seasonality, there is trend: The ever increasing number of TripAdvisor users means that clicks will tend to increase over time. Add to that the effects of marketing campaigns, competition, changing exchange rates, and political factors and there is a lot of noise obscuring whatever signal is in the data. A cross-sectional study controls for all that.  How is similarity measured? The matched pairs methodology calls for each subscriber to be paired with the non-subscriber most similar to it. For this study, there is a list of features that must match exactly and another list of features which, as a group, must be \u201cpretty close.\u201d The exact match features are categorical. The pretty close features are numeric.  Exact match features \u00b7   Same price business listing. \u00b7   Same geographic region. \u00b7   Same category (Hotel, B&B, Specialty Lodging). \u00b7   Same chain status (a Hilton can match a Marriott, but neither can match an independent property). \u00b7   Matching properties are both on the first page of listings for a destination or both on some other page. \u00b7   Presence or absence of reviews supplied by our users. Hotels that match on all of the above are candidates for matching. A hotel\u2019s actual match is its closest neighbor as determined by the \u201cpretty close\u201d features. The exact match features control for many variables that are not mentioned explicitly. For example, the price charged for a business listing depends on the popularity of the destination and the size of the property so hotels in the same pricing slice are similar in size and traffic. Matching on geography controls for currency, climate, language, and much else. Pretty close features \u00b7   Average daily rate. \u00b7   Number of rooms. \u00b7   Popularity ranking. \u00b7   Review page views.  The values of these features place each property at a point in a four-dimensional space so it is easy to calculate the Euclidean distance between any pair of properties. The closest candidate by Euclidean distance is picked as the match. Because the features are all measured on different scales, they must first be standardized to make distance along one dimension comparable to distance along any other. A few pairs are so well matched that, according to this measure, they are distance 0 from each other.               The hotels on the left have business listings. The ones on the right are their twins without business listings. Podere Perelli and Agriturismo il Borghetto are twins because each has 12 rooms, each got exactly 72 page views during the observation period, and each is seventh on its page. The results Deciding on the distance metric and creating the matched pairs was most of the work. Once I had the pairings, I loaded 36,000 closely matched pairs into JMP, a data exploration and analysis tool that includes a matched pairs module.   In the diamond-shaped chart, the horizontal axis represents increasing number of clicks on link A (\u201ccommerce clicks\u201d in the figure). To the left, where the number of clicks is low, there are some dots below the red line indicating pairs where the non-subscriber got more link-A clicks, but as the number of clicks increases, the business listings subscriber nearly always wins. In conclusion, after controlling for differences due to geography, traffic, popularity, hotel category, number of rooms, presence or absence of reviews, appearance on page one, and average daily rate, we counted the number of clicks each twin received during a fixed observation period. There was a statistically significant difference in the number of clicks on link A. The average number of clicks for business listing subscribers was 597.49. The average number for non-subscribers was 411.69. This is good news for our subscribing hoteliers: In addition to the traffic we drive directly to their sites, they see increased indirect traffic as well."], "link": "http://blog.data-miners.com/feeds/131468558542513337/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://1.blogspot.com/": 2}, "blogtitle": "Data Miners Blog"}, {"content": ["It has been a while since I've contributed to the blog . . . not because I've had nothing to say. In this time, I've been spending a lot of time working with SQL Server, producing useful stored procedures (and insights). In this post, I discuss one of them, a stored procedure in SQL Server to write text to a file. This stored procedure is a utility. I learned a lot along the way while trying to write it. This post is intended to explain these learnings. The approach that I'm taking is to use xp_cmdshell to write one line at a time using the DOS echo command. A different approach uses OLE automation and the File System Object. I couldn't get this to work, possibly because it requires configurations that I don't know about; possibly because I don't have the right permissions. My stored procedure is called usp__AppendToFile and the code is at the end of this post. If you care about naming conventions, here is the reasoning behind the name. The \"usp\" prefix is for user stored procedure. Starting a stored procedure with usp or sp seems redundant to me, but appears to be a common and perhaps even a best practice. The double underscore is my convention, saying that this is a utility. It is then followed by a reasonable name. usp__AppendToFile does the following: It takes a string ( varchar(max) ) and an optional end-of-line character. It then writes the string, one line at a time, using the echo command in DOS. By passing in the end of line character, the stored procedure can work with text that uses the DOS standard end of line (carriage return followed by line feed, the default) as well as other standards. Although seemingly simple and using familiar tools, I learned several things from this effort. My first lesson is that in order to write to a file, you need to be able to access it. When running you a command in SQL Server, it is not really \"you\" that needs permissions. The SQL Server service needs to be able to access the file. And this depends on the user running the service. To see this user, go to the Control Panel, choose the Administrative Tools, and select Services. Scroll down to find the SQL Server service (called something like SQL Server Agent ), and look in the column Log On As . As an example, the user running the service on one machine used a local machine account rather than a Windows verified domain account. For this reason, SQL Server could not access files on the network. Changing the service to run on a Windows-authenticated enabled SQL Server to create a file. (The alternative of changing the permissions for the user was not possible, since I do not have network sys admin privileges.) The second lesson is that in order to write to a file using xp_cmdshell , you need to have xp_cmdshell enabled as shown here . There are good reasons why some DBAs strongly oppose enabling this option, since it does open up a security hole. Well, actually, the security hole is the fault of Microsoft, since the command is either enabled or disabled at the server level. What we really want is to give some users access to it, which denying others. Third, the DOS way to write text to a file is using the echo command. Nothing is as simple as it seems. Echo does generally write text. However, it cannot write an empty line. Go ahead. Open a CMD shell, type in echo and see what happens. Then type in echo with a bunch of spaces and see what happens. What you get is the informative message: ECHO is on. Thanks a bunch, but that's not echoing what was on the command line. I want my procedure to write blank lines when it finds them in the string. To fix this problem, use the echo. command. For whatever reason, having the period allows an empty line to be written. Apparently, other characters work as well, but period seems to be the accepted one. The problems with DOS seem solved, but they are not. DOS has another issue: some special characters are interpreted by DOS, even before echo gets to them. For instance, > is interpreted to put the results to a file; | is interpreted as a pipe between commands, and & is interpreted as a background command. Fortunately, these can be escaped using the DOS escape character, which I'm sure everyone knows is a caret (^). But, this issue does not end there, because special characters might be in a string, in which case they do not need to be escaped. Parsing a string in a stored procedure to find quotes is beyond the range of this stored procedure. Instead, if there are no double quotes in the string, then it escapes special characters. Otherwise, it does not. Combining these lessons, here is what I consider to be a useful utility to write a string to a text file, even when the string consists of multiple lines. CREATE procedure usp__AppendToFile ( @str varchar(max), @FileName varchar(255), @EOL varchar(10) = NULL ) as begin if @EOL is NULL begin  set @EOL = char(13) + char(10); end;  -- the period allows for empty lines declare @prefix varchar(255) = 'echo.'; declare @suffix varchar(255) = '>>'+@FileName;  -- Escape special characters so things work -- But escapes work funny when in double quotes (and maybe single quotes too) set @str = (case when charindex('\"', @str) = 0      then replace(replace(replace(@str, '|', '^|'), '>', '^>'), '&', '^&')      else @str    end);  while (@str <> '') begin  declare @pos int = charindex(@EOL, @str);  declare @line varchar(8000) = (case when @pos > 0 then left(@str, @pos) else @str end);  set @str = (case when @pos > 0 then substring(@str, @pos+2, 1000000) else '' end);   set @line = @prefix+@line+@suffix;   --write @line to file;  exec xp_cmdshell @line;  end; end; -- usp__AppendToFile"], "link": "http://blog.data-miners.com/feeds/4650700929458696153/comments/default", "bloglinks": {}, "links": {"http://msdn.microsoft.com/": 1, "http://www.simple-talk.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["The millions of travelers who review hotels, restaurants, and other attractions on TripAdvisor also supply a numeric rating by clicking one of five circles ranging from 1 for \"terrible\" to 5 for \"excellent.\" On the whole, travelers are pretty kind.The average review rating for hotels and other lodgings is over 3.9. The median score is 4 and since that middle review is lost somewhere in a huge pile of 4-ratings, well over half of hotel reviews give a 4 or 5 rating. So with such kind reviewers, most hotels must have a rating over 4 and hoteliers must all love us, right? Actually, no. The average of all hotel ratings is 3.6. Here's why: some large, frequently-reviewed hotels have thousands of reviews. It is hardly surprising that the Bellagio in Las Vegas has about 250 times more reviews than say, the Cambridge Gateway Inn, an unloved motel in Cambridge, Massachusetts. It may or may not be surprising that these oft-reviewed properties tend to be well-liked by our reviewers. Surprising or not, it's true: the hotels with the most reviews have a higher average rating than the long tail of hotels, motels, B&Bs, and Inns with only a handful of reviews each. The chart compares the distribution of user review scores with the distribution of hotel average scores.  For the curious, here are the top 10 hotels on TripAdvisor by number of reviews:  Luxor Las Vegas Majestic Colonial Punta Cana Bellagio Las Vegas MGM Grand Hotel and Casino Excellence Punta Cana Flamingo Hotel & Casino Venetian Resort Hotel Casino Hotel Pennsylvania New York Excalibur Hotel & Casino Treasure Island - TI Hotel & Casino  Not all of these are beloved by TripAdvisor users. The Hotel Pennsylvania drags the average down since it receives more ones than any other score. Despite that, as a group these hotels have a higher than average score. The moral of the story is that you can't extrapolate from one level of aggregation to another without knowing how much weight to give each unit. In the last US presidential election, the average state voted Republican, but the average voter voted Democrat."], "link": "http://blog.data-miners.com/feeds/2813916013080352350/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["It's been a while since I posted. My new role at TripAdvisor has been keeping me pretty busy! My first post after a long absence is about a feature of SQL that I have recently fallen in love with. Usually, I leave it to Gordon to write about SQL since he is an expert in that field, but this particular feature is one that he doe not write about in Data Analysis Using SQL and Excel . The feature is called common table expressions or, more simply, the WITH statement. Common table expressions allow you to name a bunch of useful subquerries before using them in your main query. I think of the common table expressions as subquerries because that is what they usually replace in my code, but they are actually a lot more convenient than subquerries because they aren't \"sub\". They are there at the top level so your main query can refer to them as many times as you like anywhere in the query. In that way, they are more like temporary tables or views. Unlike tables and views, however, you don't have to be granted permission to create them, and you don't have to remember to clean them up when you are done. Common table expressions last only as long as the query is running. An example will help show why common table expressions are so useful. Suppose (because it happens to be true) that I have a complicated query that returns a list of hotels along with various metrics. These could be as simple as the number of rooms, or the average daily rate, or the average rating by our reviewers, or it could be a complex expression to produce a model score. For this purpose, it doesn't matter what the metric is, what matters is that I want to compare \"similar\" properties for some definition of similar. The first few rows returned by my complicated query look something like this:  Similar hotels have the same value of feature and similar ranking. In fact, I want to compare each hotel with four others: The one with matching feature that is next above it in rank, the one with matching feature that is next below it in rank, the one with non-matching feature that is next above it in rank, and the one with non-matching feature that is next below it in rank. Of course, for any one hotel, some of these neighbors may not exist. The top ranked hotel has no neighbors above it, for instance. My final query involves joining the result pictured above with itself four times using non-equi joins, but for simplicity, I'll leave out the matching and non-matching features bit and simply compare each hotel to the one above and below it in rank. The ranking column is dense, so I can use equi joins on ranking=ranking+1 and ranking=ranking-1 to achieve this. Here is the query: with ranks (id, hotel, ranking, feature, metric1, metric2)\n  as(select . . .) /* complicated query to get rankings */ \n select r0.id, r0.hotel, \n  r0.metric1 as m1_self, r1.metric1 as m1_up, r2.metric1 as m1_down\n from ranks r0 /* each hotel */ left join\n  ranks r1 on r0.ranking=r1.ranking+1 /* the one above */ left join\n  ranks r2 on r0.ranking=r2.ranking-1 /* the one below */\n order by r0.ranking\n The common table expression gives my complicated query the name ranks . In the main query, ranks appears three times with aliases r0, r1, and r2. The outer joins ensure that I don't lose a hotel just because it is missing a neighbor above or below. The query result looks like this:  The Hotel Commonwealth has the highest score, a 99, so there is nothing above it. In this somewhat contrived example, the hotel below it is the Lenox with a score of 98 and so on down the list. To write this query using subqueries, I would have had to repeat the subquery three times which would not only be ugly, it would risk actually running the subquery three times since the query analyzer might not notice that they are identical."], "link": "http://blog.data-miners.com/feeds/514562995321351700/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 2, "http://www.data-miners.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["On Tuesday, May 24 at 1:00pm Eastern Daylight Time, I will be presenting a webcast on behalf of JMP, a visual data exploration and mining tool. The main theme of the talk is that companies tend to manage to metrics, so it is very important that the metrics are well-chosen. I will illustrate this with a small case study from the world on on-line retailing recommendations. A secondary theme is the importance of careful data exploration in preparation for modeling--a task JMP is well-suited to. -Michael Register ."], "link": "http://www.jmp.com/about/events/webcasts/jmpwebcast_detail.shtml?reglink=70130000000TVfs", "bloglinks": {}, "links": {"http://www.jmp.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Hello Readers, As some of you will already have heard, I have accepted the position of Business Intelligence Director at TripAdvisor for Business--the part of TripAdvsor that sells products and services to businesses rather than consumers. The largest part of T4B as this side of the business is called internally is selling direct links to hotel web sites that appear right next to the hotel reviews on TripAdvisor.com. Subscribers are also able to make special offers (\"free parking\", \"20% off\", \"a free bottle of wine with your meal\", . . .) directly on the TripAdvisor site. Another T4B product is listings for vacation rental properties. There is a lot of data, and a lot of questions to be answered! I will continue to contribute to this blog and I will continue to work with Gordon and Brij on the data mining courses that Data Miners produces. TripAdvisor is based in Newton, Massachusetts--not far from my home in Cambridge. It will be novel going home every night after work! -Michael"], "link": "http://blog.data-miners.com/feeds/4968314493030565134/comments/default", "bloglinks": {}, "links": {"http://www.tripadvisor.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Gordon and I spent much of the last year writing the third edition of Data Mining Techniques and now, at last, I am holding the finished product in my hand. In the 14 years since the first edition came out, our knowledge has increased by a factor of at least 10 while the page count has only doubled so I estimate the information density has increased by a factor of five! I hope reviewers will agree that our writing skills have also improved with time and practice. In short, I'm very proud of our latest effort and I hope our readers will continue to find it useful for the next 14 years!   Table of Contents Chapter 1 What Is Data Mining and Why Do It? 1 Chapter 2 Data Mining Applications in Marketing and Customer Relationship Management 27 Chapter 3 The Data Mining Process 67 Chapter 4 Statistics 101: What You Should Know About Data 101 Chapter 5 Descriptions and Prediction: Profi ling and Predictive Modeling 151 Chapter 6 Data Mining Using Classic Statistical Techniques 195 Chapter 7 Decision Trees 237 Chapter 8 Artifi cial Neural Networks 283 Chapter 9 Nearest Neighbor Approaches: Memory-Based Reasoning and Collaborative Filtering 323 Chapter 10 Knowing When to Worry: Using Survival Analysis to Understand Customers 359 Chapter 11 Genetic Algorithms and Swarm Intelligence 399 Chapter 12 Tell Me Something New: Pattern Discovery and Data Mining 431 Chapter 13 Finding Islands of Similarity: Automatic Cluster Detection 461 Chapter 14 Alternative Approaches to Cluster Detection 501 Chapter 15 Market Basket Analysis and Association Rules 537 Chapter 16 Link Analysis 583 Chapter 17 Data Warehousing, OLAP, Analytic Sandboxes, and Data Mining 615 Chapter 18 Building Customer Signatures 657 Chapter 19 Derived Variables: Making the Data Mean More 695 Chapter 20 Too Much of a Good Thing? Techniques for Reducing the Number of Variables 737 Chapter 21 Listen Carefully to What Your Customers Say: Text Mining 777 Index 823"], "link": "http://www.amazon.com/exec/obidos/ASIN/0470650931/thedataminers", "bloglinks": {}, "links": {"http://www.amazon.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["A client recently wrote to us saying that she liked decision tree models, but for a model to be used at her bank, the risk compliance group required an R-squared value for the model and her decision tree software doesn't supply one. How should she fill in the blank? There is more than one possible answer. Start with the definition of R-squared for regular (ordinary least squares) regression. There are three common ways of describing it. For OLS they all describe the same calculation, but they suggest different ways of extending the definition to other models. The calculation is 1 minus the ratio of the sum of the squared residuals to the sum of the squared differences of the actual values from their average value. The denominator of this ratio is the variance and the numerator is the variance of the residuals. So one way of describing R-squared is as the proportion of variance explained by the model. A second way of describing the same ratio is that it shows how much better the model is than the null model which consists of not using any information from the explanatory variables and just predicting the average. (If you are always going to guess the same value, the average is the value that minimizes the squared error.) Yet a third way of thinking about R-squared is that it is the square of the correlation r between the predicted and actual values. (That, of course, is why it is called R-squared.) Back to the question about decision trees: When the target variable is continuous (a regression tree), there is no need to change the definition of R-squared. The predicted values are discrete, but everything still works. When the target is a binary outcome, you have a choice. You can stick with the original formula. In that case, the predicted values are discrete with values between 0 and 1 (as many distinct estimates as the tree has leaves) and the actuals are either 0 or 1. The average of the actuals is the proportion of ones (i.e. the overall probability of being in class 1). This method is called Efron's pseudo R-squared. Alternatively, you can say that the job of the model is to classify things. The null model would be to always predict the most common class. A good pseudo R-squared is how much better does your model do? In other words, the ratio of the proportion correctly classified by your model to the proportion of the most common class. There are many other pseudo R-squares described on a page put up by the statistical consulting services group at UCLA ."], "link": "http://blog.data-miners.com/feeds/4848301117394880092/comments/default", "bloglinks": {}, "links": {"http://www.ucla.edu/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Michael will be doing a fair amount of teaching and presenting over the next several weeks: March 16-18 Data Mining Techniques Theory and Practice at SAS Institute in Chicago. March 29 Applying Survival Analysis to Forecasting Subscriber Levels at the New England Statistical Association Meeting. April 7 Predictive Modeling for the Non-Statistician at the TDWI conference in Washington, DC."], "link": "http://blog.data-miners.com/feeds/8008772406074291443/comments/default", "bloglinks": {}, "links": {"http://events.tdwi.org/": 1, "http://mberry.eventbrite.com/": 1, "https://support.sas.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["The book is done! All 822 pages of the third edition of Data Mining Techniques for Marketing, Sales, and Customer Relationship Management will be hitting bookstore shelves later this month or you can order it now . To celebrate, I am returning to the blog. One of the areas where Gordon and I have added a lot of new material is clustering. In this post, I want to share a nice measure of cluster goodness first described by Peter Rousseeuw in 1986. Intuitively, good clusters have the property that cluster members are close to each other and far from members of other clusters. That is what is captured by a cluster's silhouette.  To calculate a cluster\u2019s silhouette, first calculate the average distance within the cluster. Each cluster member has its own average distance from all other members of the same cluster. This is its dissimilarity from its cluster. Cluster members with low dissimilarity are comfortably within the cluster to which they have been assigned. The average dissimilarity for a cluster is a measure of how compact it is. Note that two members of the same cluster may have different neighboring clusters. For points that are close to the boundary between two clusters, the two dissimilarity scores may be nearly equal. The average distance to fellow cluster members is then compared to the average distance to members of the neighboring cluster. The pictures below show this process for one point (17, 27).    The ratio of a point's dissimilarity to its own cluster to its dissimilarity with its nearest neighboring cluster is its silhouette. The typical range of the score is from zero when a record is right on the boundary of two clusters to one when it is identical to the other records in its own cluster. In theory, the silhouette score can go from negative one to one. A negative value means that the record is more similar to the records of its neighboring cluster than to other members of its own cluster. To see how this could happen, imagine forming clusters using an agglomerative algorithm and single-linkage distance. Single-linkage says the distance from a point to a cluster is the distance to the nearest member of that cluster. Suppose the data consists of many records with the value 32 and many others with the value 64 along with a scattering of records with values from 32 to 50. In the first step, all the records at distance zero are combined into two tight clusters. In the next step, records distance one away are combined causing some 33s to be added to the left cluster followed by 34s, 35s, etc. Eventually, the left cluster will swallow records that would feel happier in the right cluster. The silhouette score for an entire cluster is calculated as the average of the silhouette scores of its members. This measures the degree of similarity of cluster members. The silhouette of the entire dataset is the average of the silhouette scores of all the individual records. This is a measure of how appropriately the data has been clustered. What is nice about this measure is that it can be applied at the level of the dataset to determine which clusters are not very good and at the level of a cluster to determine which members do not fit in very well. The silhouette can be used to choose an appropriate value for k in k-means by trying each value of k in the acceptable range and choosing the one that yields the best silhouette. It can also be used to compare clusters produced by different random seeds. The final picture shows the silhouette scores for the three clusters in the example."], "link": "http://blog.data-miners.com/feeds/4103659235793544203/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "https://lh4.googleusercontent.com/": 1, "https://lh3.googleusercontent.com/": 3}, "blogtitle": "Data Miners Blog"}, {"content": ["We haven't been updating the blog much recently. Data mining blogger, Ajay Ohri figured out why. We have been busy working on a new edition of Data Mining Techniques. He asked me about that in this interview for his blog."], "link": "http://decisionstats.com/2010/10/05/interview-michael-j-a-berry-data-miners-inc/", "bloglinks": {}, "links": {"http://decisionstats.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Last week, a student in our Applying Survival Analysis to Business Time-to-Event Problems class asked this question. He made clear that he wasn't looking for a mathematical derivation, just an intuitive understanding. Even though I make use of this property all the time (indeed, I referred to it in my previous post where I used it to calculate the one-year truncated mean tenure of subscribers from various industries), I had just sort of accepted it without much thought. I was unable to come up with a good explanation on the spot, so now that I've had time to think about it, I am answering the question here where others can see it too. It is really quite simple, but it requires a slight tilt of the head. Instead of thinking about the area under the survival curve, think of the equivalent area to the left of the survival curve. With the discreet-time survival curves we use in our work, I think of the area under the curve as a bunch of vertical bars, one for each time period. Each rectangle has width one and its height is the percentage of customers who have not yet canceled as of that period. Conveniently, this means you can estimate the area under the survival curve by simply adding up the survival values. Looked at this way, it is not particularly clear why this value should be the mean tenure.   So let's look at it another way, starting with the original data. Here is a table of some customers with their final tenures. (Since this is not a real example, I haven't bothered with any censored observations; that makes it easy to check our work against the average tenure for these customers which is 7.56.)  Stack these up like cuisenaire rods with the longest on the bottom and the shortest on the top, and you get something that looks a lot like the survival curve.  If I made the bars fat enough to touch, each would get 1/25 of the height of the stack. The area of each bar would be 1/25 times the tenure. If everyone had tenure of 20, like Tim, the area would be 25*1/25*20=20. If everyone had tenure of 1, like the first of the two Daniels, then the area would be 25*1/25*1=1. Since most customers have tenures somewhere between Tim's and Daniels, the area actually comes out to 7.56-- the average tenure. In J : x 20 14 13 12 11 11 11 11 10 8 8 8 7 7 7 6 5 4 3 3 3 2 2 2 1 + /x%25 NB. This is J for \"the sum of x divided by 25\" 7.56  So, the area under (or to the left of) the stack of tenure bars is equal to the average tenure, but the stack of tenure bars is not exactly the survival curve. The survival curve is easily derived from it, however. For each tenure, it is the percentage of bars that stick out past it. At tenure 0, all 25 bars are longer than 0, so survival is 100%. At tenure 1, 24 out of 25 bars stick out past the line, so survival is 96% and so on. In J :  i. 21 NB. 0 through 20 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 +/\"1 (i. 21) 25 24 21 18 17 16 15 12 9 9 8 4 3 2 1 1 1 1 1 1 0 After dividing by 25 to turn the counts into percentages, we can add the survival curve to the chart.   Now, even the vertical view makes sense. The vertical grid lines are spaced one period apart. The number of blue bars between two vertical grid lines says how many customers are going to contribute their 1/25 to the area of the column. This is determined by how many people reached that tenure. At tenure 0, there are 25/25 of a full day. At tenure 1, there are 24/25, and so on. Add these up and you get 7.56."], "link": "http://blog.data-miners.com/feeds/7517937750629757645/comments/default", "bloglinks": {}, "links": {"http://www.data-miners.com/": 1, "http://2.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://1.blogspot.com/": 2, "http://blog.data-miners.com/": 4}, "blogtitle": "Data Miners Blog"}, {"content": ["Occasionally one has an idea that seems so obvious and right that it must surely be standard practice and have a well-known name. A few months ago, I had such an idea while sitting in a client\u2019s office in Ottawa. Last week, I wanted to include the idea in a proposal, so I tried to look it up and couldn\u2019t find a reference to it anywhere. Before letting my ego run away with itself and calling it the na\u00efve Berry model , I figured I would share it with our legions of erudite readers so someone can point me to a reference. Some Context My client sells fax-to-email and email-to-fax service on a subscription basis. I had done an analysis to quantify the effect of various factors such as industry code, acquisition channel, and type of phone number (local or long distance) on customer value. Since all customers pay the same monthly fee, the crucial factor is longevity. I had analyzed each covariate separately by calculating cancellation hazard probabilities for each stratum and generating survival curves. The area under the first year of each survival curve is the first year truncated mean tenure. Multiplying the first-year mean tenure by the subscription price yields the average first year revenue for a segment. This let me say how much more valuable a realtor is than a trucker; or a Google adwords referral than an MSN referral. For many purposes, the dollar value was not even important. We used the probability of surviving one year as a way of scoring particular segments. But how should the individual segment scores be combined to give an individual customer a score based on his being a trucker with an 800 number referred by MSN? Or a tax accountant with a local number referred by Google? The standard empirical hazards approach would be to segment the training data by all levels of all variables before estimating the hazards, but that was not practical since there were so many combinations that many would lack sufficient data to make confident hazard estimates. Luckily, there is a standard model for combining the contributions of several independent pieces of evidence\u2014na\u00efve Bayesian models. An excellent description of the relationship between probability, odds, and likelihood and how to use them to implement na\u00efve Bayesian models, can be found in Chapter 10 of Gordon Linoff\u2019s Data Analysis Using SQL and Excel . Here are the relevant correspondences: odds = p/(1-p) p = 1 - (1/(1+odds)) likelihood = (odds|evidence)/overall odds Statisticians switch from one representation to another as convenient. A familiar example is logistic regression. Since linear regression is inappropriate for modeling probabilities that range only from 0 to 1, they convert the probabilities to log(odds) that vary from negative infinity to positive infinity. Expressing the log odds as a linear regression equation and solving for p, yields the logistic function. Na\u00efve Bayesian Models The Na\u00efve Bayesian model says that the odds of surviving one year given the evidence is the overall odds times the product of the likelihoods for each piece of evidence. For concreteness, let\u2019s calculate a score for a general contractor (industry code 1521) with a local number who was referred by a banner ad. The probability of surviving one year is 54%. Overall survival odds are therefore 0.54/(1-0.54) or 1.17. One-year survival for industry code 1521 is 74%, considerably better than overall survival. The survival likelihood is defined as the survival odds, 0.74/(1-0.74) divided by the overall survival odds of 1.17. This works out to 2.43. One-year survival for local phone numbers is 37%, considerably worse than overall survival. Local phone numbers have one-year survival odds of 0.59 and likelihood of 0.50. Subscribers acquired through banner ads have one-year survival of 0.52, about the same as overall survival. This corresponds to odds of 1.09 and likelihood of 0.91. Plugging these values into the na\u00efve Bayesian model formula, we estimate one-year survival odds for this customer as 1.17*2.43*0.50*0.91=1.29. Solving 1.29=p/(p-1) for p yields a one-year survival estimate of 56%, a little bit better than overall survival. The positive evidence from the industry code slightly outweighs the negative evidence from the phone number type. This example does not illustrate another great feature of na\u00efve Bayesian models. If some evidence is missing\u2014if the subscriber works in an industry for which we have no survival curve, for example\u2014you can simply leave out the industry likelihood term. The Idea If we are happy to use the na\u00efve Bayesian model to estimate the probability of a subscriber lasting one year, why not do the same for daily hazard probabilities? This is something I\u2019ve been wanting to do since the first time I ever used the empirical hazard estimation method. That first project was for a wireless phone company. There was plenty of data to calculate hazards stratified by market or rate plan or handset type or credit class or acquisition channel or age group or just about any other time-0 covariate of interest. But there wasn\u2019t enough data to estimate hazards for every combination of the above. I knew about na\u00efve Bayesian models back then; I\u2019d used the Evidence Model in SGI\u2019s Mineset many times. But I never made the connection\u2014it\u2019s hard to combine probabilities, but easy to combine likelihoods. There you have it: Freedom from the curse of dimensionality via the na\u00efve assumption of independence. Estimate hazards for as many levels of as many covariates as you please and then combine them with the na\u00efve Bayesian model. I tried it, and the results were pleasing. An Example This example uses data from a mobile phone company. The dataset is available on our web site . There are three rate plans, Top, Middle, and Bottom. There are three markets, Gotham, Metropolis, and Smallville. There are four acquisition channels, Dealer, Store, Chain, and Mail. There is plenty of data to make highly confident hazard estimates for any of the above, but some combinations, such as Smallville-Mail-Top are fairly rare. For many tenures, no one with this combination cancels so there are long stretches of 0 hazard punctuated by spikes where one or two customers leave.  Here are the Smallville-Mail-Top hazard by the Na\u00efve Berry method:   Isn\u2019t that prettier? I think it makes for a prettier survival curve as well.  The na\u00efve method preserves a feature of the original data\u2014the sharp drop at the anniversary when many people coming off one-year contracts quit\u2014that was lost in the sparse calculation."], "link": "http://blog.data-miners.com/feeds/275188128085782855/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://2.blogspot.com/": 1, "http://3.blogspot.com/": 2, "http://www.data-miners.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["For any of our readers who have been wishing they could read our book Data Mining Techniques for Marketing, Sales, and Customer Relationship Management (2nd Edition) in Korean , now you can! We don't know why the cover pictures someone playing jacks, but then we don't really understand how our publisher chooses our U.S. cover pictures either. This book was already available in Japanese , and, of course, English . Earlier editions are available in Traditional Chinese and French ."], "link": "http://mania.isbnshop.com/books/book.php?isbn=9788993292244", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://www.amazon.fr/": 1, "http://www.co.jp/": 1, "http://mania.isbnshop.com/": 1, "http://www.amazon.com/": 1, "http://goods.com.tw/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Alternate title: Data Mining Consultant with Egg on Face Last week I made a client presentation. The project was complete. I was presenting the final results to the client. The CEO was there. Also the CTO, the CFO, the VPs of Sales and Marketing, and the Marketing Analytics Manager. The client runs a subscription-based business and I had been analyzing their attrition patterns. Among my discoveries was that customers with \"blue\" subscriptions last longer than customers with \"red\" subscriptions. By taking the difference of the area under the two survival curves truncated at one year and multiplying by the subscription cost, I calculated the dollar value of the difference. I put forward some hypotheses about why the blue product was stickier and suggested a controlled experiment to determine whether having a blue subscription actually caused longer tenure or was merely correlated with it. Currently, subscribers simply pick blue or red at sign-up. There is no difference in price. I proposed that half of new customers be given blue by default unless they asked for red and the other half be given red by default unless they asked for blue. We could then look for differences between the two randomly assigned groups. All this seemed to go over pretty well. There is only one problem. The blue customers may not be better after all. One of the attendees asked me whether the effect I was seeing could just be a result of the fact that blue subscriptions have been around longer than red ones so the oldest blue customers are older than the oldest red customers. I explained that this would not bias my findings because all my calculations were based on the tenure time line, not the calendar time line. We were comparing customers' first years without regard to when they happened. I explained that there would be a problem if the data set suffered from left truncation, but I had tested for that, and it was not a problem because we knew about starts and stops since the beginning of time. Left truncation is something that creates a bias in many customer databases. What it means is that there is no record of customers who stopped before some particular date in the past--the left truncation date. The most likely reason is that the company has been in existence longer than its data warehouse. When the warehouse was created, all active customers were loaded in, but customers who had already left were not. Fine, for most applications, but not for survival analysis. Think about customers who started before the warehouse was built. One (like many thousands of others) stops before the warehouse gets built with a short tenure of two months. Another, who started on the same day as the first, is still around two be loaded into the warehouse with a tenure of two years. Lots of short-tenure people are missing and long-tenure people are over represented. Average tenure is inflated and retention appears to be better than it really is. My client's data did not have that problem. At least, not in the way I am used to looking for it. Instead, it had a large number of stopped customers for whom the subscription type had been forgotten. I (foolishly) just left these people out of my calculations. Here is the problem: Although the customer start and stop dates are remembered for ever, certain details, including the subscription type, are purged after a certain amount of time. For all the people who started back when there were only blue subscriptions and had short or even average tenures, that time had already past. The only ones for whom I could determine the subscription type were those who had unusually long tenures. Eliminating the subscribers for whom the subscription type had been forgotten had exactly the same effect as left truncation! If this topic and things related to it sound interesting to you, it is not too late to sign up for a two-day class I will be teaching in New York later this week . The class is called Survival Analysis for Business Time to Event Problems . It will be held at the offices of SAS Institute in Manhattan this Thursday and Friday, March 18-19."], "link": "http://blog.data-miners.com/feeds/4244233752559867739/comments/default", "bloglinks": {}, "links": {"http://www.data-miners.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Lately, I've been thinking about the topic of reducing the number of variables, and how this is a lot like clustering variables (rather than clustering rows). This post is about a method that seems intuitive to me, although I haven't found any references to it. Perhaps a reader will point me to references and a formal name. This method using Pearson correlation and principal components to agglomeratively cluster the variables. Agglomerative clustering is the process of assigning records to clusters, starting with the records that are closest to each other. This process is repeated, until all records are placed into a single cluster. The advantage of agglomerative clustering is that it creates a structure for the records, and the user can see different numbers of clusters. Divisive clustering, such as implemented by SAS's varclus proc, produces something similar, but from the top-down. Agglomerative variable clustering works the same way. Two variables are put into the same cluster, based on their proximity. The cluster then needs to be defined in some manner, by combining information in the cluster. The natural measure for proximity is the square of the (Pearson) correlation between the variables. This is a value between 0 and 1 where 0 is totally uncorrelated and 1 means the values are colinear. For those who are more graphically inclined, this statistic has an easy interpretation when there are two variables. It is the R-square value of the first principal component of the scatter plot. Combining two variables into a cluster requires creating a single variable to represent the cluster. The natural variable for this is the first principal component. My proposed clustering method repeatedly does the following: Finds the two variables with the highest correlation. Calculates the principal component for these variables and adds it into the data. Maintains the information that the two variables have been combined. The attached SAS code (available at sas-var-hierarchical-clustering-v01.sas ) does exactly this, although not in the most efficient and robust way. The bulk of the code is a macro, called buildcolumns , that appends the new cluster variables to the data set and maintains another table called columns which has the information about the rows. After I run this code, I can select different numbers of variables using the expression: proc sql; .... select colname .... from columns .... where counter  These variables can then be used for predictive models or visualization purposes. The inner loop of the code works by doing the following: Calling proc corr to calculate the correlation of all variables not already in a cluster. Transposing the correlations into a table with three columns, two for the variables and one for the correlation using proc transpose . Finding the pair of variables with the largest correlation. Calculating the first principal component for these variables. Appending this principal component to the data set. Updating the columns data set with information about the new cluster. The data set referred to in the code comes from the companion site for Data Analysis Using SQL and Excel . The code will fail (by running an infinite loop) if any variables are missing or if two variables are exactly correlated."], "link": "http://blog.data-miners.com/feeds/2657292361243080006/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://www.data-miners.com/blog": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["People often ask me what tools I use for data analysis. My usual answer is SQL and I explain that just as Willie Sutton robbed banks because \"that's where the money is,\" I use SQL because that is where the data is. But sometimes, it gets so frustrating trying to figure out how to get SQL to do something as seemingly straight forward as a running total or running maximum, that I let the data escape from the confines of its relational tables and into J where it can be free. I assume that most readers have never heard of J, so I'll give you a little taste of it here. It's a bit like R only a lot more general and more powerful. It's even more like APL, of which it is a direct descendant, but those of us who remember APL are getting pretty old these days. The question that sent me to J this time came from a client who had just started collection sales data from a web site and wanted to know how long they would have to wait before being able to make some statistically valid conclusions about whether spending differences between two groups who had received different marketing treatments were statistically significant. One thing I wanted to look at was how much various measures such as average order size and total revenue fluctuate from day to day and how many days does it take before the overall measures settle down near their long-term means. For example, I'd like to calculate the average order size with just one day's worth of purchases, then two day's worth, then three day's worth, and so on. This sort of operation, where a function is applied to successively longer and longer prefixes is called a scan. A warning: J looks really weird when you first see it. One reason is that many things that are treated as a single token are spelled with two characters. I remember when I first saw Dutch, there were all these impossible looking words with \"ij\" in them--ijs and rijs, for example. Well, it turns out that in Dutch \"ij\" is treated like a single letter that makes a sound a bit like the English \"eye.\" So ijs is ice and rijs is rice and the Rijn is a famous big river. In J, the second character of these two-character symbols is usually a '.' or a ':'. =: is assignment. <. is lesser of. >. is greater of. And so on. You should also know that anything following NB. on a line is comment text.  x=: ? 100#10      NB. One hundred random integers between 0 and 9  +/ x          NB. Like putting a + between every pair of x--the sum of x. 424 <. / x         NB. Smallest x 0 >. / x         NB. Largest x 9 mean x 4.24 ~. x          NB. Nub of x. (Distinct elements.) 3 0 1 4 6 2 8 7 5 9 # ~. x         NB. Number of distinct elements. 10  x # /. x         NB. How many of each distinct element. ( /. is like SQL GROUP BY.) 6 10 15 13 15 9 9 12 6 5 +/ \\ x          NB. Running total of x. 3 3 4 8 12 13 19 23 25 33 41 48 54 56 61 67 69 72 73 74 75 . . . >./ \\ x          NB. Running maximum of x. 3 3 3 4 4 4 6 6 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 . . . mean \\ x         NB. Running mean of x. 3 1.5 1.33333 2 2.4 2.16667 2.71429 2.875 2.77778 3.3 3.72727 . . . plot mean \\ x       NB. Plot running mean of x.   plot var \\ x        NB. Plot running variance of x.   J is available for free from J software . Other than as a fan, I have no relationship with that organization."], "link": "http://blog.data-miners.com/feeds/1790023569198993177/comments/default", "bloglinks": {}, "links": {"http://www.data-miners.com/blog": 2, "http://www.jsoftware.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Recently, I started a new project which has a database component. I looked around for some visual data modeling tools, and I settled on just using the diagrams capability of SQL Server. Since the client is using SQL Server, it was simple to download SQL Server Express and get started using their diagramming tool. After creating a bunch of tables, I learned that SQL Server Database Diagrams do not produce the Data Definition Language (DDL) to create the database. Instead, the tables are created in sync with the diagram. Furthermore, SQL Server does not have a command that creates the DDL for an entire database. Right clicking on two dozen tables is cumbersome. But even worse, it would not provide complete DDL, since the table DDL does not include index definitions. I have seen some debate on the web about the merits of graphical tools versus text DDL. Each has their advantages, and, personally, I believe that a decent database tool should allow users to switch between the two. The graphical environment lets me see the tables and their relationships. The text allows me to make global changes, such as: Changing all the SMALLDATETIME data types to DATE when I go to a commercial version of SQL Server. The Expression version does not support DATE, alas. Adding auditing columns -- such as user, creation date, and update date -- to almost all tables. Adding table-specific comments. Doing these types of actions in a point-and-click environment is cumbersome, inefficient, and prone to error. At the same time, the GUI environment is great for designing the tables and visualizing their relationships. So, I searched on the web for a DDL program that would allow me to create the DDL for an entire SQL Server database. Because I did not find any, I decided that I had to write something myself. The attached file contains script-all-tables.sql contains my script. This script uses SQL to generate SQL code -- a trick that I talk about in my book Data Analysis Using SQL and Excel . The script generates code for the following: Dropping all tables in the database, if they exist. Creating new versions of the tables, taking into account primary keys, data types, and identity columns. Creating foreign key constraints on the table. Creating indexes on the table. This is a very common subset of DDL used for databases. And, importantly, it seems to cover almost all that you can do using Database Diagrams. However, the list of what it is missing from fully re-creating any database is very, very long, ranging from user defined types, functions, and procedures, to the storage architecture, replication, and triggers. The script uses the view in the sys schema rather than in Information_Schema simply because I found it easier to find the information that I needed to put the SQL together."], "link": "http://blog.data-miners.com/feeds/1670521866370335345/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://www.data-miners.com/blog": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["A reader asked the following question:   Hi Michael/Gordon,   In campaign measurements, it's possible to get a larger lift at the overall level compared to all the individual decile level lifts or vice versa, because of the differences in sample size across the deciles, and across Test & Control.    According to wikipedia, it's known as Simpson's paradox (or the Yule-Simpson effect) and is explained as an apparent paradox in which the successes in different groups seem to be reversed when the groups are combined.   In such scenarios, how do you calculate the overall lift? Which methods are commonly used in the industry?   Thanks,  Datalligence http://datalligence.blogspot.com/  Simpson's Paradox is an interesting phenomenon, where results about subgroups of a population do not generalize to the overall population. I think the simplest version that I've heard is an old joke . . . \"I heard you moved from Minnesota to Iowa, raising the IQ of both states.\" How could this happen? For the joke to work, the average IQ in Minnesota must be higher than the average IQ in Iowa. And, the person who moves must have an IQ between these two values. Voila, you can get the paradox that the averages in both states go up, although they are based on exactly the same population. I didn't realize that this paradox has a name (or, if I did, then I had forgotten). Wikipedia has a very good article on Simpson's Paradox , which includes real world examples from baseball, medical studies, and an interesting discussion of a gender discrimination lawsuit at Berkeley. In the gender discrimination lawsuit, women were accepted at a much lower rate than men overall. However, department by department, women were typically accepted at a higher rate than men. The difference is that women applied to more competitive departments than men. These departments have lower rates of acceptance, lowering the overall rate for women. Simpson's Paradox arises when we are taking weighted averages of evidence from different groups. Different weightings can produce very different, even counter-intuitive results. The results become much less paradoxical when we see the actual counts rather than just the percentages. The specific question is how to relate this paradox to lift, and understanding marketing campaigns. Assume there is a marketing campaign, where one group receives a particular treatment and another group does not. The ratio of performance between these two groups is the lift of the marketing campaign. To avoid Simpson's paradox, you need to ensure that the groups are as similar as possible, except for what's being tested. If the test is for the marketing message, there is no problem, both groups can be pulled from the same population. If, instead, the test is for the marketing group itself (say high value customers), then Simpson's Paradox is not an issue, since we care about how the group performs rather than how the entire population performs. As a final comment, I could imagine finding marketing results where Simpson's Paradox has surfaced, because the original groups were not well chosen. Simpson's Paradox arises because the sizes of the test groups are not proportional to their sizes in the overall population. In this case, I would be tempted to weight the results from each group based on the expected size in the overall population to calculate the overall response and lift."], "link": "http://blog.data-miners.com/feeds/8780607061973592016/comments/default", "bloglinks": {}, "links": {"http://datalligence.blogspot.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["Classes started this week for the spring semester at Boston College where I am teaching a class on marketing analytics to MBA students at the Carroll School of Management. The class makes heavy use of Gordon's book, Data Analysis Using SQL and Excel and the data that accompanies it. Since the local database is Oracle, I have at long last added Oracle load scripts to the book's companion page . Due to laziness, my method of creating the Oracle script was to use the existing MySQL script and edit bits that didn't work in Oracle. As it happens, the MySQL scripts worked pretty much as-is to load the tab-delimited data into Oracle tables using Oracle's sqlldr utility. One case that did not work taught me something about the danger of mixing tab-delimited data with input formats in sqlldr. Even though it has nothing to do with data mining, as a public service, that will be the topic of my next post. Preview: Something that works perfectly well when your field delimiter is comma, fails mysteriously when it is tab."], "link": "http://www.data-miners.com/sql_companion.htm", "bloglinks": {}, "links": {"http://www.data-miners.com/": 2}, "blogtitle": "Data Miners Blog"}, {"content": ["Over the past three months, I have been teaching myself enough Hadoop to get comfortable with using the environment for analytic purposes. There has been a lot of commentary about Hadoop/MapReduce versus relational databases (such as the articles referenced in my previous post on the subject). I actually think this discussion is misplaced because comparing open-source software with commercial software aligns people on \"religious\" grounds. Some people will like anything that is open-source. Some people will attack anything that is open-source (especially people who work for commercial software vendors). And, the merits of real differences get lost. Both Hadoop and relational databases are powerful systems for analyzing data, and each has its own distinct set of advantages and disadvantages. Instead, I think that Hadoop should be compared to a parallel dataflow style of programming. What is a dataflow style of programming? It is a style where we watch the data flow through different operations, forking and combining along the way, to achieve the desired goal. Not only is a dataflow a good way to understand relational databases (which is why I introduce it in Chapter 1 of Data Analysis Using SQL and Excel ), but the underlying engines that run SQL queries are dataflow engines. Parallel dataflows extend dataflow processing to grid computing. To my knowledge, the first commercial tool that implements parallel dataflows was developed by Ab Initio . This company was a spin-off from a bleeding edge parallel supercomputer vendor called Thinking Machines that went bankrupt in 1994. As a matter of full disclosure: Ab Initio was actually formed from the group that I worked for at Thinking Machines. Although they are very, very, very resistant to sharing information about their technology, I am rather familiar it. I believe that the only publicly available information about them (including screen shots) is published in our book Mastering Data Mining: The Art and Science of Customer Relationship Management . I am confident that Apache has at least one dataflow project, since when I google \"dataflow apache\" I get a pointer to the Dapper project. My wish, however, is that Hadoop were the parallel dataflow project. Much of what Hadoop does goes unheralded by the typical MapReduce user. On a massively parallel system, Hadoop keeps track of the different parts of an HDFS file and, when the file is being used for processing, Hadoop does its darndest to keep the processing local to each file part being processed. This is great, since data locality is key to achieving good performance. Hadoop also keeps track of which processors and disk systems are working. When there is a failure, Hadoop tries again, insulating the user from sporadic hardware faults. Hadoop also does a pretty good job of shuffling data around, between the map and reduce operations. The shuffling method -- sorting, send, and sort again -- may not be the most efficient but it is quite general. Alas, there are several things that Hadoop does not do, at least when accessed through the MapReduce interface. Supporting these features would allow it move beyond the MapReduce paradigm, giving it the power to support more general parallel dataflow constructs. The first thing that bothers me about Hadoop is that I cannot easily take a text file and just copy it with the Map/Reduce primitives. Copying a file seems like something that should be easy. The problem is that a key gets generated during the map processing. The original data gets output with a key prepended, unless I do a lot of work to parse out the first field and use it as a key. Could the context.write() function be overloaded with a version that does not output a key? Perhaps this would only be possible in the reduce phase, since I understand the importance of the key for going from map to reduce. A performance issue with Hadoop is the shuffle phase between the map and the reduce. As I mentioned earlier, the sort-send-sort process is quite general. Alas, though, it requires a lot of work. An alternative that often works well is simply hashing. To maintain the semantics of map-reduce, I think this would be hash-send-combine or hash-send-sort. The beauty of using hashing is that the data can be sent to its destination while the map is still processing it. This allows concurrent use of the processing and network during this operation. And, speaking of performance, why does the key have to go before the data? Why can't I just point to a sequence of bytes and use that for the key? This would enable a programming style that doesn't spend so much time parsing keys and duplicating information between values and keys. Perhaps the most frustrating aspect of Hadoop is the MapReduce framework itself. The current version allows processing like (M+)(R)(M*). What this notation means is that the processing starts with one or more map jobs, goes to a reduce, and continues with zero or more map jobs. THIS IS NOT GENERAL ENOUGH! I would like to have an arbitrary number of maps and reduces connected however I like. So, one map could feed two different reduces , each having different keys. At the same time, one of the reduces could feed another reduce without having to go through an intermediate map phase. This would be a big step toward parallel dataflow parallel programming, since Map and Reduce are two very powerful primitives for this purpose. There are some other primitives that might be useful. One would be broadcast . This would take the output from one processing node during one phase and send it to all the other nodes (in the next phase). Let's just say that using broadcast , it would be much easier to send variables around for processing. No more defining weird variables using \"set\" in the main program, and then parsing them in setup() functions. No more setting up temporary storage space, shared by all the processors. No more using HDFS to store small serial files, local to only one node. Just send data through a broadcast, and it goes everywhere. (If the broadcast is running on more than one node, then the results would be concatenated together, everywhere.) And, if I had a broadcast, then my two-pass row number code ( here ) would only require one pass. I think Hadoop already supports having multiple different input files into one reduce operator. This is quite powerful, and a much superior way of handling join processing. It would also be nice to have a final sort operator. In the real world, people often do want sorted results. In conclusion, parallel dataflows are a very powerful, expressive, and efficient way of implementing complex data processing tasks. Relational databases use dataflow engines for their processing. Using non-procedural languages such as SQL, the power of dataflows are hidden from the user -- and, some relatively simple dataflow constructs can be quite difficult to express in SQL. Hadoop is a powerful system that emulates parallel dataflow programming. Any step in a dataflow can be implemented using a MapReduce pass -- but this requires reading, writing, sorting, and sending the data multiple times. With a few more features, Hadoop could efficiently implement parallel dataflows. I feel this would be a big boost to both performance and utility, and it would leverage the power already provided by the Hadoop framework."], "link": "http://blog.data-miners.com/feeds/2526895588214669915/comments/default", "bloglinks": {}, "links": {"http://freshmeat.net/": 1, "http://www.amazon.com/": 2, "http://en.wikipedia.org/": 1, "http://www.data-miners.com/blog": 2, "http://blog.data-miners.com/": 1}, "blogtitle": "Data Miners Blog"}, {"content": ["The current issue of Communications of the ACM has articles on MapReduce and relational databases. One, MapReduce a Flexible Data Processing Tool , explains the utility of MapReduce by two Google fellows -- appropriate authors, since Google invented the parallel MapReduce paradigm. The second article, MapReduce and Parallel DBMSs: Friend or Foe , is written by a team of authors, with Michael Stonebraker listed as the first author. I am uncomfortable with this article, because the article purports to show the superiority of a particular database system, Vertica, without mentioning -- anywhere -- that Michael Stonebraker is listed as the CTO and Co-Founder on Vertica's web site . For this reason, I believe that this article should be subject to much more scrutiny. Before starting, let me state that I personally have no major relationships with any of the database vendors or with companies in the Hadoop/MapReduce space. I am an advocate of using relational databases for data analysis and have written a book called Data Analysis Using SQL and Excel . And, over the past three months, I have been learning Hadoop and MapReduce, as attested to by numerous blog postings on the subject. Perhaps because I am a graduate of MIT ('85), I am upset that Michael Stonebraker uses his MIT affiliation for this article, without mentioning his Vertica affiliation. The first thing I notice about the article is the number of references to Vertica. In the main text, I count nine references to Vertica, as compared to thirteen mentions of other databases: Aster (twice) DataAllegro (once) DB2 (twice) Greenplum (twice) Netezza (once) ParAccel (once) PostgreSQL (once) SQL Server (once) Teradata (once) The paper describes a study which compares Vertica, another database, and Hadoop on various tasks. The paper never explains how these databases were chosen for this purpose. Configuration issues for the other database and Hadoop are mentioned. The configuration and installation of Vertica -- by the absence of problems -- one assumes is easy and smooth. I have not (yet) read the paper cited, which describes the work in more detail. Also, the paper never describes costs for the different system, which is a primary driver of MapReduce. The software is free and runs on cheap clusters of computers, rather than expensive servers and hardware. For a given amount of money, MapReduce may provide a much faster solution, since it can support much larger hardware environments. The paper never describes issues in the loading of data. I assume this is a significant cost for the databases. Loading the data for Hadoop is much simpler . . . since it just reads text files, which is a common format. From what I can gather, the database systems were optimized specifically for the tasks at hand, although this is not explicitly mentioned anywhere. For instance, the second tasks is a GROUP BY , and I suspect that the data is hash partitioned by the GROUP BY clause. There are a few statements that I basically disagree with. \"Lastly, the reshuffle that occurs between the Map and Reduce tasks in MR is equivalent to a GROUP BY operation in SQL.\" The issue here at first seems like a technicality. In a relational database, an input row can only into one group. MR can output multiple records in the map stage, so a single row can go into multiple \"groups\". This functionality is important for the word count example, which is the canonical MapReduce example. I find it interesting that this example is not included in the benchmark. \"Given this, parallel DBMSs provide the same computing model    as MR, with the added benefit of using a declarative language    (SQL).\" This is not true in several respects. First, MapReduce does have associated projects for supporting declarative languages. Second, in order for SQL to support the level of functionality that the authors claim, they need to use user defined functions. Is that syntax declarative? More importantly, though, is that the computing model really is not exactly the same. Well, with SQL extensions such as GROUPING SET s and window functions, the functionality does come close. But, consider the ways that you can add a row number to data (assuming that you have no row number function built-in) using MapReduce versus traditional SQL. Using MapReduce you can follow the two-phase program that I described in an earlier posting . With traditional SQL, you have to do a non-equi-self join. MapReduce has a much richer set of built-in functions and capabilities, simply because it uses java, an established programming language with many libraries. On the other hand, MapReduce does not have a concept of \"null\" built-in (although users can define their own data types and semantics). And, MapReduce handles non-equijoins poorly, because the key is used to direct both tables to the same node. In effect, you have to limit the MapReduce job to one node. SQL can still parallelize such queries. \"[MapReduce] still requires user code to parse the value portion of the record if it contains multiple attributes.\" Well, parse is the wrong term, since a Writable class supports binary representations of data types. I describe how to create such types here . I don't actually feel qualified to comment on many of the operational aspects of optimizing Hadoop code. I do note that the authors do not explain the main benefit of Vertica, which is the support of column partitioning. Each column is stored separate, which makes it possible to apply very strong compression algorithms to the data. In many cases, the Vertica data will fit in memory. This is a huge performance boost (and one that another vendor, Paracel takes advantage of). In the end, the benchmark may be comparing the in-memory performance of a database to general performance for MapReduce. The benchmark may not be including the ETL time for loading the data, partitioning data, and building indexes. The benchmark may not have allocated optimal numbers of map and reduce jobs for the purpose. And, it is possible that the benchmark is unbiased and relational databases really are better. A paper that leaves out the affiliations between its authors and the vendors used for a benchmark is only going to invite suspicion."], "link": "http://blog.data-miners.com/feeds/1189003498857208009/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://cacm.acm.org/": 2, "http://www.vertica.com/": 1, "http://www.data-miners.com/blog": 2}, "blogtitle": "Data Miners Blog"}]