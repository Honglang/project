[{"blogurl": "http://numberblog.wordpress.com\n", "blogroll": [], "title": "Big Numbers"}, {"content": ["I\u2019m going to have to focus completely on school for a while so this blog is going on hiatus for a few months. I\u2019ll start up again when I\u2019ve passed the hurdles coming my way."], "link": "http://numberblog.wordpress.com/2011/01/11/going-to-be-out-of-commission/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "Big Numbers"}, {"content": ["I\u2019m finished with finals at last. Who knows how they\u2019ll come out, but it\u2019s a relief to be done at the moment. I\u2019m finally getting my room clean, my laundry done, my presents bought, and my social life reanimated. And I\u2019m looking at an introductory ( very introductory) AI textbook here. \n Also, for those looking for a non-corny holiday movie, I recommend seeing Holiday while it\u2019s still on Hulu. Cary Grant, Katherine Hepburn, and a surprisingly modern story about following your own path."], "link": "http://numberblog.wordpress.com/2010/12/17/all-done/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://artint.info/": 1}, "blogtitle": "Big Numbers"}, {"content": ["Stop what you\u2019re doing, right now, and check this out. \n NASA scientist Felisa Wolfe Simon has discovered a microbe with a different molecular structure than all life on Earth. While our molecules are made of carbon, hydrogen, oxygen, nitrogen, sulfur, and phosphorus, these single-celled organisms use arsenic in place of phosphorus. Its DNA is different from the DNA of all other life. This is the most alien creature we know. \n The official announcement and live video is here."], "link": "http://numberblog.wordpress.com/2010/12/02/nasa-finds-new-life/", "bloglinks": {}, "links": {"http://voices.washingtonpost.com/blog": 1, "http://feeds.wordpress.com/": 7, "http://gizmodo.com/": 1}, "blogtitle": "Big Numbers"}, {"content": ["I went to the applied math talk by Mauro Maggioni today; here are notes from the presentation. \n Multiscale SVD \n We often have to deal with high-dimensional data clouds that may lie on a low-dimensional manifold. Examples: databases of faces, collections of text documents, configurations of molecules. But, we don\u2019t know a priori what the dimension of the manifold is. So we ned some mechanisms for estimating that, in the situation where we have noise in the data. \n Some terminology: the data points are , the real data plus some white Gaussian noise of variance . The intrinsic dimension of the manifold is while the dimension of the ambient space is . \n Previous work at estimating dimensions generally falls into two categories. One is volume-based (the number of points on the manifold confined within a ball should grow as where is the radius, if you\u2019ve chosen the right dimension k. The other possibility is PCA, done locally and patched together somehow to account for the fact that we have a curved manifold rather than a linear hyperplane. The problem with this is that you don\u2019t know what \u201clocally\u201d means \u2014 how many patches do we need? There\u2019s also the so-called \u201ccurvature dilemma\u201d with PCA: the more highly curved the manifold, the more likely it is to confuse the top principal value (representing the tangent direction) with the second-highest principal value (representing the direction of curvature.) \n Maggioni\u2019s alternative is multiscale SVD. We fix a point. The noise around the manifold looks like a hollow tube \u2014 most of the noise is concentrated at a radius of from the surface of the manifold itself. So looking at a small ball (smaller than the radius of the tube) picks up less noise, looking at a large ball picks up all the noise, and looking at a still larger ball begins to pick up the curvature of the manifold. We do SVD within all these balls, and look at the singular values over a range of scales. There\u2019s a \u201csweet spot,\u201d a particular interval of scales where the signal singular values have separated from the noise singular values, but the curvature singular values haven\u2019t caught up yet; this accurately represents the curvature of the manifold. (I blogged about this earlier here and the paper is here. ) \n The assumption about the data is that the tangent covariance grows differently ( ) with respect to the radius than the normal covariance ( ). This can be computed to be true explicitly for manifolds of co-dimension one, for instance. \n The result is that if is assumed to have small enough curvature and is assumed to have small enough variance, then with high probability we can determine the intrinsic dimension with only points. Compared to a large number of other dimension-estimating methods, this outperforms all of them. Isomap, in particular, really falls apart in the presence of noise, and can\u2019t tell a 6-dimensional sphere from a 6-dimensional cube. \n One interesting fact is that many real-life data sets have dimensionality that varies. A database of text documents, for instance, is low-dimensional in some regions, and very, very high-dimensional in others. (We measure dimensionality locally, so there\u2019s no reason in principle that it shouldn\u2019t vary.) \n Measures supported on K planes \nSometimes high-dimensional data is supported on a set of planes , possibly of different dimensions . We don\u2019t know how many planes there are, which planes they are, or what their dimensions are. This is the situation in face recognition; it\u2019s also relevant to image and signal processing more generally (dictionary learning, sparse approximation, etc.) \n Maggioni and collaborators developed an algorithm for doing this. \n 1) Draw random points and nearest neighbors. Do multiscale SVD at these points. Produce an estimated plane and its estimated dimension . Also estimate the noise level . \n 2. Construct an affinity matrix, with the (i, j)th coordinate \n \n This is almost a matrix composed of zeros and ones \u2014 either a point is on a plane (in which we get one) or a point is not on that plane (in which case we get 0 because the distance is large.) In practice, with noise and curvature, it\u2019s not quite binary, but it\u2019s close. \n 3. Denoise and cluster this matrix, and find updated estimates for the planes from this. \n Tested on a face database, this worked very well. It misclassified only 3 faces out of 640. What\u2019s remarkable here is that the Yale Face Database used had 64 different photos of each person, taken at different angles, different lighting conditions, etc. This has always been a hard problem for computer vision: how do you recognize that two pictures are the same person viewed frontal and profile, while two other pictures are of two different people? Apparently, this is a solution to such heterogeneity problems. \n Geometric wavelets \nThe problem of dictionary learning is as follows. Given points in , and a data matrix , construct a dictionary of m vectors such that \n \nwhere is a sparse vector. \n There\u2019s a tradeoff here between the size of and the sparsity of : obviously, you could just let be the whole of and then would just be the identity, but that would be silly \u2014 it wouldn\u2019t compress the data at all. \n One way of doing this is something called geometric wavelets. (Paper here. Shorter, more descriptive paper with cool picture here. ) \nGiven a manifold, we partition it into a dyadic tree. We do principal components analysis on the coarsest scale and get a plane; then we divide that into two smaller subsets, do PCA on each of them, and get two new planes. For each plane, at the center point of that cube in the manifold grid, we have a tangent vector , which approximates the tangent to the manifold. The wavelets keep track of the differences . If we have some smoothness in the manifold, these corrections decay quickly, so finitely many wavelets are a good approximator for any point. \n There\u2019s a fast algorithm for computing this, very like the wavelet transform, except for a point cloud rather than a function. This is useful for compressing databases, analyzing online data, and so forth. \n The idea is very like Peter Jones\u2019 construction of beta numbers (summary here ) except that it may be more efficient in terms of storage. More on geometric wavelets from Dan Lemire."], "link": "http://numberblog.wordpress.com/2010/11/30/multiscale-svd-k-planes-and-geometric-wavelets/", "bloglinks": {}, "links": {"http://numberblog.wordpress.com/": 1, "http://feeds.wordpress.com/": 7, "http://www.duke.edu/": 1, "http://ieeexplore.ieee.org/": 2, "http://lemire.me/blog": 1, "http://www.ucla.edu/": 1}, "blogtitle": "Big Numbers"}, {"content": ["I\u2019d just like to mention that I\u2019ve come across Tomasz Malisiewicz\u2019s blog on machine learning and computer vision, and I\u2019m hooked. You should be too. \n There\u2019s the usual panoply of links to interesting papers, but then there\u2019s also Tomasz\u2019s radical idea for reimagining computer vision using a memex instead of a set of categories. He thinks that the \u201cvision problem\u201d will be solved by something much closer to actual AI than is generally considered necessary today. His ideas are informed by Wittgenstein and Vannevar Bush as well as contemporary research. It sounds interesting, to say the least. Then there\u2019s also Tomasz\u2019s stirring (if somewhat intimidating) advice to students and researchers to be Renaissance men and look beyond the A+ and the well-received publication. All in all, very worth reading. \n (Sensible Sarah says: \u201cHey, wait a minute! I thought I was a math student \u2014 what\u2019s up with all this vision stuff? And I have a qual to pass in a month!\u201d Sensible Sarah throws up her hands in dismay.)"], "link": "http://numberblog.wordpress.com/2010/11/17/vision-without-categories/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://quantombone.blogspot.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Big Numbers"}, {"content": ["I was somewhat disturbed to read this article in the Chronicle of Higher Education. It\u2019s written by a man who writes papers for a custom-essay company; students pay him to write their papers for them. Obviously it\u2019s sad to see how common academic dishonesty is. But what really shocked me was something else. \n A lot of \u201cEd Dante\u201d\u2018s customers (he goes by a pseudonym) are in graduate or professional school. And apparently they\u2019re really terrible writers. When they write him asking him for help, it\u2019s something like \u201cYou did me business ethics propsal for me I need propsal got approved pls can you will write me paper?\u201d I have to wonder \u2014 why are these people in graduate school in the first place? \n I don\u2019t mean that pejoratively. I don\u2019t think you\u2019re inferior as a person if you\u2019re bad at writing or not gifted at academics. But look. I\u2019m a grad student myself. I\u2019m here because I want to be a mathematician. While the future is uncertain for all of us, I wouldn\u2019t be here if I thought that was a completely unrealistic goal. If I were so overwhelmed by the work here that I was tempted to pay someone to do it for me , and if I\u2019d already tried tutors, adjusting my sleep and study schedule, etc. then I\u2019d start reevaluating my plans. It would be insane not to! The way I see it, going to graduate school is a decision that has to be justified \u2014 you should do it only if you think it\u2019ll help you in the long run. \n If you can\u2019t write your own briefs in law school, you really think you\u2019re going to be a successful lawyer? If you can\u2019t write your own PhD thesis, do you really think you have any chance to make it in academia? \n This article really makes me worry that lots of people are going for extra degrees just for the sake of doing it, or to escape the job market, and not because they\u2019re really pursuing a passion or talent that has a realistic chance of paying off. That\u2019s disturbing \u2014 it seems like a bad sign for the economy as a whole."], "link": "http://numberblog.wordpress.com/2010/11/16/the-custom-essay-man/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://chronicle.com/": 1}, "blogtitle": "Big Numbers"}, {"content": ["Currently reading this paper by Alex Bronstein et al. \n The idea here is to make searchable databases for shapes (2d and 3d objects) in the same way a text engine responds to search queries. \n There are two main parts to this task: feature detection, and feature description. Different methods can be employed to define what a feature is; a dense descriptor just selects all the points in the image as descriptors, but there are other possibilities. SIFT looks for local maxima of the discrete image Laplacian that persist through different scales. MSER finds level sets that show the smallest variation of area when traversing the level-set graph. \n Feature description aims to produce a \u201cbag of words\u201d from the features, by performing vector quantization on the feature space. Two images can then be compared by comparing their bags of features with plain old Euclidean distance. \n There are extra challenges if we want to do this with 3d objects rather than images: for one, conformations of 3d objects (like a body in different poses) are much more complicated than the (usually affine) transformations seen on a given image. Also, while images are typically represented as matrices of pixels, 3d objects can be represented as meshes, point clouds, level sets, etc, so computations have to be portable between formats. Also, 3d objects don\u2019t have a universal system of coordinates. \n The feature description process used by the authors is based on heat kernel signatures. \nRecall that the heat kernel is the fundamental solution of the heat equation \n \nThe heat kernel can also be seen as the transition density function of a Brownian motion. \nFor any point on the surface, we give its heat kernel signature as \n \nThis is invariant under isometric deformations of the space, since it depends only on the Riemannian metric. It captures local information in a small neighborhood of x for small t, and global information for large t. It can be proven that any continuous map preserving the heat kernel signature must be an isometry. And computing it depends on computing the eigenvalues of the Laplacian, which can be done with various formats of representing 3d shapes. \n \n So, since the eigenvalues of the Laplacian decay exponentially, we can truncate this sum for computation of the heat kernel. \nIn the discrete case, instead of the Laplacian we would use a discretization of the form \n \n After vector quantization to get a bag of features, the authors generalize this notion to a spatiall sensitive bag of features: \n \ngiving a matrix that represents the distribution of nearby words. \n From there, we can compare these matrices by embedding them into a Hamming Space. (Hamming distances happen to be very quick to compute on modern CPU architectures. \n ShapeGoogle was tested against various possible transformations of shapes (isometries, topology changes, holes, noise, scale changes) and performed well, outperforming other methods like Shape DNA, part-based bag of words, and clock matching bag of features. \n It\u2019s an interesting project, and I suspect that the ability to search shape databases will prove useful. Here\u2019s a list of current content-based image retrieval search engines: both Google and Bing use these methods (as opposed to only looking at metadata.)"], "link": "http://numberblog.wordpress.com/2010/11/15/shape-google/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://en.wikipedia.org/": 4, "http://visl.ac.il/": 1}, "blogtitle": "Big Numbers"}, {"content": ["Last week\u2019s applied math seminar was Arthur Szlam, talking about alternating methods for SVD. I\u2019m putting up some notes here; several results are presented without proof. \n Given an m x n matrix A, we want to find the best rank k approximation to A. \nThere is an explicit solution to this problem, known as the singular value decomposition. We write \n \nwhere is diagonal, are orthogonal. The columns of are the eigenvectors of and the columns of are the eigenvectors of . This costs operations, and it\u2019s a completely explicit solution. To get a rank-k approximation, we take \n \nwhere the \u201chat\u201d means taking the first k columns of each matrix. \nThis is the best rank-k approximation, both in the sense of Frobenius norm and operator norm. \n However, if , time is wasteful. There are recent attempts to do this faster, in time. \n We fix k, and a small oversampling parameter p, and let . Draw an random Gaussian matrix , and set . \nCalculate the SVD, , and set to be the first k columns of , etc. \n Why does this work? \n Well, pretend A is really of rank 5, for instance. Hit A with a random matrix. It is unlikely that any vector will be in the kernel of A. Whatever's left is in the range. So we're very likely to have the full range of A; we probably preserve the SVD. However, in real life we don't have a known rank k matrix. \n What we have is a guarantee of quality, in terms of the decay of the singular values: \n \n Here the expected value is taken over the random Gaussian draw, the norm is the operator norm, and is the k + 1th singular value; if A is really rank k, this is zero, and the expected error is therefore zero. If is small, then the bound is good. We're in danger if m is large and is not super small. \n This actually happens, unfortunately. For example, in term document matrices, where a collection of text documents form the rows, and words form the columns, with a word count in each matrix entry indicating how often a word was used in each document. (A little imagination should indicate the importance of these kinds of matrices for semantic analysis.) These matrices are sparse and large, but their singular values decay slowly. So what can we do here? \n We hit by some more powers of A, to drive down the smaller eigenvalues. In practice, 10 or 15 times. \n \nNow we have a new estimate, depending on the power q. \n \n This works, but it can be memory intensive to save all the intermediate powers. This is where the \"alternating\" method comes in. \n Here we look at a related problem: matrix completion. You're given a subset of the entries of a matrix A, and you know A is low rank. How do you fill in the rest of the entries? The simplest approach is alternating least squares. \n Write where P is m-by-k and Q is n-by-k. \nAlternate between updates of P and Q: \n \n \nThese are least-squares problems, and can be solved explicitly using \n Step 1: form Gaussian, . \nStep 2: form matrix st \n \n(a QR-decomposition, with Q orthogonal and R upper triangular.) \nStep 3: iterate for \n \n \n Coming back to our own problem, we can use the same formula to calculate . \n \n This turns out to work well in practice; the example set was a database of labeled faces, which have the first 20 eigenvalues decaying fast, then the next decaying very slowly. This kind of alternating SVD works well for faces."], "link": "http://numberblog.wordpress.com/2010/11/09/alternating-methods-for-svd/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "Big Numbers"}, {"content": ["Guy Fawkes Day. It\u2019s a good day for radical ideals. No, not that kind! This kind. \n I\u2019m currently reading Charles Siegel\u2019s series of posts on Algebraic Geometry from the beginning and I highly recommend. It\u2019s some of the clearest, loveliest writing ever, and it\u2019s got me really interested in the topic."], "link": "http://numberblog.wordpress.com/2010/11/05/remember-remember-the-fifth-of-november/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://en.wikipedia.org/": 1, "http://rigtriv.wordpress.com/": 1}, "blogtitle": "Big Numbers"}, {"content": ["I am now a TA holding office hours for some calculus classes, and I actually enjoy it a lot. I\u2019m generally impressed with the students. General advice that ought to hold if you\u2019re taking an intro calc class: \n 1. Don\u2019t freak out because a problem is taking a long time and seems unfairly hard. You\u2019ll say \u201cThere must be some mistake!\u201d No, there isn\u2019t. This isn\u2019t high school; you\u2019re being taught by professors, who don\u2019t always know how to gauge the right difficulty level. They do tend to underestimate how long it takes to finish your homework. Also, calculus is actually harder than high school math. If you have to go through a lot of computations and false steps \u2014 don\u2019t worry! That\u2019s what math is actually like! \n 2. I do not have magical TA superpowers of Mathematica . Most of the time, if you ask me for help, I\u2019m going to look at the example on your worksheet and look in the help documentation. You could do that too! The xkcd Tech Support Cheat Sheet is relevant here. \n 3. L\u2019Hopital\u2019s rule is your friend. Seriously. So is big-O notation. These will save your bacon. \n 4. 90% of mistakes in multivariable calc result from not drawing pictures. Draw a picture. You are never too cool to draw a picture."], "link": "http://numberblog.wordpress.com/2010/11/03/advice-for-calculus-students/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://xkcd.com/": 1}, "blogtitle": "Big Numbers"}]