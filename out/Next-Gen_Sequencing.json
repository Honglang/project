[{"blogurl": "http://nextgenseq.blogspot.com\n", "blogroll": [], "title": "Next-Gen Sequencing"}, {"content": ["Julia Meyer is a Ph.D. student in the lab of Bill Carroll in the NYU Cancer Institute, and I have the good luck to sit on her thesis advisory committee. Julia has been using RNA-seq to look for mutations that are specific to relapse of childhood acute lymphoblastic leukemia (ALL). ALL can often be cured in children, but years after remission about 20% of patients relapse, and prognosis for relapsed ALL is very poor.  Julia studied pairs of RNA samples from 10 patients taken at original diagnosis and again after relapse (Illumina RNA-seq). The data analysis was very difficult since she initially found millions of variants, and even after extensive stringent filtering and matching between diagnosis and relapse, there were many false positives. Eventually she narrowed it down to just 20 non-synonymous mutations that were specific to the relapse samples. Two patients harbored (different) relapse-specific mutations in the same gene, NT5C2, which codes for the Cytosolic 5\u2019-nucleotidase II. Full exon sequencing of NT5C2 was completed in 61 additional relapse specimens (using 454 amplicon protocol), identifying 5 additional mutations which were also confirmed as relapse specific.  Conclusions: Mutations in NT5C2 are associated with the outgrowth of drug resistant cells in childhood ALL.  This work was published as an ASCO abstract at the 2012 ASCO annual meeting.  http://www.asco.org/ASCOv2/Meetings/Abstracts?&vmview=abst_detail_view&confID=114&abstractID=94400  As a member of the thesis committee, I got a view of some really interesting followup studies. The NT5C2 gene product is a purine nucleotidase. Structural modeling of the relapse-associated mutations in the encoded protein suggests alteration of enzyme subunit association/dissociation. Julia has found that cells transfected with the mutant version of the NT5C2 are RESISTANT to 6-mercaptopurine, which is one of the drugs used for long term maintenance chemotherapy of ALL. She also found very low levels of the mutant allele in some diagnosis samples (early stage disease). The obvious implication is that under long term drug treatment, a clone of tumor cells with activating mutations in NT5C2 increases and create a drug resistant relapse.   Wow. This is the first molecular model for the cause of relapse of ALL. It could lead directly to diagnostics and therapy."], "link": "http://nextgenseq.blogspot.com/feeds/1006266072155199833/comments/default", "bloglinks": {}, "links": {"http://www.asco.org/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["My Next-Generation DNA Sequencing Informatics book has gone live on the Cold Spring Harbor Laboratory Press website (pre-orders). http://www.cshlpress.com/link/nextgendna.htm It has the following chapters (see below). I am going to leak a few pages as a teaser, so I need to know which chapter is most interesting to some random selection of people. Votes will be counted in the comment section for this post.   1. Introduction to DNA Sequencing Stuart Brown 2. History of Sequencing Informatics Stuart Brown 3. Visualization of Next-Generation Sequencing Data Phillip Ross Smith, Kranti Konganti, and Stuart Brown 4. DNA Sequence Alignment Efstratios Efstathiadis 5. Genome Assembly Using Generalized de Bruijn Digraphs D. Frank Hsu 6. De Novo Assembly of Bacterial Genomes from Short Sequence Reads Silvia Argim\u00f3n and Stuart Brown 7. Genome Annotation Steven Shen 8. Using NGS to Detect Sequence Variants Jinhua Wang, Zuojian Tang, and Stuart Brown 9. ChIP-seq Zuojian Tang, Christina Schweikert, D. Frank Hsu, and Stuart Brown 10. RNA Sequencing with NGS Stuart Brown, Jeremy Goecks, and James Taylor 11. Metagenomics Alexander Alekseyenko and Stuart Brown 12 High-Performance Computing in DNA Sequencing Informatics Efstratios Efstathiadis"], "link": "http://nextgenseq.blogspot.com/feeds/5916939451481358020/comments/default", "bloglinks": {}, "links": {"http://www.cshlpress.com/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I have started writing a new blog \"channel\" about Next Generation Sequencing for a website called BiteSizeBio . The first article went live today: NGS- A Revolution in Technology These articles will be more basic and broad then the internal lab stuff (and random personal observations) that I post here. I plan to write about one per month, and I will probably post links here when they go live."], "link": "http://nextgenseq.blogspot.com/feeds/7654022722472233590/comments/default", "bloglinks": {}, "links": {"http://bitesizebio.com/": 2}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I gave a short presentation at the NYU CTSI Translational Research in Progress seminar this week about our sequencing work on strains of Streptococcus mutans associated with severe tooth decay in children. We sequenced and did de novo assembly on each of 20 strains, then did an in silico subtraction to find unique genomic elements associated with health or disease. the details are in this PDF file: https://www.box.com/s/fbea9b1d5cdb96f07348"], "link": "http://nextgenseq.blogspot.com/feeds/5498346662386592845/comments/default", "bloglinks": {}, "links": {"https://www.box.com/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I led a workshop on 7/31/2012 about NGS for Drug Development at this Hanson Wade conference in Boston: NGS Bioinformatics for Drug Developers I made a huge PPT slide deck to fill 2 hours, but in the workshop we ended up in a multi-way discussion for most of the time, so I never got to show half the slides. I am posting them on BOX.com so I don't feel like the effort was wasted. NGS drug dev PPT slides"], "link": "http://nextgenseq.blogspot.com/feeds/3567227182166065398/comments/default", "bloglinks": {}, "links": {"http://nextgeneration-sequencing.com/": 1, "https://www.box.com/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["One of my Bioinformatics students, Laura Cox, is working on a Human Microbiome Project study with Martin Blaser. Yesterday, she presented a lab report to a standing room only crowd about her sequencing work on bacterial populations using the Illumina MiSeq machine. Up till now, HMP work was about the only sequencing that we consistently ran on the 454 machine. Laura showed that with the MiSeq paired-end 150 bp sequencing protocol, it was possible to sequence 16S amplicons (in the V4 region) from both ends and stitch them together using the ea-utils FASTQ-join [ Erik Aronesty (2011). ea-utils : \"Command-line tools for processing biological sequencing data\";  http://code.google.com/p/ea-utils ] to get about 260 bp reads on each amplicon. Laura used a custom multiplex scheme to get 192 different samples into one MiSeq run, which after demultiplexing, gave about 2,000 P-E reads per sample.   She also demonstrated that the resulting sequence data could be processed with QIIME [ http://www.qiime.org ] to get reasonable taxonomy information, build phylogenetic trees, and apply all the cute tools to calculate diversity and compare groups of samples by PCA and UNIFRAC [ http://bmf2.colorado.edu/unifrac ].   The economics of MiSeq are persuasive. It is giving amplicon data at about 40X less cost than 454. As our HMP protocols shift over to MiSeq, this will be the last year that we keep the 454 machine in the Genomics Core Lab."], "link": "http://nextgenseq.blogspot.com/feeds/7850934810353099878/comments/default", "bloglinks": {}, "links": {"http://www.qiime.org/": 1, "http://bmf2.colorado.edu/": 1, "http://code.google.com/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I have noticed that a fair number of people who actually work with Next-Gen Sequence data read this blog, so perhaps we can use it for a collaborative project. I want to write a paper about uneven coverage in exome sequencing leading to incorrect SNP calls. Our data is from tumor-normal pairs, and we see a lot of false negatives - failure to detect a SNP in a sample due to low coverage at that spot. Exome capture methods seem to have more than their fair share of low coverage spots (even with an average coverage over 100x), and these low coverage spots do differ somewhat from sample to sample. I'd like some other people to share data with us and/or do some similar analysis on other data sets so that we can make a stronger paper."], "link": "http://nextgenseq.blogspot.com/feeds/5724126189012965921/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["Illumina has announced an updated and improved cancer genome sequencing service for $18,000. This will provide genome sequencing of the tumor at 80x and of normal tissue from the same patient at 40x. This is similar to the coverage offered by Complete Genomics for a similar service (at $12K). Illumina also offers a novel sample prep method (by partnership with the Broad Inst.) for very small samples and FFPE. Perhaps the most interesting thing about the Illumina service is the bioinformatics support, which will include a new variant detection algorithm that looks at both the tumor and normal together, in order to reduce the false positives. The standard approach, available from other software, does variant detection separately for each sample, then tries to subtract the variants found in the normal from those found in the tumor. This method works very poorly, since many variants cannot be called accurately in tumor samples which contain various amounts of normal tissue mixed in as well as tumor genomic heterogeneity. Many other existing variants are simply not called in the normal sample (ie. false negatives) due to poor coverage, poor quality, nearby insertion/deletions or any other feature that fails stringent variant detection software. We have been working on this same approach to the problem, but Illumina brings a much bigger team with a access to a LOT more data. Illumina will also provide custom annotation of discovered variants provided by a team of human bioinformaticians (rather than just running the data through a static annotation software pipeline). I think this is a more realistic milestone for clinical sequencing than the mythical $1000 genome. Cancer patients are one of the few (common) clinical scenarios where whole genome sequencing could really pay off with actionable discoveries - allowing genetic information to be used to chose targeted drugs and other interventions. A simple genome sequence (at whatever coverage) of a healthy person does not provide much medically actionable data today. Furthermore, the informatics that can currently be applied to a single, cheaply acquired, genome sequence range from relatively inexpensive but simplistic one-size-fits-all software pipelines to the equally mythical $100,000 interpretation (presumably provided by a dedicated project team of expert informaticians and medical geneticists)."], "link": "http://nextgenseq.blogspot.com/feeds/817746312005309372/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["One of my very good informatics people is leaving at the end of the year, so I have a job vacancy to fill in our Sequencing Informatics unit (funded as an Institutional core to support our Nex-Gen Sequencing Lab and our investigators, not from any one grant). I want someone with either a Masters and some experience with Next-Gen sequencing informatics, or a PhD. (bioinformatics or computer science, or something similar) who is looking for a more stable, service oriented position, rather than the usual highly competitive postdoc. There will be opportunity for both collaborative and independent work on various projects, and publications are expected. UNIX/Perl/Java skills are necessary. The job previously involved informatics support for 454 sequencing, but that turned out to be less than 30% of the actual work. Looking forward, our Microbiome work will be done mostly on Illumina, bacterial genomes on Illumina... you get the idea. Send cv's to stuart.brown@gmail.com"], "link": "http://nextgenseq.blogspot.com/feeds/244481811097220083/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["The storage of NGS data has reached and passed the critical point. The owner of a HiSeq machine can expect to generate hundreds of Terabytes per year. Even more critical than the current large data volumes is the trend over the next few years - sequencing will grow faster and cheaper much more rapidly than hard drives. Current trends show the doubling of drive capacity (at a constant cost) every 18 months, but the doubling of sequencing output (also at constant cost) every 5 months. So you can expect to pay 3X more for NGS data storage every year. The Pistoia Alliance, a trade group that includes most of the big Pharma companies and a bunch of software/informatics companies (but no sequencing machine vendors), has proposed a \"Sequence Squeeze\" challenge with a prize of $15,000 for the best novel open-source NGS compression algorithm. Nice.  www.sequencesqueeze.org   I think the basic outline of a solution has already been published in this paper by Hsi-Yang Fritz , Leinonen , Cochrane , and Birney :     Efficient storage of high throughput DNA sequencing data using reference-based compression . http://www.ncbi.nlm.nih.gov/pubmed/21245279 Their basic idea is to reduce the amount of data stored that exactly reproduces a Reference Genome. Why store the same invariant data over and over again? Just save the interesting differences, and the quality scores near these differences. First align all reads to a Reference Genome, then compress high quality reads (all bases Q>20) that perfectly match the Reference down to just a start position and a length. For Illumina reads, all the read lengths are the same, so that value just needs to be saved once for the entire data file. The aligned reads are sorted and indexed, so the position of each read can be marked just as an increment from the previous read. Groups of identical reads can be replaced by a count. For reads that do not perfectly match the Ref. Genome, there may still be stretches of high quality matching bases. These can be represented by a set of start-stop coordinates with respect to the read start position, then an efficient formula to store differences for non-matching bases and the qualities of surrounding bases. Many such variant summaries already exist. Another interesting idea is to use many different Reference Genomes (for humans), and match each sample to the most similar Reference. This might reduce the number of common variants observed by anything from 2x to 10X."], "link": "http://nextgenseq.blogspot.com/feeds/3281395983780415461/comments/default", "bloglinks": {}, "links": {"http://www.sequencesqueeze.org/": 1, "http://www.nih.gov/": 5}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["A number of new companies have recently been created, or have refocused their primary business effort on opportunities in clinical sequencing and personalized medicine. This area has received a lot of speculative attention in the past few years, but the recent development of \u201cExome\u201d sequencing technology has suddenly made it a practical area for commercial investment.  There are several challenges in that must be overcome to make DNA sequencing a clinically relevant tool:   1)  the cost of the assay, which includes sample collection from the patient, sample preparation, and operation of the DNA sequencing machine    2)  bioinformatics to identify sequence variants in the patient\u2019s DNA     3)  filtering and interpretation of sequence variants for clinical relevance \u2013 i.e. identify variants that provide information that directly impacts disease treatment decisions.   Exome sequencing addresses all three of these challenges. The exome is defined as the protein coding exons of genes, which make up approximately 50 MB of the human genome \u2013 about 1.5% of the entire genome. New sample preparation reagents make it possible to capture this portion of the genome in a single step in a single tube for less than $100. The current Illumina HiSeq sequencing machine produces about 20 Gbp per lane for about $1500, which is equivalent to 400X coverage of the exome. Since current bioinformatics methods require only 50-100X coverage for optimal discovery of sequence variants, this allows 4 to 8 samples to be multiplexed into a single lane. Therefore exome sequencing can be used to scan all of a patient\u2019s genes for under $500 in sequencing and sample preparation costs. The $1000 genome is available right now.  Since the exome is a much smaller amount of sequence than the entire genome, and it is focused on the best characterized regions, the task of identifying variants is simplified. The problem of false positives is reduced both by the smaller extent of sequence and by the deeper coverage (\u226550X). The challenge of interpretation is also greatly reduced since exons are by definition protein coding. All exon sequence variants can be characterized as changing amino acids or not (or creating frameshifts &/or stop codons), and the likely impact on a protein of an amino acid change can be assessed by a number of existing algorithms. Most genes can be further characterized by existing knowledge about protein function such as metabolic and regulatory pathways, as well as databases of clinical genetic and pharmacogenetic information.  Since the technical ability to perform exome sequencing and basic discovery of sequence variants is available to anyone with a HiSeq machine (and a few skilled bioinformaticians), companies are currently trying to distinguish themselves with the clinical interpretation that they can offer. Some companies are skipping the sequencing entirely and focusing solely on the interpretation of clinical sequence data.  \u2022 Ambry Genetics  Ambry Genetics is the first laboratory to provide CLIA-approved exome services for applications in clinical diagnostics along with clinical interpretation and classification of variant data. The expert bioinformatics team makes Clinical Diagnostic Exome\u2122 possible with a robust data analysis pipeline for Mendelian disease discovery.  \u2022 Knome Offers Whole-Genome Sequencing, Interpretation for $5K Founder: George Church  KnomeSelect, a targeted sequencing service that covers the exome, costs $24,500 for individuals. A comparative analysis of genomes includes a short list of suspect variants, genes and networks. Custom desktop software is provided for further analysis, including KnomeFinder for candidate variant discovery and KnomePathways for finding gene-gene interactions and gene networks. The company recently opened up its services to scientists interested in sequencing exomes or genomes of small numbers of humans as part of research studies.  \u2022 Personalis  Founders: Stanford founders are Russ Altman, chair of the bioengineering department; Euan Ashley, director of the Stanford Center for Inherited Cardiovascular Disease; Atul Butte, chief of the division of systems medicine at the department of pediatrics; and Michael Snyder, chair of the genetics department and director of the Stanford Center for Genomics and Personalized Medicine. John West, the former CEO of Solexa, is the new firm's CEO.  \u201c Its core capability will be the medical interpretation of human genomes. Personalis expects to work closely with a variety of sequencing technology and service providers \u2014 including Illumina, Complete Genomics, and others.\u201d   \u2022 Omica is a new startup company. It has developed and published the VAAST system for annotating sequence variants. VAAST is a probabilistic search tool that identifies disease-causing variants in genome sequence data. It combines elements from existing amino acid substitution and aggregative approaches that increase accuracy and make it easy to use. The tool can score both coding and non-coding variants, and evaluate rare and common variants. The platform, to be used for clinical annotations of both whole genomes and more targeted data such as exomes or gene panels, is currently in beta testing with several undisclosed collaborators. Besides VAAST, which generates disease candidate lists, the Omica service will also include annotation tools that will provide additional information about the role of the genes. Users can submit their genome sequence, and it puts all the clinical annotations on top of it. It also has an interface that can relate variants to diseases.  \u2022 GenomeQuest is a provider of cloud-based computing solutions for analysis of Next Generation sequencing data. The GQ-DxSM product analyzes and reports comprehensive genomic information about variations and changes in genes and proteins to improve disease treatment. The workflow can be used for Whole-Genome, Whole-Exome, and selected Gene Panels including: - Automated transfer of raw data from sequencing machines - Alignment of the reads against reference genomes - Variant detection and annotation - Mapping and documentation of variants against known inherited and somatic mutations - Integration with other clinical data systems such as Electronic Health Records and therapy protocols to create a comprehensive patient diagnostic record Designed for academic research laboratories, diagnostics labs, IVD manufacturers, and pharmaceutical companion diagnostic groups, GQ-Dx is already being used in clinical research. In collaboration with GenomeQuest, pathologists at Beth Israel Deaconess Medical Center, a teaching hospital of Harvard Medical School, are developing \u201cclinical grade\u201d annotation methods and databases for cancer diagnoses. GenomeQuest has also created a GeneTests-based diagnostic panel that generates a comprehensive report on disease susceptibility, diagnosis, and treatment on more than 2,000 disorders from a single, whole-genome sequence of a patient.  \u2022 Foundation Medicine has narrowed the focus even further. They provide diagnostic exome sequencing of 300 cancer related genes on FFPE tumor samples submitted by clinical pathologists. They sequence these 300 genes to very deep coverage (500X) to allow detection of rare somatic variants in heterogeneous tumor tissue. The selected gene set is intended to include only genes with directly disease related functions that impact cancer treatment decisions. The test is intended to replace many different single-gene diagnostic tests currently on the market.   \u2022 23andMe  has started a pilot program that offers full exome sequencing for $999. While the company\u2019s regular personal genome service uses Illumina genotyping arrays with around 1 million SNPs (single nucleotide polymorphisms), the exome sequencing actually sequences around 50 million DNA bases with 80x coverage.  Customers will get the raw data, without any additional reports, so it will only be useful to people who actually know how to handle this raw genetic data. 23andMe plans to eventually add a limited set of tools and content that utilize exome sequence data.  23andMe is not the first company to offer whole-genome sequencing to consumers, but it is the first to do so at a sub-$1000 pricepoint. For hardcore bioscientists who know their way around raw genetic data, this is as good a deal as you can currently get."], "link": "http://nextgenseq.blogspot.com/feeds/3873097829931683630/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I was at the CHI APPLYING NEXT-GENERATION SEQUENCING conference in Providence RI, where I heard an extremely interesting presentation from a new Genomics company called Foundation Medicine. This company plans to offer a clinical diagnostic test based on very deep sequencing of all exons from about 300 cancer related genes. They will sequence directly from pathologist's FFPE blocks using Illumina HiSeq to a depth of 500 to 1000X. Here is a recent poster they presented at ASCO, but the information at the CHI conference was updated and more in depth. ASCO poster Here is why I think this is very important. First, this test will include all existing genes that are currently being tested for any type of cancer (BRCA1&2, KRAS, BRAF, HER2, EGFR, etc), but will include all exons and greater diagnostic sensitivity for mutations present in low abundance in heterogenous samples which may suffer from mixed tumor and normal tissue, multiple clones, mixed aneuploidy etc. It will likely also contain the majority of known pharmacogenomic genes. So this one test could put all the other providers of cancer related genetic tests out of business. It is also very important that the test is highly targeted only at \"actionable\" genes. Foundation Med. plans to deliver a report for each patient (in 14 days) that lists all mutations observed in the diagnostic genes, as well as some key items drawn from the literature, clinical trials, and a curated knowledge base about treatments relevant to those genes. In the presentation, COO Kevin Krenitsky said that they typically found 2-3 mutated genes per patient. This is an amount of data that the oncologist or pathologist can reasonably be expected to deal with \u2014 rather than the hundreds to thousands of mutated genes with questionable to zero clinical implications that will be produced by whole genome sequencing. Another interesting discovery reported by Foundation Med. was that in a small number of cases (perhaps 5%), they found mutations for genes that were associated with a different type of cancer. This suggests the use of a non-traditional drug, possibly in combination with other more typical therapies, as an individualized treatment for that one patient. There are currently about 30 drugs for which genetic information can aid in treatment decisions, but this is clearly an area of intense development. Foundation Med. can easily modify its test to include any relevant new genes. We are clearly heading to the point where every cancer patient will benefit from an individualized genomics workup."], "link": "http://nextgenseq.blogspot.com/feeds/5836235249603331945/comments/default", "bloglinks": {}, "links": {"http://www.foundationmedicine.com/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I learned something interesting today about the SNP arrays used for GWAS. There has been a lot of discussion about the nature of mutations/alleles discovered by GWAS studies in terms of the \"common disease: common variant\" hypothesis. It is clear that SNP arrays are designed to cover common variants - alleles that are present in at least 2% of the human population (or at least of some population). Contrary-wise, genome sequencing studies tend to focus on rare variants. In fact a number of recent studies show that major diseases such as cancer and autism tend to be associated with novel, very severe mutations in coding regions of genes. Now this is the interesting part. We took a look at the intersection between the Illumina 2.5 M SNP array and the regions targeted by the Agilent Sure Select exon enrichment kit. It turns out that only about 90K of the Illumina SNPs are in the exon regions. This matches up with Illumina's own annotation file showing that more than 80% of the SNPs on the array are intron or intergenic. My human genetics colleague suggests that the SNP array targets sequence variants (alleles) with small effects, while the exon sequencing strategy targets mutations with large effects. So we can't really replace the SNP array with exome sequencing, they are looking at completely different things."], "link": "http://nextgenseq.blogspot.com/feeds/6583389421099316872/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I am amazed by the success reported in recent papers finding mutations by Next-Gen Sequencing in rare genetic diseases and cancer. In our lab, the sequence data for SNPs is messy and difficult to interpret. The basic problem is that NGS data, particularly Illumina data in our case, contains a moderate level of sequencing errors. We get somewhere between 0.5% and 1% errors in our sequence reads from the GAII and HiSeq machines. This is not bad for many practical purposes (ChIPseq and RNAseq experiments have no trouble with this data) and this error level \"is within specified operating parameters\" according to Illumina Tech support. The errors are not random, they occur much more frequently at the ends of long (100 bp) reads. Some types of errors are systematic in all Illumina sequencing ( A>T miscalls are most common ), and other types of errors are common to a particular sample, run, or lane of sequence data. Also, when you are screening billions of bases, looking for mutations, rare overlaps of errors will occur.  So if sequence data contains errors, and the point of your experiment is to find mutations, then when you find a difference between your data and the reference genome (a variant), you had better make doubly sure that the difference is real. There is a lot of software designed to filter out real mutations (SNPs) from the random sequence errors. The basic idea is to first filter out bad, low quality bases using the built-in quality scores produced by the sequencer. Second, require that multiple reads show the same variant, and that the fraction of reads showing the variant makes sense in your experiment: 40-60% might be good for a heterozygous allele in a human germline sample, 10% or less might make sense if you are screening for a rare variant in a sample from a mixed population of cells. Also, it is usually wise to filter out all common SNPs in the dbSNP database - we assume that these are not cancer causing, and they have a high likelihood of being present in healthy germline cells as well as tumor cells.  We have used the SNP calling tools in the Illumina CASAVA software, the MAQ software package, similar tools in SAMtools , and recently the GATK toolkit . In all cases, it is possible to tweak parameters to get a stringent set of predicted mutations, filtering out low quality bases, low frequency mutations, and SNPs that are near other types of genomic problems such as insertion/deletion sites, repetitive sequence, etc. Using their own tools Illumina has published data showing a false positive detection rate of 2.89% ( Illumina FP Rate ). Under many experimental designs, validating 97% of your predicted mutations would be excellent.  Unfortunately, our medical scientists don't want predicted SNPs vs. an arbitrary reference genome. They want to find mutations in cancer cells vs. the normal cells (germline or wild type) of the same patient. This is where all the tools seem to fall apart. When we run the same SNP detection tools on two NGS samples, and then look for the mutations that are unique to the tumor vs the wild type (WT), we get a list of garbage, thousands of lines long. We get stupid positions with 21% variant allele detected in tumor and 19% variant in WT. Or we get positions where the 80% variant allele frequency is not called as a SNP in WT because 2 out of 80 reads have a one base deletion near that base. So the stringent settings on our SNP discovery software create FALSE NEGATIVES where we miss real SNPs in the WT genome, which then show up as tumor-specific mutations in our SNP discovery pipeline.  Zuojian Tang is creating a post-SNP data filter that imposes a sanity check on the data based on allele frequencies. We are trying out various parameters, but something like a minimum of 40% variant in the tumor and less than 5% variant in the WT narrows the list of tumor-specific mutations down to a manageable number that could be validated by PCR or Sequenom ."], "link": "http://nextgenseq.blogspot.com/feeds/100489810657643322/comments/default", "bloglinks": {}, "links": {"http://www.nyuinformatics.org/": 1, "http://samtools.sourceforge.net/": 1, "http://onlinelibrary.wiley.com/": 1, "http://maq.sourceforge.net/": 1, "http://www.sequenom.com/": 1, "http://www.illumina.com/": 1, "http://www.broadinstitute.org/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["Two interesting projects came through our informatics group last week, both in the 'data drop' mode were the investigator asks for help to analyze data as it comes off of the sequencers. I have noted many times before, that our informatics effort is much greater on the poorly designed and failed experiments. Experiment #1 was a seemingly standard SNP detection using exome sequences with 100 bp paired-end reads on Illumina HiSeq (Agilent Sure Select capture) - the entire thing done by an private sequencing contractor. The contractor also supplied SNP calls using Illumina CASAVA software. Our job was simply to find overlaps between the SNP calls for various samples and controls, and to annotate the SNPs with genomic information (coding or non-coding, conservative mutations, biological pathways, etc). However, we have an obsession with QC data, which the vendor was very reluctant to supply. Turns out that these sequencing reads have a 1.5% error rate, while our internal sequencing lab generates 0.5% error. We also see 10K novel SNPs in each sample with only minimal overlap across samples (a red flag for me). More QC data is extracted from the vendor, and now we see a steep increase in error at the ends of reads. So we wish to trim all reads down by 10-25% and recall SNPs - extract more files from vendor 3x (Illumina requires a LOT of runtime and intermediate files in order to run CASAVA for SNP calling). Meanwhile, Experiment #2 is an RNAseq project where the investigator is interested in alternative splicing. We analyzed one earlier data set with 50bp reads with only moderate success. It seems that very deep coverage is needed to get valid data for alt-splicing, especially when levels of a poorly expressed isoform are suspected to change by a small amount due to biological treatment. The investigator saw some published results suggesting that paired-end RNAseq data would provide more information about splicing isoforms. So, WITHOUT a bioinformatics consult, they sent an existing sample (created for 50bp single end sequencing) to the lab for 100 bp paired-end sequencing. This data came out of our pipeline with more than 20% error and a strange mix of incorrectly oriented read pairs (facing outward rather than inward). After a few days of head scratching and escalating levels of Illumina bioinformatics tech support, we have an explanation. A 225 bp library fragment contains 130 bp of primers and adapters. Thus the insert has an average size of about 95 bp. Some are shorter! Thus, our 100 cyle reads go off the far end of most sequences, adding 5 or more bases of adapter sequence where the alignment software is expecting genomic sequence. In addition, the paired ends overlap more than 100% - so the start of one read is inside the end of the other. Thus they map in the opposite orientation, with an insert size of 5-10 bp. Our best effort to analyze this data will involve chopping all reads back to 36 bp and repeating the Paired-End analysis. So that was 3 days of bioinformatics analysis time not so well spent on forensic QC. Now we are looking back to Experiment #1 and wondering about insert sizes in that library. What if that library's insert size was about 110 or 120 bp (perhaps with a sizeable tail of much smaller fragments), and a fraction of the reads also run off into the adapter, adding mismatched bases at the ends of alignments, and thus jacking up the overall error rate. Two conclusions: 1) talk to bioinformatics BEFORE you build your sequencing libraries 2) if you want something done right, do it yourself."], "link": "http://nextgenseq.blogspot.com/feeds/5415744483713166315/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["It is now possible to get 100x coverage of the exome sequence for a cancer sample (or any other type of human genomic sample) on one lane of an Illumina HiSeq machine. With the Sure Select 50 MB exome kit, it still costs quite a bit more than one thousand dollars to get this data, but it is getting close. At maximum yield, it might currently be possible to multiplex 4 samples into a singe lane and still get 100x coverage of each. This will certainly be true when planned upgrades to the HiSeq machine are available.  Illumina provides some nice software (called CASAVA) that is typically run at the default settings by Core labs and sequencing outsourcing companies. This software gives high-quality genome alignments and pretty good SNP calls - useful for many purposes. However, real-world research needs are often not satisfied with default automated bioinformatics analysis. Narrowing down hundreds of thousands of SNP calls to the few real disease-related mutations is difficult hands-on work for skilled bioinformaticians. Today in my lab group, we are fighting with false-negatives: SNPs that were present but not called in the germ line sample, leading to false identification of mutations unique to the tumor. It looks like we will have to re-run the SNP detection software many times with small changes in various parameters to optimize specificity vs. sensitivity in each sample. Investigators may sub-contract this type of work to the lab that does the sequencing, they may have skilled bioinformaticians in their lab group, or they may hire bioinformatics consultants. In any case, $1K of sequence data may cost more than $10K for analysis."], "link": "http://nextgenseq.blogspot.com/feeds/7123260517996370976/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["Anyone who has worked with NextGen sequence data quickly gains an appreciation for the difficulties associated with long term data storage. The current 'state of the art,' at least for Illumina machines, involves saving some fairly raw data files such as fastq text to the NCBI Short Read Archive (SRA).  SRA_Submission_Guidelines.pdf  Our GAIIx is producing about 30 million reads per lane, which gives files of 8-10 GB (72 cycles) per lane in either qseq (completely unfiltered) or fastq (quality scored) format. If we max out two runs per week, that is about 140 GB of raw sequence data per Illumina machine per week.  There has been some recent discussion about the possibility of phasing out the SRA at NCBI. [see this post which claims to be a memo from NCBI director David Lipman: \" The Sequence Read Archive (SRA) will also be phased out over the next 12 months.\" ] If cost cutting is truly necessary for our national biomedical research infrastructure, I can see why the raw SRA data might be growing at an awkwardly rapid rate and have less value than the higly used databases of GenBank non-redundant nucleotide, GEO, etc.  I think that it is interesting to turn this discussion around and ask why are we archiving all of this raw sequence data? The trivial argument is that: Journals require open access to raw data as a condition of publication.\" But that argument ignores the more interesting question: What is the 'raw data' for a sequencing project? No one is loading Illumina (or SOLID or 454) image data into public archives. The impracticality of saving multiple terabytes of image data for each run made that approach moot a couple of years ago. We are saving raw qseq or fastq files right now because our methods for basecalling and SNP calling (and indel/translocation/copy number calling) are imprecise. I have seen data analysts go back into primary sequence reads for a single sample and find a SNP that was not called because a few reads had below threshold quality scores.  If we consider the actual \"useful\" data content of a NGS run on a single sample, the landscape looks quite different. ChIP-seq is our most common NGS application. The useful data from a ChIP-seq run is actually just a set of genome positions where read starts are mapped. At most, this is 20-30 million positions. In actuality, 30% of reads are not mapped, and another 10-50% are duplicates (multiple reads that map to the exact same position), so the final data set might be compressed to about 10 million genomic loci with a read count at each spot. After sorting and indexing, this information could be efficiently stored in a very compact file.  RNA sequencing is becoming increasingly popular. Our clients are typically not interested in the sequence data itself, only in gene expression counts - essentially the same data as produced by a microarray. However, there are some cool new applications that look at alternative splicing, so we may have to keep the actual sequence reads on hand for a while longer.  Human (and mouse) SNP/indel/cnv detection is another popular NGS application. We are only really interested in the variants. However, SNP calling software requires both numbers of reads with reference vs. variant bases and quality scores for each basecall. Some software also uses context dependent quality metrics, such as distance from other SNPs, distance from indels, etc. Given the highly diverse collection of existing SNP detection software, and the likelihood of new software development, it seems impossible to compress this class of data to a set of variant calls and discard the raw reads. This is very unfortunate, since typical variant detection projects use anything from 20x to 50x coverage of the genome. So we are storing 150 GB of raw sequence data in order to track a few million bytes worth of actual variation in the genome of each research sample.  Other applications, such as de novo genome sequencing of new organisms, or metagenomic sequencing of environmental or medical samples will not be easily compressed. Fortunately, these data are currently archived in places other than the SRA."], "link": "http://nextgenseq.blogspot.com/feeds/754014510549468881/comments/default", "bloglinks": {}, "links": {"http://phylogenomics.blogspot.com/": 1, "http://www.nih.gov/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["A couple of recent papers demonstrate a significant opportunity for the use of NextGen Sequencing in the diagnosis of genetic disease. Dennis Lo et al, at The Chinese University of Hong Kong, have published results for a NGS fetal genetic diagnostic test based on recovery of fragments of fetal DNA from the mother's blood plasma. Preliminary results show that complete coverage of the fetal diploid genome is possible [ Science Translational Medicine ] at a resolution that allows for differentiation of heterozygous vs. homozygous mutations in disease genes; and also that aneuploidy, such as trisomy 21 can be detected with high specificity and sensitivity [ British Medical Journal ]. The key benefit of this approach is that it can be done non-invasively from simple blood draw from the mother, so it avoids the relatively high incidence of pregnancy complications created by amniocentesis or chorionic villus sampling procedures.   Meanwhile, the lab of Stephen Kingsmore at the US National Center for Genome Resources reported results of a targeted sequencing carrier screen for a total of 448 severe (rare) recessive genetic diseases [ Science Translational Medicine ] . This work is particularly significant because the screen is designed to work in multiplex, allowing for a potential total cost per patient of below $500 (less than $1 per disease screened). While each gene is rare in isolation, the combined screen shows an average of 2.8 mutations per individual tested in the proof-of-concept phase of the study.   Taken together, these advances suggest that routine clinical applications of NGS will soon be practical, attractive, and economically feasible for large numbers of healthy people (pregnant women and marriage minded couples). This is great news for NGS equipment vendors, and also suggests a software engineering opportunity for the development of much more robust bioinformatics pipelines for processing this data and including it in electronic medical records. At the same time, I am worried that the lab folks may be progressing much more rapidly than the thinking in the ELSI community. What kind of databases will be created when every pregnancy and every marriage license is associated with gigabyte files of deep sequencing data? This issue is all the more problematic because disease carrier testing and Down syndrome screening are already so widely accepted. Changing prenatal tests to use sequencing in order to reduce complications in pregnancy, and adding pre-conception tests for diseases that were previously thought to be too rare to merit widespread screening are non-controversial medical advances. The downside might come from the unintentional discovery of other genetic information, the availability to law enforcement and other organizations of large files of genetic information on every person, etc."], "link": "http://nextgenseq.blogspot.com/feeds/5295184608733511882/comments/default", "bloglinks": {}, "links": {"http://stm.sciencemag.org/": 2, "http://www.bmj.com/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["This showed up in my inbox today:  Integromics launches SeqSolve, a Next Generation Sequencing functional ...  PharmaLive.com (press release) SeqSolve is the first NGS analysis software on the market specifically developed for data interpretation without requiring bioinformatics expertise. ...    Might be a bit presumptious, but I would guess that a lot of scientists would like to have someone with \"bioinformatics expertise\" do data analysis for their Next-Gen Sequencing projects. But hey, if someone wants to spend $10-$50K on sequencing, but doesn't want an expert to look at the data, good luck with that. Our \"Sequencing Informatics Group\" at NYU is now taking outside consulting work for all types of NGS bioinformatics projects. It is sort of like fixing your Porche - you can go to the Foreign Car specialist mechanic, or you can go to PepBoys and buy some spark plugs and a wrench kit.    We (Ross Smith) have been developing our own visualization toolkit for NGS. Latest version allows us to integrate RNAseq and ChIPseq with RefSeq or other annotation. Net result is much more accurate assignment of TF or histone modification sites to genes, and the ability to clearly see which of multiple TSS are actually being used in a particular sample/cell type. It is very beautiful."], "link": "http://nextgenseq.blogspot.com/feeds/1418837235759653767/comments/default", "bloglinks": {}, "links": {"https://mail.nyumc.org/": 2}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I am speaking at the CHI Next-Gen Sequencing conf in Providence 9.26.2010 (Sunday short course). My topic is going to be about the role of bioinformatics in QC for NG sequencing, with examnples from ChIPseq, where I have the most experience. My main point is that the informatics team works hardest on experiments that produce poor data - or where the data contradict the investigator's expectations. When the experiment is beautiful, then you can use your automated (or semi-automated) pipeline, and hand over the analyzed data with a standard report. For a transcription factor type ChIPseq, the standard result is a set of peaks with p-value and fold change vs. an input DNA, annotated by distance to the nearest gene's Transcription Start Site. If pressed, we can deliver this about 2 days after the sequencing run is complete. For an epigenomics type ChIPseq (histone methylation, acetylation, etc) we deliver both peaks vs input DNA and some type of fold-change for each peak comparing one biological condition vs. another. However, we spend a lot more time squabbling about runs with high PCR duplication, weird artifacts, low yield, peaks in the input DNA lane, etc. To deal with this, we have been developing a variety of tools to quantify overall data quality in a ChIPseq run. We are looking as the overall clustering of mapped reads on the Reference genome (average spacing of adjacent/overlapping reads), as well as coverage at various depths. Some of these metrics make intersting graphs, but we have not completely pinned down their predictive power for understanding the data. We have recently been playing with selecting sets of genes based on external data such as gene exprssion values from microarray or RNAseq experiments, and looking at the aggregate profile of reads mapped near the TSS of groups of genes that are upregulated, downregulated, unchanged, etc. By combining reads for a bunch of genes, we get smooter curves and you can actually say fairly clearly that upregualted genes have (or do NOT have) a change in histone methylation near the TSS as compared with downreg or unchanged genes."], "link": "http://nextgenseq.blogspot.com/feeds/4526333115589734616/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I am not a biostatistican, but I can play on on Blogger. Attention science news writers: \"Correlation does not equal causation.\" Can we institute a simple electroshock penalty for those who cite such fraudulent statistics?  Check this out: \" new study finds that older women who use multivitamins may be more likely than non-users to develop breast cancer\" http://www.reuters.com/article/idUSTRE62S4F520100329  Is there any chance of a sampling bias in this study? Any chance at all that the population of elderly women who take multivitamins may be different in any health parameters from the population that finds no need for such supplements? Furthermore, is there any chance that the study subjects taking the vitamins are more likely to be examined more frequently and therefore more likely to have cancer detected?  This reminds me of the brilliant study (cited from the grocery store tabloids by my sister-in-law) that diet soda makes you fat, because a study found that people who drink diet soda were more likely to be overweight than those who do not drink it."], "link": "http://nextgenseq.blogspot.com/feeds/8126836150212274538/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["I was reading the NY Times on Sunday (3/21/2010). I do that. In the Business section there was an article about the relatively poor sales of the direct-to-consumer genetic tests offered by the startup company 23andMe. Other competitors in this market have done even worse. Several scientists interviewed in the article said basically that there is very little predictive medical value to these SNP profile tests. Then they interviewed Esther Dyson, who is apparently on the board of directors of 23andMe. She provided this clunker of a quote: Ms. Dyson called it \u201cappallingly paternalistic,\u201d to think consumers could not interpret genetic information without help of a doctor. \u201cPeople can understand statistics about baseball,\u201d she said, \u201cand I think they ought to understand statistics about genetics.\u201d Does this not trivialize all of the work of medical geneticists, biostatisticians, and bioinformaticians. Is our work really no more challenging than interpreting baseball statistics? Gee thanks Ms. Dyson."], "link": "http://nextgenseq.blogspot.com/feeds/5642117203845065841/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["Targeted Resequencing is one area of the DNA sequencing landscape that has not yet been revolutionized by Next-Gen technologies. Targeted resequencing typically investigates a few genes (or a few dozen) across large populations. The largest portion of the effort involves lots of PCR to collect all the exons \u2014 or in some projects entire gene regions, then sequencing each amplification product, while keeping track of which PCR product comes from which individual. Even a small project - 10 genes, with 10 exons each, on 100 individuals means 10,000 PCR reactions, and 10,000 sequencing reactions (while keeping accurate track of 10,000 different DNA fragments and avoiding cross-contamination). The Next-Gen approach would amplify the genomic regions in larger chunks, combine all of the chunks from one individual together, then run the library prep protocol (fragment, attach linkers, etc). So how does this play out in reality? I read a paper in Genome Biology yesterday ( Harismendy et al http://genomebiology.com/2009/10/3/R32 ) about targeted sequencing. They looked at six genes, which were covered by 28 large PCR amplicons (all exons plus some introns ) which ranged in size from 3 to 14 kb , for a total of 266 kb of genomic DNA. These PCR products were then combined, and used in the sample prep protocols for 454, ABI SOLID, and Illumina GA sequencing. The same genes also were sequenced by standard Sanger methods using 273 short PCR reactions (88 kb ). Overall, the NG seq methods showed distinct bias favoring the ends of PCR products, and required very high coverage (34-fold, 110-fold and 101-fold for Roche 454, Illumina GA, and ABI SOLiD , respectively) to achieve a 10% false positive rate - false negative rates were much lower. Lets talk about costs. Sanger sequencing costs from $3-10 per sample. I've got an Internet offer here for $4 per reaction, so lets use that for this study: Sanger: $4 x 273 = $1092 per individual Illumina is about $1000 per sample plus about $300 per sample for the library prep kit. So I think they are about the same. However, the Next Gen methods come out far ahead if you multiplex a group of individuals together in the same sequencing reaction. This is not possible with Sanger methods since the sequence is read from the average of a large number of molecules. Then the question becomes how deep can you multiplex while still producing enough reads from each individual research subject to achieve the depth of coverage needed? Our Illumina GAII currently produces about 2 million (usable) 35 bp reads per lane, but we are ramping up toward 5 million 50 bp reads with the latest upgrades.  2 M X 35 bp = 70 M bases 5 M X 50 bp = 250 M bases So for 250 kb X 100x coverage = 25 M bp So it looks like the current generation of NG machines do have a cost advantage over Sanger methods if you include 8, 10, or 12 X multiplexing. Improved accuracy and reduced sampling bias (sample prep methods) could bring down the coverage requirements and increase the advantage of NG methods. I'd really like to hear some other opinions about this issue. We are writing several grant proposals for projects like these and I need some convincing arguments. \u2014Stuart"], "link": "http://nextgenseq.blogspot.com/feeds/3526041440052693528/comments/default", "bloglinks": {}, "links": {"http://genomebiology.com/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["The Pervasive Effects of an Antibiotic on the Human Gut Microbiota, as Revealed by Deep 16S rRNA Sequencing Dethlefsen L, Huse S, Sogin ML, Relman DA PLoS Biology Vol. 6, No. 11, e280 doi:10.1371/journal.pbio.0060280  A paper in PLOS Biology from the Relman lab investigates the effect of a treatment with the antibiotic ciprofloxacin on the bacteria in the intestine. They collected over 7,000 full-length 16S rDNA sequences (1100-1400 bp) by Sanger sequencing and over 900,000 reads (~250 bp) from 454 sequencing of the V3 and the V6 regions.\u00a0  There are many important results in this paper, but it is particularly relevant that 454 sequencing reveals more taxonomic variation with greater stability than traditional sequencing. In my own work, I have found that sequence variants that occur only once in the experiment cannot be used to differentiate samples. Deep sequencing reveals more taxa, and also reduces the frequency of singletons. A rare sequence variant (OTU) that occurs only once in the ~7000 full-length sequences occurs about 65 times in the 454 data set, providing more than enough \"probability of detection\" to be used for comparisons between samples.\u00a0  \"This set of 7,208 sequences is among the largest datasets of full-length 16S rRNA sequences from the human microbiota (or any environment), the rarefaction curves for V6 and V3 tag pyrosequencing eventually rise higher and display more curvature toward the horizontal than the OTU0.01 curve. These features show that a single run of the [454] FLX sequencer targeting V6 or V3 tags from the human gut microbiota can reveal more taxa, and capture a larger proportion of the detectable taxa, than a more extensive effort directed toward full-length 16S rRNA clone sequencing.\""], "link": "http://nextgenseq.blogspot.com/feeds/465352133144764724/comments/default", "bloglinks": {}, "links": {"http://biology.plosjournals.org/": 1}, "blogtitle": "Next-Gen Sequencing"}, {"content": ["CisGenome - just published in Nov. Nature Biotechnology. An integrated software system for analyzing ChIP-chip and ChIP-seq data. Ji H, Jiang H, Ma W, Johnson DS, Myers RM, Wong WH. Nat Biotechnol. 2008 Nov;26(11):1293-300. A full-function integrated bioinformatics suite for ChIP-chip and ChIP-Seq including peak-finding, FDR control for single samples, subtraction of control lane, visualization and annotation of peaks on known genomes, and Motif finding. \u00a0Functional GUI on Windows and Mac. Wow.\u00a0  Software website here: \u00a0CisGenome http://www.biostat.jhsph.edu/~hji/cisgenome/index.htm  Abstract: We present CisGenome, a software system for analyzing genome-wide chromatin immunoprecipitation (ChIP) data. CisGenome is designed to meet all basic needs of ChIP data analyses, including visualization, data normalization, peak detection, false discovery rate computation, gene-peak association, and sequence and motif analysis. In addition to implementing previously published ChIP\u2013microarray (ChIP-chip) analysis methods, the software contains statistical methods designed specifically for ChlP sequencing (ChIP-seq) data obtained by coupling ChIP with massively parallel sequencing. The modular design of CisGenome enables it to support interactive analyses through a graphic user interface as well as customized batch-mode computation for advanced data mining. A built-in browser allows visualization of array images, signals, gene structure, conservation, and DNA sequence and motif information. We demonstrate the use of these tools by a comparative analysis of ChIP-chip and ChIP-seq data for the transcription factor NRSF/REST, a study of ChIP-seq analysis with or without a negative control sample, and an analysis of a new motif in Nanog- and Sox2-binding regions."], "link": "http://nextgenseq.blogspot.com/feeds/2196854592174654153/comments/default", "bloglinks": {}, "links": {"https://www.box.net/": 1, "http://www.jhsph.edu/": 1}, "blogtitle": "Next-Gen Sequencing"}]