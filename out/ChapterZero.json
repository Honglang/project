[{"blogurl": "http://thousandfold.net/cz\n", "blogroll": [], "title": "ChapterZero"}, {"content": ["Well damn. It\u2019s been a long while. Time for another post about my not having posted in a while. \n I\u2019m wrapping up my internship here at IBM, preparing the exit talk on my research. It\u2019s entitled \u201cNew results on the accuracy of randomized spectral methods.\u201d I\u2019ll put up the talk after we\u2019ve wrapped up the paper and submitted it to ArXiv, in a month or so, but here\u2019s a synopsis. Motivated by the fact that a lot of applications of randomized projection-based low-rank approximations use them to approximate subspaces rather than matrices (the canonical example being spectral clustering), we investigated the accuracy of the subspaces so obtained. It turns out that you have a pretty nice dependence on the number of steps of the power method you take, and the oversampling \u2026 the error bounds are almost usable. That\u2019s a coup for machine learning applications! However, I think a sharper analysis (ours is as simple as possible) might be able to make the bounds tight enough for practical use. Along the way, we revisited the analysis of the classical subspace iteration method (unintentionally), and found a geometric interpretation of that \u201crandom space interaction with singular spaces\u201d term that shows up in the analysis of random projection-based low-rank approximation error bounds. \n I think the results are nice enough for a conference paper, but I\u2019m (as usual) a little leery of claiming a strong original contribution, since a lot of what we did turns out to be only a few steps removed from others\u2019 work. But them\u2019s the breaks when you work in numerical linear algebra!"], "link": "http://thousandfold.net/cz/2012/08/10/drawing-to-a-close-at-ibm/", "bloglinks": {}, "links": {}, "blogtitle": "ChapterZero"}, {"content": ["Let \\(\\mat{F},\\mat{G}\\) be positive definite matrices (do they have to be definite?) and \\(0 \\leq p \\leq 2.\\) Show that \n\\[ \n\\tr\\left(\\mat{F}^{p/2} - \\mat{G}^{p/2}\\right) \\leq \\frac{p}{2} \\tr\\left((\\mat{F} - \\mat{G})\\mat{G}^{p/2 - 1}\\right). \n\\] \n I\u2019m working on it. See if you can get a proof before I do"], "link": "http://thousandfold.net/cz/2012/07/03/problem-another-inequality/", "bloglinks": {}, "links": {}, "blogtitle": "ChapterZero"}, {"content": ["There are matrices for which the spectrum of \\(\\mat{A}\\) and \\(\\mat{A}\\mat{A}^T\\) are identical. As an example, take \n\\[ \n\\mat{A} = \\begin{pmatrix} \n\\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\ \n0 & \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ \n0 & 0 & 0 & 0 \\\\ \n0 & 0 & 0 & 0 \n\\end{pmatrix}. \n\\] \nFreaky, right? I wouldn\u2019t have believed it. I know one class of matrices for which this is true, but it\u2019s a bit funky: if \\(\\mat{C}\\) and \\(\\mat{B}\\) are rectangular matrices related in a nice way (essentially, \\(\\mat{B}\\) is \\(\\mat{C}\\) plus a perturbation that is orthogonal to the range space of \\(\\mat{C}\\)), then \\(\\mat{A} = \\mat{C} \\mat{B}^\\dagger\\) has this property. Here \\(\\dagger\\) denotes pseudoinversion. The proof of this special case is complicated, and not particularly intuitive. So I wonder if there\u2019s a simpler proof for the general case."], "link": "http://thousandfold.net/cz/2012/06/22/when-do-a-and-aat-have-the-same-spectrum/", "bloglinks": {}, "links": {}, "blogtitle": "ChapterZero"}, {"content": ["This summer\u2019s the summer of languages for me. I\u2019m learning R piecemeal, because I\u2019m working on a data analytics project that requires a lot of statistical analysis: learning a new language is a bother, but the unprecedented amount of statistical packages available for R justifies the investment. I also decided to dive into Julia , despite its youth, mostly because of this : \n \nWe want a language that\u2019s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that\u2019s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled. \n (Did we mention it should be as fast as C?)\n \n Well, that and the fact that Matlab isn\u2019t readily available here at IBM. I could use my Caltech license, but the writing\u2019s on the wall. I might as well train myself in a good alternative for when I no longer have that option. Octave isn\u2019t an option I\u2019m happy with: it looks kludgy, Matlab code doesn\u2019t *actually* run without modification, and the plotting options are embarassingly sad (Gnuplot in 2012? come on)."], "link": "http://thousandfold.net/cz/2012/06/22/summer-of-languages/", "bloglinks": {}, "links": {"http://www.johnmyleswhite.com/": 1, "http://julialang.org": 1}, "blogtitle": "ChapterZero"}, {"content": ["I\u2019ve started a separate blog to track my mathematical reading . Check it out, comment, and suggest material I might find interesting."], "link": "http://thousandfold.net/cz/2012/06/21/new-reading-blog/", "bloglinks": {}, "links": {"http://thousandfold.net/": 1}, "blogtitle": "ChapterZero"}, {"content": ["I\u2019m working on an internal project that motivated me to look into survival analysis. Cool stuff, that. Essentially, you have a bunch of data about the lifetimes of the some objects and potentially related covariates, and from that data you\u2019d like to estimate the functional relationship between the lifetimes and the covariates, in the form of the survival function \\(P(T > x) = f(x, X_1, \\ldots, X_n).\\) It\u2019s not clear that this is the best lens to approach the particular problem I\u2019m looking at ( churn )\u2014 for instance, you could think of this in machine learning or approximation theory terms rather than statistical terms\u2014, but it\u2019s certainly a very sensible approach. \n I still have access to the online Springer books through Caltech, so I started to read Dynamic regression methods for survival analysis , which is more than a bit ambitious given my lack of statistical training. The first chapter\u2019s quite inspiring though: I now want to learn about compensators and martingale central limit theorems. Apparently one can view a lot of statistical estimators as compensators (or something close), and the difference between the estimators and the actual quantities as a martingale (or, more generally, a zero-mean stochastic process). Then the asymptotic error of the estimator can be understood through the central limit theorem (\\emph{or}, in the more general case, through empirical process theory\u2014 which for various reasons, including the connection to sums of independent r.v.s in Banach spaces, I\u2019m also very interested in learning). \n Of course, my first question is: is there a nonasymptotic version of the martingale central limit theorem? And my second question is: do these concepts make sense in the matrix case (whatever that is), and what\u2019s known there? Is this something worth working on? Unfortunately, I know nothing about statistics \u2026 I think maybe there\u2019s a relation to the self-normalizing processes that Joel mentioned I should look into. So there\u2019s one more thing to add to my list of too-little-time investigations. \n Links to interesting literature: \n \n Pollard\u2019s notes on empirical process theory (the most concrete/approachable I can find) \n Sara van de Geer\u2019s notes on empirical process theory (concise and simple) \n An introduction to survival analysis"], "link": "http://thousandfold.net/cz/2012/06/13/third-week-of-the-ibm-internship/", "bloglinks": {}, "links": {"http://www.ethz.ch/": 1, "http://www.amstat.org/": 1, "http://www.yale.edu/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "ChapterZero"}, {"content": ["This question has been bothering me off and on for several months now: how *exactly* is the truncated SVD computed, and what is the cost? I\u2019ve gathered that most methods are based on Krylov subspaces, and that implicitly restarted Arnoldi processes seem to be the most popular proposal \u2026 in what miniscule amount of literature I\u2019ve been able to find on the subject (like only two or three papers, tops, that actually focus on computing truncated SVDs, as opposed to the multitudes that propose using them). However, I\u2019ve no idea if that\u2019s the case, or just the impression I have. \n But mostly, I want to know the compute time of the most efficient truncated SVD algorithm. Why? A naive estimate of getting the rank-\\(k\\) truncated SVD of an \\(m\\times n\\) matrix using Krylov subspace techniques is \\(\\mathrm{O}(mnk)\\), but in practice the compute time actually depends on properties of the matrix. On the other hand, basically all randomized low-rank techniques take around that many operations but without depending on the properties of the matrix. Soooo, it\u2019d be nice to know when randomized techniques are competitive. Since there\u2019s so much literature on them, I assume they are competitive (and the Halko\u2013Martinsson\u2013Tropp survey paper has some plots that show speedups)."], "link": "http://thousandfold.net/cz/2012/06/07/truncated-svd-how/", "bloglinks": {}, "links": {}, "blogtitle": "ChapterZero"}, {"content": ["I started reading Matousek\u2019s discrete geometry book. Specifically, chapter 12 on applications of high-dimensional polytopes. Mostly because he apparently draws a connection between graphs and the Brunn-Minkowski theory , and I\u2019m interested to see exactly what that connection is and if it has any interesting implications. \n I just finished the proof of the weak perfect graph conjecture (it\u2019s been a while since I read a nontrivial pure math proof, so I haven\u2019t actually *digested* it entirely). Now I\u2019m on the exercises for that portion.\u00a0So, here\u2019s an interesting question involving the concept of total unimodularity, which is apparently one of those foundational concepts in an area called polyhedral combinatorics. \n A matrix \\(\\mat{A}\\) is called totally unimodular if every square submatrix of \\(\\mat{A}\\) has determinant 0 or \\(\\pm 1.\\) The question is to show that every nonsingular totally unimodular \\(n \\times n\\) matrix maps the lattice \\(\\mathbb{Z}^n\\) bijectively onto itself. \n Update (solution) \nIt\u2019s easy peasy. Clearly each entry of \\(\\mat{A}\\) is in \\(\\{-1,0,1\\}\\), so \\(\\mat{A}\\) maps \\(\\mathbb{Z}^n\\) into itself. The fact that the mapping is bijective follows easily from Cramer\u2019s rule, the fact that \\(|\\mathrm{det}(\\mat{A})| = 1,\\) and the fact that all the minors of \\(\\mat{A}\\) are integral."], "link": "http://thousandfold.net/cz/2012/05/08/qotd-total-unimodularity/", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "ChapterZero"}, {"content": ["Let \\(\\mat{P}_{\\mathcal{U}}\\) denote the projection unto a \\(k\\)-dimensional subspace of \\(\\C^{n}.\\) We say \\(\\mathcal{U}\\) is \\(\\mu\\)-coherent if \\((\\mat{P}_{\\mathcal{U}})_{ii} \\leq \\mu \\frac{k}{n} \\)\u00a0for all \\(i = 1, \\ldots, n.\\) \n Let \\(\\mat{A} \\in \\C^{n \\times n} \\) be a SPSD matrix whose top \\(k\\)-dimensional invariant subspace is \\(\\mu\\)-coherent. Given \\(\\delta > 0\\) and \\(\\mat{S} \\in \\C^{n \\times \\ell}\\), where \\(k < \\ell \\ll n,\\) \u00a0is there a matrix \\(\\mat{E}_{\\delta,\\mat{S}} \\in \\C^{n \\times n} \\) such that \n \n \\(\\hat{\\mat{A}} := \\mat{A} + \\mat{E}_{\\delta,\\mat{S}}\\) is SPSD, \n The top \\(k\\)-dimensional invariant subspace of \\(\\hat{\\mat{A}}\\) is also \\(\\mu\\)-coherent, \n \\(\\|\\hat{\\mat{A}} \u2013 \\mat{A}\\|_2 < \\delta \\), and \n \\(\\mat{S}^t \\hat{\\mat{A}} \\mat{S} \\succeq \\delta \\cdot \\mat{I}\\) ? \n \n It\u2019d be fine if instead of (2) holding, the coherence of the top \\(k\\)-dimensional invariant subspace of \\(\\hat{\\mat{A}}\\) is slightly larger than that of \\(\\mat{A}\\)."], "link": "http://thousandfold.net/cz/2012/04/14/are-there-perturbations-that-preserve-incoherence-and-give-nicely-conditioned-submatrices/", "bloglinks": {}, "links": {}, "blogtitle": "ChapterZero"}, {"content": ["I had a dozen eggs in the fridge that I need to use up by Wednesday; now I\u2019m down to four. I made Jaffa cakes last week (not bad, but not memorable, and definitely not worth the effort: my main complaint is that the cake portion is bland and has a too-tough texture) and I left some chocolate blueberry cookie dough sitting in the fridge overnight which I\u2019ll bake up tonight. That accounted for four of the eggs. This post is about the two meals that account for another four, and maybe the remaining four also \n  Steak, eggs, and vegetables in sauce \n I\u2019m in love with this dish because it\u2019s super simple to prepare, filling, and not too unhealthy: steak, eggs, and steamed veggies in a Dijon sauce. Surprisingly, considering how much I love meat, the star of the show here is the veggies in sauce: steamed baby carrots and French green beans (haricot verts) with an emulsion of Dijon mustard, red wine vinegar, EVOO, and some salt. Basically, it\u2019s a quick and dirty substitute for Sauce Dijon. \n If I\u2019d known sauce + steamed vegetables was such a winning combination (don\u2019t ask me why it took so late in life for me to get a clue) I would be a much healthier person My mission for next week is to learn how to make Hollandaise sauce and its derivatives, and find other vegetables to smother in sauce. No doubt Alton Brown has something interesting to say on the matter."], "link": "http://thousandfold.net/cz/2012/03/25/in-which-i-discover-vegetables-taste-good-in-dijon-sauce-then-proceed-to-spread-the-gospel/", "bloglinks": {}, "links": {"http://www.kimberlycun.com/": 1, "http://thousandfold.net/": 1}, "blogtitle": "ChapterZero"}]