[{"blogurl": "http://lemire.me/blog\n", "blogroll": [], "title": "Daniel Lemire's blog"}, {"content": ["For human beings, reading is much faster than writing. The same is often true for computers: adding a record to a database may take an order of magnitude longer than reading it. But the speed differential can often be explained by the overhead of concurrency. For example, writing to a database may involve some form of locking whereas the database systems must first acquire the exclusive writing rights over a section of memory. \n If you minimize the overhead, by writing straight C/C++ code, it is less obvious that reading and writing to memory should have different speeds. And, indeed, in my tests, the memset instruction in C which initializes a range of bytes with a given value, takes exactly half the running time of memcpy which reads data from one location and writes it to another. Because memset involves just writing to memory, and memcpy does as much reading as it does writing, we can conclude that these tests show symmetry: reading is as fast as writing. \n However, these tests (with memset and memcpy ) are maybe not representative of how writing and reading is done most commonly. Thus I designed a second batch of tests. In these tests, I have two arrays: \n \n A small array (32KB or less) that fits easily in fast L1-L2 CPU cache. It has size M . \n A larger array (between 1MB to 512MB) that might not fit in CPU cache. It has size M times N so that the small array fits N times in the larger array. \n \n Then I consider two tests that should run at the same speed if reading and writing are symmetrical: \n \n I copy the small array over the large one N times so that all of the larger array is overwritten. In theory, this test reads and writes M times N elements. In practice, we expect the computer to read the small array only once and to keep its data in close proximity with the CPU afterward. Hence, we only need to read the M elements of the small array once and then write N times M elements in the large array. Hence, this test measures mostly writing speed. The simplified source code of this test is as follows:\n for ( size_t x = 0 ; x < N ; + + x ) { \n  memcpy ( bigdata , data , M * sizeof ( int ) ) ; \n bigdata + = M ; \n } \n \n \n I copy N segments from the large array to the small array so that at the end, all of the large array has been read. Again, this test reads and writes M times N elements. However, we can expect the processor to delay copying the short array to memory: it might keep it in fast CPU cache instead. So that, in effect, this second tests might be measuring mostly reading speed. The simplified source code is:\n for ( size_t x = 0 ; x < N ; + + x ) { \n  memcpy ( data , bigdata , M * sizeof ( int ) ) ; \n bigdata + = M ; \n } \n \n \n \n Notice how similar the source codes are! \n Can you predict which code is faster? I ran these tests on a desktop Intel core i7 processor: \n \n \n M \n N \n test 1 (time)/ test 2 (time) \n \n \n 2048 \n 32 M \n 3.4 \n \n \n 8192 \n 16384 M \n 1.6 \n \n \n Analysis : The test 1 (writing test) is significantly slower than test 2 (reading test). That is, it is slower to take a small array (located near the CPU) and repeatedly write it to slower memory regions, than doing the reverse. That is, reading is faster than writing. \n \u201cBut Daniel! That\u2019s only one test and 4 lines of code, it proves nothing!\u201d \n Ok. Let us consider another example example. In C/C++, boolean values ( bool ) are stored using at least one byte. That\u2019s rather wasteful! So it is common to pack the boolean values. For example, you can store 8 boolean values in one byte using code such as this one: \n void pack ( bool * d , char * compressed , size_t N ) { \n for ( size_t i = 0 ; i < N / 8 ; + + i ) { \n  size_t x = i * 8 ; \n compressed [ i ] = d [ x ] | \n     d [ x + 1 ] < < 1 | \n     d [ x + 2 ] < < 2 | \n     d [ x + 3 ] < < 3 | \n     d [ x + 4 ] < < 4 | \n     d [ x + 5 ] < < 5 | \n     d [ x + 6 ] < < 6 | \n     d [ x + 7 ] < < 7 ; \n } \n } \n \n The reverse operation is not difficult: \n void unpack ( char * compressed , bool * d , size_t N ) { \n for ( size_t i = 0 ; i < N / 8 ; + + i ) { \n  size_t x = i * 8 ; \n d [ x ] = compressed [ i ] & 1 ; \n d [ x + 1 ] = compressed [ i ] & 2 ; \n d [ x + 2 ] = compressed [ i ] & 4 ; \n d [ x + 3 ] = compressed [ i ] & 8 ; \n d [ x + 4 ] = compressed [ i ] & 16 ; \n d [ x + 5 ] = compressed [ i ] & 32 ; \n d [ x + 6 ] = compressed [ i ] & 64 ; \n d [ x + 7 ] = compressed [ i ] & 128 ; \n } \n } \n \n These two pieces of code (pack and unpack) are similar. The main difference is that the pack function reads 8 bools (1 byte each) and writes one byte whereas unpack function reads one byte and writes 8 bools. That is, the pack function is a reading test whereas the unpack function is a writing test. \n Can you guess which one is faster? In my tests, unpacking is 30% slower than packing. This is consistent with the result of my other analysis: it seems that reading is faster than writing, probably due to caching issues. \n Of course, you should only expect to see a relative slowdown if you write much more than you would otherwise read (say by a factor of 8). Also, I expect the results of these tests to vary depending on the compiler and the machine you use. Always run your own tests! \n Source code : As usual, you can find my source code online . \n Further reading : I know why DRAM is slower to write than to read, but why is the L1 & L2 cache RAM slower to write? via Reverend Eric Ha \n Update : Nathana\u00ebl Schaeffer points out that the latency of the MOV operation differs depending on whether you are copying data from memory to a register, or from a register to memory. On my test machine, it takes at least 3 cycles to copy data from a register to memory, but possibly only 2 cycles to copy data from memory to a register ( Fog, 2012 ). The minimal throughput of the two operations is the same however. \n Credit : Thanks to L. Boystov, S. H Corey, R. Corderoy, N. Howard and others for discussions leading up to this blog post."], "link": "http://lemire.me/blog/archives/2012/10/29/is-reading-memory-faster-than-writing-to-memory-on-a-pc/", "bloglinks": {}, "links": {"http://electronics.stackexchange.com/": 1, "https://github.com/": 1, "https://plus.google.com/": 1, "http://users.isterre.fr/": 1, "http://www.agner.org/": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["You can represent a list of distinct integers no larger than N using exactly N bits: if the integer i appears in your list, you set the i th bit to true. Bits for which there is no corresponding integers are set to false. For example, the integers 3, 4, 7 can be represented as 00011001. As another example, the integers 1, 2, 7 can be represented as 01100001. \n Bitmaps are great to compute intersections and unions fast. For example, to compute the union between 3, 4, 7 and 1, 2, 7, all I need to do is compute the bitwise OR between 00011001 and 01100001 (=01111001) which a computer can do in one CPU cycle. Similarly, the intersection can be computed as the bitwise AND between 00011001 and 01100001 (=00000001). \n Though it does not necessarily make use of fancy SSE instructions on your desktop, bitmaps are nevertheless an example of vectorization . That is, we use the fact that the processor can process several bits with one instruction. \n There are some downsides to the bitmap approach: you first have to construct the bitmaps and then you have to extract the set bits. Thankfully, there are fast algorithms to decode bitmaps . \n Nevertheless, we cannot expect bitmaps to be always faster. If most bits are set to false, then you are better off working over sets of sorted integers. So where is the threshold? \n I decided to use the JavaEWAH library to test it out. This library is used, among other things, by Apache Hive to index queries over Hadoop. JavaEWAH uses compressed bitmaps (see Lemire at al. 2010 for details) instead of the simple bitmaps I just described, but the core idea remains the same. I have also added a simpler sparse bitmap implementation to this test. \n I generated random numbers using the ClusterData model proposed by Vo Ngoc Anh and Alistair Moffat . It is a decent model for \u201creal-world data\u201d. \n Consider the computation of the intersection between any two random sets of integers. The next figure gives the speed (in millions of integers per second) versus the density measured as the number of integers divided by the range of values. \n \n I ran the test on a desktop core i7 computer. \n Conclusion : Unsurprisingly, the break-even sparsity for JavaEWAH is about 1/32: if you have more than 1000 integers in the range [0,32000) then bitmaps might be faster. Of course, better speed is possible with some optimization and your data may differ from my synthetic data, but we have a ballpark estimate. A simpler sparse bitmap implementation can be useful over sparser data though it comes at a cost: the best speed is reduced compared to EWAH. \n Source code : As usual, I provide full source code so that you can reproduce my results. \n Update : This post was updated on Oct. 26th 2012. \n \n Related posts (automatically generated): \n \n\t\t Fast bitmap decoding"], "link": "http://lemire.me/blog/archives/2012/10/23/when-is-a-bitmap-faster-than-an-integer-list/", "bloglinks": {}, "links": {"https://github.com/": 3, "http://onlinelibrary.wiley.com/": 1, "http://arxiv.org/": 1, "http://en.wikipedia.org/": 1, "http://lemire.me/blog": 2, "http://hive.apache.org/": 1, "http://code.google.com/": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["As a teenager, I was genuinely impressed by communism. The way I saw it, the West could never compete. The USSR offered a centralized and efficient system that could eliminate waste and ensure optimal efficiency. If a scientific problem appeared, the USSR could throw 10, 100 or 1000 scientists at it without having to cajole anyone. \n I could not quite understand why the communist countries always appeared to be technologically so backward. Weren\u2019t their coordinated engineers and scientists out-innovating our scientists and engineers? \n I was making a reasoning error. I had misunderstood the concept of economy of scale best exemplified by Ford. To me, communism was more or less a massive application of the Fordian approach. It ought to make everything better and cheaper! \n The industrial revolution was made possible by economies of scale: it costs far less per car to produce 10,000 cars than to make just one. Bill Gates became the richest man in the world because software offers an optimal economy of scale: it costs the same to produce one copy of Windows or 100 million copies. \n Trade and employment can also scale: the transaction costs go down if you sell 10,000 objects a day, or hire 10,000 people a year. Accordingly, people living in cities are typically better off and more productive. \n This has lead to the belief that if you regroup more people and you organize them, you get better productivity. I want to stress how different this statement is from the previous observations. We can scale products, services, trade and interaction. Scaling comes from the fact that we need reproduce many copies of the essentially the same object or service. But merely regrouping people only involves scaling in accounting and human ressources: if these are the costs holding you back, you are probably not doing anything important. To get ten people together to have much more than ten times the output is only possible if you are producing an uniform product or service. \n Yet, somehow, people conclude that regroup people and getting them to work on a common goal, by itself, will improve productivity. Fred Brooks put a dent in this theory with his Brook\u2019s law: \n Adding manpower to a late software project makes it later. \n While it is true that almost all my work is collaborative, I consistently found it counterproductive to work in large groups. Of course, as an introvert , this goes against all my instincts. But I also fail to see the productivity gains in practice whereas I do notice the more frequent meetings. \n Abramo et al. (2012) looked seriously at this issue and found that you get no more than linear scaling. That is, a group of two researchers will produce twice as much as one researcher. Period. There is no economy of scale when coordinating human brains. Their finding contradicts decades of science policy where we have tried to organize people into larger and better coordinated groups (a concept eerily reminiscent of communism). \n We can make an analogy with computers. Your quad-core processor will not run Microsoft Word four times as far. It probably won\u2019t even run it twice as fast. In fact, poorly written software may even run slower when there are more than one core. Coordination is expensive. \n The solution is to lessen the need for coordination: have different people work on different things, use smaller teams, and employ fewer managers."], "link": "http://lemire.me/blog/archives/2012/10/15/you-cannot-scale-creativity/", "bloglinks": {}, "links": {"http://www.springerlink.com/": 1, "http://lemire.me/blog": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["In Quebec, we have had massive student protests . Students were asking for free higher education. It seems that things have quieted down as the new government has vowed to freeze tuition in constant dollar. Though it is never spelled out as such, affordable higher education is viewed as a tool to favor social mobility. \n Most of us are in favor of social mobility: it is a good thing if a kid from a poor background can grow to become wealthy. \n Sociologists will argue that we have no other social engineering tool to favor social mobility like affordable higher education. That is what justifies massive government involvement in higher education even in countries as right-leaning as the United States. In more left-leaning countries like France, higher education is often entirely free. \n Before I pursue this line of thought, there is another unstated assumption: college graduates are smarter and they make better citizens. While the first assumption (social mobility) is founded on solid research, this second one has much weaker support. Spending time in college will improve the mastery of your major, but your overall cognitive skills may not improve. For example, getting a degree in English will not help you if you need to learn software programming later. Arguably, a degree in mathematics might help you become a better programmer, but even that is doubtful. \n In any case, it has been historically true that college degrees have translated into better jobs, especially for the poorer half of the population. But, at some point, we have confused the means and the goals. What the young people in Quebec should be asking for is the means to start their careers. \n Meanwhile, as a college professor I sometimes have to answer difficult questions such as \u201cif I get this diploma or degree, will I get a job?\u201d That is, many students care very much about the first unstated assumption: attending college helps you get jobs. \n Most university-level professors, me included, are uneasy with this function. My job is simply not geared toward providing job training. For example, I try to get our students to program if only because it is such a useful skill to have in industry. Unfortunately, if a student only programs within our classes, he is very unlikely to become a sought-after expert programmer. For most people, it takes 10 years to be great at programming. So, degrees are not a straight path to a career in the software industry. The same is true of most degrees and most industries. Universities are simply not great at job training and they don\u2019t make people all-around smarter. \n But this used not to matter. The old industrial-age model was: get a degree or diploma in whatever you care for, get an office job, retire. The purpose of the degree was not to train you but rather to fail those who could not conform to a typical office job. This model still work if you are lucky enough to get a job in government or a big corporation. \n Yet conformity is less important in the post-industrial age. And correspondingly, we are slowly moving to a post-industrial career model where a badge like a degree or diploma is only one of the things that are nice to have. \n Increasingly, my answer to \u201cWill I get a job with this degree?\u201d is \u201cIf that\u2019s all you have, no, you won\u2019t\u201d. \n Effectively, I predict that the effectiveness of higher education as a tool for social mobility will weaken. A focus on conformity-based badges like degrees will fail the younger generation. The routine factory and office jobs are being automated and outsourced too fast. I am especially concerned when I hear the student representatives present the degree as a goal in itself. As if we were in the seventies. \n Further reading : \n \n \nBrown and Lauder, The great transformation in the global labour market , Soundings, Number 51, July 2012."], "link": "http://lemire.me/blog/archives/2012/10/08/will-i-get-a-job-with-this-degree/", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1, "http://www.eurozine.com/": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["Since the second world war, science has relied on what I call traditional peer review. In this form of peer review, researchers send their manuscript to journal. An editor then reviews the manuscript and if it is judged suitable, the manuscript is sent to reviewers who must make a recommendation. If the recommendations are good, the editor might ask the authors to revise their work. The revised manuscript can be sent back to the reviewers. Typically, the authors do not know who the reviewers are. \n In computer science and engineering, we often rely also on peer reviewed conferences: they work the same way except that the peer review process is much shorter and it typically only involves one round. That is, the manuscript is either accepted as is or rejected definitively. \n Governments attribute research grants according to the same peer review process. However, in this case, a research proposal is reviewed instead and there is typically only one round of review. You can theoretically appeal of the decisions, but by the time you do, the funds have been allocated and the review committees might have been dismissed. \n Researchers trust peer review. There is a widely held belief that this process can select the best manuscripts reliably, and that it can detect and reject nonsense. \n However, most researchers have never looked at the evidence. \n So how well does peer review fare? We should first stress that a large fraction (say 94% ) of all rejected manuscripts are simply taken to another journal and accepted there. The editor-in-chief of a major computer science journal once told me: you know Daniel, all papers are eventually accepted, don\u2019t forget that . That is, even if you find that some work is flawed, you can only temporarily sink it. You cannot denounce the work publicly in the current system. \n Another way to assess the reliability of peer review is to look at inter-reviewer agreement. We find that as soon as one reviewer feels that the manuscript is acceptable, the inter-reviewer agreement falls to between 44% and 66% ( Cicchetti, 1991 ). That is, consensus at the frontiers of science is as elusive as in other forms of human judgment ( Wessely, 1998 ). Who reviews you is often the determining factor: the degree of disagreement within the population of eligible reviewers is such that whether or not a proposal is funded depends in large proportion of cases upon which reviewers happen to be selected for it ( Cole et al., 1981 ). \n \n Related posts (automatically generated): \n \n\t\t How peer review is supposed to help you! \n\t\t Peer review is an honor-based system \n\t\t The purpose of peer review"], "link": "http://lemire.me/blog/archives/2012/09/19/how-well-does-peer-review-work/", "bloglinks": {}, "links": {"http://www.lutz-bornmann.de/": 1, "http://lemire.me/blog": 3, "http://dx.doi.org/": 3}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["About a year ago, I read Made by Hand: Searching for Meaning in a Throwaway World by Mark Frauenfelder. It is a simple book with a simple message. \n How to be happy? Frauenfelder thought that moving to tropical island might be it for him and his family. It failed. Location is not related to your happiness. \n But then, in the process of moving, he realized something important. He was happier when he made stuff for himself. \n It is not about anti-consumerism. While I do not know Frauenfelder, I expect he has a big screen TV and a nice car. \n Rather, it is the idea that if you can afford it, making your own stuff makes your life richer and more meaningful. \n This message has changed my life. What have I done with it? \n \n I have learned to make thick yogurt. My recipe is simple but it still took me months to get it right. Mostly, I had to learn the hard way that the common recipes cannot be trusted. We never buy commercial yogurt. I make 4l of yogurt a week and I have been doing so for about a year. \n I have learned to make my own wine, port and beer. When I drink wine, I drink my wine. Of course, I don\u2019t start from grapes, but rather from grape juice. But it is meaningfully my wine since I can screw it up. People love my port wine. \n We no longer buy commercial bread. I have learned to make several great tasting breads with very little effort. Do not do as I did and try to use a bread machine. It is a waste of time and money. Instead go buy these books:\n \n \n My Bread: The Revolutionary No-Work, No-Knead Method by Jim Lahey and Rick Flaste \n \n Peter Reinhart\u2019s Artisan Breads Every Day \n \n If you want to save money, you can find these recipes online or on YouTube (e.g., see the GardenFork.TV recipe ). It took me months but now I can reliably produce bread that is superior to what you can find in a store, with little effort. \n I also make my own pizza, entirely from scratch, that has exactly the same great taste as a professional one. The only thing I have not gotten quite right is how to shape the pizza as a nice round pie. I have been making my own pizza for about 10 years, but I only got it right in the last two months or so (thanks to Peter Reinhart\u2019s books). If you are curious, I use a stone and I cook the pizza at 500F.\n \n I produce my own herbs, tomatoes and lettuce from my garden. I have canned enough tomatoes to last a year. \n I build my own radio-control models. I have built two sailboats from scratch. My latest effort was a crawler : built from parts. I have also built a Mario-themed clock for my son. My basement is filled with half-completed robots, arduinos and random other items.\n \n \n My wife is sometimes annoyed at my projects. I fail all the time. Sometimes I make a mess. I waste money. \n Over time, however, I build up expertise. And more importantly, the stuff I make has meaning. My kids eat my pizza, not just any pizza. My kids love my bread, not just any bread. \n It is not about saving money. On the contrary, it is about being able to afford to do your own stuff. It makes me happy that I can afford to make my own bread and that I am good at it. \n Update : I also roast my own coffee. I picked that up from John Cook ."], "link": "http://lemire.me/blog/archives/2012/09/19/happier-annoying-wife/", "bloglinks": {}, "links": {"http://www.amazon.com/": 3, "http://www.youtube.com/": 3, "http://www.johndcook.com/blog": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["Databases and search engines often store arrays of integers. In search engines, we have inverted indexes that map a query term to a list of document identifiers. This list of document identifiers can be seen as a sorted array of integers. In databases, indexes often work similarly: they map a column value to row identifiers. You also get arrays of integers in databases through dictionary coding: you map all column values to an integer in a one-to-one manner. \n Our modern processors are good at processing integers. However, you also want to keep much of the data close to the CPU for better speed. Hence, computer scientists have worked on fast integer compression techniques for the last 4 decades. One of the earliest clever techniques is Elias coding . Over the years, many new techniques have been developed: Golomb and Rice coding , Frame-of-Reference and PFOR-Delta , the Simple family, and so on. \n The general story is that while people initially used bit-level codes (e.g., gamma codes ), simpler byte-level codes like Google\u2019s group varint are more practical. Byte-level codes like what Google uses do not compress as well, and there is less opportunity for fancy information theoretical mathematics. However, they can be much faster. \n Yet we noticed that there was no trace in the literature of a sensible integer compression scheme running on desktop processor able to decompress data at a rate of billions of integers per second. The best schemes, such as Stepanov et al.\u2019s varint-G8IU report top speeds of 1.5 billion integers per second. \n As your may expect, we eventually found out that it was entirely feasible to decoding billions of integers per second. We designed a new scheme that typically compress better than Stepanov et al.\u2019s varint-G8IU or Zukowski et al.\u2019s PFOR-Delta , sometimes quite a bit better, while being twice as fast over real data residing in RAM (we call it SIMD-BP128). That is, we cleanly exceed a speed of 2 billions integers per second on a regular desktop processor. \n We posted our paper online together with our software . Note that our scheme is not patented whereas many other schemes are. \n So, how did we do it? Some insights: \n \n We find that it is generally faster if we compress and decompress the integers in relatively large blocks (more than 4 integers). A common strategy is bit packing . We found that bit packing could be much faster (about twice as fast) if we used vectorization (e.g., SSE2 instructions). Vectorization is the simple idea that your processor can process several values (say 4 integers) in one operation: for example, you can add 4 integers to 4 other integers with one instruction. Because bit unpacking is the key step in several algorithms, we can effectively double the decompression speed if we double the bit unpacking speed. \n Most of the integer compression schemes rely on delta coding. That is, instead of storing the integers themselves, we store the difference between the integers. During decompression, we effectively need to compute the prefix sum. If you have taken calculus, think about it this way: we store the derivative and must compute the integral to recover the original function. We found out that this can use up quite a bit of time. And if you have doubled the speed of the rest of the processing, then it becomes a serious bottleneck. So we found that it is essential to also vectorize the computation of the prefix sum. \n \n After all this work, it is my belief that, to be competitive, integer compression schemes need to fully exploit the vectorized instructions available in modern processors. That is, it is not enough to just write clean C or C++, you need to design vectorized algorithms from the ground up. However, it is not a matter of blindly coming up with a vectorized algorithm: getting a good speed and a good compression rate is a challenge. \n Credit : This work was done with Leonid Boytsov . We have also benefited from the help of several people. I am grateful to O. Kaser for our early work on this problem. \n Software : A fully tested open source implementation is available from github. As caveat, we used C++11 so that a C++11 compiler is required (e.g., GNU GCC 4.7). \n Limitations : To be able to compare various alternatives, we have uncoupled some decoding steps so that at least two passes are necessary over the data. In some cases, better speed could be possible if the processing was merged into one pass. We are working on further optimizations. \n Further reading : More database compression means more speed? Right? , Effective compression using frame-of-reference and delta coding \n Quick links : The paper , the software . \n \n Related posts (automatically generated): \n \n\t\t Fast bitmap decoding \n\t\t Bit packing is fast, but integer logarithm is slow"], "link": "http://lemire.me/blog/archives/2012/09/12/fast-integer-compression-decoding-billions-of-integers-per-second/", "bloglinks": {}, "links": {"http://arxiv.org/": 2, "https://github.com/": 2, "http://boytsov.info/": 1, "http://dl.acm.org/": 1, "http://research.google.com/": 1, "http://en.wikipedia.org/": 6, "http://lemire.me/blog": 6, "http://oai.cwi.nl/": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["I have always been a fan of the personal computer. I worked all summer once to buy myself a cloned PC XT . I probably would not be a computer science researcher without the personal computer. It has shaped my life. \n We may not remember, but the PC was a somewhat surprising disruption. You could explain the success of some early PCs (e.g., from Commodore) as game machines. But if this is all you had to go on, the success of the IBM PC was puzzling. It was not a very good machine to play games: it had minimal video and sound capabilities. It worked as a word processor, but it was awkward. And yet it soon became your PC, just like you had your car and your phone. It brought freedom. You got software on it to print birthday cards. You wrote your first novel on it. You could run your own business with a PC. \n People have been predicting the death of the PC for decades now. For example, several corporations predicted that thin clients would replace the PC in business. That is, businesses would go back to having a few large well-maintained servers and people would use cheap, interchangeable, devices called thin clients to connect to these servers. The thin clients would require no software-related maintenance. \n Unfortunately, the PCs kept getting cheaper. So the savings from using thin clients instead of PCs were not worth it. And, unlike a thin client, the PC could keep working when the servers were down. Moreover, the CEO has a PC at home: he does not want to switch to a thin client at the office. \n More recently, traditional PCs were almost disrupted by netbooks . Though cheaper and smaller, they were still PCs. And they failed to gain enough traction to displace the PCs. \n Something funny was happening in another market however. Phones were getting more powerful. The iPhone is just as powerful than a PC was ten years ago. Importantly, this meant that a whole set of mobile technologies were getting cheap and widely available: tiny cameras, super small CPUs, and so on. \n So this made the tablet, as envisioned by the scifi authors for decades, a possibility. Apple was the first one to market it successfully as the iPad. Right now, we can buy a Google Nexus 7 tablet for $200. It has a 4-core 1.3GHz processor, 1GB RAM and many things that most PCs did not have even 5 years ago. Oh! Did I mention that it is $200? \n So, I think that this time around, tablets will kill PCs. To be precise, I make the following prediction: \n \nIn some quarter of 2015, the unit sales of tablets will be at least twice the unit sales of traditional PCs, in the USA. \n Greg Linden calls this prediction incautious because experts predict that, at best, the tablet sales will match PC sales by 2015 . Indeed, for my prediction to come true, it is not enough for the tablet market to grow, people must stop renewing their PCs and come to rely on their tablets. If my prediction comes true, the PC industry will have begun a slow march toward irrelevance. \n I even put my money where my mouth is: I bet $100 against Greg that I am right. The loser gets to hand over $100 to a charity chosen by the winner. \n I should make it clear that I am not silly enough to think that I can meaningfully predict the future. But I also think that the analysts have too much confidence in their own predictions. Consider this beautiful quote for example ( source ): \n Another software technology will come along and kill off the web, just as it killed news, gopher, et al. And that judgment day will arrive very soon \u2014 in the next two to three years, not 25 years from now. (George F. Colony, Forrester Research CEO, 2000) \n As far as tablets are concerned, tech. people cannot imagine tablets replacing traditional PCs. Yet I see regular folks like my father forgetting all about PCs and using exclusively his iPad. I have seen people showing up at meetings with tablets for at least a year now\u2026 some of them just regular people who are not trying to look good. They genuinely get a lot out of their tablets. They have been frustrated with their laptops for too long. A lot of these people are decision makers. \n As others have remarked, not everything is great about the tablet. You can write your first novel on it, but I would urge you to get a physical keyboard connected to it (and then it is no longer a real tablet). But PC technology has stagnated. All the faults PCs had 5 years ago are still with us: slow boot sequences, viruses, confusing configurations\u2026 and PCs have gotten less and less hackable. \n I admit, the desktop computer is a good match for our office jobs: a desk with a desktop PC on it. It goes together. I don\u2019t know how companies and governments can reduce their stock of PCs. But they sure spend less money renewing their PCs. They will need to if they keep buying more tablets. \n My prediction conveys also a belief that the typical job will soon be different from the current office job. I believe that we are not out of the Great Recession yet. If jobs are scarce but people need to access the net, then a $100 tablet will be a compelling offer. \n I admit that I am not quite sure how students will get by without a PC, but they won\u2019t be using their PCs to watch videos or read in 2015. PCs will start to look like this old typewriter you have in the basement. \n I also think that tablets are a much better match for retirees. PCs as sold by Dell are currently a mess. They are confusing and hard to maintain. Cheap tablets are a much better match. \n Conclusion : I don\u2019t know whether tablets will kill PCs. But if they do, this will be a big deal and I want to be able to say \u201cI told you so\u201d."], "link": "http://lemire.me/blog/archives/2012/09/10/will-tablets-kill-pcs/", "bloglinks": {}, "links": {"http://articles.businessinsider.com/": 1, "https://plus.google.com/": 1, "http://en.wikipedia.org/": 4, "http://glinden.blogspot.ca/": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["In Computer Science, we often informally judge intelligence by using the Turing test . The Turing test is quite simple: if you can convince an observer that you are a human beings, then you are probably at least as smart as a human being. \n Yet no organization could ever convince you that it is a real person. Corporations and governments make you sign forms. They are rude, impersonal. They typically react slowly. They only apologize after days of consultations with lawyers. No sane human being behaves this way. \n Have you ever tried to have a social exchange with a government or a corporation? No matter how nice they try to be (\u201cyour call is important to us\u201d) they fail to convince. In fact, I would rather deal with software constructs than organizations. They are often much closer to passing the Turing test. \n Why is it then, that so many (both from the left and the right) want to grant more power to these organizations, to rely more fully on them? \n Update : John Regeh contributed the following quote: \n A crowd is not the sum of the individuals who compose it. Rather it is a species of animal, without language or real consciousness, born when they gather, dying when they depart. ( Gene Wolfe )\n \n \n Related posts (automatically generated): \n \n\t\t My Fight Againt Comment SPAM: Spambots pass the Turing test! \n\t\t Comments are back! But you need to pass a reverse Turing test! \n\t\t Breaking news: HTML+CSS is Turing complete"], "link": "http://lemire.me/blog/archives/2012/09/03/organizations-would-not-pass-the-turing-test/", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1, "http://en.wikiquote.org/": 1, "http://lemire.me/blog": 3}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["In Mathematics, we typically require equality to form equivalence classes . That is, it should be reflexive: A should be equal to A. Moreover, it should be symmetric: if A is equal to B, then B should be equal to A. Finally, it should be transitive : if A is equal to B, and B is equal to C, then A must be equal to C. \n These conditions appear to make perfect sense. Yet almost all programming languages fail to enforce reflexivity, symmetry and transitivity. \n \n Languages such as XPath fail to be transitive. Indeed, consider the following numbers\n x = 10000000000000001; \ny = 10000000000000000.0; \nz = 10000000000000000; \n Then x = y and y = z are true, but x = z is false. The problem with XPath is that it automatically casts integer-to-float comparisons to float-to-float comparisons. \n But even if you restrict yourself to integer types, XPath still fails to provide transitivity: (1,2) is equal to (2,3) which is equal to (3,4), yet (1,2) fails to be equal to (3,4). \n If you try the same test with the values x,y, z in JavaScript, you will find that transitivity is preserved because\u2026 x = z. That\u2019s because JavaScript has no integer type. JavaScript uses finite-precision floating point numbers to represent all numbers.\n However, floating point numbers have problems of their own. For one thing, they are not reflexive. That is, there is a value such as x = x evaluates to false. (The NaN marker .) This is only one of several challenges that floating point numbers represent.\n \n \n Conclusion . When working out a theory, it is easy to come up with simple axioms. People often think that software is just a representation of mathematics that can be whatever we want it to be. Thus, software should obey intuitive and simple axioms. However, what we get, instead, are organically grown compromises. There is sometimes little difference between studying software and studying nature. Both can be surprising. \n Update : John Regehr pointed me to comparisons in PHP . It is slightly insane trying to figure out what is equal to what. \n Update 2 : Jeff Erickson reminded me of a very funny presentation by Gary Bernhardt . \n \n Related posts (automatically generated): \n \n\t\t Language, Mathematics and Programming \n\t\t The best software developers are great at Mathematics?"], "link": "http://lemire.me/blog/archives/2012/09/03/programming-elementary-mathematics/", "bloglinks": {}, "links": {"http://php.net/": 1, "https://www.destroyallsoftware.com/": 1, "http://en.wikipedia.org/": 3, "http://lemire.me/blog": 2}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["We would all like to be smarter, to produce better software, better research papers or better art. It is not difficult to see that, by just about any metric, productivity amongst human beings follow a power law. For example, 80% of all research papers are written by 20% of the researchers. (I made this one up, but the actual numbers are similar: see Lotka\u2019s law .) \n However, it is a difficult topic to study because the very idea of measuring intellectual productivity is, at best, controversial. \n Yet we can\u2019t help but ask: what does determine your intellectual productivity? Here are some tentative answers: \n \n Luck . A lot of intellectual work is based on trial and error. A couple of early breakthroughs, due to pure chance, can motivate you and inspire you to work for years. Repeated failures can discourage you and push to abandon leading edge work. Luck alone can explain the power law in the distribution of intellectual productivity. \n Persistence . Some people are pig headed. I sure am. They will keep working even when things are not working. This is closely related to the willingness to take chances. Most great intellectuals dive in and take risks. \n People . Some people drain you without helping you produce. Other people energize you and force you to improve your game. Even though I am an introvert and sometimes an insular person, I have not found a better way to boost my productivity than finding exactly the right people at the right time. Sometimes I will have worked for months on a problem just to have several breakthroughs because I started working with someone smart. The quality of my work can also be strongly affected by the people I work with: people with higher standards than mine tend to push me up, people with lower standards tend to make me more lenient. But it is not just a matter of meeting smart people. I do not think I would get a lot out of meetings with Einstein. I am not interested in Physics right now and I doubt Einstein would share my interests. You need to meet people who have truly compatible goals and interests. Some people have an innate ability to connect with others, to find the right collaborators. Others spend a lot of time meeting people and are therefore more likely to find good matches.Though I have been extremely lucky in this respect for the last few years, collaboration is not and will never be my strong point. \n Method . Some people pay attention to what works and to what does not work. They experiment. They read about how great people worked. They learn meta-strategies like divide-and-conquer . They constantly tune their approach to intellectual productivity. Some people figure out that working seriously a few hours every day is better than doing pseudo-work all day. I find that a common source of low productivity is excessive pondering: you cannot be productive if you fail to deliver in time. Other people just stumble on the right approach early on. For example, ever since I was a teenager, I have collected ideas in a notebook. This means that I am never out of crazy (and usually bad) ideas. \n Focus . Intellectual work is boring in a way that manual labor is not. Intellectual work can become frankly alienating. Some people seem to never get bored. They can do narrow mathematics for days and days without ever getting bored. They are satisfied by the work itself. I have weak boredom resilience myself: I constantly question the relevance of my work . While it sounds good, it can actually be quite bad as you can be far more productive if you just keep your head down and push forward. I also see a lot of people who willingly engage in low productivity activities. Then they wonder why they aren\u2019t being productive! A lot of work is actually busy work : you feel as if you were working, but you aren\u2019t producing lasting value. \n Fire . You have to care to have some intellectual productivity. You must feel bad when nothing was achieved. And I mean really bad. Some people are only driven by fear of losing their financial support. Others cannot stand not producing anything intellectually. They do not wait for permission nor do they plan to retire. These people, those who feel an urge to produce, will naturally be much more productive. \n \n \n Related posts (automatically generated): \n \n\t\t Workout to improve\u2026 intellectual productivity? \n\t\t The secret to intellectual productivity \n\t\t Emotions killing your intellectual productivity"], "link": "http://lemire.me/blog/archives/2012/08/24/to-improve-your-intellectual-productivity/", "bloglinks": {}, "links": {"http://www.daniel-lemire.com/blog": 1, "http://en.wikipedia.org/": 1, "http://lemire.me/blog": 5}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["As an graduate, finding useful references was painful. What the librarians had come up with were terrible time-consuming systems. It took an outsider (Berners-Lee) to invent the Web. Even so, the librarians were slow to adopt the Web and you could often see them warn students against using the Web as part of their research. Some of us ignored them and posted our papers online, or searched for papers online. Many, many years later, we are still a crazy minority but a new generation of librarians has finally adopted the Web. \n What do you conclude from this story? \n Whenever you point to a difficult systemic problem (e.g., it is time consuming to find references), someone will reply that \u201ctime fixes everything\u201d. A more sophisticated way to express this belief is to say that systems are self-correcting. \n Yet few systems are self-correcting. We simply bear the cost of the mistakes and inefficiencies. If some academic discipline fails to make us better off, we barely notice and we keep paying for it. What we see as corrections are often disruptions brought by people who worked from outside the supposedly self-correcting system. \n Far from self-correcting, the system resists changes. For example, lectures are a terrible way to teach, yet they have been around for centuries and we are likely to lecture for at least decades, if not centuries. They are provably not cost-effective from a learning perspective. But it will take strong outside forces to get any change. \n Maybe librarians would have eventually invented the Web. I doubt it, but given enough time everything is possible. Yet \u201cin the long run we are all dead.\u201d (Keynes) The argument that if enough time passes, the problem will be solved mostly make sense if you plan to live forever. Neither you nor our civilization will be around forever. \n We need better science and technological innovation today. It is not ok to say that if we wait another generation, we will find out how to control the climate and generate free energy. Our survival as a civilization depends on our ability to remain innovative today. \n The stock market has been flat for the last ten years. It is not at all automatic that new prosperity will emerge through new innovations and inventions. It may very well not happen in my lifetime. I\u2019m a techno-optimist, so I believe we will out-innovate our problems, but I don\u2019t believe that the system will do it. Crazy people will have to act outside of the norm. We won\u2019t have Internet-enabled brain implants if we just wait long enough: some Ph.D. student needs to sign waivers and make his mother cry so we have any chance to know how it feels to have the Internet in your brain. \n Free-market advocates believe that free markets are self-correcting. That is a more believable notion except for the fact that there is no such thing as a free market in the real world. \n By the way: things often become worse over time. My body is living proof: I\u2019m weaker than when I was in my 20s. \n Conclusion : People too often believe that the systems they are in are self-correcting. Yet corrections, when they happen, are often actually disruptions brought forth by outsiders. Trust that systems, when left alone, will do the right thing, is overly optimist. \n We should celebrate outsiders and protect them from the wrath of the insiders."], "link": "http://lemire.me/blog/archives/2012/08/18/does-time-fixes-all/", "bloglinks": {}, "links": {}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["Can you guess the speed difference between these two lines of code? The first line of code does N additions: \n for ( int i = 0 ; i < N ; i + + ) sum + = arr [ i ] ; \n \n The second line of code does N/16 additions: \n for ( int i = 0 ; i < N ; i + = 16 ) sum + = arr [ i ] ; \n \n A naive programmer might expect the second option to be 16 times faster. The actual answer is much more complicated and worth further study. I have implemented a simple test . \n I used the GNU GCC 4.5 compiler with the -O2 optimization flag. We can compute the complete sum faster using different compiler flags (-O3 -funroll-loops) and I present these results in a separate column (sum-unroll). In this last version, the compiler makes aggressive use of SSE instructions to vectorize the problem. \n \n \n N \n sum \n sum-unroll \n 1/16 \n \n \n 20K \n 1100 \n 6700 \n 20,000 \n \n \n 400K \n 1000 \n 3700 \n 5100 \n \n \n 500M \n 2100 \n 3900 \n 4200 \n \n \n The speeds are expressed in millions of integers per second. \n On tiny arrays, most of the data resides close to the CPU. Hence, computations are essentially CPU bound: doing fewer computations means more speed. \n The story with large arrays is different. There, skipping almost all of the data (15 integers out of 16) only buys you twice the speed! Moreover, once we take into account the vectorized version that the compiler produced (sum-unroll), the difference becomes almost insignificant! \n My source code is available as usual. \n Conclusion : We are in the big data era. Maybe ironically, I sometimes get the impression that our processors are drinking data out of a straw. Whereas a speed of 20,000 million integers per second is possible when the data is cached, I barely surpass 4000 million integers per second when reading from RAM. \n Source : I stole the problem from Elazar Leibovich who posted it privately on Google+."], "link": "http://lemire.me/blog/archives/2012/08/13/on-feeding-your-cpu-with-data/", "bloglinks": {}, "links": {"https://github.com/": 2}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["We can roughly sketch human history as follows: \n \n Initially, everything was expensive for human beings. \n Farming made food cheap. \n The industrial revolution made goods and services cheap. \n In the post-industrial age, we are making the design of new products and services cheap. \n \n Once you adopt this (admittedly simplistic) point of view, a few things about our current economy become clear: \n \n The very concept of a job is squarely rooted in the industrial age. A job is a standardized and regimented occupation. It is expensive to create a new job, but we can amortize the cost over many years. I predict that fewer and fewer people will be interested in creating jobs. It takes too long and it is too expensive. \n Schools and colleges are organized for the industrial age. They train a large volume of people in a consistent manner. The concept of a degree, that is, an expensive badge that you acquire once and amortize over many years, is probably quickly becoming obsolete. It will probably fall along with the concept of a job. \n It does not matter much how many products a company is selling, what matters is whether it can out-innovate its competitors. Corporations may produce more than they ever did in volume alone, but they have never been so short-lived. I don\u2019t worry about how many people Google is serving, I worry about whether they\u2019ll keep innovating fast enough to stay in business. In fact, I worry about whether corporations can even survive in the post-industrial age, as they tend to be geared toward the accumulation of capital, not fast innovation. \n \n This view of the world defines how I view others and my own job: \n \n We used to spend a great deal of time teaching using a standardized format. What I wish I could ask to young people today is: Can you learn something like Calculus very cheaply and very quickly if you need to? What would you do if you needed to learn calculus, do you know? \n So, you are are unemployed or underemployed? You aren\u2019t very rich? I don\u2019t care about any of this. However, have you ever created something new and interesting? There is a counterpart to this: if you are rich and hold a prestigious position, but you are just sitting on top of an industrial machine, then I don\u2019t care about you. You are not interesting. \n \n \n Related posts (automatically generated): \n \n\t\t The Internet is a product of the post-industrial age"], "link": "http://lemire.me/blog/archives/2012/08/03/a-post-industrial-point-of-view/", "bloglinks": {}, "links": {"http://lemire.me/blog": 1}, "blogtitle": "Daniel Lemire's blog"}, {"content": ["The Internet is on fire with this question: who invented the Internet? \n A couple of weeks ago, the president of the USA said: \n Government research created the Internet so that all the companies could make money off the Internet. \n Crovitz replied in the Wall Street Journal : \n It was at the Xerox PARC labs in Silicon Valley in the 1970s that the Ethernet was developed to link different computer networks. \n Manjoo replied in Slate : \n Researchers working directly for the government and at university labs funded by the government were some of the first people on the planet to think up a worldwide network, (\u2026) \n Did Xerox or the US government invent the Internet? \n Neither. \n The Internet is not an industrial product or service. It lacks the uniformity and consistency. Instead, the Internet is one of the first product of the post-industrial age. \n Industrial products, like the iPad, can be neatly attributed to a single individual (e.g., Steve Jobs). They are produced within a regulated and somewhat hierarchical environment. The Internet is of a higher order: it could simply not be invented in this setting. \n Governments and corporations supported specific technologies, like the transistor and TCP/IP. They made money, collected taxes and created jobs. But it is simply not possible to get venture capital to build something like the Internet that nobody can control. Likewise, no government agency would build something it can\u2019t easily regulate like the Internet. The Internet is not a highway. \n Some of us are worrying, instead, that governments and corporations are trying to kill the Internet. After all, the US government specifically wants to install a kill switch on the Internet. Many corporations are complaining that the Internet is a threat to their intellectual property. It is a bit disingenuous that they would also claim credit for it. \n Further reading : The Government Did Too Invent the Internet \n \n Related posts (automatically generated): \n \n\t\t A post-industrial point of view"], "link": "http://lemire.me/blog/archives/2012/07/30/the-internet-is-product-of-the-post-industrial-age/", "bloglinks": {}, "links": {"http://online.wsj.com/": 1, "http://math-blog.com/": 1, "http://www.slate.com/": 1, "http://lemire.me/blog": 1, "http://www.usatoday.com/": 1}, "blogtitle": "Daniel Lemire's blog"}]