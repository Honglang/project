[{"blogurl": "http://martynplummer.wordpress.com\n", "blogroll": [], "title": "JAGS News"}, {"content": ["Xavier Fern\u00e1ndez i Mar\u00edn, who maintains the jags package on Gentoo Linux, writes to tell me he is developing the R package ggmcmc . This package is for visualizing Markov Chain Monte Carlo output using ggplot2 graphics and\u00a0 should complement the existing plots for base and lattice graphics provided by coda . A comparison of all three graphical styles is given below. If you want to know more,\u00a0 see the\u00a0 tutorial by Xavier. \n  Trace plot from the ggmcmc package \n  xyplot using lattice graphics from coda \n  Standard coda trace plot \n The samples are from the Classic BUGS example \u201cOxford\u201d, after forcing\u00a0 JAGS 3.3.0 to use the IWLS sampler in the glm module. As you can see, IWLS has an extremely low acceptance rate in this example, but if you run it long enough then you will see that it does generate valid samples from the posterior."], "link": "http://martynplummer.wordpress.com/2012/10/21/ggmcmc-diagnostic-plots-for-mcmc-with-ggplot2/", "bloglinks": {}, "links": {"http://martynplummer.wordpress.com/": 3, "http://feeds.wordpress.com/": 1, "http://xavier-fim.net/": 1, "http://cran.r-project.org/": 3}, "blogtitle": "JAGS News"}, {"content": ["Bill Northcott\u2019s binary distribution of JAGS 3.3.0 for Mac OS X is now available . This distribution supports 10.8 (Mountain Lion) and 10.7 (Lion). As explained by Bill in the installation manual , changes in Apple\u2019s developer tools make it difficult to support earlier versions."], "link": "http://martynplummer.wordpress.com/2012/10/19/jags-3-3-0-for-mac-os-x/", "bloglinks": {}, "links": {"http://sourceforge.net/": 2, "http://feeds.wordpress.com/": 1}, "blogtitle": "JAGS News"}, {"content": ["In response to a comment by Emmanuel Charpentier, I should write a few words about what has changed in JAGS 3.3.0. \n Firstly, this is entirely a bug-fix release: there are no new features.\u00a0 As the number of JAGS users increases, people are using it in more diverse ways and uncovering\u00a0 bugs that were previously hidden.\u00a0 Feature development has been almost entirely on hold while I hunt down these bugs. \n Bugs that cause JAGS to fail with an error message are the easiest to deal with. Even if the message makes no sense to you, I can usually track down the problem if you send me a worked example.\u00a0 The more concerning kind of bugs are silent. These are bugs that return an invalid set of samples without any warning.\u00a0 We have to wait until JAGS returns some egregiously wrong samples before such bugs are uncovered. \n JAGS 3.3.3 fixes three silent bugs: \n \n It was possible for an initial value to overwrite a data value.\u00a0 This meant that you could get different results from the same model by supplying different initial values, (because in fact you were looking at different data). JAGS will now give an error if you try to overwrite an observed value during initialization. \n The uniform distribution was giving the wrong likelihood when used as an outcome variable.\u00a0 Note that there were no problems with uniform priors on hyper-parameters, which is the most common use of the uniform distribution. \n There were a number of problems with the IWLS sampler in the glm module.\u00a0\u00a0 Fortunately, the IWLS sampler has low precedence so would normally not be used. This has now been more thoroughly tested and it will give correct samples, although it may still have very poor efficiency. \n \n If you want to see more details of what has changed, see the NEWS file in the source distribution."], "link": "http://martynplummer.wordpress.com/2012/10/08/what-has-changed-in-jags-3-3-0/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "JAGS News"}, {"content": ["The source tar ball and Windows installer for JAGS 3.3.0 are now available from Sourceforge . Binary packages for other platforms should be available shortly: see the JAGS homepage for details of how to get hold of a binary version for your platform."], "link": "http://martynplummer.wordpress.com/2012/10/02/jags-3-3-0-is-released/", "bloglinks": {}, "links": {"http://sourceforge.net/": 1, "http://feeds.wordpress.com/": 1, "http://mcmc-jags.sourceforge.net": 1}, "blogtitle": "JAGS News"}, {"content": ["In May we published an article on the burden of cancer attributable to infection in The Lancet Oncology . On the left is Figure 2 from the article, which shows that the majority of the burden is attributable to just four infectious agents. This is essentially the same as the grey scale image we submitted (drawn with R) but has been tinted to match The Lancet\u2019s pink colour scheme. \n On the right is the reproduction of our figure that appears in the September issue of newsletter HPV Today .\u00a0 As you can see, they have added some details to our graph, none of which actually convey information: 3D perspective, gaps in between the components, a shadow effect, and a gentle fading of the background across the x-axis.\u00a0 There is an interesting use of colour too. The deep red colour emphasizes \u201cOther infectious agents\u201d, which was rather the opposite of what we intended.\u00a0 The x-axis label \u201cDevelopmental Status\u201d \u2013 now strangely detached from the rest of the graph \u2013 is the same colour as the bar component corresponding to HPV for reasons that are not entirely clear. \n At least they didn\u2019t turn it into a pie chart."], "link": "http://martynplummer.wordpress.com/2012/09/28/bad-graphs/", "bloglinks": {}, "links": {"http://martynplummer.wordpress.com/": 1, "http://feeds.wordpress.com/": 1, "http://www.thelancet.com/": 1}, "blogtitle": "JAGS News"}, {"content": ["Back in December I wrote about the online machine learning course from Stanford, and how I was looking forward to the course on probabilistic graphical models (PGMs).\u00a0 Unfortunately the second course did not work out so well for me, despite my obvious interest in the topic, and I never completed it. The workload was simply too intense and the weekly deadlines were incompatible with my busy travel schedule during the springtime. \n The Stanford courses are now part of an umbrella organization called Coursera , which is aggregating online courses from Universities all over the world.\u00a0 This 20-minute TED talk \u00a0 by Daphne Koller, co-founder of Coursera and lecturer on the PGM course, explains Coursera\u2019s social mission.\u00a0 I now realise I am not really part of the target audience. In fact Coursera has its eye on the global marketplace for education. By removing\u00a0 financial and logistic barriers to entry they aim to make higher education accessible to people who would never otherwise have a chance to follow such courses. \n An interesting point made by Koller is that online courses generate large amounts of data, which can be analyzed to improve the course the next time it is repeated. The same idea is reiterated by Peter Norvig in a 6-minute TED talk . Norvig\u2019s talk also explains why the weekly deadlines \u2013 which I complained about above \u2013 are necessary to make these courses work. \n Not discouraged by my failure on the PGM course, I have signed up for two autumn courses: Heterogeneous Parallel Programming and Functional Programming Principles in Scala . Both are relatively short (6 and 7 weeks respectively) so should be more manageable although I am sure I will have to give up one of them."], "link": "http://martynplummer.wordpress.com/2012/09/05/online-courses/", "bloglinks": {}, "links": {"http://martynplummer.wordpress.com/": 1, "http://www.coursera.org": 1, "http://www.ted.com/": 2, "http://feeds.wordpress.com/": 1, "https://www.coursera.org/": 2}, "blogtitle": "JAGS News"}, {"content": ["Andrew Gelman has announced the release of Stan version 1.0.0 and its R interface RStan.\u00a0 Stan \u2013 named after Stanislaw Ulam , the inventor of the Monte Carlo method \u2013 is a new MCMC program that represents a major technological leap forward. It works flawlessly on my Linux desktop and is very, very fast. (Note that I have nothing to do with the Stan project: I am just posting this here because it is clearly of interest to JAGS users). \n The starting point for Stan was that some large hierarchical models are intractable by Gibbs sampling.\u00a0 Everyone has come across examples in which the Gibbs sampler will not converge in a reasonable time because the variables are highly correlated. Older BUGS users will remember re-parametrizing their models to reduce this correlation, e.g. by centring predictor variables like this: \n x.mean <- mean(x[]) \nfor (i in 1:N) {\n y[i] ~ dnorm(mu[i], tau)\n mu[i] <- alpha + beta * (x[i] - x.mean) \n} \n OpenBUGS introduced updaters that sample blocks of correlated parameters together.\u00a0 The glm module in JAGS also uses block sampling for more efficient sampling of generalized linear models.\u00a0 Stan takes this approach to its logical conclusion by using Hamiltonian Monte Carlo [1] to do block updating of all the parameters in the model together.\u00a0 It can thereby tackle problems that remain beyond the scope of OpenBUGS and JAGS. \n Stan works a bit differently from OpenBUGS and JAGS. It takes a description of the model and converts it into C++ code, then compiles the C++ program into a standalone executable file. This executable is run on a data set to produce the sampled output.\u00a0 The model description used by Stan is a BUGS-like language, which BUGS users should find easy to read.\u00a0 To help BUGS users further, the Stan User\u2019s Guide contains an appendix that highlights the similarities and differences between Stan and BUGS.\u00a0 The Stan distribution also contains the familiar BUGS examples converted to Stan format. \n Despite the similarities, there are still some surprises for BUGS users. Since the model description is converted to C++ before being compiled, the programming style is imperative, not declarative. This means you can do some things that are impossible in BUGS, such as defining multiple likelihoods for the same variable \n y \u02dc normal(0,1);\ny \u02dc normal(2,3); \n The contributions to the log-likelihood are added together.\u00a0 You can also have local variables that are modified inside a for loop: \n {\n total <- 0;\n for (i in 1:n) {\n  theta[i] \u02dc normal(total, sigma);\n  total <- total + theta[i];\n }\n} \n Rethinking everything from the ground up has allowed the Stan developers to correct some of the design problems of BUGS, such as the counter-intuitive parametrization of distributions in terms of the precision rather than the standard deviation or variance. This was important in the early days of BUGS to allow efficient Gibbs sampling of conjugate distributions. However, nobody can express an informative prior in terms of the precision, which leads to extensive use of parameter transformations and rather verbose BUGS code. \n That\u2019s about all I have to say at this point. Stan 1.0.0 was only released two days ago so I have not had much time to explore further. To find out more, visit the Stan web site: http://mc-stan.org \n [1] For an explanation of Hamiltonian Monte Carlo, see the introduction by Radford Neal in The Handbook of Markov Chain Monte Carlo . The sampler used by Stan is the No U-Turn Sampler , or NUTS."], "link": "http://martynplummer.wordpress.com/2012/09/02/stan/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.crcpress.com/": 1, "http://www.openbugs.info/": 1, "http://andrewgelman.com/": 2, "http://en.wikipedia.org/": 1, "http://mc-stan.org": 2}, "blogtitle": "JAGS News"}, {"content": ["Kodi Arfer sent a script that reveals a bug in the glm module.\u00a0 If you set the random number generator seed in the initial values, then the output from jags should be identical every time you run the model.\u00a0 Unfortunately, Kodi\u2019s script shows that this is not the case.\u00a0\u00a0 In the interests of transparency I am putting a note here on the blog but I currently have no idea how to fix this. \n The script, and some sample output, is below \n \n library(rjags)\n\u00a0\u00a0\u00a0 load.module(\"glm\")\n\nset.seed(55)\n\nilogit = function (v)\u00a0 1 / (1 + exp(-v))\n\nN = 100\nx1 = (1:N - N/2)/100\nx2 = rnorm(N, 0, 3)\nz = .1 + .3 * x1 - .7 * x2\ny = rbinom(N, 1, ilogit(z))\n\nmodel = 'model\n\u00a0\u00a0 {for (i in 1:N)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 {y[i] ~ dbern(ilogit(b0 + b1 * x1[i] + b2 * x2[i]))}\n\u00a0\u00a0\u00a0 b0 ~ dnorm(0, 100^(-2))\n\u00a0\u00a0\u00a0 b1 ~ dnorm(0, 100^(-2))\n\u00a0\u00a0\u00a0 b2 ~ dnorm(0, 100^(-2))}'\n\ngo = function()\n\u00a0\u00a0 {inits = list(b0 = 0, b1 = 0, b2 = 0)\n\u00a0\u00a0\u00a0 jags = jags.model(textConnection(model),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 data = list(N = N, y = y, x1 = x1, x2 = x2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 inits = c(inits, list(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .RNG.name = \"base::Wichmann-Hill\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .RNG.seed = 5\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 n.chains = 1, n.adapt = 200, quiet = T)\n\u00a0\u00a0\u00a0 samp = coda.samples(jags,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 variable.names = names(inits),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 n.iter = 500, quiet = T)\n\u00a0\u00a0\u00a0 list(jags = jags, samp = samp)}\n\nprint(replicate(10,\n\u00a0\u00a0 {x = go();\n\u00a0\u00a0\u00a0 as.numeric(x$samp[[1]][1,1])})) \n The script prints one of the sampled values after 500 iterations. This value should be the same across all 10 replications, which is what we see without the glm module: \n \u00a0[1] 0.1390914 0.1390914 0.1390914 0.1390914 0.1390914 0.1390914\n\u00a0[7] 0.1390914 0.1390914 0.1390914 0.1390914 \n But when the glm module is loaded, the sampled values are not identical across runs \n \u00a0[1] -0.20043641\u00a0 0.38309648\u00a0 0.38309648\u00a0 0.38309648\u00a0 0.38309648\n\u00a0[6]\u00a0 0.38309648\u00a0 0.38309648\u00a0 0.38309648\u00a0 0.38309648 -0.08749869 \n Of course, the statistical properties of the sampled output are still correct. This does not mean you should avoid using the glm module, just that you will not necessarily be able to reproduce the simulations exactly in a later run."], "link": "http://martynplummer.wordpress.com/2012/08/28/lack-of-reproducibility-in-the-glm-module/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "JAGS News"}, {"content": ["Statistical Practice in Epidemiology with R is a one week course aimed at promoting the use of R among epidemiologists.\u00a0 This annual course has been running almost every year since 2004. Last year it took place in Lyon, France, but is now returning to its spiritual home in Estonia.\u00a0 The course will take place from 25 to 30 May 2012 at the Strand Hotel in the seaside resort town of P\u00e4rnu . \n For more details about the course, including how to apply, see the SPE home page .\u00a0 The deadline for applications is April 1 2012."], "link": "http://martynplummer.wordpress.com/2012/02/17/course-statistical-practice-in-epidemiology-with-r/", "bloglinks": {}, "links": {"http://martynplummer.wordpress.com/": 1, "http://www.strand.ee/": 1, "http://feeds.wordpress.com/": 1, "http://www.bendixcarstensen.com/": 1, "http://www.visitparnu.com/": 1}, "blogtitle": "JAGS News"}, {"content": ["JAGS 3.2.0 has been released. The source tarball and the windows installer are on Sourceforge.\u00a0 Binary packages for other platforms should be available in due course, according to the schedules of the various binary maintainers. \n Full details details of the changes can be found in the NEWS file that comes with the distribution, but I would like to highlight two important changes. \n \n Binomial Bug \n This release fixes an unpleasant bug in the Binomial distribution, introduced in JAGS 3.0.0 which gave the wrong likelihood when the size parameter is unobserved.\u00a0 An example of a model that fails is: \n model {\n Y ~ dbin(p, N)\n N ~ dpois(lambda)\n p <- 0.5\n lambda ~ dgamma(0.001, 0.001)\n} \n When Y is observed, the posterior for N should centred around 2*Y, but JAGS 3.0.0 and 3.1.0 gave incorrect posteriors. Thanks to Richard Barker for pointing this out. \n In fact, the posterior distribution of N can be expressed in closed form in this case. With a slight abuse of BUGS notation: \n N | Y, p, lambda ~ Y + dpois(lambda*(1 - p)) \n JAGS 3.2.0 includes a new ShiftedCount sampler, which recognizes this closed form solution and samples directly from the posterior.\u00a0 Likewise, the new ShiftedMultinomial sampler draws N efficiently when it has a prior multinomial distribution, e.g. \n model {\n for (i in 1:ncat) {\n  Y[i] ~ dbin(p, N[i])\n }\n N ~ dmulti(prob[], Ntotal)\n p <- 0.5\n prob[1:ncat] ~ ddirch(alpha[])\n} \n It should be noted that OpenBUGS has had a shifted Poisson sampler for many years, so JAGS is playing catch up here \n Binary GLMs \n Those of you have have used the Holmes-Held sampler from the glm module will know that it can be numerically unstable, and crashes with a frustrating message about this being a \u201cknown problem\u201d.\u00a0 I have spent quite a lot of time improving the implementation of this algorithm and I hope that this problem has now been resolved."], "link": "http://martynplummer.wordpress.com/2012/01/03/new-year-new-jags/", "bloglinks": {}, "links": {"http://sourceforge.net/": 2, "http://feeds.wordpress.com/": 1}, "blogtitle": "JAGS News"}]