[{"blogurl": "http://thegenomefactory.blogspot.com.au\n", "blogroll": [], "title": "The Genome Factory"}, {"content": ["Once you get your names on a few papers, the influx of academic spam increases. I probably get at least two of these spams a day in my inbox (not counting the ones that are diverted by Google's spam filter). I suspect those academics higher up the chain get even more. The most common ones are requests for conference attendance, session chairing, and reviewing; closely followed by international students and graduates looking for jobs or Ph.D. placements. They are nearly always totally unrelated to what I work on (computational genomics).   I got a classic example today:   To: torsten.seemann@monash.spam.edu  Dear Torsten Seeman:   Umm, you spelled my surname incorrectly. Sorry for misleading you with the correct spelling in my email address, websites, blog, and Twitter account.  The four sponsoring societies of DDW invite you to submit an abstract for presentation at DDW 2013 Scientific Sessions, to be held Saturday, May 18-Tuesday, May 21, 2013 in Orlando, FL. The DDW 2013 online abstract submission site is now open and will close on Saturday, December 1, 2012 at 9 p.m. ET.    So it's in \"Orlando, FL\". Because I am Australian, I do have some idea of what else is in the world, and can deduce it is in the USA. But you are ignorant if you think the whole of the world knows all your two-letter USPS state codes! eg. CA = California or Canada? And unlike you, I know Canada is a country and not just a French speaking state of the USA.    <snip> DDW 2013 Abstract Submission Site: http://ddw2013.abstractcentral.com  Poster sessions and DDW programming will start on Saturday, May 18, 2013. If your abstract is accepted, it may be scheduled for presentation on Saturday, Sunday, Monday or Tuesday. Sincerely,  DDW Administration DDWprogram@gastro.org  Hmm, you still haven't explained what the hell \"DDW\" is. Your email domain is giving me a hint, but that was the last line of the email. OK, I'll click on the link to find out - I can't help myself.  \"Digestive Disease Week\" eh?  Thanks for giving me the shits.   [Report Spam] clicked."], "link": "http://thegenomefactory.blogspot.com/feeds/7292590052493700195/comments/default", "bloglinks": {}, "links": {"http://ddw2013.abstractcentral.com/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["Introduction As more labs get into the \"next generation sequencing\" game, they are finding themselves unwittingly entering into the \"high performance computing\" game too. Some lucky labs will be able to access a local or national HPC service, or collaborate with those who do, or pay for access to one like Amazon EC2. But ultimately any bioinformatician will want (and need) a \"general server\" to experiment, develop, and test on. This is particularly true in terms of large, fast storage which is usually in short supply or costly in HPC environments.  Details  With the small lab in mind, here I outline how, for about A$2500/US$2600/ \u20ac 2000), you can build your own server with 6/12 cores, 64GB RAM, and 12TB RAID storage from affordable commodity parts:  CPU  Intel CORE i7 3930K - LGA 2011 $580 The Intel CPUs are about 40% faster for the same clock rate than the equivalent AMD. This is a 3.2 Ghz 6 core CPU (12 threads) and can go to 3.8 Ghz when only using some of the cores. It has good RAM bandwidth too.  CPU Cooler  Thermaltake Contac 39 $49 The above CPU doesn't come with a stock cooler.  Motherboard  Gigabyte GA-X79-UP4 Intel Mainboard - LGA 2011 $269 This motherboard was chosen as it takes the new LGA2011 CPU, has 8 DIMM slots, and 8 SATA ports, including 6.0Gbsp SATA3 ones which we will want for the SSD boot disk described later. We will save money by using Linux software RAID.  RAM  G.SKILL DDR3 32GB (4x8GB) PC-12800/1600 RipjawsZ F3-12800CL10Q-32GBZL $318 (2 kits) Using 8 x 8GB DIMMs for 64GB total RAM. This is about the most you can affordably get on a consumer motherboard today.  Root disk  OCZ Agility 3 Series 120GB 2.5\" SSD Sata 6Gbs $170 (2 drives) For the boot/root disk, we will use 120GB SSD instead of mechanical hard disk, for speed. Two disks in RAID1 (mirror) configuration. This would store the operating system, and any important databases eg. BLAST indices, SQL etc.  Data disk  Seagate Barracuda 3TB $924 (6 drives) In bioinformatics you can never have enough disk space! Here I suggest using six 3TB 7200rpm drives in RAID6 arrangement for a total of 12TB storage. Reading will be fast, but writing a bit slower. We need to compromise on a budget. Case  Antec P280 $135 To hold all this stuff, this case is great value. It has good ventilation, which is crucial for keeping all the disks and the CPU cool.  Power supply  Antec TruePower New Series 650W $120 The case doesn't come with a power supply. I've chosen this one because it has 9 SATA power connectors for the disks, and some unneeded cable sets can be removed in a modular fashion.  Operating system  Ubuntu Server $0 Ubuntu Server edition is pre-tuned for server settings as opposed to interactive desktop use. Because it is Debian based, you have access to far more packaged bioinformatics applications than Centos, including those in the BioLinux project.  Miscellaneous Extra 120mm cooling fans (for the hard disk zones) Extra SATA3 cables (ones with locking clips) CPU thermal paste (the grey stuff) PCIe graphics card (for the screen, any old one will do)  Conclusion For about A$2500/US$2600/ \u20ac 2000 and a little bit of Linux know-how, you can build your own \"high-end\" server. It is missing some of the features of commercial servers from Dell etc (eg. hardware RAID controller, IPMI remote management, hot-swap disks) but it is much more affordable."], "link": "http://thegenomefactory.blogspot.com/feeds/1976432153853214975/comments/default", "bloglinks": {}, "links": {"http://www.com.au/": 8, "http://www.ubuntu.com/": 1, "http://nebc.ac.uk/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["Yesterday we got an email from a Nesoni user who said that the SHRiMP aligner was failing on his FASTQ data. Then again today we got another similar report from our trusted colleague Tim Stinear with the same issue. The evidence was mounting for a bug either in Nesoni or in the FASTQ file, rather than user error . Tim had recently upgraded his PGM Sequencer to Torrent Suite v3.0 (point zero release alarm bells!), and Nesoni saved the shrimp_log.txt file and it contained this:  note: quality value format set to PHRED+33 done r/hr r/core-hr The qv-offset might be set incorrectly! Currenty qvs are interpreted as PHRED+33 and a qv of 62 was observed . To disable this error, etiher set the offset correctly or disable this check (see README).  Wow! The PGM has improved dramatically, calling some bases at Q62, that's better than 1 in 1 million error rate... Here's a breakdown of the Q values in the file:    Symbol ASCII Q+33 Frequency + 43 10 1105774 , 44 11 347753 - 45 12 1167099 . 46 13 673276 / 47 14 137220 0 48 15 225893 1 49 16 1621714 2 50 17 1731775 3 51 18 2726736 4 52 19 4280447 5 53 20 4272951 6 54 21 2556953 7 55 22 7783535 8 56 23 5153631 9 57 24 2362016 : 58 25 2406869 ; 59 26 2517450 < 60 27 5762153 = 61 28 4334469 > 62 29 7109066 ? 63 30 11113780 @ 64 31 13934507 A 65 32 9227417 B 66 33 12758868 C 67 34 8228985 D 68 35 9935410 E 69 36 1459950 F 70 37 2358692 G 71 38 682190 H 72 39 158322 I 73 40 168311 J 74 41 269 K 75 42 199121 L 76 43 83457 M 77 44 4971 N 78 45 464143 O 79 46 8657 P 80 47 0 Q 81 48 0 R 82 49 0 S 83 50 0 T 84 51 0 U 85 52 0 V 86 53 0 W 87 54 0 X 88 55 0 Y 89 56 0 Z 90 57 0 [ 91 58 0 \\ 92 59 0 ] 93 60 0 ^ 94 61 0 _ 95 62 746    Ok, there really are 746 bases with Q62. Some grep work shows me they are all occuring alone in 746 reads, all in position 1 in the read, like this:   @QWRK0:02580:02076 GGGAATCAAAACGCTGATTTTTGATGAACAGAATAACGAA + _ 8,59=<<@@6@BCCDDDFFF3FEEEFAEC@@;77-7777   The table also shows quite a few >Q41 bases, which aren't typically seen in FASTQ files. These ones are probably ok (?) but the Q62 ones surely must be some artifact or bug in the version 3.0 of Torrent Suite. In the meantime, our solution has been this:   sed 's/^_/H/' < dodgy.fastq > fixed.fastq   Be interested to see if others have encountered this problem."], "link": "http://thegenomefactory.blogspot.com/feeds/5619217515017686147/comments/default", "bloglinks": {}, "links": {"http://www.vicbioinformatics.com/": 1, "http://www.edu.au/": 1, "http://ir.lifetechnologies.com/": 1, "http://en.wikipedia.org/": 2, "http://compbio.toronto.edu/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["Introduction Illumina sequencing instruments (HiSeq, MiSeq, Genome Analyzer) can produce three main types of reads when sequencing genomic DNA: Single-end Each \"read\" is a single sequence from one end of a DNA fragment. The fragment is usually 200-800bp long, with the amount being read can be chosen between 50 and 250 bp. Paired-end Each \"read\" is two sequences (a pair) from each end of the same genomic DNA fragment ( more info ). The distance between the reads on the original genome sequence is equal to the length of the DNA fragment that was sequenced (usually 200-800 bp). Mate-pair: Like paired-end reads, each \"read\" is two sequences from each end of the same DNA fragment, but the DNA fragment has been engineered from a circularization process ( more info ) such that the distance between the reads on the original genome sequence is much longer (say 3000-10000 bp) than the proxy DNA fragment (200-800 bp). Single-end library (\"SE\") When we got the original Illumina Genome Analyzer, all it could do was 36 bp single-end reads, and each lane gave us a massive 250 Mbp, and we had to walk 7 miles through snow in the dark to get it. Ok, that last bit is clearly false as we don't get snow in Australia and we speak metric here, but the point is that there is still plenty of legacy SE data around, and SE reads are still used in RNA-Seq sometimes. Let's imagine our data was provided as a standard FASTQ file called SE.fq :  velveth Dir 31 -short -fastq SE.fq  velvetg Dir -exp_cov auto -cov_cutoff auto  I strongly recommend enabling the -exp_cov auto and -cov_cutoff auto options. They will almost always improve the quality of your assemblies.  Paired-end library (\"PE\") Paired-end reads are the standard output of most Illumina sequencers these days, currently 2x100bp for the HiSeq and 2x150bp for the GAIIx and MiSeq, but they are all migrating to 2x250bp soon. The two sequences per paired read are typically distributed in two separate files, the \"1\" file contains all the \"left\" reads and the \"2\" file contains all the corresponding \"right\" reads. Let's imagine our paired-end run gave us two files in standard FASTQ format, PE_1.fq and PE_2.fq : velveth Dir 31 -shortPaired -separate -fastq PE_1.fq PE_2.fq  velvetg Dir -exp_cov auto -cov_cutoff auto  Previously you had to interleaved the left and right files for Velvet, but we recently added support to Velvet for the -separate option which we hope is now saving time and disk space throughout the Velvetsphere! Mate-pair library (\"MP\") Mate-pair reads are extremely valuable in a de novo setting as they provide long-range information about the genome, and can help link contigs together into larger scaffolds. They have been used reliably for years on the 454 FLX platform, but used less often on the Illumina platform. I think the main reasons for this are the poorer reliability of the Illumina mate-pair protocol and the larger amount of DNA required compared to a PE library. We can consider MP reads as the same as PE reads, but with a larger distance between them (\"insert size\"). But there is one technical difference due to the circularization procedure used in their preparation. PE reads are oriented \"opp-in\" (L=>.....<=R), whereas MP reads are oriented \"opp-out\" (L<=.....=>R). Velvet likes its paired reads to be in opp-in orientation, so we need to reverse-complement all our MP reads first, which I do here with the EMBOSS \"revseq\" tool . revseq -sequence MP_1.fq -outseq rcMP_1.fq -notag revseq -sequence MP_2.fq -outseq rcMP_2.fq -notag velveth Dir 31  -shortPaired -separate  -fastq  rcMP_1.fq rcMP_2.fq velvetg Dir -exp_cov auto -cov_cutoff auto -shortMatePaired yes   Early Illumina MP libraries are often contaminated with PE reads (the so-called shadow library ) which are the result of imperfect selection of biotin-marked fragments in the circularization process. There is a special option in velvetg (not velveth ) called -shortMatePaired added by Sylvain Foret which informs Velvet that a paired channel is MP but may contain PE reads, which helps it to account for them and avoid mis-scaffolding. I recommend using this no matter how pure you think your MP library is.  Combining them all! (SE + PE + MP) When de novo assembling multiple libraries in Velvet, you should order them from smallest insert size to largest insert size. For our case, this means the SE first, then the PE, then the MP. Each library must go in its own \"channel\" as it comes from a differently prepared DNA library. Channels are specified in Velvet with a numerical suffix on the read-type parameter (absence means channel 0): velveth \\  Dir 31 \\  -short -fastq SE.fq \\   -shortPaired2 -separate  -fastq  PE_1.fq PE_2.fq \\  -shortPaired3 -separate  -fastq  rcMP_1.fq rcMP_2.fq   velvetg \\  Dir \\  -exp_cov auto -cov_cutoff auto \\  -shortMatePaired3 yes  Note that the -shortMatePaired option has been allocated to channel 3 now (the -shortPaired3 library) as that is the MP channel. Conclusions It's relatively to get up and running with Velvet, but when your projects become more complicated, the methods in this post should help you. But if you prefer a nice GUI to take care of most of the issues discussed here, I recommend using our Velvet GUI called VAGUE (Velvet Assembler Graphical User Environment)."], "link": "http://thegenomefactory.blogspot.com/feeds/7862821123611122573/comments/default", "bloglinks": {}, "links": {"http://bioinformatics.net.au/": 1, "http://www.illumina.com/": 2, "http://biology.edu.au/": 1, "http://emboss.sourceforge.net/": 1, "http://biodiv.tw/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["If you are a bioinformatician working in microbial genomics, then you should know this URL:  ftp://ftp.ncbi.nih.gov/genomes/   If you click on the URL, there is a big list of folders, and it does look like a mess. But for those of us in microbial genomics there are a few key folders you should know about, and probably even have mirrored on your own servers:  ftp://ftp.ncbi.nih.gov/genomes/Bacteria/  ftp://ftp.ncbi.nih.gov/genomes/Bacteria_DRAFT/  ftp://ftp.ncbi.nih.gov/genomes/Plasmids/  ftp://ftp.ncbi.nih.gov/genomes/Viruses/  ftp://ftp.ncbi.nih.gov/genomes/Fungi/  ftp://ftp.ncbi.nih.gov/genomes/Fungi_DRAFT/  Most of my work is in bacterial genomics, so I'll discuss the contents of the first four folders only. I'll leave the last two to an experienced mycogenomicist .  1. Bacteria This directory contains a folder for each completed bacterial genome. That is, the genome has been finished to a single DNA sequence per replicon (usually just one chromosome) and is fully annotated. There are currently around 1000 completed bacterial genomes, of which I've been involved in about 10.  Let's have a look at one. I chose Dickeya dadantii because it's a lovely sounding alliteration for a plant pathogen: ftp://ftp.ncbi.nih.gov/genomes/Bacteria/Dickeya_dadantii_3937_uid52537/  NC_014500.asn 15.9 MB 13/06/2012 12:11:00 NC_014500.faa 1.7 MB 13/06/2012 12:11:00 NC_014500.ffn 4.5 MB 19/11/2011 11:00:00 NC_014500.fna 4.8 MB 29/09/2010 10:00:00 NC_014500.frn 49.1 kB 29/09/2010 10:00:00 NC_014500.gbk 16.7 MB 13/06/2012 12:11:00 NC_014500.gff 1.8 MB 03/04/2012 03:41:00 NC_014500.ptt 407 kB 10/03/2012 13:18:00 NC_014500.rnt 7.1 kB 29/09/2010 10:00:00 NC_014500.rpt 281 B 25/04/2011 10:00:00 NC_014500.val 7.0 MB 13/06/2012 12:11:00  You can see a bunch of files, all with the same prefix (NC_104500) and a bunch of different suffixes or file extensions (gbk, gff) - some of which should be familiar to you. The NC_014500 is the RefSeq accession ID for the single chromosome of Dickeya dadantii. The most important files are:  fna : FASTA file of the chromosomal sequence (think \"n\" = nucleotide) gbk : Genbank file containing meta-data, sequence, and annotations gff : GFF3 file containing annotations only (coordinates relative to the .fna file) faa : FASTA file of the translated coding regions (proteins) annotated in the .gbk/.gff (think \"aa\" = amino acids)  In terms of usefulness, the .gbk file contains (nearly) all the information that the other files contain - the .faa and .fna files are easily generated from the .gbk using BioPerl etc. If you want to get the .gbk files for all the finished genomes, you can download the tarball NCBI provides: ftp://ftp.ncbi.nih.gov/genomes/Bacteria/all.gbk.tar.gz  2. Bacteria_DRAFT This directory contains folders for each draft bacterial genome. That is, the genome has been de novo assembled into contigs/scaffolds (eg. using Newbler for 454 data) but has not been, and probably never will be, finished. They are usually annotated, either by the submitter or automatically by NCBI, but sometimes there may be only sequences. There is about 2600 draft genomes currently.  Here's the contents of the Thiocapsa marina str. 5811 genome folder - it's a purple sulphur coccus from the Mediterranean Coast if you are interested.  NZ_AFWV00000000.asn   13.5 kB 03/04/2012 03:19:00 NZ_AFWV00000000.contig.asn.tgz 1.7 MB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.faa.tgz 1.0 MB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.ffn.tgz 1.5 MB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.fna.tgz 1.6 MB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.frn.tgz 4.1 kB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.gbk.tgz 4.6 MB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.gbs.tgz 4.2 kB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.gff.tgz 393 kB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.ptt.tgz 119 kB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.rnt.tgz 1.5 kB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.rpt.tgz 2.5 kB 21/07/2012 02:13:00 NZ_AFWV00000000.contig.val.tgz 1.6 MB 21/07/2012 02:13:00 NZ_AFWV00000000.gbk   4.7 kB 03/04/2012 03:19:00 NZ_AFWV00000000.rpt   257 B 03/04/2012 03:19:00 NZ_AFWV00000000.val   6.0 kB 03/04/2012 03:19:00  This folder looks a bit different to the finished genomes. It has a .gbk file, but you will notice it is quite small (4700 bytes), and if you look at it, you can see it has no sequence or annotation, only some meta-data and a reference to \" WGS NZ_AFWV01000001-NZ_AFWV01000062\" . This means that this genome record consist of 62 other records; one for each contig in the assembly. These are stored in the compressed tar file NZ_AFWV00000000.contig.gbk.tgz as follows:   % tar ztf NZ_AFWV00000000.contig.gbk.tgz  NZ_AFWV01000001.gbk NZ_AFWV01000002.gbk NZ_AFWV01000003.gbk  ...  NZ_AFWV01000061.gbk  NZ_AFWV01000062.gbk  So, in summary, instead of getting a nice neat single .gbk or .faa file for each replicon as you do for the completed genomes, you get a tarball of files for each assembly, with each file representing a contig in the draft genome. Any extra chromosomes or plasmids will be mixed in the bag of contigs. 3. Plasmids The plasmids folder is not known to many people, it seems a bit hidden away frankly. It contains ~3000 completed plasmid sequences. Confusingly, ~1000 of these are duplicated from the Bacteria folder (as the plasmid was sequenced with its parent), while the other ~2000 are novel. Even more annoying is that the folder structure is different:  faa/  21/07/2012 19:39:00  fna/  21/07/2012 19:40:00  gbk/  21/07/2012 19:41:00  ...  plasmids.all.faa.tar.gz  43.2 MB  23/07/2012 19:43:00  plasmids.all.fna.tar.gz  75.1 MB  23/07/2012 19:43:00  plasmids.all.gbk.tar.gz  199 MB  23/07/2012 19:43:00  ...  Now we have a folder for each file extension, which each contains 3000 files. So the files for a particular plasmid are spread out over multiple folders. Fortunately they provide compressed tar files of the whole archive to download directly: plasmids.all.gbk.tar.gz  4. Viruses Some of you may be wondering why I am including Viruses in this story. Well, some viruses infect Bacteria too - they are called bacteriophage . There are ~3000 folders in the Viruses division, but not all of them are bacteriophage. A simple grep for \" phage \" suggests ~600 are bacterial viruses. The folder structure is the same as for the finished Bacteria genomes.  It is important to realise that most of these virus sequences are natively dsDNA and will also appear integrated into the chromosomal DNA of many of the entries in Bacteria and Bacteria_DRAFT."], "link": "ftp://ftp.ncbi.nih.gov/genomes/Bacteria/all.gbk.tar.gz", "bloglinks": {}, "links": {"http://www.blogger.com/": 5, "ftp://ftp.ncbi.nih.gov/genomes/Bacteria/Dickeya_dadantii_3937_uid52537/": 1, "ftp://ftp.ncbi.nih.gov/genomes/": 1, "http://fungalgenomes.org/blog": 1, "http://en.wikipedia.org/": 1, "ftp://ftp.ncbi.nih.gov/genomes/Viruses/": 1, "ftp://ftp.ncbi.nih.gov/genomes/Bacteria/all.gbk.tar.gz": 1}, "blogtitle": "The Genome Factory"}, {"content": ["A few weeks ago we managed to lure an old colleague , Dave Powell, back into academia to join the VBC team. One of his many talents is software engineering, so I immediately got him working on some projects related to the Velvet de novo assembler that I really wanted to do but could never find time. One of the plans was to write a GUI for Velvet. Browsing sourceforge and the like, it seems a few people had attempted and failed. Last year I supervised a 4th year software engineering group project, who managed to get a basic prototype working, but the code was unmaintainable. So Dave and I planned it out, and he had a prototype done by the next day. He implemented it in JRuby, which can be distributed as a standard portable Java .jar file. We called it VAGUE. However, there are few issues with the current command line Velvet that was limiting the usefulness of VAGUE to its beginner target audience: Paired-end read files have to be interleaved first, even though Illumina produce them as separate files The user needed to know what format they were in eg. fastq, fasta It only supports GZIP compression, even though a lot of sequencing centres are using BZIP2 People don't know what K value to start with We could have put these features into VAGUE, but it seemed much more sensible to put them directly into Velvet itself. So that's what Dave did! With a bit of code refactoring, Velvet can now do these things. velveth now has a -separate option for paired-read files (default is -interleaved ): velveth dir 31 -shortPaired -fastq -separate reads_R1.fq reads_R2.fq  velveth now has a -fmtAuto option which auto-detects file-type and compression-type. Note that each file can be of different format and compression - very elegant. velveth dir 31 -shortPaired -separate -fmtAuto left.fastq.gz right.fa.bz2  velveth now supports BZIP2 compressed files, and if you have pbzip2 (parallel bzip) installed it will use that, and if you have pigz (parallel gzip) it will also use that For the issue of choosing K, I wrote a very simple Perl script which actually counts the K-mers in your actual reads and tells you the K-mer coverage for your target genome for all possible values of K. It's called VelvetK and it has a --best option which we use in VAGUE to automatically choose a reasonable K-mer value to assemble with.  I think these new features and tools will help introduce Velvet and de novo assembly to more people, and hopefully help move power to the biologists and give more time to the bioinformaticians to develop better tools.  VAGUE - Velvet GUI Velvet 1.2.07 - minimum version required for VAGUE VelvetK - optional VAGUE add-on to estimate K using your data VelvetAdvisor - web page to help choose K for your data"], "link": "http://thegenomefactory.blogspot.com/feeds/59416193137978887/comments/default", "bloglinks": {}, "links": {"http://bioinformatics.net.au/": 2, "http://dna.edu.au/": 1, "http://scholar.com.au/": 1, "https://github.com/": 1, "http://www.ac.uk/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["While browsing SeqAnswers.com today I came across a post where Uwe Appelt provided a couple of lines of Unix shell wizadry to solve some problem. What attracted my attention was the following: paste - - - - < in.fq | filter | tr \"\\t\" \"\\n\" > out.fq Now, I've done a reasonable amount of shell one-liners in my life, but I'd never seen this before. I've used the paste command a couple of times, but clearly its potential power did not sink in! Here is the man page description for paste: Write lines consisting of the sequentially corresponding lines from each FILE, separated by TABs, to standard output. With no FILE, or when FILE is -, read standard input. So what's happening here? Well, in Unix, the \"-\" character means to use STDIN instead of a filename. Here Uwe is providing paste with four filenames, each of which is the same stdin filehandle. So lines 1..4 of input.fq are put onto one line (with tab separator), and lines 5..8 on the next line and so on. Now, our stream has the four lines of FASTQ entry on a single line, which makes it much more amenable to Unix line-based manipulation, represented by filter in my example. Once that's all done, we need to put it back into the standard 4-line FASTQ format, which is as simple as converting the tabs \"\\t\" back to newlines \"\\n\" with the tr command. Example 1: FASTQ to FASTA     A common thing to do is convert FASTQ to FASTA, and we don't always have our favourite tool or script to to this when we aren't on our own servers:   paste - - - - < in.fq | cut -f 1,2 | sed 's/^@/>/' | tr \"\\t\" \"\\n\" > out.fa  paste converts the input FASTQ into a 4-column file cut command extracts out just column 1 (the ID) and column 2 (the sequence) sed replaces the FASTQ ID prefix \"@\" with the FASTA ID prefix \">\" tr conversts the 2 columns back into 2 lines   And because the shell command above uses a pipe connecting four commands (paste, cut, sed, tr) the operating system will run them all in parallel, which will make it run faster assuming your disk I/O can keep up.   Example 2: Removing redundant FASTQ ID in line 3   The third line in the FASTQ format is somewhat redundant - it is usually a duplicate of the first line, except with \"+\" instead of \"@\" to denote that a quality string is coming next rather than an ID. Most parsers ignore it, and happily accept a blank ID after the \"+\", which saves a fair chunk of disk space. If you have legacy files with the redundant IDs and want to conver them, here's how we can do it with our new paste trick:    paste -d ' ' - - - - | sed 's/ +[^ ]*/ +/' | tr \" \" \"\\n\"  paste converts the input FASTQ into a 4-column file, but using SPACE instead of TAB as the separator character sed finds and replaces the \"+DUPE_ID\" line with just a \"+\" tr conversts the 4 columns back into 4 lines  That's it for today, hope you learnt something, because I certainly did."], "link": "http://thegenomefactory.blogspot.com/feeds/1470134594100714643/comments/default", "bloglinks": {}, "links": {"http://seqanswers.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["Prokka is a software tool I have written to annotate bacterial, archaeal and viral genomes. It is based on years of experience annotating bacterial genomes, both automatically and via manual curation.   It's main design considerations were to be:  fast  supports multi-threading hierarchical search databases  simple to use  no compulsory parameters bundled databases  clean  standards-compliant output files pipeline-friendly interface  thorough  finds tRNA, rRNA, CDS, sig_peptide, tandem repeats, ncRNA  includes /gene and /EC_number where possible, not just /product traceable annotation sources via /inference tags  useful  produce files close-to-ready for submission to Genbank complete log file   The first release is a monolithic, but followable Perl script. It only uses core Perl modules, but has quite a few external tool dependencies, some of which I can't bundle due to licence restrictions. Eventually I hope to have a public web-server version, and a version of it in the Galaxy Toolshed.   It currently takes about 10 minutes on a quad Intel i7 for a typical 4 Mbp genome.   You can download it from here and read the manual here."], "link": "http://thegenomefactory.blogspot.com/feeds/787571772825153689/comments/default", "bloglinks": {}, "links": {"http://bioinformatics.net.au/": 2}, "blogtitle": "The Genome Factory"}, {"content": ["A while back I came across a cool LEGO model of a Hi-Seq 2000 sequencing instrument created by Dawei Lin at the UC Davis Genome Centre.  My good colleague Tim Stinear was one of the first Australian labs to purchase an Ion Torrent PGM sequencing instrument, which looks like this:    Now, my oldest child Oskar just turned 6 years old, and he was ready for LEGO. Being a former LEGO nerd, my nostolgia went into overdrive and I \"invested\" in some generic LEGO sets for his and Zoe's Xmas presents, subsequently supplemented by some Ebay bulk lots. So now I had an excuse to lie on the rug all day playing with LEGO.  And here is my first attempt at a LEGO model of a PGM:   Not perfect or quite to proportions, but not a bad effort I reckon!"], "link": "http://thegenomefactory.blogspot.com/feeds/8356658362811437558/comments/default", "bloglinks": {}, "links": {"http://www.edu.au/": 1, "http://3.blogspot.com/": 1, "http://www.realtimegenomics.com/": 1, "http://bioinformaticscore.com.au/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["Today I took FASTQ file with 3.5M reads, which was Read1 from a paired-end Illumina 100bp run - it was about 883Mb in size. As many have shown before me, GZIP compresses to about 1/4 the size, and BZIP2 about 1/5. 883252 R1.fastq 233296 R1.fastq.gz 182056 R1.fastq.bz2 I then split the read file into 3 separate files: (1) The ID line, but with the mandatory '@' removed, (2) the sequence line, but uppercased for consistency, and (3) the quality line unchanged. It ignored the 3rd line of each FASTQ entry, as it is redundant. This knocked 1% off the total size. 189588 id.txt 341756 seq.txt 341756 qual.txt 873100 TOTAL Now, I compressed each of the three streams (ID, Sequence, Quality) independently with GZIP. The idea is that these dictionary-based compression schemes will work better on more homogeneous data streams, than when they are interleaved in one stream. As you can see this does improve things by about 15%, but still not as good as BZIP2 without de-interleaving.  20608 id.txt.gz  84096 qual.txt.gz 102040 seq.txt.gz  206644 TOTAL (was 233296 combined)  If we use BZIP2 to compress the interleaved stream, it does only 5% better than when it was a single stream. This is testament to BZIP2's ability to cope with heterogeneous data streams better than GZIP.  16560 id.txt.bz2  66812 qual.txt.bz2  93564 seq.txt.bz2  176936 TOTAL (was 182056 combined)  So in summary, we've re-learnt that BZIP2 is better than GZIP, and that they are both doing quite well adapting to the three interleaved data types in a FASTQ file."], "link": "http://thegenomefactory.blogspot.com/feeds/5597033883407628240/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "The Genome Factory"}, {"content": ["Every command line bioinformatician has a suite of specific utility tools in their $PATH for doing core operations on common data files, such as counting the number of sequences in a .fasta file. Sometimes however, you end up on someone else's server where those tools are no longer available. It's in this situation when a good working knowledge of the standard Unix tools can be valuable. Here's a short list of the most useful ones for chaining together:  cat - show grep - filter sed - modify wc - count cut - extract columns sort - sort uniq - identify duplicates head - extract start tail - extract end expr - arithmetic For example, the classic example is to count the number of sequences in a .fasta file:  % grep '>' in.fasta | wc -l  Because each sequence has a \">\" character for its ID line, the grep selects out all the ID lines, which the number of is equal to the number of sequences. The wc command counts its input, and -l means to count lines, rather than characters or words.  Most of us would be using modern GNU/Unix systems, where the grep command has a -c option which counts matching lines rather than printing them out one by one, removing the need for wc :   % grep -c '>' in.fasta  Please ensure you use quote characters around the \">\", otherwise the shell will think you want to send the output of grep to in.fasta, which will result in in.fasta being truncated to a zero length file. Not ideal.  More commonly today is the .fastq file , used for storing millions of short reads from high-throughput sequencing machines. An example of one entry is shown below:  @ HWUSI-EAS-100R_0002:7:1:2596:12829#ACAGTG/1 TCAAAAATCAGCCGTCACCGAGTATTACCTGAATCACGGCAAATGGCCGGAAAACAC  + HWUSI-EAS-100R_0002:7:1:2596:12829#ACAGTG/1 ffffffffffdfffedaddbaa\\ba\\Yb`]_a`a```_^`_]YT\\Q`]]TT]^BBBB  At first glance, the simplest way to count entries in a .fastq file would be to just extend what we did with .fasta files, but use the \"@\" character for the ID line matching.  % grep -c '@' in.fasta   Unfortunately, this doesn't always work, as \"@\" is also a valid symbol in the encoded quality string! No worries, let's use the \"+\" character on the second ID line. Arrggh, it's also a valid quality symbol! WTF? At this stage you start muttering expletives about moron file format designers, but then calm down when you realise it could have been much worse ie. XML.  Technically, the sequence and quality parts of a .fastq entry can span multiple lines, just like you see 60 column wrapped .fasta files. However, most vendors stick to 4 lines per entry, putting the sequence and quality strings on a single line each. This means we can count entries by dividing the number of lines in the file by four:  % LINES=`cat in.fasta | wc -l` % READS=`expr $LINES / 4` % echo $READS  The above is traditional Bourne shell, but most people are using modern-ish shells like BASH, where this can be written more concisely as:  % expr $(cat in.fasta | wc -l) / 4   That's it for now. More posts to follow."], "link": "http://thegenomefactory.blogspot.com/feeds/1726917915834030017/comments/default", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1}, "blogtitle": "The Genome Factory"}, {"content": ["I don't know what all the fuss is about. If molecular biologists simply learnt some basic Unix command line tools, they could rid themselves of messy PCRs, oligo design, optical maps, primer walking etc. % echo \">ClosedGenome\" > closed.fasta % grep -v '>' 454AllContigs.fna | sed 's/[^AGTC]//gi' >> closed.fasta % mail -s \"Genbank genome submission\" genomes@ncbi.nlm.nih.gov < closed.fasta Too easy! :-P"], "link": "http://thegenomefactory.blogspot.com/feeds/8706677468866766081/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "The Genome Factory"}]