[{"blogurl": "http://www.r-chart.com\n", "blogroll": [], "title": "R-Chart"}, {"content": ["Graph a github user's followers (and follower's followers).   Each programming language tends to develop its own idiomatic set of data structures. In R, data frames are often the structure of choice. JSON (a subset of Javascript) has emerged as an open standard for data interchange that has largely displaced XML for many web based APIs. This post gives an example of how to read JSON from GitHub, manipulate the data within R and produce a graph of the results (like the one above). A few standard R libraries are required: RCurl to retrieve JSON data (or anything else) from a URL rjson to to parse the JSON data In this particular API call ( an example of json returned - no API key is required for the Github API), the JSON data will represent a GitHub user's (first thirty) followers. This implies the use of a graph to represent the data so the igraph library will be used as well.  With the RCurl and rjson libraries available, the json results can be retrieved and converted to an R list as follows: o <- fromJSON(getURL('https://api.github.com/users/EzGraphs/followers'))  You can check the class for yourself using class(o) and view the length of the list using length(o)  To convert the results to the data frame where rows represent followers and columns represent attributes, unlist the results, transpose the results and cast as a data frame: df <- as.data.frame(t(sapply(o, unlist)))  That is the basic process - the rest of the code is the details related to getting the data into the iGraph object which can then be rendered using plot, tkplot (show above) or rglplot."], "link": "http://www.r-chart.com/feeds/1451722197159796986/comments/default", "bloglinks": {}, "links": {"http://developer.github.com/": 1, "http://3.blogspot.com/": 1, "https://api.github.com/": 1, "https://github.com/": 1, "http://cran.r-project.org/": 3}, "blogtitle": "R-Chart"}, {"content": ["There has been a lot of chatter during the past week on HN generated by with Jeff Atwood's \"Please don't learn to code\". Actual posts included: Please don't learn to code (www.codinghorror.com) Please Don't Become Anything, Especially Not A Programmer (learncodethehardway.org) Please Learn to Code (sachagreif.com) Please learn to write (www.randsinrepose.com) Each post had a relatively high number of comments associated with them as well. Granted, Jeff selected a provocative headline that he knew would generate a response. Call it link-bait if you will, but his actual article was pretty well reasoned. A number of the responses and comments suggested that Jeff was promoting some sort of elitist snobbery where the programmer guild was restricting entrance to the uninitiated. The entire reaction by some was predicated by the title of the article alone. Which is why I am going to ask: Please Learn to Read If you are going to react - and react strongly - to an article, be responsible and actually read what the author is saying. Creating a strawman to kick around is great entertainment. If your goal is to entertain, have at it I suppose. If your goal is to inform, as most of the people posting and commenting were, why not respond to the actual content of the author's argument? One of the tremendous values that the HN community has demonstrated over time is a certain objectivity and critical analysis that is not widely available elsewhere. Granted, we all have our biases, but ad-hominem assaults and polarizing characterizations don't promote better understanding of a subject. They tend to shut down substantial conversation and the discourse devolves into a virtual shouting match. Active engagement in a debate is a good thing. Again, the HN community allows for interactive participation in a way that is somewhat unique. It is something worth preserving. It is worth a few minutes of time before firing of a response that indicates the author you are opposing is mindless and malicious. In most cases, he is not - if the post made to the front page of HN chances are that it has been vetted by a few folks and introduces some ideas worth considering. It seems to have become a requirement to title articles in a way that will get them initial attention. This sound-byte mentality is perhaps unfortunate, but to be expected in our day where we filter relevant and interesting information so quickly. It is often worth critiquing the selected title on its own merits or for not associated being with the actual content. It is a different thing to read the title of an article, infer the content of the article based upon this alone (or a cursory reading) and responding on this basis alone. Of course there are qualifications qualifications to what has been stated above... much of the conversation was civil and intelligent, some posts might promote ideas that demand a stronger response, there are links to materials immature, misinformed or downright evil. But there has been a trend recently that became evident enough in this last set of posts to motivate me to say something on the subject. I expect that there will be responses to this post indicating that I question the intelligence of the folks who responded and that I am inferring that they are illiterate ( I am not - many of these folks are downright brilliant in terms of brute intelligence). It is not a question of intelligence. It is a matter of discipline and character. It is a challenge use restraint when responding to another. It is easier to simply allow one's initial reaction to a headline to result in a response that is at best confusing and counterproductive and at worst hurtful. What is easier is not always what is better."], "link": "http://www.r-chart.com/feeds/1813598398331387516/comments/default", "bloglinks": {}, "links": {"http://www.codinghorror.com/blog": 1, "http://hn4d.com/": 1, "http://www.randsinrepose.com/": 1, "http://learncodethehardway.org/blog": 1, "http://sachagreif.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["R often comes up in discussions of heavy duty scientific and statistical analysis (and so it should). However, it is also incredibly handy for a variety of more routine developer activities. And so I give you\u2026 log file analysis with R!  I was just involved in the launch of gradesquare.com (go ahead \u2013 click on the link and check it out. We will still be here later!). With the flurry of recent activity, I needed a way to visualize and communicate site activity to the rest of the team. It only takes a few lines of R to read in a log file (of a reasonable size), format the data, and generate some usable charts. Like most good ideas - it is not new . Most log files follow a similar format (such as common log format ) so there may be some minor variations to the following exercise. The only library that I used for this example was ggplot2 for charts.  library(ggplot2)  Read the Log File A sample of the log file (miserably wrapped - my apologies):  66.12.71.25 - - [21/Feb/2012 23:44:11] \"GET /course/1894/detail HTTP/1.1\" 200 7017 5.0829 66.12.71.21 - - [21/Feb/2012 23:44:39] \"GET /search_by_author?search_learn_exp=Khan+Academy&page=193 HTTP/1.1\" 200 8019 0.3288 66.12.71.25 - - [21/Feb/2012 23:45:21] \"GET /course/19/detail HTTP/1.1\" 200 6851 0.1213 18.4.5.14 - - [21/Feb/2012 23:45:59] \"GET /search_by_subject?search_learn_exp=algebra-i-worked-examples HTTP/1.1\" 200 7939 0.0370  If you can't make that out - just know that it is a relatively typical log file that includes the IP address of the client request, the date and time, the HTTP method and URL path, the HTTP request status code, a count of bytes returned and the time required for the request to process.  The log file can be read into a data frame as follows .  df = read.table('webapp.log') There are a lot of different options available - and you might want to take advantage of these to minimize the amount of additional cleanup required after loading the file. For details: help(read.table)     Clean Up and Format  I chose to clean up manually after the fact. To start, we name the columns in the data frame.  colnames(df)=c('host','ident','authuser','date','time','request','status','bytes','duration')  The date and time were split up when read in above. I am not concerned with the time at this point but do want the date to be cast to a date type. df$date=as.Date(df$date,\"[%d/%b/%Y\")  To see the column names and first few rows of our data frame...  head(df)  There are a number of different ways of getting a quick handle on the data - you could do a summary for instance. One item that you might want to have is a the number of requests for HTTP status. table(df$status)  But the item of immediate interest is simply the number of requests. The following will provide the number of requests by date. reqs=as.data.frame(table(df$date)) R is really great for these quick summarizations, and if you memorize a few functions you will be able to address most needs easily. At a certain point, I can better visualize data problems using SQL, and so use the sqldf library. For now - on to some charts using ggplot2. Make Some Charts   One \"gotcha\" that I hit fairly often with R and ggplot2 is the need to cast variables in a way that allows them to be treated as either continuous or discrete. In the following casting the Var1 field as a Date allows it to be treated as continuous and geom_line() renders a line as intended. ggplot(data=reqs, aes(x=as.Date(Var1), y=Freq)) + geom_line() + xlab('Date') + ylab('Requests') + opts(title='Traffic to Site')  On the other hand, the format function is used in this example to cause the (http) status value to be treated as discrete. ggplot(data=df, aes(x=format(status))) + geom_bar() + xlab('Status') + ylab('Count') + opts(title='Status')    By the way, the images were exported as pngs for the blog by assigning the chart to a variable p and printing like so:   png(\"imagename.png\")  print(p)  dev.off()  So there you have it - functional, useful R that addresses a practical every day need of web developers. It is also a great, practical task that can introduce you to R with a simple relevant exercise that provides immediate value.  The next time Google Analytics falls short, pull out R and give it a try!"], "link": "http://www.r-chart.com/feeds/3427234779316482700/comments/default", "bloglinks": {}, "links": {"https://en.wikipedia.org/": 1, "http://stackoverflow.com/": 1, "http://1.blogspot.com/": 2, "http://www.gradesquare.com/": 1, "http://gradesquare.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["Crazy busy - no time to blog recently. Time enough for pretty pictures based upon previous words though...(thanks http://www.wordle.net )."], "link": "http://www.r-chart.com/feeds/6837203126169082016/comments/default", "bloglinks": {}, "links": {"http://www.wordle.net/": 1, "http://2.blogspot.com/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["Significance Magazine According to Significance Magazine (jointly published by Royal Statistical Society and the American Statistical Association ) the following are the top ten stories of 2010. 1. Progress in the prevention of HIV:  Public health studies result in HIV treatment advancements. 2. Drug regulation: restrictions and retractions: Related to breast cancer and type 2 diabetes. 3. Measuring a teacher's value: LA Times graded teachers based on standards tests results.  4. Political rhetoric finds a helpmeet in statistics: \"a statistical recovery and a human recession.\" 5. Census of Marine Life: The first census of the world's seas completed in 2010. 6. Death of Frederick Jelinek : a pioneer in speech recognition and statistical methods of NLP.  7. The genetic key to Shangri-La: Dr. Paola Sebastiani genetics advancements related to longevity.  8. Screening saves: CT Scanning definitively associated with a reduced risk of lung cancer mortality. 9. Fat kills: Quantitative reviews in various areas of health and nutrition.   10. Words, words words: Culturomics project produces the Google Ngram Viewer .   The details are parceled out in 5 articles: Part 1 | Part 2 | Part 3 | Part 4 | Part 5  Other Stories - or my $0.02. The following are not exactly in the same category as the listings in significance magazine - but they involve personalities and events that affect many members of the R community and have some sort of analytical/statistical significance. World Statistics Day I mean, I missed picking up a greeting card - but the objective of the celebration is pretty worthwhile: building support and better understanding for official statistics among the general public and the policy-makers worldwide.  R-Bloggers  For the R community, R-Bloggers has had a banner year and provided a great deal of visibility for the R community. They are looking for sponsorship - so please consider supporting them.   U.S. Economic News News involved the use of additional zeros tacked on to end of numbers. The recovery.org web site has been somewhat underwhelming. Edward Tufte's nomination to serve on the Recovery Independent Advisory Panel was a fascinating development. His emphasis on clear and truthful presentation of information could be a Good Thing.   New Era of Data Journalism The World Bank has continued to provide more data on economic and social topics.  A couple of blog posts covered this, and an R package is also available to access the World Bank Data API. There has been an increased refinement in data journalism as well as controversy surrounding WikiLeaks during 2010.     Data Marketplace  InfoChimps is pioneering an online marketplace for buying and selling data. Seems that they have a plausible idea - they recently landed 1.2 million dollars in funding.   Benoit Mandelbrot Another noteworthy death this year that was not mentioned was the loss of the \"Father of Fractals\" - Benoit Mandelbrot ."], "link": "http://www.r-chart.com/feeds/1594521702624711592/comments/default", "bloglinks": {}, "links": {"http://www.edwardtufte.com/": 1, "http://www.recovery.gov/": 1, "http://www.org.uk/": 1, "http://ngrams.googlelabs.com/": 1, "http://www.co.uk/": 1, "http://www.amstat.org/": 1, "http://cran.r-project.org/": 1, "http://unstats.un.org/": 1, "http://www.jhu.edu/": 1, "http://infochimps.com/": 1, "http://www.nytimes.com/": 1, "http://data.worldbank.org/": 1, "http://www.r-bloggers.com/": 1, "http://www.statsoft.com/": 1, "http://projects.latimes.com/": 1, "http://www.r-statistics.com/": 2, "http://www.plosone.org/": 1, "http://www.r-chart.com/": 3, "http://ten%20news%20stories%20of%202010%20-%20and%20the%20statistics%20that%20made%20them.%20part%204/": 1, "http://www.culturomics.org/": 1, "http://venturebeat.com/": 1, "http://www.significancemagazine.org/": 5}, "blogtitle": "R-Chart"}, {"content": ["Thanks to everyone who visited and commented here at R-Chart over the last year! Blogging has forced me to crystallize my thoughts and I hope others have benefited a bit from these meanderings. It it great to interact with the knowledgeable, educated and friendly folks in the R community. I make no claims to be an expert or authority on statistics, visualization, design or any of the myriad of other topics touched on over the past year. I appreciate all who have provided encouragement, suggestions and corrections. Unlike many of you more scientifically minded types who meticulously verify all conclusions before speaking, I tend to throw ideas out in the blog and make adjustments and corrections based upon feedback. This is really one of the great values of blogging - and so again, thank you for your responsiveness. It was unexpected and very helpful. Lessons Learned In case you blog or are thinking of blogging, I thought you might be interested in how things have worked here at R-Chart to this point. Make Good Titles It was interesting to find out which items were of most interest (based upon the number of hits per page). A great deal seems to be based upon the headline to the blog - never underestimate the value of a well-constructed-sound-byte of a title. This often dictates the future of a posting. Bad title = no response. I really never gave much thought to how important it is to construct a meaningful, attention grabbing title. Blog Promotion Promotion of each article also took more time than I expected. Tal over at R-Bloggers really does the R community a service - bloggers who sign up have content aggregated automatically. If you want to draw additional readers you have to do a certain amount of footwork yourself. I get about 15% of total traffic to the site from search engines - which is kind of low. Most of the generic sites that I submitted the blog to didn't send any traffic. Content that was of specific interest to a given community ended up resulting in the most traffic. The top sites that have sent traffic this way are shown below. www.reddit.com   15,218 www.google.com  8,932 news.ycombinator.com 7,211 www.r-bloggers.com 4,885 www.dzone.com  3,682 habrahabr.ru    1,167 (Hi to friends in Russia for this - the highest ranking non-English site) twitter.com    689 www.google.co.in    565 www.google.co.uk    531 www.rubyflow.com    470 R is International I was really amazed at the international response - folks from 164 countries around the world hit the blog since its inception. Germany was the top non-English site in total visits and France was also well represented. This probably is of no surprise to many - R has been widely used in academic research and there are a relatively small number of highly specialized professionals around the world using R. It's obvious that the web reaches everywhere - it is not obvious who will end up visiting a given site. Interest as Indicated by Traffic A few other numbers of note: 96,928  R-Chart Pageviews all time history as of 01/31/2010. 620   Downloads of the free R-Chart iPhone application 237   Total days blogging at blogspot (as.Date('2010-12-31') - as.Date('2010-05-08')) 195    Days blog has lived at r-chart.com (as.Date('2010-12-31') - as.Date('2010-06-19')) 158   Comments on this blog  Advertising Apologies to folks who are put off by the advertising. I had a goal to dip into this area a bit to come to offset costs and maybe buy a book or two. This may happen eventually...  $ 42.89 AdSense Revenue $ 13.46 Advertising Revenue through Amazon affiliates  Again - thanks to all - and have a Happy New Year"], "link": "http://www.r-chart.com/feeds/2226286964020013103/comments/default", "bloglinks": {}, "links": {"http://www.reddit.com/": 1, "http://www.rubyflow.com/": 1, "http://www.dzone.com/": 1, "http://news.ycombinator.com/": 1, "http://www.r-chart.com/": 1, "http://habrahabr.ru/": 1, "http://twitter.com/": 1, "http://www.r-bloggers.com/": 2, "http://4.blogspot.com/": 1, "http://www.co.uk/": 1, "http://www.co.in/": 1, "http://www.google.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["A quick follow up to the previous post : about the the scores in the 2010 Google AI competition relative to programming language. The chart above makes each language visible and discrete - and the scales are the same.  library(ggplot2)  df<- read.csv('googleAI2010.csv',sep=';',header=FALSE)  df$V7 <- NULL  names(df)<- c('rank', 'username','country','organization','language','elo_score')    ggplot(data=df, aes(x=rank, y=elo_score, color=language)) +  + geom_point(size=1) +  + facet_wrap(~ language) + opts(title='Google AI 2010: Score by Rank for each Language')  It is based upon a simple comparison of rank and score.    df<- read.csv('googleAI2010.csv',sep=';',header=FALSE)  df$V7 <- NULL  names(df)<- c('rank', 'username','country','organization','language','elo_score')  ggplot(data=df, aes(x=rank, y=elo_score)) + geom_point(size=1) + opts(title='Google AI Score by Rank')  Another approach to viewing this information is a histogram by score (which ignores rank). With a binwidth of 100 (and ignoring low scores of people who signed up but who dropped out relatively early) a (nearly) bimodal distribution appears.   qplot(data=df, x=elo_score, geom='histogram', binwidth=100)  Any ideas about why this is not normal? Is there some aspect of ELO scoring that leads to this shape? Or are there different types of programmers represented? This can be broken down by language. To avoid difficulty distinguishing colors, the rainbow palette is used and a few languages are not reported (since they were not highly represented in the competition). library(sqldf)  df2=sqldf(\"select * from df where language not in ('Groovy','Scala','Go','OCaml')\")  df2$language=factor(df2$language)  qplot(data=df2, x=elo_score, fill=language, geom='histogram', binwidth=100) + scale_fill_manual(values=rainbow(12))    As mentioned in the previous post , the data is available at GitHub - feel free to post some of your own visualizations of this data."], "link": "http://www.r-chart.com/feeds/1042341025364158802/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://3.blogspot.com/": 2, "http://www.r-chart.com/": 2}, "blogtitle": "R-Chart"}, {"content": ["The Google AI Challenge recently wrapped up with a Lisp developer from Hungary as the winner. The competition challenges contestants to create bots that push the limits of AI and game theory. These bots compete against one another, and a complete ranking of competitors is available. The big story today is that the winner ( G\u00e1bor Melis ) used Lisp to beat out over 4000 other contestants around the world using a host of different programming languages.      Paul Graham has stated that Java was designed for \"average\" programmers while other languages ( like Lisp ) are for good programmers. The fact that the winner of the competition wrote in Lisp seems to support this assertion. Or should we see Mr. Melis as an anomaly who happened to use Lisp for this task?  Programming Languages Usage   Java, C++, Python and C# were heavily used overall.   language count(*)  1  Java  1634  2   C++  1232  3  Python  948  4   C#  485  5   PHP  80  6  Ruby  55  7  Haskell  51  8  Perl  42  9  Lisp  33  10 Javascript  19  11   C  18  12  OCaml  12  13   Go  6  14  Scala  4  15  Groovy  1  In the Top 200   language count(*)  1  Java  70  2   C++  64  3  Python  34  4   C#  17  5   C  4  6  Haskell  3  7   PHP  3  8  Ruby  2  9 Javascript  1  10  Lisp  1  11  OCaml  1  Top 100  1  Java  33  2  C++  32  3 Python  20  4  C#  9  5  C  3  6 Haskell  1  7  Lisp  1  8 OCaml  1  Top 10   language count(*)  1  Java  4  2  C++  3  3  C#  2  4  Lisp  1  The plot above is a bit difficult to discern due to the number of languages represented (and similarity in colors). So here is a breakdown by language.  Lisp does appear to be skewed towards higher ranking. But even more striking are the C hippies:  The functional crowd represented with Haskell also ranked on the higher end:  How about Java? There is a trend towards the average - but a significantly larger number of entrants used Java. It also is a language taught in many colleges, and might reflect greater student participation in these languages (although MIT did focus on Lisp back in the day...).  How about representatives from the Microsoft? Einstein and Elvis showed up - Mort was not interested.  I can post charts of other languages if anyone asks - otherwise, download the files for yourself and draw your own conclusions. And congratulations to   G\u00e1bor Melis - I am again feeling the inspiration to delve into the mysteries of Lisp and meander among mountains of parenthesis...  Methodology Used No need to proceed further unless you are interested in how the results listed above were derived. Basically, I used Ruby to scrape the results from the Google AI Rankings site . The results were read into Ruby, and ggplot2 and sqldf libraries were used to analyze the results. Get the Data into R So to find out more...I whipped up a ruby script to create a delimited file from the 47 page listing online. (Feel free to get these from their GitHub location and do some additional validation/analysis of your own). Read this file into R:  df<- read.csv('googleAI2010.csv',sep=';',header=FALSE)  df$V7 <- NULL  names(df)<- c('rank', 'username','country','organization','language','elo_score')  Sanity Check Most of this work can be done in idiomatic R (which has some significant Lisp influences) - which might be a better way to honor the winner. However, I find myself using sqlite more and more these days - particularly in mobile development. So I used the sqldf library which uses this database behind the scenes. Country rankings are available online, and the following emulates these results. Specifically, the number of entrants in the top 200 ranked contestants from each country can be derived as follows:     library('sqldf')     top200=df[df$rank <= 200,]     sqldf('select country, count(*) from top200 group by country order by 2 desc')   Organization rankings are similar, representing the top organizations within the top 100. There are some anomalies here, the highest ranking \"Other\" is not shown in the online version for obvious reasons, and only the most of these have only one entrant in the top 100 an are listed in an arbitrary manner. However, the results are otherwise the same in R.      t op100=df[df$rank <= 100,]   sqldf('select organization, count(*) from top100 group by organization order by 2 desc')     R Code The following are additional snippets of R code used to generate the results above.    # Language Usage    sqldf('select language, count(*) from df group by language order by 2 desc')   sqldf('select language, count(*) from top200 group by language order by 2 desc')  sqldf('select language, count(*) from top100 group by language order by 2 desc')    top10=df[df$rank <= 10,]  sqldf('select language, count(*) from top10 group by language order by 2 desc')   If you fiddle enough with the bucket size for histograms, you might be able to draw some conclusions... but the density plot seemed like a nicer option.     library('ggplot2')   # Substitute your favorite language of those available for Lisp below  qplot(data=df[df$language=='Lisp',], x=rank, geom='histogram', binwidth=1000) + opts(title='Lisp')      # The density plot at the top of this posting:    ggplot(data=df, aes(rank, fill=language)) +  geom_density(alpha = 0.2) +      xlim(0,5000) +    opts(title='2010 Google AI Challenge Rankings')   ggsave('program_language_density_plot.png')   # Breakdown by language:  ggplot(data=df[df$language=='Scala',], aes(rank, fill=language)) + geom_density(alpha = 0.2) + xlim(0,5000) + opts(title='Scala')      Update: I have been keeping up with the comments - and sketched out some other ways of looking at the data in another post ."], "link": "http://www.r-chart.com/feeds/6168320883529187100/comments/default", "bloglinks": {}, "links": {"http://ai-contest.com/": 4, "http://3.blogspot.com/": 2, "http://www.zdnet.com/blog": 2, "http://quotenil.com/": 2, "http://1.blogspot.com/": 5, "http://www.r-chart.com/": 1, "https://github.com/": 3, "http://www.paulgraham.com/": 2}, "blogtitle": "R-Chart"}, {"content": ["Mortgage rates have been at historic lows recently. The rates are posted various places online along with simple mortgage calculators. Such calculators illustrate the payment schedule for a mortgage based upon selected terms. But with less than a dozen lines of R code, you can do a far more sophisticated analysis. Mortgage Calculation Function Rather than reinvent the wheel, you can work with this nice R function by Thomas Girke (Associate Professor of Bioinformatics over at UC Riverside). At the R prompt, you can grab it from its home online by calling source: source(\"http://faculty.ucr.edu/~tgirke/Documents/R_BioCond/My_R_Scripts/mortgage.R\")   This loads the function and outputs a helpful description of the function:   The monthly mortgage payments and amortization rates can be calculted with the mortgage() function like this:    mortgage(P=500000, I=6, L=30, amort=T, plotData=T)      P = principal (loan amount)      I = annual interest rate      L = length of the loan in years  So keep in mind that there is a huge amount of R code available online:  CRAN  Github  Google Code  are just the beginning.   Instant R Graphical User Interfaces Rather than simply calling the function directly, you can quickly construct a GUI input widget using the fgui library. library(fgui) gui(mortgage)   With this trivial invocation, a window pops up.         Not terribly fancy, but about the simplest way you will ever be able to construct a GUI! In this case a mortgage amount of $90,000 for 10 years at 3.75% is illustrated.     After entering these values, click OK to actually call the function. This results in a good deal of interesting output. Close the pop up window and look at the R Console:      Monthly payment: $900.5512 (stored in monthPay)         Total cost: $108066.1   As indicated in this message, an R object named monthPay contains the amount of the monthly payment and can be used in subsequent R commands and calculations. You also are greeted with a graph illustrating annual interest and payments as a stacked bar chart.   Plenty of useful information! But that's not all...  Beyond the Basics You might have noticed a number of messages regarding data stored in R objects. This is where the power of R exceeds that of any standard mortgage calculator. These objects can serve as input to other calculations or charting operations.  The aDFmonth object contains amortization data for each month, while aDFyear contains annual information. In the following example, a new data frame is created from the monthly data that does not include the amortization information and plot it using ggplot2. (The amortization data is a significantly different scale and better viewed independently). library(ggplot2) DF=melt(aDFmonth[-1], id.vars='Year')  ggplot(DF, aes(x=Year,y=value, group=variable)) + geom_line() + facet_wrap(~ variable, ncol=1)   You can quickly manipulate the data frame to view amortization information instead. Use the exact same ggplot call (though the facet_wrap is removed below as unnecessary for a single variable) to create a chart scaled to fit the values relevant to the amortization.   DF=melt(aDFmonth[c(1,5)], id.vars='Year') ggplot(DF, aes(x=Year,y=value, group=variable))+ geom_line()     The limits of calculations and visualizations available in a web calculator or Excel are reached pretty quickly. R provides the means to create relatively full featured solutions in only a few lines of code."], "link": "http://www.r-chart.com/feeds/1179246055042215866/comments/default", "bloglinks": {}, "links": {"https://github.com/": 1, "http://faculty.ucr.edu/": 1, "http://www.bankrate.com/": 1, "http://1.blogspot.com/": 3, "http://3.blogspot.com/": 1, "http://code.google.com/": 1, "http://2.blogspot.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "R-Chart"}, {"content": ["'Indeed, I am moving on: my new project is about methods on how to domesticate the unknown, exploit randomness, figure out how to live in a world we don't understand very well . While most human thought (particularly since the enlightenment) has focused us on how to turn knowledge into decisions, my new mission is to build methods to turn lack of information, lack of understanding, and lack of \"knowledge\" into decisions\u2014how, as we will see, not to be a \"turkey\".' - Nassim Nicholas Taleb  With thanksgiving on the way, an economic lesson provided by a turkey's statistical department seems appropriate. Our turkey - let's call him auRthur - like most turkeys has a statistical department at his disposal. His department is in fact tracking an index - the Turkey Welfare Index which is a reflection of how much the human race cares about auRthur. Notice the relatively positive trend... until Thanksgiving Day...   Evidently, our auRthur's statistical department utilized a model that had some flaws - \"past performance is not necessarily a predictor of future returns\". This is because the harvesting of the turkey is a \"rare event.\" Rare (unprecedented) events are difficult to predict. The story is not terribly amusing to turkeys to begin with - but becomes less amusing to humans when understood as a metaphor of the financial meltdown and statistical modeling in use by banking institutions. Essentially, banking institutions assumed a huge amount of risk because a catastrophic meltdown was simply outside the realm of consideration. It was not represented in most of the models in use. A great and vivid illustration. See Nassim Nicholas Taleb's essay where this chart and illustration originally appeared at edge.org . This article discusses the limits of statistical thinking and is a good springboard to other writings by Taleb - who was a practitioner of risk as he ran a hedge fund for a number of years and saw many of the practices in the financial industry up close and personal. The chart above was created using R and ggplot2. The data frame named DF was populated with data related to the Turkey Welfare Index. > DF  TWI Day color  1 14 1 black  2 15 2 black  3 16 3 black  4 17 4 black  5 18 5 black  6 19 6 black  7 20 7 black  8 -100 8 red   UPADTE: This can be entered in a few different ways. One is through a grid (which requires that you specify the Day as a factor).  DF=edit(data.frame())  DF$Day=factor(DF$Day)  Plotted using ggplot2:   library(ggplot2)   ggplot(data=DF, aes(x=Day, y=TWI, fill=color)) +  geom_bar() +  scale_fill_manual(value= c(\"black\", \"red\")) +  theme_bw() + scale_x_discrete(breaks = NA) +  opts(legend.position='none', axis.title.x=theme_blank(),    axis.title.y=theme_blank(),    title='Turkey Welfare Index')  This included a couple of somewhat unusual settings to shut off labels and axes that results in the simple \"plain\" appearance you see above.  So - Happy Thanksgiving - understand statistics and don't be a turkey..."], "link": "http://www.r-chart.com/feeds/8351171166433109175/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://1.blogspot.com/": 1, "http://edge.org/": 1, "http://www.edge.org/": 2}, "blogtitle": "R-Chart"}, {"content": ["Complaints about the iPhone App Store are not uncommon among developers. The submission process is frustrating at best, you can expect arbitrary rejections, and Apple's policies have not always been particularly open or welcoming. If you make it through the process and get an app accepted, it can be essentially buried where it will remain unused unless you dedicate significant energy to marketing it.  And so I figured I would do a small scale experiment to check App Store response in these areas (and App Store user behavior)....and post the results here in the hopes that developers planning to write iPhone apps would benefit from my experience. Cheaper by the (Half) Dozen Having a limited amount of time available, and appreciating the value of quick iterations in development or in any business process, I limited myself to submitting a few apps with similar functionality and different target audiences. What follows is my findings (with a bit of R code used for analysis). Six free iPhone apps were submitted for publication during a 21 day period between July 23, 2010 and August 13, 2010. Four apps were accepted and two were rejected. The four apps that were accepted:  R-Chart (R Programming Community News) has been available 96 days and averaged 4.44 downloads per day. ( Download it if you want to keep up with this blog and/or R community news).  FRB (U.S. Federal Reserve Board News) has been available 82 days and averaged 4.15 downloads per day.  Duq News (Duquesne University News) has been available 92 days and averaged 2.52 downloads per day.   Visit the Lehigh Valley (Lehigh Valley Tourism Info) has been available 93 days and averaged1.31 downloads per day.  Two apps were rejected  DeSales U (DeSales University News) Blender Buzz (Blender Software Blog/News )  App Functionality and Subject Area All apps contained essentially equivalent functionality, but differed by subject area and graphic and styling qualities. The \" R-Chart \" and \"Blender Buzz\" apps reference resources of interest to software users and were intended to promote this blog and the Blender Buzz blog . They are topical and not limited by locale or institution. \" Duq News \" and \"DeSales U\" provide news from Duquesne and DeSales Universities . \" Visit the Lehigh Valley \" provides information about places and events for visitors to Eastern Pennsylvania. The \" FRB \" app provides latest publicly available news from the U.S. Federal Reserve Board. Review Process and Time It took between 7 and 9 days for the App Store to review an app and either accept or reject it. It does not appear that subject area of the app contributed to its acceptance. Both R-Chart and Blender Buzz were directed at programming communities - one was accepted and the other was rejected. Likewise, one of the University apps was accepted, the other was rejected. Each app was submitted once only. None were resubmitted after initial rejection.   Download Counts A total of 1120 have been downloaded. As noted above, the result is between 1 and 4 downloads per day.     I have not expended much effort in promotion. I tweeted about the R-Chart App and mentioned it here on this blog but never promoted any of the others. So the downloads were the result of folks searching for an app that was of interest to them. Conclusion... So getting an app accepted by the app store is not an insurmountable process - but does require time and planning. It is not an activity that will just take care of itself. And because of inconsistency in the process, you would do better to allot a bit of extra time for the app store process. And unless your app fills a rather unique niche, you will need to do marketing in the same way that you would for a web site or any other resource. R Code used in the Analysis is Below  > # Read in the data  > df = read.csv('app_stats.txt')  > df       App Downloads Submitted_Date Response_Date Response  1     R-Chart  426  2010-07-23 2010-07-30 Accepted  2      FRB  340  2010-08-04 2010-08-13 Accepted  3    Duq News  232  2010-07-26 2010-08-03 Accepted  4 Visit the Lehigh Valley  122  2010-07-25 2010-08-02 Accepted  5    DeSales U   0  2010-08-02 2010-08-09 Rejected  6   Blender Buzz   0  2010-07-26 2010-08-03 Rejected   > # Get the total downloads  > sum(df$Downloads)  >  > Do a plot of downloads     > ggplot(data=df, aes(x=App, y=Downloads, fill=Response))      + geom_bar() + coord_flip() + theme_bw()  > ggsave('app_downloads.png')  >  > ggplot(data=df, aes(x=App, y=as.numeric(Processing_Time), fill=Response)) + geom_bar() + coord_flip() + theme_bw()  > ggsave('processing_time.png')  >     > # Cast the date columns as such    > df$Submitted_Date=as.Date(df$Submitted_Date)  > df$Response_Date=as.Date(df$Response_Date)  >  > # Find out total dates each application was on the market  > as.Date('2010-11-03') - min(df$Submitted_Date)  >  > # Determine total number of days apps were being submitted for review  > max(df$Response_Date) - min(df$Submitted_Date) > > # Processing time at the app store   > df$Processing_Time = df$Response_Date - df$Submitted_Date  >  > # Time each app has been available for download  > df$days_available = as.Date('2010-11-03') - df$Response_Date  >  > # Downloads per day  > df$downloads_per_day = df$Downloads/as.numeric(df$days_available)  >  > # Limit view to selected columns  > df[c(1,7,8)]"], "link": "http://www.r-chart.com/feeds/6556038199488581490/comments/default", "bloglinks": {}, "links": {"http://www.desales.edu/": 2, "http://3.blogspot.com/": 2, "http://itunes.apple.com/": 9, "http://www.r-chart.com/": 1, "http://blenderbuzz.blogspot.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["Yet another reason to check out the ggplot2 wiki!"], "link": "http://www.r-chart.com/feeds/4939968277579407625/comments/default", "bloglinks": {}, "links": {"http://github.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["This is the second post (previous one here ) that provides an analysis of Cramer's stock recommendations based upon the Mad Money Stock Screener as of 10/15/2010.  Recommendations by Segment As mentioned in the previous post , recommendations are referenced either by a number below or by name. # Description  5 Buy  4 Positive  3 Hold  2 Negative  1 Sell  Referencing calls by number is used to provide average scores and to plot results. Also as noted previously, not all segments of the show are available through the Stock Screener. Calls made during the Lightning Round and Mail Bag have an average of less than 4 (positive). Calls driven by questions by the audience have a lower average, while those in interviews are higher.  Segment   Average Call Discussed  4.387156 Featured  4.351351 Interview  4.895954 Lighting Round 3.764465 Mail Bag  3.605691    Calls Trail the Market This is a bit more speculative a representation. The S & P and Down Jones Industrial Average are used in this comparison. The call (on a scale of 1 to 5) is multiplied by a factor so that a smoothed condition mean line is generated on the chart. The factor is arbitrary, it just makes the line fit on the chart in a reasonable location, so the directionality of the call line is relevant - not the degree. The S & P appears as the red line and the average of the calls for each day (times a factor of 3000) appears in blue.  The DJI is multiplied by a factor of 3000 in the chart below.  Although it is not completely clear, it appears that stock picks tend to trail the market movement. For this reason they generally sound plausible. Calls tend to be more pessimistic at the time the market has been moving down. 5 of Cramer's Favorites A few of the stocks that Cramer has recommended in the last year qualify in a special way as \"favorites.\" They are the stocks that appear the most times in the data with only a buy recommendation.       Wynn Resorts (WYNN) had a range of $27.00 between the lowest and highest 19 buy recommendations.         Weatherford Int'l (WFT)        had a range of         $6.18 between its 16 buy recommendations.               NVIDIA (NVDA) had range of 9.05 between its 15 buy recommendations      Cypress Semiconductor (CY) had a range of 4.58 between its 13 buy recommendations.      Teva Pharmaceutical (TEVA) had a 17.03range in its 12 buy recommendations.   Conclusion We are bombarded with predictions and promises regularly in the news and media. Over time, I have grown more and more suspicious of the ability of individuals to consistently predict stock prices by simply having a superficial knowledge of current market motions and a general awareness of current financial news.     I often think about verifying the claims, but often don't have the time... figured I would at least take a cursory look. I hope this sort of thing becomes more common to keep the media honest.   At least Jim Cramer's show is cast somewhat in the realm of entertainment. Again, I'll refer you to Bill Alpert of Barrons who has done more extensive analysis and reporting on Cramer's recommendations."], "link": "http://www.r-chart.com/feeds/7793013875051299367/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://online.barrons.com/": 1, "http://www.r-chart.com/": 2, "http://1.blogspot.com/": 6, "http://4.blogspot.com/": 2, "http://www.thestreet.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["The 2010 ggplot2 Case Study Competition Winners have been announced ! Congratulations to the winners!  Grand Prize : David Kahle, Rice University Finalist : Michael Lavine, UMass Amherst Finalist : Claudia Beleites, TU Dresden & Uni. Trieste Check out their entries to get a glimpse of what is possible with R and ggplot2."], "link": "http://www.r-chart.com/feeds/2241698458412841038/comments/default", "bloglinks": {}, "links": {"http://github.com/": 4}, "blogtitle": "R-Chart"}, {"content": ["Bill Alpert of Barron\u2019s has demonstrated the use of R in financial journalism as he criticized the performance of Jim Cramer\u2019s stock picks. Patrick Burns was an advisor for the analysis done in the article. R was an important tool that allowed them to do their research as indicated by Burns in his article and by Alpert in his presentation and in an article published in R News. Their specific critique were centered around obtaining objective verification of claims by CNBC that by following Cramer's advice, one could beat the S & P 500 index. I recommend you to these sources if you are interested in a more comprehensive analysis.  The following is is based upon data available on the Mad Money Stock Screener as of 10/15/2010.  Mad Money Stock Screener Since Cramer's Mad Money Stock Screener is available on line there is at least an \"unofficial\" group of Cramer's recommendations available to analyze. It is apparent that the data is not complete, as several program segments in the drop down are not represented. Selecting any of the following segments results in no records returned:  Caller's Stock Game Plan Sudden Death  In addition, the date provided includes month and day (but not year information). It appears that data from one calendar year is available through the web site. This was born out by plotting the price at the time of the recommendation on charts for individual equities.  Always A Bull Market Somewhere According to his profile, \"Jim Cramer believes that there is always a bull market somewhere, and he wants to help you find it\". His optimistic, entertaining and confident approach that he exudes on screen is reflected in his history of stock picks. Recommendations are indicated either by a number below or by name:  # Description  5 Buy 4 Positive 3 Hold 2 Negative 1 Sell  Cramer is functioning as an entertainer with financial knowledge. The show is geared towards providing action oriented advice (buy/sell) and tends to be skewed towards positive actions. This fits with his profile description - if there is always a bull market, there is always something to buy.  Analysis of All Recommendations The following jitter demonstrates that a 5 (Buy Recommendation) is most often given, and the smoothing indicates that Cramer is generally positive in his ratings.   The program segment in use might also shed some light on the recommendations given. As mentioned earlier, not every program segment is represented in the data.   The specific totals represented above:    Buy  Hold Negative Positive  Sell   2336  7  229  422  559  The vast majority of the time, Cramer gives a buy recommendation. The second most often provided recommendation is to sell. A clear \"Buy\" or \"Sell\" is certainly more entertaining to hear than a \"Meh... hold.\" None of this in and of itself means that Cramer's ratings are bad or inferior to other sources. It simply serves to illustrate that the program is geared towards entertainment. At best, one might hope that only the clear winners are topics of conversation on the program. However, further analysis at least calls this into question.  Individual Stock Recommendations Not every company has an equal number of recommendations. The top 5 (in terms of total recommendations made) are Apple, Citigroup, Intel, Bank of America and Ford Motor Company. In order to get a sense of the a given recommendation in the scope of wider market history, the data from the stock screener can be superimposed on a stock chart.  Apple   Apple was given a recommendation on 90 occasions (more than twice times as many than the next most popular companies recommended). The average recommendation was 4.933333 and Cramer recommended Buy 84 times and Positive 6 times. The clear upward trend in Apple's price justifies an optimistic view in recent history. None of the remaining stocks in the top 5 recommended fits this pattern though.  Citigroup    Citigroup was given a recommendation on 40 occasions and had an average recommendation of 4.850000, Cramer recommended Buy 36 times and Positive 3 times and Negative 1 time. The negative rating was on the 12/08/2009 Lighting Round.    Intel     Intel was tied for second place with Citigroup with 40 recommendations. It had an average recommendation of 4.900000. He recommended Buy on 36 occasions and Positive on 4 occasions.   Bank of America   Bank of America had an average of 4.820513 in the 39 times it was recommended. It was given a Buy recommendation 34 times, a Positive 4 times and a negative once (During the 11/24/2009 Lighting Round).  Ford Motor Ford Motor Company was given an average of 4.777778 in the 36 times it was recommended (a Buy 28 times and a Positive 6 times).  A Negative Example One additional stock that is of interest is British Petroleum (BP), which had an rough ride this year due to the Deepwater Horizon Oil Spill .     Cramer issued a Sell recommendation on 10 occasions, a Positive on 1 occasion and a Buy on 6 Occasions (a total of 17 recommendations).  Conclusion I'll leave it to you to draw your own conclusions about how to interpret recommendations given by Jim Cramer. For myself, I find him entertaining and well informed about financial news for a wide range of stocks. He also has the track record as a successful hedge fund manager over the course of a number of years. However, I am skeptical about the ability of analysts to consistently predict the direction of the market.  Depending upon the reception of this post, I can provide additional information about the methods used to obtain the data and create the charts above and also show how other stocks recommended on Mad Money have performed. Let me know if you have any interest in the comments."], "link": "http://www.r-chart.com/feeds/1700837471508690785/comments/default", "bloglinks": {}, "links": {"http://www.cnbc.com/": 1, "http://www.flickr.com/": 1, "http://www.r-chart.com/": 1, "http://1.blogspot.com/": 1, "http://online.barrons.com/": 1, "http://burns-stat.com/": 2, "http://4.blogspot.com/": 2, "http://2.blogspot.com/": 5, "http://www.thestreet.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "R-Chart"}, {"content": ["Nicholas Taleb and Recent tweets indicate that Benoit Mandelbrot has died at age 85. Mandelbrot was a French and American mathematician, best known as the father of fractal geometry. His official biography at Yale does not yet reflect this (it was last updated in March 2010) and his Wikipedia page is protected from editing for the moment. UPDATE: The New York Times has confirmed this. He is known for changing the way researchers perceive and characterize the phenomenon of natural growth and for the implications for his work for scientists and mathematicians. But fractals have become iconic to the masses as expressive of the idea that beauty and mathematics are inextricably related. As I mentioned elsewhere my brother and I were inspired at an early age by the wonderful designs that could be created by simple mathematical equations. He will be missed."], "link": "http://www.r-chart.com/feeds/2838026290767710922/comments/default", "bloglinks": {}, "links": {"http://www.nytimes.com/": 1, "http://www.fooledbyrandomness.com/": 1, "http://www.r-chart.com/": 1, "http://twitter.com/": 1, "http://www.yale.edu/": 1, "http://en.wikipedia.org/": 1, "http://upload.wikimedia.org/": 1, "http://kottke.org/": 1}, "blogtitle": "R-Chart"}, {"content": ["In the twisting paths of human discovery, you never quite know what intellectual enterprise is going to result in a world changing discovery. For instance, the mathematical notion of expected value did not grow up in a sterile, academic environment. In 1654 Blaise Pascal was approached by Chevalier de M\u00e9r\u00e9 who was interested in gambling problems. Pascal corresponded with Fermat and thus the mathematical theory of probabilities was born. In recent days reports on economic upheaval have often cast financial industries as institutions based upon greed and power that contribute nothing of value to society. Defenders of the free market are quick to respond with the immediate economic benefits provided by such institutions as they mitigate risk and serve as \"middle men\" in modern financial markets. What is seldom considered is that discoveries in one area often find application in a separate area of life that was never considered during the initial investigation. And so one day, perhaps Wall Street calculations might be put to non-financial use that benefit mankind in other ways. There is historical precedent. For instance, interests in the insurance industry served to popularize and apply a 19th century Belgian calculation in a manner that is now used on modern exercise machines. A Belgian Astronomer: Adolphe Quetelet Adolphe Quetelet (1796\u20131874) was a Belgian mathematician, astronomer and statistician. While studying astronomical activities in Paris he interacted with Joseph Fourier (1768\u20131830), Sim\u00e9on Poisson (1781\u20131840) and Pierre Laplace (1749\u20131827). He went on to put his new found appreciation of probability to practical use in the study of the human body (a subject he had initially approached as a painter and sculptor). One calculation he created, dubbed the Quetelet Index, is a number that expresses a relationship between a person's height and weight. Quetelet was not specifically interested in the use of his index for health purposes, but simply for defining the characteristics of \"normal\" or \"average\" man . The Financial Industry In the mid 20th century actuaries observed increased mortality in overweight policyholders. And so in an effort to construct more accurate mortality tables the relationship between weight and cardiovascular disease became the subject of epidemiological studies. Weight tables were first used to predict life expectancy as far back as 1913. But tables of ideal or desirable weight were developed by the Metropolitan Life Insurance Company in the 1940's. In the 1960s, a small group insurance industry experts began to use the Quetelet Index. But it remained for a an American scientist to perform a comparative study of available indices and rename the Quetelet Index to the form that we know it today where it has become a subject related to health and nutrition . An American Oceanographer, Biologist, and Physiologist Ancel Benjamin Keys was a scientist who wrote an article for the July 1972 \"Journal of Chronic Diseases\" that coined the phrase \"body mass index\" or BMI as a modern designation for the Quetelet Index. Interestingly enough, Keys early studies culminated in a B.A. in economics and political science in 1925. His first Ph.D. was in oceanography and biology but his later work was related to his second Ph.D. focused on physiology. He is best known for two dietary contributions - the K-Ration and the Mediterranean Diet . Keys (and others today ) considered the tendency in the insurance industry to equate relative body weight with excess risk of death to be somewhat simplistic . There is worldwide variation according to diet and physical activity habits. In most industrial countries people in the middle range of body weight are healthier than those at an extreme. Because of these types of concerns, BMI is often considered along with other concerns that can indicate potential health risks. Specifically : A BMI in the overweight category along with certain diseases A BMI of less than 25 and a waist size above the standard (35 for women or 40 for men) The actual BMI ranges considered healthy or at risk are still being debated. In 1998, the U.S. National Institutes of Health changed U.S. definition of normal from 27.8 to 25 to conform to World Health Organization Standards . In addition, other countries in the world are encouraging the upper limit for BMI to be even lower than 25. BMI and R One\u2019s optimal weight can be derived using the BMI and height as follows: optimal_weight = function (height, bmi){  round((height**2 * bmi) / 703)  } A grid, similar to one found here and the chart above can be constructed a script found at GitHub . For example: > create_bmi_dataframe(bmi_end=30)   19 20 21 22 23 24 25 26 27 28 29 30  60 97 102 108 113 118 123 128 133 138 143 149 154  61 101 106 111 116 122 127 132 138 143 148 153 159  62 104 109 115 120 126 131 137 142 148 153 159 164  63 107 113 119 124 130 135 141 147 152 158 164 169  64 111 117 122 128 134 140 146 151 157 163 169 175  65 114 120 126 132 138 144 150 156 162 168 174 180  66 118 124 130 136 143 149 155 161 167 173 180 186  67 121 128 134 140 147 153 160 166 172 179 185 192  68 125 132 138 145 151 158 164 171 178 184 191 197  69 129 135 142 149 156 163 169 176 183 190 196 203  70 132 139 146 153 160 167 174 181 188 195 202 209  71 136 143 151 158 165 172 179 186 194 201 208 215  72 140 147 155 162 170 177 184 192 199 206 214 221  73 144 152 159 167 174 182 190 197 205 212 220 227  74 148 156 164 171 179 187 195 203 210 218 226 234  75 152 160 168 176 184 192 200 208 216 224 232 240  76 156 164 173 181 189 197 205 214 222 230 238 246  77 160 169 177 186 194 202 211 219 228 236 245 253  78 164 173 182 190 199 208 216 225 234 242 251 260  79 169 178 186 195 204 213 222 231 240 249 257 266  80 173 182 191 200 209 218 228 237 246 255 264 273 There is some variation with the government site \u2013 perhaps related to rounding."], "link": "http://www.r-chart.com/feeds/487357515084765504/comments/default", "bloglinks": {}, "links": {"http://www.slate.com/": 1, "http://1.blogspot.com/": 1, "http://profiles.nih.gov/": 1, "http://ndt.oxfordjournals.org/": 2, "http://www.nih.gov/": 3, "http://en.wikipedia.org/": 6, "http://github.com/": 1, "http://mbbnet.umn.edu/": 1, "http://apps.who.int/": 1}, "blogtitle": "R-Chart"}, {"content": ["Physical fitness has become increasingly technical and data driven. I started running a bit in the last few months and have been delving into the prevailing wisdom related to assessing ones health as a baseline for pursuing various fitness goals. Some of the terms related to tracking a heart rate gave me visions of white lab coats, cardiac monitors and sophisticated formulas based upon years of scientific analysis. And while there may be truth to this, the practical reality is quite a bit simpler.  In many workout routines, a target heart rate is calculated which is supposed to identify a range (usually in beats per minute) during an exercise routine that will provide optimal cardiovascular value. The basic idea is that you want a work out that is rigorous enough to derive a benefit from the exercise without harming your body. It is appealing in that it provides an objective measure to evaluate your workout. And once your workout can be measured, it is possible to set goals and work to improve your heath. What is implied in the idea of a target heart rate is that there is some upper limit that cannot safely be exceeded. You might think that you need to be hooked up to a bunch of cardiac sensors to find out this value - and although this might be optimal, it is not the technique used by most folks. Instead, there are relatively simple formulas that are used to calculate a maximum heart rate for an individual. They are usually based only upon age (although some calculations consider gender as well).  Maximum Heart Rate Calculations Various formulas (most of them simple linear formulas at that) have been devised to estimate individual Maximum Heart Rates. However actual maximum heart rates vary significantly between individuals based upon physiology, physical fitness and other factors so the value of the metric is disputed. Nevertheless, I was interested in comparing the available formulas to get a sense of a range (based upon \"ensembling\" if you will) of what is being reported or suggested by health sites, software and machines that use this value. One of the gizmos I have begun using is the Garmin GPS with heart rate monitor. I am impressed with its performance so far. It includes its own software that does most of the types of data aggregation and summary that you would like - but I look forward to geeking out and seeing what can be done with the data in R in later posts.   Method A ruby script was used to create a semicolon delimited file with the maximum heart rate from ages 18 through 90 for various calculation methods described in the Wikipedia article . The resulting data can be read into an R script to produce the charts in this blog. A summary that combines the calculations combined does not make a whole lot of sense since two of the calculations in use are for women only and one is for men only. However, all of the techniques fit within a relatively narrow range (since we human beings aren't quite that random). Besides, the two calculations for women are among the most divergent presented, and so cancel each other out in part (though they probably pull down the average for they younger and older ends of the spectrum).   This average is included in the chart below - which is easier to see if you generate it yourself and stretch it to a size suitable for your monitor.    The only input value considered in calculations is gender - two of which are specific to women and one for men. It seems that the most popular calculations don't bother with gender anyway.  There are a number of possibilities for using R with fitness devices that provide heart rate information, geographic data, time, distance, caloric intake and consumption, etc. I was was not able to find much in the way of open source fitness related calculation software APIs - so this could be an new area for R developers to address. (It also provides some balance to the relatively sedentary life of developing and blogging)."], "link": "http://www.r-chart.com/feeds/571095303318431682/comments/default", "bloglinks": {}, "links": {"http://www.americanheart.org/": 1, "http://1.blogspot.com/": 1, "http://en.wikipedia.org/": 2, "http://4.blogspot.com/": 1, "http://github.com/": 3, "http://2.blogspot.com/": 4}, "blogtitle": "R-Chart"}, {"content": ["In the world of data preparation a common task is to identify duplicate records in a file or data set. A few years ago, I did most development work in Java, and shudder to think of the amount of code required to accomplish this sort of task. Since that time I been involved in many projects that did not require programming in a specific language, but simply \"getting the job done.\" With that in mind, \"removal of duplicate records in a file\" can be construed as manipulating a data set rather than an exercise in file processing. The following shows how R compares with other technologies when performing this task.  A file named \"file.txt\" containing semicolon delimited records appears in the examples below:  1;abc;123;etc  1234;qwer;4321;etc  1235;asdf;4341;etc  1;abc;123;etc  1234;qwer;5555;etc   Identifying rows in which every field is duplicated is relatively straight forward using Unix utilities. The file can be outputted using the cat (concatenate) command, the results sorted, and a unique list of results (prefaced by the count of occurrences in the file) can be filtered by a regular expression that indicates any row that has a number of occurrences not equal to one.   cat file.txt | sort | uniq -c | grep \"^ [^1 ]\"  Often, it is more concise to speak Unix than English. This starts to break down a bit when considering each line in the file as a record with distinguishable fields.  General purpose programming languages can do the same thing, but are a bit more verbose. In ruby, the file can be read into an array of arrays.    a=File.open('file.txt').readlines.map{|l|l.split(';')}  A list of unique rows can be outputted using the following one liner:    a.uniq.each{|l|puts l.join(';')}  And with a bit more effort, you can write a program that will filter the results as needed. This type of processing can also be done declaratively in R where such a file is read in as a data frame.    df=read.csv('file.txt',sep=';',header=FALSE)   unique(df)  The duplicated function can also be used to identify the single row that is duplicated.   df[duplicated(df),]  That is to say, the all fields in all records in the following data frames are equal.   unique(df)==df[!duplicated(df),]  The situation gets a bit more complicated when you want to only use some of the delimited fields to identify duplicate records. In the data set above, rows 1 and 4 are identical. Consider the requirement to recognize lines 2 and 5 as identical (due to the first two fields matching). In Unix, you could use awk with the -F option to process the delimited fields. In ruby you could store key fields in variables and compare them with each row. If you come from the SQL world, you could use the R sqldf package to treat the data frame as a database table.   sqldf('select * from df group by V1, V2 having count(*) >1')  The R way of getting this information is to identify the indices of the duplicated rows.   df[duplicated(df[c(1,2)]),]   I enjoy looking at the overlapping aspects of programming languages and utilities (like this OTN Article from a few years ago). It is helpful for highlighting the right tool for the right job, and aids in communication with other technical professionals."], "link": "http://www.r-chart.com/feeds/5372288810399868062/comments/default", "bloglinks": {}, "links": {"http://2.blogspot.com/": 1, "http://www.oracle.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["Just announced: World Bank Data features and data are available. Previous posts have demonstrated how to access and plot this data using R (including the use of the R WDI package ). The chart above can be created using the following program in R.  library(ggplot2) library(WDI)    DF <- WDI(country=c(\"US\",\"FR\",\"DE\",\"GB\",\"CN\",\"RU\",\"IN\"), indicator=\"SP.POP.TOTL\", start=1990, end=2008)   ggplot(DF, aes(year, SP.POP.TOTL, ggplot(DF, aes(year, SP.POP.TOTL, color=country))+geom_line(stat=\"identity\")+theme_bw()+xlab(\"Year\")+opts(title=\"Total Population\")+ylab(\"\")"], "link": "http://www.r-chart.com/feeds/9014193283880751967/comments/default", "bloglinks": {}, "links": {"http://data.worldbank.org/": 1, "http://2.blogspot.com/": 1, "http://www.r-chart.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "R-Chart"}, {"content": ["... or what I did on my summer vacation...  Just got back from the Elder Research Two Day Course \" Tools for Discovering Patterns in Data \". It was a great course that (while not R specific) provides a great overview of Data Mining tools and techniques and insight into current applications in a wide variety of industries.   Dr. Elder is a coauthor of a book available online (and provided with the course) called \"Handbook of Statistical Data Analysis and Data Mining Applications.\" This book contains a wealth of practical examples and tutorials (most using the Statsoft Statistica software). It has a decidedly practical emphasis that allows you to see how algorithms are used to discern patterns in the data and to evaluate and compare how effective they are with specific data sets. Functional areas covered in the tutorials include aviation safety, movie box office receipts, customer services, credit scoring, automobile brand review, quality control, business administration in a medical industry, psychological evaluation, dentistry and profit analysis. This is very helpful for those who prefer to work from the concrete to the general (rather than being provided mathematical abstractions that you then apply to specific situations). They might also be helpful for showing a business user why data mining matters and what value it brings to a business or organization.    The conference covered many of the same topics discussed in Introduction to Data Mining by Tan, Steinbach and Kumar. However, there were many more concrete examples and applications of techniques in specific areas of finance, industry, government and education. A section of the book on ensemble methods is included in a larger section simply titled \"Classification: Alternative Techniques\". Dr. Elder went into greater detail on these topics and demonstrated the effectiveness of combining multiple models into a single model that is usually more accurate than the best of the individual component classifiers. It seems that different classifiers \"see\" certain parts of data sets better than others, and that combining classifiers results in a final analysis in which the best (most accurate) elements of each classification are retained while the worst aspects are largely ignored. By combining classifiers and manipulating the training set and input features a more accurate final model can be obtained.   More detail about ensemble methods is available in another book coauthored by Dr. Elder entitled Ensemble Methods in Data Mining. This book goes into greater detail about how and when to use ensembling and includes some examples in R. The use of multiple classification techniques raises a number of interesting issues - on the one hand they seem to work in practice, but there use makes it more difficult to trace how a final combined model is constructed from the original data set. This has raised some interesting issues about the definition of complexity and the quest for simple accurate models. Dr. Andrew Fast presented on Text Mining and Social Network Analysis - and provided some valuable insights into these rapidly developing fields. There were also a number of software demos and time to interact with other members of Elder Research staff and conference participants.   The conference took place in Charlottesville VA which is a great setting with many historical and recreational attractions nearby. So that's what I did on my summer vacation..."], "link": "http://www.r-chart.com/feeds/718438522319707387/comments/default", "bloglinks": {}, "links": {"http://www.datamininglab.com/": 2, "http://www.blogger.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["As announced by David Smith over at Revolution Analytics , a ggplot2 Case Study Competition is on...      Rather than blogging for the last few days, I cobbled together an entry . It is not a particularly mind bending use of ggplot2, but the subject matter is relatively original. It is an brief analysis and visualization of a J.S. Bach 2 Part Invention. And because Bach's music is so well structured, the visualization itself is nice looking and well balanced. Perhaps suitable for geeky tee shirts...  Check it out when you get a chance."], "link": "http://www.r-chart.com/feeds/7052403659735605620/comments/default", "bloglinks": {}, "links": {"http://upload.wikimedia.org/": 1, "http://github.com/": 3, "http://blog.revolutionanalytics.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["So if you had a robot that was an expert at botany - would you have a bot botanist? Among other things, it would need to to distinguish flowers through vision and image processing, and be able to classify various kinds of plants based upon specific characteristics. What do both of these requirements have in common? They can be done using the k-means clustering . Image segmentation can be used to allow our robot to recognize objects. Based upon petal and sepal size, it could determine say - the species of an iris. The well-known iris data set has been featured in other posts . K-Means in R If you look up the k-means algorithm online or in a reference book, you will be met with a flurry a mathematical symbols and formal explanations. The basic principal (informally stated) is rather simple... given set of observations (picture a scatter plot of points), and a number of groups or clusters that you wish to group them in, the k-means algorithm finds the center of each group and associates observations with the groups with the \"closest\" center. To use k-means in R, call the kmeans function with a matrix of values and the number of centers. The function seeks to partition the points into k groups (the number of centers) such that the sum of squares from points to the assigned cluster centers is minimized. Each observation (point) belongs to the cluster with the nearest mean. How-To To start, we will copy the iris data set to a separate data frame. Not strictly speaking necessary, but makes it easier me to reflexively enter df whenever the data frame is in view. Next we create a matrix object containing only the Petal Length and Width.  df=iris  m=as.matrix(cbind(df$Petal.Length, df$Petal.Width),ncol=2) Now we will do the actual clustering.  cl=(kmeans(m,3)) Simple eh? The cl object contains a number of interesting attributes associated with the model.  cl$size  cl$withinss  Next we do a bit of data formatting and preparation for subsequent calls to graph the data. Notice that we add the cluster information back to our original data frame. This is a good organization of the data and also a requirement for working with ggplot2 which is designed to use data frames. df$cluster=factor(cl$cluster)  centers=as.data.frame(cl$centers) The following graph color codes the points by cluster. We also add the centers and a semi transparent halo around the center to emphasize the place of the center... and its role in classifying the observations into clusters. library(ggplot2)  ggplot(data=df, aes(x=Petal.Length, y=Petal.Width, color=cluster )) +  geom_point() +  geom_point(data=centers, aes(x=V1,y=V2, color='Center')) +  geom_point(data=centers, aes(x=V1,y=V2, color='Center'), size=52, alpha=.3, legend=FALSE)   This plot is an interesting example of how several different sets of data (in this case the actual observations as well as the centers) in separate data frames can be included in a single ggplot2 chart. Misclassified Observations The models is pretty accurate, but not perfect. The following SQL statement highlights the few misclassified observations: sqldf('select Species, cluster, count(*) from df group by Species, Cluster')     Species cluster count(*)  1  setosa  2  50  2 versicolor  1  48  3 versicolor  3  2  4 virginica  1  6  5 virginica  3  44  So we grab the outliers into their own data frame....  df2 = sqldf('select * from df where (Species || cluster) in     (select Species || cluster from df group by Species, Cluster having count(*) < 10)')  Now we can enhance the previous graph to put a diamond around misclassified points. l ast_plot() + geom_point(data=df2, aes(x=Petal_Length, y=Petal_Width, shape=5, alpha=.7, size=4.5), legend=FALSE)   And so with a bit of Data Mining knowledge and the R programming language, even our machines can stop and smell the roses..."], "link": "http://www.r-chart.com/feeds/6836947123869493862/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://en.wikipedia.org/": 2, "http://www.r-chart.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["The NIST 's The Engineering Statistics Handbook includes an Introduction to Time Series Analysis which provides a great way of demonstrating how R can be used to make such calculations. This post replicates the analysis of the data set introduced under Averaging Methods using R. As you might expect, Time Series Analysis is a broad subject that has been investigated in depth elsewhere. If you need more information, a book such as Time Series Analysis and Its Applications provides a much more in depth look at the mathematical theory involved as well as providing practical examples of the use of R for analysis and forecasting. But back to the NIST handbook... the data set they used represents supplier deliveries to a warehouse. The calculations that follow demonstrate how to perform the calculations they do in this section of the handbook using R.    Supplier Amount(in 1000 of $)     1   9     2   8  3   9     4   12     5   9     6   12     7   11  8   7  9   13  10   9  11   11  12   10  Simple Average (Mean) In R the series can be represented as a vector. v=c(9,8,9,12,9,12,11,7,13,9,11,10) The average of the series is 10. mean(v)  The \"error\" amount that each entry in the vector differs from the mean can be calcuated as follows. s - mean(s)  This value can serve as the basis for a measure to ascertain how well a model fits (Error Squared). (v - mean(v))^2  Finally, the sum or mean of these results can be used to compute values that represent the overall fit (or amount of error) for the estimate. sum((v - mean(v))^2) # SSE\" is the sum of the squared errors.  mean((v - mean(v))^2) # MSE\" is the mean of the squared errors. Now that we have a simple values that indicate how good an estimate for a set is, we can test with other values. Rather than writing out an entire calculation each time, we can create a function in R and apply the function to each value in a vector. sse = function(x, series){sum((series - x)^2)}  mse = function(x, series){mean((series - x)^2)} To compare the estimate (10) with 7, 9, and 12. sapply(c(7,9,10,12),sse,v)  sapply(c(7,9,10,12),mse,v)  Analyzing Time Series Data A time series is simply a sequence of data points in time. Time series data has unique characteristics which allow it to be processed in a similar manner regardless of the underlying data represented. Many disciplines deal with this type of data including statistics, signal processing, econometrics and mathematical finance. Such data appears in business in relationship to sales forecasting, budgetary analysis, yield projections, and in the process / quality control arena. In other blog entries, they are used in relation to stock market analysis and economic data . They are relevant to web sites and are available through tools like Google Analytics. So time series data is widely applicable but has common features regardless of its application. It can be analyzed to identify its characteristics and patterns. This often leads to forecasting in which a model is used to predict future events based upon past data. All time series data has the following common qualities: a natural temporal ordering often events that are close together are generally more closely related than those further apart in most cases, past values are assumed to influence future values (rather than the other way around) usually spaced at uniform intervals The data set we are working with is a bit odd to consider as a time series - a supplier is not a unit of time. However, it is useful for making the point that a \"simple\" average (or mean) of all past observations is only a useful estimate for when there are no trends. Not sure what to make of this. I emailed the government and asked for clarification. Will post the answer here if I receive a response.   In R, a vector can be cast to a time series object as follows: s=as.ts(c(9,8,9,12,9,12,11,7,13,9,11,10))  Moving Average A moving average is described in the NIST Handbook and is also referred to as \"smoothing\" - a term that comes up in ggplot2 (geom_smooth). There are a myriad of functions available in R that involves some sort of lagged calculation of a series of numbers. A simple example that almost does the trick involves rollapply: rollapply(s, 3, mean) This works, but it is not clear that the first two entries were skipped. Better to use a library that has additional checks coded in... library(TTR)  SMA(s,3) If you take a look at the code inside... you can get an idea of the additional verification and error checking (which accounts for missing values at the beginning of the list). To view the source, simply input the function name without any parenthesis: SMA You can drill down into the internally called methods in this case: runMean  runSum With this method available, we can calculate the Error and the Error Squared: s - SMA(s,3)  # Error  (s - SMA(s,3))^2 # Error Squared Note that the calculated mean replaced missing entries as zeroes... x=((s - SMA(s,3))^2)  x[ is.na(x) ] <- 0  mean(x)  Oh - in case you were interested in the plot: library(ggplot2)  df = as.data.frame(as.ts(v))  df$idx = as.numeric(rownames(df))    df$x= as.numeric(df$x) qplot(data=df, idx, x) + geom_line() + geom_smooth()"], "link": "http://www.r-chart.com/feeds/4802408713335086512/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://www.nist.gov/": 6, "http://www.r-chart.com/": 2, "http://en.wikipedia.org/": 2, "http://www.amazon.com/": 1}, "blogtitle": "R-Chart"}, {"content": ["Atte Tenkanen had a blog on fractals using R for a time. Much of his source code is still available online. To produce his version of the Mandelbrot set:  source(' http://users.utu.fi/attenka/mandelbrot_set.R ')  Fractals (such as the Mandelbrot Set pictured above) are objects that display self-similarity on all scales. Fractal are mathematical concepts with practical applications. For example, fractal dimensions provide a solution to measuring a coastline - where you come up with different lengths depending upon the length of the ruler you use. (the Coastline Paradox ). There also pretty and fun to look at.... Back in the 80's my little brother and I would type in fractal equations into a Radio Shack Color Computer. We would spend half a day typing in a program and debugging it and the other half a day waiting for the image to render. He later improved the process by using a Commodore Amiga and more than two fingers for typing.  You can also do this type of plot using ggplot2 - and most of the effort is related to turning off axes and legends. The source is on github and can be run from there. source(' http://github.com/ezgraphs/R-Programs/raw/master/mandelbrot.R ')  So now I can download source code from somewhere out on the interweb thingy and use open source software to render these images in a matter of seconds.      Just like we used to do when we were kids...."], "link": "http://www.r-chart.com/feeds/7919296186803098682/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 2, "http://fractalswithr.blogspot.com/": 1, "http://users.utu.fi/": 2, "http://en.wikipedia.org/": 3, "http://github.com/": 1}, "blogtitle": "R-Chart"}]