[{"blogurl": "http://geomblog.blogspot.com\n", "blogroll": [], "title": "The Geomblog"}, {"content": ["I was in Aarhus recently for a MADALGO workshop on large-scale parallel and distributed models , where I did a sequence of lectures on GPU algorithms . I was briefly interviewed by a university reporter for an article, and did a little video on why I think big data/big iron problems are interesting. \n \nAt the risk of embarrassing myself even more than I usually do, here's the video . Note that this was recorded at a time of great crisis across the globe, when all hair styling products had mysteriously disappeared for a few days."], "link": "http://geomblog.blogspot.com/feeds/6248061110420783467/comments/default", "bloglinks": {}, "links": {"http://katrinebjerg.au.dk/": 1, "http://feedads.doubleclick.net/": 2, "http://www.au.dk/": 2}, "blogtitle": "The Geomblog"}, {"content": ["We had an incredible hiring season two years ago, making seven offers and hiring seven new faculty. And now we're doing it again ! \n \nOur department is looking to hire five new faculty (at least four are at the assistant professor level). I'm particularly excited that we're hiring two people in the general area of big data (following up on our data mining and database hires from two years ago). \n \nOne slot is in what I'll call \"big data meets big performance\": I'll have more to say about this shortly, but the challenges of large data analysis are not just about managing the data, but about managing large numbers of machines to crunch this data (MapReduce is perhaps the most well known example of this). We're looking for people who can \"speak extreme data and extreme processors\" fluently - these could be on the data/systems management side, or on the analysis side, or the modelling side. \n \nUtah has a strong presence in high performance computing (the Supercomputing confererence is happening in Salt Lake, and Mary Hall is the general chair), and we're one of the few places that has a good understanding of both sides of the large data story (i.e machines and bits). \n \nThe second slot is in machine learning, with ties to language. Text (and language) provide one of the best large data sources for scalable machine learning, and we're looking for people interested in the challenges of doing ML at scale, especially when dealing with NLP. If you're that person, you'll be coming into a department that has the entire range of data analysis folks from algorithms to analysis to systems to NLP (with many other faculty that are heavy users of ML technology). \n \nOur plan, once we fill these slots, is to make Utah a one-stop shop for large-scale data analysis and visualization - in addition to the above slots, we're also looking to hire in information visualization to complement our strong viz presence. \n \nIn addition to the above slots, we are also hiring in computer security and HCI/user interfaces. While I'm equally excited about these positions, I know much less about the areas :). I will point out that we have a big systems group that covers many aspects of security (language security, verification, and network security) already. We've also had strong demand from students and industry for research in HCI, which will complement our info-viz efforts (and even our data mining work) \n \nFor more details on how to apply, see our ad . We'll start reviewing applications after Dec 1 . Feel free to email me if you have questions about the slots (but don't send me your application material - send them in directly ) \n \nDisclaimer: the above views are my own personal views, and don't represent the views of the department or the hiring subcommittees."], "link": "http://geomblog.blogspot.com/feeds/6039199345611351920/comments/default", "bloglinks": {}, "links": {"http://www.utah.edu/": 3, "http://feedads.doubleclick.net/": 2, "http://sc12.supercomputing.org/": 1}, "blogtitle": "The Geomblog"}, {"content": ["Without quite realizing it, I managed to create a (tiny) meme in the rarefied circles of TCS/math with my post \" Things a TCSer should have done at least once \". \n \nFirstly, you should check out the G+ post for even more scathing commentary on my original post. \n \nNext, you should see the followups (in case you haven't already): \n \n Complexity blog: Things a complexity theorist.... \n Turing's invisible hand: Things an AGTer.... \n Peter Krautzberger (on Boole's rings): Things a set theorist.. \n Computational Combinatorics: Things a graph theorist... \n Quantum Pontiff : Things a quantum information theorist .... \n \nLet me know if there are more - I'm still waiting for a quantum computing version. (thanks, Pontiff!)"], "link": "http://geomblog.blogspot.com/feeds/4582283063278490988/comments/default", "bloglinks": {}, "links": {"https://plus.google.com/": 1, "http://computationalcombinatorics.wordpress.com/": 1, "http://agtb.wordpress.com/": 1, "http://feedads.doubleclick.net/": 2, "http://geomblog.blogspot.com/": 1, "http://blog.computationalcomplexity.org/": 1, "http://boolesrings.org/": 1, "http://dabacon.org/": 1}, "blogtitle": "The Geomblog"}, {"content": ["I've been asked by Rafail Ostrovsky and David Shmoys to highlight that s tudent support for travelling to FOCS 2012 is still available, and the deadline is this Friday. \n \nThey note that they can fund 24-25 people, so people in the US should apply EVEN IF they don't have a paper . \n \nThe NSF has been very generous of late in supporting student travel to conferences, and the best way to encourage them to continue is to show that we use the money they give us :). My students have taken advantage of these opportunities in the past, and it's been a great experience for them. \n \nSo if you're a student reading this (and I know there are many of you!) or if you're an advisor who may not have the funding to send your student(s) to FOCS, do take advantage of this chance."], "link": "http://geomblog.blogspot.com/feeds/2651875496273218048/comments/default", "bloglinks": {}, "links": {"http://dimacs.rutgers.edu/": 2, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["There are many discussions about what things a TCS grad student should know. I thought it might be useful instead to list out some things a theoretician should have done at some point early in their career. \n \nRules of the game: \n \n You lose points if you do it as part of a class. If you decide to round an LP in a class on approximations, that's something you're being taught to do. But if you do it as part of solving a problem, then you've also done the difficult job of recognizing that an LP needs to be rounded. That latter skill demonstrates some mastery. \n The goal is not complete coverage of all of TCS; rather, it's coverage of techniques that will come up fairly often, no matter what kinds of problems you look at. \n This list is necessarily algorithms-biased. I doubt you'll need many of these if you're doing (say) structural complexity. \n A similar caveat applies to classical vs quantum computing. While it seems more and more important even for classical computations that one knows a little quantumness, I don't know enough about quantum algorithm design to add the right elements. Comments ? \n \nWith those caveats out of the way, here's my list: \n \n Show that a problem is NP-hard (for bonus points, from some flavor of SAT via gadgets) \n Show a hardness of approximation result (bonus points for going straight from a PCP) \n Prove a lower bound for a randomized algorithm \n Prove a lower bound via communication complexity or even information theory \n Round an LP (bonus points for not just doing the obvious rounding) \n Show an integrality gap for an LP \n Design a primal-dual algorithm \n Use projective duality to solve a problem (bonus points for using convex duality) \n Apply a Chernoff bound (bonus for using negative dependence, Janson's inequality, and an extra bonus for Talagrand's inequality) \n Design an FPT algorithm (maybe using treewidth, bonus for using bidimensionality or kernelization) \n Design a nontrivial exponential time algorithm (i.e an algorithm that doesn't just enumerate, but does something clever) \n Do an amortized analysis (for extra bonus get a log*n bound) \n use an advanced data structure - something beyond van emde Boas trees (extra bonus for exploiting word-size) \n invoke VC dimension to solve a problem \n \nWhat else ?"], "link": "http://geomblog.blogspot.com/feeds/7669180708879974570/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["Four times makes it an institution ? \n \nAfter the poster events at STOC 2011, STOC 2012 and SoCG 2012, FOCS 2012 makes it four times. Tim Roughgarden notes that the deadline for submitting posters to FOCS 2012 is in two weeks (Sep 11). So if you have interesting work you'd like to air out in the theory comunity, and a deep longing to visit New Brunswick, New Jersey, then submit your entry. \n \nBy this time, there's a good chance that you've already experienced the posters event either as presenter or audience at one of these venues. If not, I'll reiterate what I've said before : presenting a poster is a great way to disseminate your work with more personalized interaction than you often get with a paper presentation."], "link": "http://geomblog.blogspot.com/feeds/1675997537022256927/comments/default", "bloglinks": {}, "links": {"http://dimacs.rutgers.edu/": 1, "http://geomblog.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["Andrew McGregor and I are tag-blogging the workshop. Read his post on day 1 here . \n \nDay II could roughly be summarized as: \n \n \nSitting by the stream recovering from a sparse attack of acronyms. \n \nThere were a number of talks on sparse recovery, which includes compressed sensing, and asks the question: How best can we reconstruct a signal from linear probes if we know the signal is sparse. \n \n Eric Price led things off with a discussion of his recent breakthroughs on sparse FFTs. While the regular DFT takes $n \\log n$ time, it's reasonable to ask if we can do better if we know the signal has only $k$ nonzero Fourier coefficients. He talked about a sequence of papers that do this \"almost optimally\", in that they improve the $n\\log n$ running time as long as the sparsity parameter $k = o(n)$. \n \n Anna Gilbert provided an interesting counterpoint to this line of work. She argued that for real analog signals the process of sensing and recording them, even if the original signal was extremely sparse, can lead to discrete signals that have $\\Theta(n)$ nonzero Fourier coefficients, and this is in some way intrinsic to the sensing process. This was part of an attempt to explain why a long line of sublinear methods (including Eric's work) don't do very well on real signals. This line of attack is called 'off the grid\" reconstruction because you're off the (discrete) grid of a digital signal. \n \nIn sparse recovery, there are two parameters of interest: the number of measurements you make of the signal, and the time for reconstruction. Obviously, you'd like to get both to be as small as possible, and information-theoretic arguments show that you have to spend at least $k \\log(n/k)$ measurements. Martin Strauss focused on speeding up the reconstruction time while maintaining measurement-optimality, in a setting known as the \"for-all\" formulation, where the adversary is allowed to pick a signal after seeing the probe matrix (there's a related \"foreach\" model that is subtlely different, and I'm still a little confused about the two). \n \nOn a slightly different note (but not that different as it turns out), Atri Rudra talked about a fun problem: Given the \"shadows\" of a set of 3D points along three orthogonal planes, can you find the minimum number of points that could yield the specific shadows ? If all projections have complexity $n$, it's well known that the right bound is $n^{3/2}$. While this bound was known, it wasn't constructive, and part of Atri's work was providing an actual construction. There are all kinds of interesting connections between this problem, join queries, triangle counting in graphs, and even the scary-hairy 3SUM, but that's a topic for another time. \n \nOther talks in the afternoon: Alex Andoni talked about finding the eigenvalues of a matrix efficiently on streams (specifically finding the \"heavy\" eigenvalues). I learnt about the Cauchy interlacing theorem from his talk - it's a neat result about how the eigenvalues of submatrices behave. Ely Porat talked about the problem of preventing evil entities Hollywood from poisoning a BitTorrent stream of packets, and presented ideas involving homomorphic signatures for packets via error correcting codes. \n \n Joshua Brody returned to the basic streaming setup. Most stream algorithms that estimate some quantity introduce two-sided error (the true estimate could be above or below the reported value). He asked whether this was necessary to stay with sublinear bounds: it turns out for some problems, limiting yourself to 1-sided error can worsen the space complexity needed to solve the problem (note that for problems like the Count-min sketch, the estimate is one-sided by design, and so there's no deterioriation) \n \nComing up next: Andrew's summary of day three, and a report on our heroic tree-swinging adventures."], "link": "http://geomblog.blogspot.com/feeds/3003812356353680395/comments/default", "bloglinks": {}, "links": {"http://polylogblog.wordpress.com/": 1, "http://www.umich.edu/": 1, "http://web.umich.edu/": 1, "http://www.buffalo.edu/": 1, "http://u.ac.il/": 1, "http://en.wikipedia.org/": 1, "http://www.mit.edu/": 2, "http://cs.au.dk/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["I was reading Larry Wasserman's post on the modern two-sample test and a few thoughts came to mind.\n\n \n1. I've known about permutation tests for a long time, but it only just occurred to me that the permutation test is exactly what you do in the AM protocol for Graph non-isomorphism. The principle is that if the two distributions are identical, your test statistic should not be able to tell them apart, and the way to test this is by randomly changing labels. Replace distributions by graphs and test by Merlin, and it looks the same. Is this a trivially trite observation, or are there other examples of protocols that use standard statistical tests in disguise ? (hmm I see a cstheory question here)\n \n \n2. The kernel test he mentions, if you look closely, is merely computing the kernel distance between the two distributions. And the energy test he mentions later is doing something akin to earthmover. The kernel distance strikes again !!!"], "link": "http://geomblog.blogspot.com/feeds/6967677870416939902/comments/default", "bloglinks": {}, "links": {"http://normaldeviate.wordpress.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["This is the second part in Samira Daruki's report from the Women in Theory workshop at Princeton. The first part focused on the technical talks at the workshop, and this part focuses on the non-technical events. \n \n As an aside, many of the topics discussed would be quite useful to have at a general conference as well: it would be nice to have a similar panel discussion at a STOC/FOCS/SODA. The opening talk of the workshop was by Joan Girgus (Princeton). She presented statistics about the percentage of women at the undergrad and graduate level in different fields of science and engineering in the last five decades. She mentioned that nearly fifty years ago, those who wanted to combine a career with raising a family were seen as anomalies. Today, combining family and the career is the norm with its own complexities and difficulties. However, even now women continue to be underrepresented in science and engineering beginning from undergraduate level till the faculty and research positions. Joan presented several possible reasons for this and also suggested approaches that could be taken by universities to improve the participation of women in academic and research careers. The other interesting talk on the first day was by Maria Klawe (Harvey Mudd) who argued (and actually convinced us!) that it is the best time ever to be a woman in theory, and discussed opportunities for the future. On the second day, there was a \"work-life balance\" panel led by Tal Rabin. All the speakers and organizers of the workshop were gathered to answer the questions by students. This panel was one of the most useful sessions in the workshop, because we could hear the real experiences and challenges that pioneering female researchers faced in their career and life. The panel began by Tal asking speakers to just give one piece of advice to the audience. Some highlighted points were: \n \n \" Be aware of young male talkers\u201d \n \u201cmake conscious choices\u201d \n \u201cMake a right decision and go for it\u201d, \n \u201cGo for what really interests you\u201d \n \u201cThe path is difficult, and so you must acquire the ability to talk about yourself and your work\u201d, \n \u201cdo the work you like and be excited about that\u201d, \n \u201cTry to be more self promoting\u201d \n \n The floor was then opened for questions. There were different types of questions. Some of the questions were about family life for female researchers and the right time to have children. Some speakers believed that having children is an option, rather than a default, and there should be no pressure to have children. While it might seem that everybody by default expects you to raise a family and have children, you don\u2019t need to listen to people and do what they want. It was also mentioned that there is no \"right time\" to have children, and that it was a very personal decision. You should just decide for yourself when it's the right time for you. Valerie King said that some of the conditions at one's job can affect decisions regarding one's family. She pointed out that in Canada there are child-friendly policies for women in academia. But she also mentioned that sometimes you have to sacrifice something because of your family life, and in those cases you should find some other alternative ways to minimize the impact, like choosing a less taxing problem to work on or... There were different views on how family life and having children could affect the career life of young female researchers. Some believed that it wasn't a major issue - you could take off for a few years and wait till you reach a stable point for going back to your job - but some argued against this, pointing out that research in fields like CS develops very fast, and coming back after being away from research for a while can be very difficult and destructive for your research career without a careful and conscious plan. There were also discussions among speakers about how choosing a supportive partner can help young female researchers to deal with these difficulties more easily, (and that actually finding a proper partner is time-consuming and challenging by itself!). Another question was about the challenges and pressures of graduate study. One of the highlighted issues was about challenges in working with colleagues in academia. It was mentioned that you should be careful with the people around you, and make sure to have some people around you that talk about your contribution and acknowledge you. Panelists talked about different experiences they had with male colleagues. some of whom would make sure to acknowledge your contributions explicitly in their presentations, and some who would use your ideas without any acknowledgement. Clearly if you want to be more successful you should avoid being surrounded by this latter group of people. It was mentioned that one of the techniques in dealing with problems that you might face with male colleagues (if you find yourself unable to solve it by yourself) is to go to your manager or boss and push him to help you in that situation. Another challenge that was highlighted was finding a research problem to work on during graduate study and also for one's research career after that. Many of the speakers agreed that was one of the biggest challenges in their research work). Other discussed challenges were about choosing the right advisor and changing research problems or advisors during one's PhD. It was mentioned that usually the most common mistake new students make in doing research is that they decide on some topic, do a wide search on the current and previous work, and then come to the conclusion that all the problems had already been solved and that there was nothing new to do. But in fact in most research topics there are always ways to make the area broader and find new directions: this is the creative aspect of research. This is the main distinction between doing research and \"re-search\" There were also some discussions about the different aspects of working as a researcher at research labs or at a university as faculty. Lisa Zhang from Lucent mentioned that research labs have good quality of life and encourage a lot of flexibility. However, there are issues relating to job security versus tenure and there is a trade-off between these two kinds of research positions. There was discussion about collaboration between researchers. Valerie King mentioned that one should not be afraid to contact people and ask to work with them. In fact, people like it that others come and work with them on interesting research problems. She related experiences where she got stuck in solving some problem and found someone more expert in that area to collaborate with. One such collaboration was with two other female researchers resulting in what she called the only \u201c Three Women SODA paper \u201d. At the end of the panel, Shubhangi Saraf (IAS, Rutgers) talked about her experiences during graduate study. She said that it was very important for one's research career to do multiple internships, travel and visit different schools and research labs, find good people to work with and build a good network and connections. Shubhangi was one of the participants that first attended the Women In Theory workshop as a student four years ago and is now, at the third workshop, one of the organizers. She mentioned that this workshop was one of the ways that she was able to meet new people and make connection to do internships. At the end of the second day there was a banquet in Palmer House at which we were able to meet other professors from Princeton University and talk with them. To conclude this post, I think this workshop was successful in its main goal of bringing together theory women from different departments and fostering a sense of kinship and camaraderie among them. It was really a good feeling to talk about challenges you faced or times when you got stuck during your research and realize that other students and researchers have had the same experience! You feel a lot more powerful, because now when you're stuck with a problem and don\u2019t know what to do, you know there are some other people with a similar situations that you can just shoot an email to and say: \u201cHey! I'm stuck and need to talk with you! \u201d."], "link": "http://geomblog.blogspot.com/feeds/6746594203889112398/comments/default", "bloglinks": {}, "links": {"http://dl.acm.org/": 1, "http://www.utah.edu/": 1, "http://geomblog.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["My student Samira Daruki recently attended the Women In Theory workshop at Princeton. This is the first part of her (lightly edited) report from the workshop, focusing on the technical talks. For more on the workshop (from a presenter's perspective) see Glencora Borradaile's post . \n \nThis past week, I was at the third workshop on \u201c Women in Theory (WIT) \u201d workshop held in Princeton University with about 46 female students and 12\nspeakers. Most of the students came from different schools in USA: University of Maryland, College\nPark, Brown, Oregon State, UMass, MIT, Stony Brook, Berkeley, Princeton,\nColumbia, Boston U, Washington, Stevens Institute of Technology, , UCSD, Northwestern, Harvard, UW-Madison,\nUCLA, CUNY, GMU, Harvey Mudd and Uta . There were also some international\nstudents from India (IIT), ETH Zurich, Germany, Canada (Toronto), City University Hong\nKong, Moscow Engineering Institute and Israil (Technion, Weizmann, Hebrew). \n \nParticipants were from a wide range of research fields like Cryptography\nand Privacy, Graph theory and algorithms, Computational Geometry,\nComputational Biology, Complexity, Game theory and mechanism design and\nSocial Networks. But the interesting thing was that you could\nfind more students working on cryptography than other fields. \n \nTal Rabin, Shubhangi Saraf and Lisa Zhang were organizers of the workshop\nand there were many junior and senior women in the field as speakers:\n Gagan Aggarwal (Google), Glencora Borradaile (Oregon State), Joan\nGirgus(Princeton), Nicole Immorlica (Northwestern University), Valerie King\n(University of Victorial),Maria Klawe (Harvey Mudd), Vijaya Ramachandran\n(UT Austin), Jennifer Rexford (Princeton),Dana Ron (Tel Aviv University),\nRonitt Rubinfeld (MIT and Tel Aviv University), Shubhangi Saraf(IAS),\nRebecca Wright (Rutgers and DIMACS). \n \nBeside the non-technical parts of the workshop, there were many\ninteresting technical talks by different researchers on different topics.\n Among them, one of the very wonderful and interesting talks was presented\nby Dana Ron (Tel-Aviv University, Columbia) on sublinear-time algorithms. When we refer to \u201cefficient algorithms\u201d we usually mean\n\u201cpolynomial-time algorithm\u201d and at the best we can try to lower the degree\nof the polynomial as much as possible, leading to \u201clinear time algorithms\u201c. However when we are dealing with massive data sets, even\nlinear time might be infeasible. In these cases we look to design even\nmore efficient algorithms, namely \u201csub-linear time algorithms\u201d. These\nalgorithms do not even go through the whole input data, so we\njust expect to output approximately-good answers. They are\nalso probabilistic, and so are allowed to have a failure probability (i.e are Monte Carlo). \n \nOne such type of algorithms are property testing algorithms, in which one has to decide with high success probability whether an input (like a\ngraph), has a certain property (for example is bipartite), or is relatively\nfar from having that property (for example in this case a relatively large\nfraction of its edges should be removed so that the graph become\nbipartite). In this talk several properties and algorithms were discussed. Other types of sublinear algorithms discussed in her talk were\nalgorithms for estimating various graph parameters, like the number of\nconnected components, the size of a minimum vertex cover, and so on.\nI think Dana wase able to give a flavor of analysis techniques for sublinear-time algorithms with great clarity, and it was definitely one\nof the best talks in this workshop. \n \nAnother great talk was given by Ronitt Rubinfeld (Tel Aviv University\nand MIT), on estimating properties of distributions. In her talk, she\nsurveyed a body of work regarding the complexity of testing and estimating\nvarious parameters of distributions over large domains, when given access\nto only few samples from the distributions. Such properties include testing\nif two distributions have small statistical distance, testing if the\nmarginal distributions induced by a joint distribution are independent,\ntesting if a distribution is monotone, and approximating the entropy of the\ndistribution. In this kind of problems, the classical techniques such as\nthe Chi-squared test or the use of Chernoff bounds have sample complexities\nthat are at least linear in the size of the support of the underlying\ndiscrete probability distributions. However, algorithms whose sample\ncomplexity is \u201csublinear\u201d in the size of the support were shown for all of\nthe above problems. \n \nNicole Immorlica (Northwestern University) also gave an interesting talk\nabout cooperation in social networks, presented as a game among\nstudents. In this talk, she explored the impact of networks on behavior\nthrough a study of the emergence of cooperation in the dynamic, anonymous\nsocial networks that occur in online communities. In these communities,\ncooperation (for example in business deal) results in mutual benefits,\nwhereas cheating results in a high short-term gain. \n \nSome of the other interesting talks was presented by Gagan\nAggarwal (Google) on mechanism design for online advertising, Glencora\nBorradaile (Oregon State) on designing algorithms for planar graphs (and it\nwas the only talk on the blackboard without any slides!), Vijaya\nRamachandran (U of Texas, Austin) on Parallel and Multicore Computing Theory\nand Valerie King (University of Victoria) on Dynamic Graph Algorithms for\nmaintaining connectivity. In her talk, Valerie discussed this problem that\nhad been studied for over 30 years, and she reviewed the area and described\na new direction for achieving polylogarithmic worst case performance. At\nthe end of her talk she also mentioned Mihai Patrascu as a pathbreaking researcher who was\ntaken too soon from us. \n \nUnfortunately there were no talks on topics like Computational\nGeometry, but we had a few students working on CG related topics and\nthey presented their research problems in the rump session. My presentation at the rump session was on building core sets for uncertain data ."], "link": "http://geomblog.blogspot.com/feeds/7344435163899904568/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.utah.edu/": 2, "http://blogs.oregonstate.edu/": 1, "http://womenintheory.wordpress.com/": 2}, "blogtitle": "The Geomblog"}, {"content": ["My student Amirali Abdullah attended a recent MRC event on discrete and computational geometry at Snowbird, organized by Satyen Devadoss, Vida Dujmovic, Joe O'Rourke, and Yusu Wang. This is his (lightly edited) report from the event. Note that the organizers requested that the problems discussed not be widely disseminated, so no specific technical questions will be discussed here . \n \nRecently, there was an MRC workshop for Discrete and Combinatorial Geometry held right in my hometown, at Snowbird in Utah. Suresh invited me to share my experience of the event. \n \nWorking with trained mathematicians illustrated to me just how many tools are out there \nthat I wasn't familiar with beyond a \"oh I heard that word somewhere\" head-nod. Ergodic theory, configuration spaces of cohomologies, measure theoretic ideas, algebraic geometry techniques and variational calculus tools and more. Now, there's always a strong need for self-study and picking up techniques independently in a Ph.D but I can't help but feel that most theoretical CS students would benefit from required courses and curricula more tailored to supplementing our math backgrounds. \n \nBut more than being exposed to the mathematical depth of knowledge out there, I loved the intoxicating energy of a room filled with curious mathematicians. One group would be eagerly folding origami cubes, another playing with colored bricks and crayons on a coloring problem, \na trifecta of mathematicians lying passed out by a whiteboard decoated with scribbled figures and gazing dreamily into the distance .I finally understand the cliche of mathematicians having the enthusiasm and boundless energy of kindergarteners,playing with ideas and constructs-- no disinterested 'suits' here! \n \nMore so, it was good for me to associate real faces and people with many of the papers and fields I had read of. One grad student speaks of how he had been burned out for several months after his last paper, another stares blankly at his beer at 10:30 pm after a twelve hour session discussing a tricky problem, another discuss the merits of wine vs scotch, one raves about an advisor who recommend his students go out hiking on a beautiful day instead of staying in their labs (Suresh, hint, hint!!), another of handling the two-body problem in an already restricted job market. And of course, the common theme of how wonderful and at times frustrating math is. \n \nThere were many light-hearted moments too, to brighten a few faces. For example after mathematicians A and B had spent all day on a problem only to realize their approach was all wrong- \n \nMathematician A: \"I guess doing math is all about cursing sometimes.\" \nMathematician B: \"$%#@! F-bomb. #@%#@ F-bomb. Censored here for family audiences\". \nOr another light-hearted conversation between Organizer A and a student who had invited him to speak at a conference - \n \n\"So, am I the entertainment for the high school students \nthen?\" \n \nStudent-\"Yes, we have your talk on geometry the evening after the sword-swallower performs.\" \n \nLet me give a shout out to the wonderful facilities provided us by the organizers, especially the amazing food.We were fed three square meals a day, plus tea twice a day and another informal snacks and beer session after nine pm. Most of the meals were supplemented by confectionaries including red velvet cake or pastries, the meals were generally 3-4 courses (including mushroom and cheese garlic pizza, salmon fillet, chicken teriyaki, beef broccoli and more) and there were several rounds of free wine and scotch during the week. I may or may not have been slightly tipsy on a few occasions, and most of us put on at least a couple of pounds in a gluttonous week of math and fine cuisine. Several of us also went on a hike up the nearby trails, or enjoyed the swimming pools. I'm from Utah, of course, so I've been spoiled to always have the outdoors easily available. \n \nThere was a lovely talk given by the organizers on the job hunt process and pointers on finding the best fit institution. We've all heard the horror tales of how tight the academic job market is, but it's \ndisconcerting nonetheless to hear firsthand of several hundred applicants for a single faculty position, or of how many of the workshop participants had sent in over a 100 applications to various universities for their first job. Despite this, the independence of a research university position is still THE holy grail for those of a more mathematical bent - most of those attending seemed uninterested in the compromises involved in a teaching intensive or industry position, and I can certainly understand that sentiment. \n \nFinally a shout out for my favorite screening of the session- Diana Davis showed us her entry for \" Dance your Ph.D thesis \", which drew much approval from an audience worn out by the excessive number of dry beamer and powerpoint presentations we've seen. ."], "link": "http://geomblog.blogspot.com/feeds/5910979836973786586/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.utah.edu/": 1, "http://www.youtube.com/": 1, "http://www.ams.org/": 1}, "blogtitle": "The Geomblog"}, {"content": ["Jeff Phillips and I organized a workshop on Geometry In The Field at SoCG 2012. I haven't blogged about the workshop yet because Jeff and I are working on a more detailed report for the SIGACT Computational Geometry column, and I'll post it here when it's done. \n \nMany people had asked me when the talk slides would be available, and I'm happy to announce that all the talk slides are now up at the workshop site . \n \nI strongly recommend looking over the slides if you have the time: these were excellent talks by great speakers that gave a masterful overview of their specific areas, as well as challenges for the future."], "link": "http://geomblog.blogspot.com/feeds/6353185237681971107/comments/default", "bloglinks": {}, "links": {"http://www.utah.edu/": 2, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["Some highlights from day 2 of CGWeek: \n \n \n On Laplacians of Random Complexes , by Anna Gundert and Uli Wagner. \n \n The Cheeger inequality is a well known inequality in spectral graph theory that connects the \"combinatorial expansion\" of a graph with \"spectral expansion\". Among other things, it's useful for clustering, because you can split a graph using Cheeger's inequality to find a \"thin cut\" and then repeat. There's been work recently on a \" higher-order\" Cheeger inequality , that allows you to cut a graph into $k$ pieces instead of two by connecting this to observations about the first $k$ eigenvectors of the Laplacian. \n \nThe above paper generalizes the notion of combinatorial expansion versus spectral expansion in a very different way. Think of a graph as the 1-skeleton of a simplicial complex (i.e the set of faces of dimension at most 1). Is there a way to define the Laplacian of the complex itself ? And is there then an equivalent of Cheeger's inequality ? \n \nIt turns out that this can indeed be done. Recent work by Gromov and others have shown how to define the notion of \"edge expansion\" for a simplicial complex. Roughly speaking, you compute the edge expansion of a cochain (a function of a chain) by endowing it with a norm and then looking at the norm of the edge operator with respect to the norm (sort of) of the cochain itself. What is interesting is that if you choose the underlying coefficient field as $\\mathbb{R}$ and the norm as $\\ell_2$, you get the spectral equivalent, and if you choose instead $\\mathbb{Z}_2$ and the Hamming distance, you get the combinatorial equivalent. \n \nIt's known that for 1-skeletons, and even for things like persistence, the underlying field used to define homology doesn't really matter. However, for this problem, it matters a lot. The authors show that there is no equivalent Cheeger's inequality for simplicial complexes ! They also look at random complexes and analyze their properties (just like we can do for random graphs). \n \n \n Add Isotropic Gaussian Kernels at Own Risk: More and More Resilient Modes in Higher Dimensions , by Edelsbrunner, Fasy and Rote \n \nSuppose you have three Gaussians in the plane, and you look at the resulting normalized distribution. You expect to see three bumps (modes), and if the Gaussians merge together, you'd expect to see the modes come together in a supermode. \n \nCan you ever get more than three modes ? \n \nThis is the question the above paper asks. It was conjectured that this cannot happen, and in fact in 2003 it was shown that it was possible to get 4 modes from three Gaussians (you can get a little bump in the middle as the three Gaussians pull apart). In this paper, they show that in fact you can get a super-linear number of \"bumps\" or critical points for $n$ Gaussians, and these modes are not transient - they \"persist\" in a certain sense. \n \nThis is quite surprising in and of itself. But it's also important. A plausible approach to clustering a mixture of Gaussians might look for density modes and assign cluster centers there. What this paper says is that you can't really do that, because of the \"ghost\" centers that can appear. \n \nOther quick hits: \n \n Chris Bishop ran a very interesting workshop on Analysis and Geometry . While I couldn't attend all the talks, the first one was by Ranaan Schul gave an overview of the analytic TSP , in which you're given a continuous set of points in the plane, and want to know if there's a finite length curve that passes through all the points. The techniques used here relate to multiscale analysis of data and things like local PCA. \n One of the cool innovations in CGWeek was the fast-forward session: each afternoon before the workshops started, speakers were allowed to give a 1-slide overview of what would happen in their events. The slides were all placed in a single presentation ahead of time, and it was clocked so that people couldn't run on. It was great - I didn't have to do careful planning, and got a much better idea of what was coming up. We should do this for all talks at the conference !"], "link": "http://geomblog.blogspot.com/feeds/7947361822784163881/comments/default", "bloglinks": {}, "links": {"http://www.ethz.ch/": 1, "http://feedads.doubleclick.net/": 2, "http://arxiv.org/": 1, "http://www.sunysb.edu/": 2, "http://pub.ac.at/": 1, "http://lucatrevisan.wordpress.com/": 1}, "blogtitle": "The Geomblog"}, {"content": ["CGWeek day 1 has my head spinning. Attending a mix of conference talks and workshops is something I'm used to in the DB/data mining world, but not in theory conferences. It's refreshing, exhilarating, and exhausting, all at the same time. \n \nThere were a number of great talks today at the conference and the workshops -- too many to do justice to. Here are some random thoughts: \n \n This year, there's a best student presentation award. My student Amirali Abdullah is one of the competitors, so I don't want to say too much about it, but I'll say that everyone uppped their game - the presentations by students were great ! Maybe they should have best presentation awards in general to improve everyone's presentation skills ! \n Amir's talk went pretty well (if I say so myself). It was his first conference talk, but I thought it was well above the average (maybe this says something about the average :)). The talk was about approximate Bregman near neighbors . \n Very intriguing paper by Adamaszek and Stacho on the connection between homology structure of flag complexes and matchings that induce maximal independent sets. \n The workshop on geometric learning was fantastic. Lots to think about on multiscale representations, and distances to measures. \n Jeff Erickson has a four part liveplussing ( one , II , trois , fear ) of the business meeting \n Most importantly: Rio in 2013 !! Kyoto in 2014 (boo) !! OSU no way (I kid, I kid). \n Even more importantly: Herbert proposes changing the name of the conference to \"Symposium on Computational Geometry and Topology\". Major concern - what about the sausage ? Audience votes confusedly after Herbert promises a Klein bottle of beer for everyone."], "link": "http://geomblog.blogspot.com/feeds/8886769628333330283/comments/default", "bloglinks": {}, "links": {"http://www.utah.edu/": 1, "http://geomblog.blogspot.com/": 1, "https://plus.google.com/": 4, "http://feedads.doubleclick.net/": 2, "http://www.toronto.edu/": 1}, "blogtitle": "The Geomblog"}, {"content": ["Ed: Graham Cormode kindly agreed to send a missive from the recently concluded big data workshop at Duke \n \nThere seem to be no shortage of Big Data events at the moment, reflecting some of the enthusiasm around this topic. Last week, there were meetings at NIST \nand at Duke \nand there are upcoming events at Stanford \nand SAMSI \nto name but a few. (Ed: also the new Simons Foundation program  on big data ) \n \n I attended the Duke event, and was hoping to blog about the meeting, but suffered a big data problem of my own: with 25 speakers, plus panels and breakout sessions, how to pull out any meaningful summary? So instead, I'll just pick on one issue that was the subject of much discussion: what exactly is new about big data? In particular, there has been much interest in what we called 'massive' data over the last years (as captured by MADALGO , the center for massive data algorithmics \nand the accompanying MASSIVE workshop ). Is Big Data just a rebranding of Massive Data? Which is bigger, Big Data, Massive Data, or Very Large Data ? \n \nWhat came out from our discussions is that we believe that Big Data is qualitatively different from the challenges that have come before. The major change is the increasing heterogeneity of data that is being produced. Previously, we might have considered data that was represented in a simple form, as a very high-dimensional vector, over which we want to capture key properties. While Big Data does include such problems, it also includes many other types of data: database tables, text data, graph data, social network data, GPS trajectories, and more. Big Data problems can consist of a mixture of examples of such data, not all of which are individually \"big\", but making sense of which represents a big problem. Addressing these challenges requires not only algorithmics and systems, but machine learning, statistics and data mining insights. \n \nThis is by no means a new observation: the axes of Volume, Velocity and Variety have been widely discussed before .\nBut it did remind me that we should not get too hung up on how big our big data is, since size is not the sole defining characteristic of handling big data (although it is a necessary characteristic). \n \nMany other familiar topics came up in discussions, such as: how can big data sets be made more accessible to researchers? how can privacy concerns with big data be addressed? what are the right models for big data, for both the data and the computation? what are the key techniques for working with big data? do we even know enough to teach a course on big data? \n \nWhat struck me about these discussions is that not only were we far from reaching a consensus on any point, but also no one seemed to think they had any strong candidate solutions. Consequently, I left assured that Big Data is much more than a rebranding of existing work. It represents a significant research agenda that will challenge us for years to come."], "link": "http://geomblog.blogspot.com/feeds/8072456154637643738/comments/default", "bloglinks": {}, "links": {"http://vldb.org/": 1, "http://madalgo.au.dk/": 2, "http://draft.blogger.com/": 1, "http://radar.oreilly.com/": 1, "http://www.stanford.edu/": 1, "http://simons.berkeley.edu/": 1, "http://www.samsi.info/": 1, "https://sites.google.com/": 1, "http://www.nist.gov/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["As Jeff mentioned , we're organizing a workshop at SoCG called \" 8F-Computational Geometry \" that looks at the past and future of the impact computational geometry has had \"in the field\". It was quite difficult to narrow things down to 8 speakers, and even harder to list only four areas where CG has already had impact. \n \n The abstracts for the talks are now up at the workshop page. We hope to see you there !"], "link": "http://geomblog.blogspot.com/feeds/5063996934683411456/comments/default", "bloglinks": {}, "links": {"http://www.utah.edu/": 2, "http://geomblog.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["I just got word from Mikkel Thorup that Mihai Patrascu passed away. I had heard that he was ill, and only realized the seriousness of his condition when I saw him at STOC 2012. He came in a wheelchair to be honored for getting a Presburger award, and the entire room gave him a standing ovation. \n \nI can only hope that he's out there proving lower bounds in the sky, and arguing fiercely with the clouds while he does so. \n \n Update : There is now a memorial page for Mihai at mipmemorial.blogspot.com . Please visit there to post your comments, and have a drink for Mihai !"], "link": "http://geomblog.blogspot.com/feeds/512574905848963981/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://mipmemorial.blogspot.com/": 2}, "blogtitle": "The Geomblog"}, {"content": ["(while this could have been a question on cstheory, I think it's a little too vague - but it's perfect for a blog post!) \n \nI was at dinner with some faculty colleagues (assistant professor beer night - best invention known to mankind!) and we were in the traditional \"all our students are terrible\" phase of the evening * . \n\nThe discussion turned to \"what programming concepts do students understand after an intro-CS class\", which led to the following claim: \n \n\"you can't write an exponential time algorithm without knowing recursion\" \nWhile this seemed plausible, I was skeptical. Thankfully my PL colleague (and super blogger) Matt Might was at the table, and he pointed out that the lambda calculus formally has no recursive constructs and yet is Turing-complete, via the magic of the Y-combinator (which essentially computes fixed points directly). \n \nOf course one might argue that a fixed point operator does give you recursion (indeed the Y-combinator is often referred to as anonymous recursion). So my question really is: \n \nIs there a well defined notion of \"programming languages without recursion or recursion-like substances\" and if so, what is the most expensive algorithm that can be expressed in such a language ? \n \np.s BDDs seem like an obvious candidate... \n \n \n 1 (other regularly discussed phases include \"I can't believe < latest cryptic thing > senior faculty member said\", \"How many frequent flyer miles do you have\" and \"chickens!\". Beer night is awesome ! \u21a9"], "link": "http://geomblog.blogspot.com/feeds/5473788006395218559/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://geomblog.blogspot.com/": 2, "http://www.blogger.com/blog": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "The Geomblog"}, {"content": ["I didn't do any STOC blogging this time, but I do want to say something about the extracurricular events. \n \nFor a while now, people (including me) have been clamoring for an expansion of the traditional three-days-of-talks format at theory conferences, and now STOC and FOCS (and SoCG soon!) are doing workshops, tutorials and poster sessions on a regular basis. Chandra Chekuri, Sergei Vassilvitskii and I organized the poster session at STOC this year, and Avrim Blum and Muthu (Muthu!) were in charge of workshops. \n \n \n Workshops \n \n \nHow did they go ? The workshops, the day before the conference, looked extremely successful. Michael Kearns' morning tutorial on computational finance was standing room only in a large auditorium - I don't know the exact numbers but there were easily more than 150 people in the room. The workshops went pretty well as well: I spent much of my time at the distributed streaming workshop, and that was also standing room only, with probably at least 60 people in the room at all times. \n \nI'm really glad that we had these events, and even happier that FOCS 2012 plans to continue the event. Speaking of which, Avrim is looking for workshop proposals for FOCS 2012, and specifically asked me to suggest that geometry folks suggest a workshop. The deadline is June 20 , and all you need is a 2-page proposal and 2-3 people who'd organize it. \n \n \n Posters \n \n \nI'm of course biased about the poster session, but I do think it went off quite well, fueled by Howard Karloff's brilliant idea to provide ice cream sandwiches as refreshments during the event. There were 30 posters in all, and a ton of discussion. It was really nice to walk around looking at the posters and seeing the level of engagement. I polled a number of presenters and attendees, and they all seemed to enjoy the session. \n \nThe only complaint was that the poster session was too short, and that we should have left the posters up for people to browse while taking breaks. I think this is an excellent idea that the next posters committee (at FOCS 2012) should take into account. \n \n \n The business meeting \n \n \n \nWe had an incredibly long business meeting - even our most rambunctious SODA meetings haven't gone on this long (almost 3 hours). For those not yet in the know, STOC 2012 is going to a two-tier format. Joan Feigenbaum is the chair, and the \"must-not-be-called-senior\"-PC will have 9 people on it. Their primary role will be managing the review process with the \"certainly-not-junior\"-PC consisting of 70-80 people who will review no more than 10 papers each, AND will be allowed to submit papers. \n \n \n \nThis is a major change for a theory conference, albeit one that was brought up for discussion at SODA 2012 . I'm particularly curious to see how the whole \"letting PC members submit papers\" goes. Before everyone starts freaking out, it's worth pointing out that all this is effectively doing is bringing the unofficial reviewers into the fold, thereby giving them a chance to do reviews in context of the entire pool, rather than making isolated decisions. Since the unofficial reviewers were mostly drawn from the author pool, this is not too different from how it's always been. I'm hoping that the reduced load per reviewer will bring in better reviews and more consistency in decisions. \n \n \n \nAll in all, a time of experimentation - workshops, posters, tutorials, two-tier PCs - the theory community moves slowly, but when it does move, things happen quickly :)"], "link": "http://geomblog.blogspot.com/feeds/1222179508154084082/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://geomblog.blogspot.com/": 1}, "blogtitle": "The Geomblog"}, {"content": ["Atri Rudra asks me to post an announcement for the second incarnation of the workshop on coding theory, complexity and sparsity. The first event went off quite well - I was sure that Dick Lipton had a post on it but I couldn't find it (Update: here it is , thanks to Atri). At any rate, do attend and get your students to attend: they have a great roster of speakers and travel support for students. \n\n \n \nEfficient and effective transmission, storage, and retrieval of information on a large-scale are among the core technical problems in the modern digital revolution. The massive volume of data necessitates the quest for mathematical and algorithmic methods for efficiently describing, summarizing, synthesizing, and, increasingly more critical, deciding when and how to discard data before storing or transmitting it. Such methods have been developed in two areas: coding theory, and sparse approximation (SA) (and its variants called compressive sensing (CS) and streaming algorithms). \n \nCoding theory and computational complexity are both well established fields that enjoy fruitful interactions with one another. On the other hand, while significant progress on the SA/CS problem has been made, much of that progress is concentrated on the feasibility of the problems, including a number of algorithmic innovations that leverage coding theory techniques, but a systematic computational complexity treatment of these problems is sorely lacking. \n \nThe workshop organizers aim to develop a general computational theory of SA and CS (as well as related areas such as group testing) and its relationship to coding theory. This goal can be achieved only by bringing together researchers from a variety of areas. We will have several tutorial lectures that will be directed to graduate students and postdocs.\n \nThese will be hour-long lectures designed to give students an introduction to coding theory, complexity theory/pseudo-randomness, and compressive sensing/streaming algorithms. \n \nWe will have a poster session during the workshop and everyone is welcome to bring a poster but graduate students and postdocs are especially encouraged to give a poster presentation. \n \nConfirmed speakers: \n \n \n Eric   Allender   Rutgers\n \n Mark  Braverman  Princeton\n \n Mahdi  Cheraghchi  Carnegie Mellon University\n \n Anna  Gal     The University of Texas at Austin\n \n Piotr  Indyk    MIT\n \n Swastik Kopparty   Rutgers\n \n Dick  Lipton    Georgia Tech\n \n Andrew McGregor  University of Massachusetts, Amherst\n \n Raghu Meka    IAS\n \n Eric  Price    MIT\n \n Ronitt Rubinfeld   MIT\n \n Shubhangi Saraf   IAS\n \n Chris  Umans   Caltech\n \n David Woodruff   IBM \n \n \nWe have some funding for graduate students and postdocs. For registration and other details, please look at the workshop webpage:\n https://sites.google.com/site/umccsworkshop2012/"], "link": "http://geomblog.blogspot.com/feeds/4372719397857600698/comments/default", "bloglinks": {}, "links": {"http://rjlipton.wordpress.com/": 1, "http://feedads.doubleclick.net/": 2, "https://sites.google.com/": 1}, "blogtitle": "The Geomblog"}, {"content": ["What is North Carolina known for? \n Basketball ?\n Barbecue ?\n Great Universities ?\n \n \nWell, this June it will be known for geometry, as SoCG will be held this year very near Durham, NC. Unfortunately, it will be at UNC and not Duke. (I kid, I know Jack will do a great job organizing.)\n\n \n\nIf those things are not enough, Suresh and I ( Jeff ) are organizing an \n 8F-Computational Geometry \nworkshop at \n SoCG . \nIn case thats not clear, 8F stands for AITF, and AITF stands for Algorithms in the Field. \n\n \n\nThe idea is to both showcase how computational geometry has had impact on \"the field\" and highlight some areas that we hope are ripe for future impact from the CG community. There are two days of talks Tuesday, June 19 on ongoing impact and Wednesday, June 20 on potential (more) impact. We already have a great line-up of speakers; on the first day we have talks by \n Valerio Pascuci on Visualization, \n Lars Arge on GIS,\n David Mount on Data Retrieval, and\n Nina Amenta on Meshing and Surface Reconstruction.\n On the second day we have talks by \n Dan Halperin on Robotics and Automation, \n Jie Gao on Mobile Networks,\n Michael Mahoney on High Dimensional Data Analysis, and\n Tom Fletcher on Medical Imaging.\n\n \n\nWe encourage you all to attend, and to see what the Durham area has to offer. There is actually a full 4 days of activities at SoCG this year. The early registration deadline is next week (May 23) and its only $100 for students! \n\n \n \n \n\nWe should mention that this 8F-Computational Geometry workshop is part of a larger push (through sponsorship) by NSF. There was a more general \n 8F Workshop last year, and there are more in the works."], "link": "http://geomblog.blogspot.com/feeds/1710795668840344657/comments/default", "bloglinks": {}, "links": {"http://www.ucdavis.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://cs.stanford.edu/": 1, "http://www.duke.edu/": 1, "http://acg.ac.il/": 1, "http://maps.google.com/": 1, "http://dimacs.rutgers.edu/": 1, "http://pascucci.org/": 1, "http://www.unc.edu/": 1, "http://www.goduke.com/": 1, "http://www.sunysb.edu/": 1, "http://www.utah.edu/": 3, "http://cs.au.dk/": 1, "http://socg2012.unc.edu/": 2, "http://www.umd.edu/": 1}, "blogtitle": "The Geomblog"}, {"content": ["\"In the long run, we're all dead\" is a famous quote attributed to John Maynard Keynes. The context was his arguments with economists of the time: he was trying to argue for government intervention in the markets to control inflation, rather than just letting it play out. \n \nIt's an apt response also to occasional claims that asymptotics will eventually win out, especially with large data. Asymptotics will eventually win out, as long as everything else stays fixed . \n \nBut that's the precise problem. Everything else doesn't stay fixed. Well before your $C n \\log n$ algorithm beats the $c n^2$ algorithm, we run out of memory, or local cache, or something else, and the computational model changes on us. \n \nWe come up with external memory models, and are informed that in fact even a factor of log N is too much. We design streaming models and Mapreduce and so on and so forth, and realize that all this communication is burning a hole in our atmosphere. \n \nLesson to be learned (and re-learned): asymptotic analysis is a powerful tool, but it's only as good as the model of computation you're REALLY working with."], "link": "http://geomblog.blogspot.com/feeds/6023362477311050401/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "The Geomblog"}, {"content": ["The multiplicative weight update method (MWU hereafter) is a neat algorithm design technique with applications in machine learning, geometry and optimization among others. However, it's viewed (and discussed) as an advanced technique, with very technical examples requiring lots of background (see the Arora-Hazan-Kale survey first example). \n \nAfter pondering the use of MWU for a while now, it seems to me that this should be taught as a standard algorithms tool that naturally follows from divide and conquer and prune-and-search. In what follows, I'll sketch out how this might work. \n \nA quick caveat: the MWU, like many powerful tools, has multiple interpretations, and the survey hints at many of them (for example, MWU = derandomization of Chernoff bound). I don't intend to imply that there's only one way to interpret the technique, but that the approach I'll describe is the most accessible one when learning the method for the first time. \n \n \n \nThe divide and conquer unit in an algorithms class might cover sorting and FFTs, driven by the standard D&C recurrence relation. Prune and search is introduced as a variation, with median finding and binary search being the prototypical examples. \n \nPrune-and-search works like magic, if you're seeing it for the first time. Do some work, throw away the rest, and your $n \\log n$ running time goes down to linear. (As an Indian of a certain age, this always reminds me of \" thoda khao, thoda phenko \" (eat a bit, throw a bit) from Jaane Bhi Do Yaaro ). \n \nBut what makes it tick is determining the constant factor that needs to be thrown away. The most direct way to do this is deterministically: on a linear time budget, find a point that splits the input into two roughly balanced parts. \n \nBut that's expensive in practice. Indeed, algorithms that use median finding as a subroutine try to avoid using a deterministic procedure because it can be slow. When you get to more advanced methods like parametric search, it gets even worse. \n \nThe neat trick to apply here is to randomize ! We know that we don't have to find an exact split point - merely one that will approximately balance the two sides of the recursion. For median finding, we can pick three points at random, and take their median. With a sufficiently high probability, this median will create a 1/3-2/3 split, and away we go ! \n \nThis idea surfaces again and again, especially in the many randomized algorithms in computational geometry. As a design strategy, it's quite effective - design an algorithm that works if you can do balanced splits, and then find the split by choosing randomly. Invoke the Chernoff God and some satellite deities, and you're done. \n \nWhich brings us to the MWU. \n \nRandomized splitting says that we're willing to lie a little about the split process, and things still work out. But whether you do randomized splitting or deterministic, the end result is still that you throw away some reasonable fraction of the input, NEVER TO SEE IT AGAIN. \n \nSuppose you can't even do that ? \n \nThink of noisy binary search (or 20 questions with a liar). Now, even your decision on what to prune is error-prone. You might be eliminating things that you need to consider later. So it's not clear that you can make any kind of search progress. But let's be reasonable and limit the adversary (or the noise) in some manner. Let's say that you'll only misclassify a small fraction of the input in each iteration (where small is \"strictly less than half\"). Let's also assume that I can tell you how important certain points are, so that you have to take that into consideration when defining your \"small fraction\". \n \nSo how does MWU now work ? I tell you a distribution over the input, and you give me back a rule that's reasonably good at (say) classifying points. I increase the importance of things you made mistakes on (so you try not to do it again), and repeat. \n \nEventually, what happens is that the weight of things that you make mistakes on increases rapidly. But the overall weight of the input can't increase too much, because I only increase the weight of things you make mistakes on, which is not a lot. Intuitively, what happens is tha the first weight catches up with the second, at which point the algorithm is complete. You can show that if the updating schedule is chosen correctly, the process terminates in logarithmic steps. \n \nEssentially, MWU functions as a zombie binary search. You want to kill elements, but they keep coming back. Thankfully, each time they come back they're slightly weaker, so you have a better chance of hitting them next time. Eventually, head is severed from neck, and your zombies are dead (and your points are found). \n \nMy only wish at this point is that whenever you think of MWU you think of zombies. Now that's impact :)"], "link": "http://geomblog.blogspot.com/feeds/3675340728333004626/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.youtube.com/": 1, "http://en.wikipedia.org/": 1, "http://www.princeton.edu/": 1}, "blogtitle": "The Geomblog"}, {"content": ["Communication is now the key to modelling distributed/multicore computations. Jim Demmel has been writing papers and giving talks on this theme for a while now, and as processors get faster, and the cloud becomes a standard computing platform, communication between nodes is turning out to be the major bottleneck. So suppose you want to learn in this setting ? Suppose you have data sitting on different nodes (you have a data center, or a heterogeneous sensor network, and so on) and you'd like to learn something on the union of the data sets. You can't afford to ship everything to a single server for processing: the data might be too large to store, and the time to ship might be prohibitive. So can you learn over the (implicit) union of all the data, with as little discussion among nodes as possible ? This was the topic of my Shonan talk , as well as two papers that I've been working on with my student Avishek Saha, in collaboration with Jeff Phillips and Hal Daume. The first one will be presented at AISTATS this week, and the second was just posted to the arxiv . We started out with the simplest of learning problems: classification. Supppose you have data sitting on two nodes (A and B), and you wish to learn a hypothesis over the union of A and B. What you'd like is a way for the nodes to communicate as little as possible with each other while still generating a hypothesis close to the optimal solution. It's not hard to see that you could compute an $\\epsilon$-sample on A, and ship it over to B. By the usual properties of an $\\epsilon$-sample, you guarantee that any classifier on B's data combined with the sample will also classify A correctly to within some $\\epsilon$-error. It's also not too hard to show a lower bound that matches this upper bound. The amount of communication is nearly linear in $1/\\epsilon$. But can you do better ? In fact yes, if you let the nodes talk to each other, rather than only allowing one-way communication. One way of gaining intuition for this is that $A$ can generate classifiers, and send them over to $B$, and $B$ can tell $A$ to turn the classifier left or right. Effectively, $B$ acts as an oracle for binary search. The hard part is showing that this is actually a decimation (in that a constant fraction of points are eliminated from consideration as support points in each step), and once we do that, we can show an exponential improvement over one-way communication. There's a trivial way to extend this to more than 2 players, with a $k^2$ blow up in communication for $k$ players. This binary search intuition only works for points in 2D, because then the search space of classifiers is on the circle, which lends itself naturally to a binary search. In higher dimensions, we have to use what is essentially a generalization of binary search - the multiplicative weight update method. I'll have more to say about this in a later post, but you can think of the MWU as a \"confused zombie\" binary search, in that you only sort of know \"which way to go\" when doing the search, and even then points that you dismissed earlier might rise from the dead. It takes a little more work to bring the overhead for k-players down to a factor k. This comes by selecting one node as a coordinator, and implementing one of the distributed continuous sampling techniques to pass data to the coordinator. You can read the paper for more details on the method. One thing to note is that the MWU can be \"imported\" from other methods that use it, which means that we get distributed algorithms for many optimization problems for free. This is great because a number of ML problems essentially reduce to some kind of optimization. A second design template is multipass streaming: it's fairly easy to see that any multipass sublinear streaming algorithm can be placed in the k-player distributed setting, and so if you want a distributed algorithm, design a multipass streaming algorithm first. One weakness of our algorithms was that we didn't work in the \"agnostic\" case, where the optimal solution itself might not be a perfect classifier (or where the data isn't separable, to view it differently). This can be fixed: in an arxiv upload made simultaneously with ours, Blum, Balcan, Fine and Mansour solve this problem very neatly, in addition to proving a number of PAC-learning results in this model. It's nice to see different groups exploring this view of distributed learning. It shows that the model itself has legs. There are a number of problems that remain to be explored, and I'm hoping we can crack some of them. In all of this, the key is to get from a 'near linear in error' bound to a 'logarithmic in error' bound via replacing sampling by active sampling (or binary search)."], "link": "http://geomblog.blogspot.com/feeds/7796617482591160233/comments/default", "bloglinks": {}, "links": {"http://arxiv.org/": 3, "http://feedads.doubleclick.net/": 2, "http://www.ac.jp/": 1}, "blogtitle": "The Geomblog"}, {"content": ["(tl;dr: Our upcoming paper in SoCG 2012 shows that with a nontrivial amount of work, you can do approximate Bregman near neighbor queries in low dimensional spaces in logarithmic query time ) One of the things that has haunted me over the years (yes, haunted!) is the geometry of Bregman divergences. A Bregman divergence is a very interesting creature. It's defined as the difference between a convex function and its linear approximation starting at another point: $$ d_\\phi(p,q) = \\phi(p) - \\phi(q) - \\langle \\nabla \\phi(q), p - q\\rangle $$ Because the Bregman divergences are parametrized by the function $\\phi$, they yield a family of divergences. The most familiar one is $\\ell_2^2$, but the most important one is the Kullback-Leibler divergence. There are a ton of reasons why one should study Bregman divergences, and in a shameless plug I'll refer to you my slides ( one , two , three ) from a geometry summer school last year. Suffice it to say that it's possible to unify a number of different algorithms in machine learning under a single framework by realizing that they're all particular instances of doing something with a Bregman divergence: two notable examples of this are Adaboost and information-theoretic clustering . \n So what's the geometric perspective on Bregman divergences ? They generalize duality ! \n \nThere's a standard way to think about traditional point-hyperplane duality via a lifting map: take a point, lift it to the paraboloid one dimension up, and find the tangent plane. But suppose we replace the paraboloid by a generic convex function ? what we get is a general convex duality (technically a Fenchel-Legendre duality) defined by $$\\phi^*(u) = \\sup_v \\langle u,v \\rangle - \\phi(v)$$ The reason this doesn't come up in Euclidean space is that the mapping is self-dual if we set $\\phi(x) = (1/2) \\|x\\|^2$, which explains the symmetry in $\\ell_2^2$. It turns out that this observation is enough to import much of standard combinatorial geometry over to general Bregman spaces. We can compute convex hulls, Voronoi Diagrams, delaunay triangulations etc, as long as we're careful to keep primal and dual straight. for example, the locus of points equidistant from two points under a Bregman divergence is either a primal straight line or a dual straight line (depending on how you measure things), and so on. This was all elegantly worked out by Nielsen, Nock and Boissonnat a while ago. Alas, this beautiful connection doesn't help us with approximation problems. One of the most interesting questions regarding Bregman divergences is solving the near-neighbor problem. This is interesting because the Kullback-Leibler distance is often used (for example) to compare images, and so there's been a lot of empirical work on this problem. But here's the problem. Let's consider the low dimensional ANN problem. In Euclidean space (or even in spaces of bounded doubling dimension), here's how you solve the problem. You build some kind of quad-tree-like data structure, using the triangle inequality to reason about which cells you need to explore, and using packing bounds to bound the number of cells explored. You also need a crude bound on the near neighbor to start with, and to do this, you use some variant of a ring-tree. The key ingredients: triangle inequality (twice over) and packing bounds. Bregman divergences don't even satisfy a directed triangle inequality in general. And to date, we didn't even know how to define packing bounds properly for these directed distances. In an upcoming paper at SoCG 2012 with my students Amirali Abdullah and John Moeller, we finally figured out how to get some \"approximation\" of the ANN machinery to work with Bregman divergences, and get logarithmic query times with small space. If I say so myself, there are some nice ideas in the paper. Firstly, in a somewhat surprising twist, a Bregman divergence satisfies a \"reverse triangle inequality\" on the line: \\[ d_\\phi(a,b) + d_\\phi(b,c) \\le d_\\phi(a, c), a, b, c, \\in R\\] This is neat, because it gives packing bounds ! Intuitively, if the sum of lengths of subintervals along an interval is at most the length of the interval, then you can't have too many subintervals. The next trick is an observation that the square root of a Bregman divergence satisfies a kind of relaxed triangle inequality that we call $\\mu$-defectiveness: \\[ |d(x, y) - d(x,z)| \\le \\mu d(y, z)\\] This allows us to import some of the ring-tree machinery to get a coarse approximation. And I should mention that the $\\mu$ values involved here are quite small. If you're comparing distributions with the KL-divergence, then the value of $\\mu$ is less than 2 even quite close to the boundary of the simplex. Even with all of this in place, the quad-tree argument breaks. This is because of $\\mu$-defectiveness: we can't assume that cell sizes are \"reasonably large\" at some number of levels below the root of the tree, and so our packing bounds look terrible. It takes a lot more work to fix this problem: essentially we exploit second-order structure of the Bregman divergences to bound the cell sizes and get the desired packing bound. \nAfterthoughts: \n \n Most of the complexity of the algorithm is in the analysis: the actual algorithm looks mostly like a Euclidean ANN procedure. While we haven't implemented it, I'm hopeful that when we do, we'll be able enjoy the empirical behavior of our Euclidean cousins. \n We're looking at high dimensional Bregman near neighbors, as well as some other approximate Bregman geometry questions. While our low-D ANN result comes from the \"I will break your limbs one by one till you submit\" school of algorithms, the hope is that we can start to exploit more of the dual geometry as we learn more."], "link": "http://geomblog.blogspot.com/feeds/5616567354784864140/comments/default", "bloglinks": {}, "links": {"http://arxiv.org/": 3, "http://feedads.doubleclick.net/": 2, "http://citeseerx.psu.edu/": 1, "http://jmlr.mit.edu/": 1, "http://cgl.uni-jena.de/": 3}, "blogtitle": "The Geomblog"}]