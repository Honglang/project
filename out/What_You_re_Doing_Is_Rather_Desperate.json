[{"blogurl": "http://nsaunders.wordpress.com\n", "blogroll": [], "title": "What You're Doing Is Rather Desperate"}, {"content": ["June 23, 2004. BMC Bioinformatics publishes \u201cMistaken Identifiers: Gene name errors can be introduced inadvertently when using Excel in bioinformatics\u201d. We roll our eyes. Do people really do that? Is it really worthy of publication? However, we admit that if it happens then it\u2019s good that people know about it. \n October 17, 2012. A colleague on our internal Yammer network writes: \n \n \nSad but true. I keep finding newbie bioinformatics errors in the Cancer Genome Atlas project data. This time a text download of 450K methylation from the Cancer Genome Atlas project reveals that Excel has had its evil way with the data at some point. Gene names such as MAR1, DEC1, OCT4 and SEPT9 are now reformatted as dates. \n For example: \n \nbarcode\t      probe name  beta value  gene symbol chromosome position\nTCGA-06-0125-02A-11D-2004-05 cg13918206  0.92035091902012 1-Dec\t  9    118159781\n \n \n I click through the CGA data portal in search of more datasets and choose, more or less at random, another file containing data from the Illumina 450K platform. It\u2019s called jhu-usc.edu__HumanMethylation450__TCGA-G4-6628-01A-11D-1837-05__methylation_analysis.txt . Let\u2019s get that into R: \n \ntcga <- read.table(\"jhu-usc.edu__HumanMethylation450__TCGA-G4-6628-01A-11D-1837-05__methylation_analysis.txt\", sep = \"\\t\", header = T)\ndim(tcga)\n\n# [1] 485577  6\n\nhead(tcga$gene.symbol)\n\n# [1] 1-Dec 1-Dec 1-Dec 1-Dec 1-Dec 1-Dec\n# 25107 Levels: 10-Mar 11-Mar 11-Sep 13-Sep 14-Sep 1-Dec 1-Mar 1-Sep ... ZZZ3\n \n Seems we have a problem. Let\u2019s count up gene names: \n \ngenes <- as.data.frame(table(tcga$gene.symbol))\nhead(genes, 20)\n\n  Var1 Freq\n# 1   119652\n# 2 10-Mar  5\n# 3 11-Mar  21\n# 4 11-Sep  32\n# 5 13-Sep  18\n# 6 14-Sep  4\n# 7 1-Dec  8\n# 8 1-Mar  30\n# 9 1-Sep  10\n# 10 3-Mar  33\n# 11 4-Mar  25\n# 12 5-Mar  4\n# 13 5-Sep  12\n# 14 6-Mar  21\n# 15 7-Mar  13\n# 16 8-Sep  2\n# 17 9-Mar  6\n# 18 9-Sep  16\n# 19 A1BG  6\n# 20 A1CF  5\n \n Yes, we have a problem. \n \u201cNewbie bioinformaticians\u201d is one thing. Large institutes, awarded millions of dollars to contribute to \u201cbig science\u201d projects is another. \n Despair at the quality of public data, fears about reproducibility in science. Must be Monday. \n Filed under: bioinformatics , research diary Tagged: cancer , errors , excel , reproducibility , spreadsheet , tcga"], "link": "http://nsaunders.wordpress.com/2012/10/22/gene-name-errors-and-excel-lessons-not-learned/", "bloglinks": {}, "links": {"http://www.biomedcentral.com/": 1, "http://feeds.wordpress.com/": 1, "http://cancergenome.nih.gov/": 1, "http://www.hopkinsmedicine.org/": 1, "http://nsaunders.wordpress.com/": 8, "https://tcga-data.nih.gov/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["Updates from RStudio support: \n(1) \u201cThanks for reporting and I was able to reproduce this as well. I\u2019ve filed a bug and we\u2019ll take a look.\u201d \n(2) Taking a further look, this is actually a bug in the Markdown package and we\u2019ve asked the maintainer (Jeffrey Horner) to look into it. \n \n As juejung points out in a comment on my previous post, applying custom CSS to R Markdown by sourcing the custom rendering function breaks the rendering of inline equations. \n I\u2019ve opened an issue with RStudio support and will update here with their response. In the meantime, one solution to this problem is: \n \n Do not create the files custom.css or style.R, as described yesterday \n Instead, just put the custom CSS at the top of your R Markdown file using style tags, as shown below \n \n \n<style type=\"text/css\">\ntable {\n max-width: 95%;\n border: 1px solid #ccc;\n}\n\nth {\n background-color: #000000;\n color: #ffffff;\n}\n\ntd {\n background-color: #dcdcdc;\n}\n</style>\n \n Filed under: programming , R , research diary , statistics Tagged: markdown , rstudio"], "link": "http://nsaunders.wordpress.com/2012/08/28/addendum-to-yesterdays-post-on-custom-css-and-r-markdown/", "bloglinks": {}, "links": {"http://support.rstudio.org/": 1, "http://nsaunders.wordpress.com/": 7, "http://feeds.wordpress.com/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["People have been telling me for a while that the latest version of RStudio , the IDE for R, is a great way to generate reports. I finally got around to trying it out and for once, the hype is justified. Start with this excellent tutorial from Jeremy Anglim. \n Briefly: the process is not so different to Sweave , except that (1) instead of embedding R code in LaTeX, we embed R code in a document written using R Markdown ; (2) instead of Sweave, we use the knitr package; (3) the focus is on generating HTML documents for publishing to the Web (see e.g. RPubs ), although knitr can also generate PDF documents, just like Sweave. \n It took me a little while to figure out a couple of things. First, how best to generate HTML tables, ideally using the xtable package. Second, how to override the default RStudio/R Markdown style. I\u2019ve documented those tasks in this post. \n \nI\u2019m using RStudio version 0.96.330, installed on Ubuntu 12.04. \n 1. HTML tables using xtable \nYou can, of course, use R Markdown syntax to generate HTML tables. However, if like me you\u2019re a fan of xtable , you can keep using it in R Markdown documents. This works because (1) xtable can generate HTML output and (2) where HTML syntax appears in a markdown document, it\u2019s processed \u201cas is\u201d ( i.e. ignored and output as HTML). \n A simple example. In RStudio, open a new R Markdown document, enter the following code and save it with the extension .Rmd, e.g. tables.Rmd : \n \nTables\n======\n\n```{r table1, comment=NA, results='asis'}\nlibrary(xtable)\ndata(iris)\nprint(xtable(head(iris, 10)), type = \"html\", include.rownames = F)\n``` \n  A simple table using xtable + R Markdown This outputs the first 10 rows of the iris dataset as a table. Things to note: \n \n Use comment=NA in the knitr chunk options to prevent lines of output beginning with ## \n Use results=\u2019asis\u2019 in the knitr chunk options so as the HTML is rendered, not printed \n Use type = \u201chtml\u201d as an argument to print.xtable() for HTML tables \n \n (update: comment=NA is redundant here; see the helpful comment from Yihui at end of post) \n Hit the \u201cKnit HTML\u201d button in RStudio and you should see something like the image, right (click for larger version). \n 2. Custom CSS \nLet\u2019s say we want to alter the style of that table. First question: from where does RStudio get its default style? Answer: this CSS file which on my system, is found at /usr/lib/rstudio/resources/markdown.css . A glance at its contents shows, for example, why our HTML table has no border even though the HTML from xtable specifies \u201cborder=1\u2033 ; table, td and th are all given the attribute border: none . \n The document \u201c Customizing Markdown Rendering \u201d provides instructions to alter styles, but I found the structure of the document difficult to follow; it is not written in a step-by-step instructional style. Here\u2019s what I did: \n \n Define your custom styles; I created a file named custom.css from the original markdown.css and saved it in the same directory as the R Markdown file, tables.Rmd with these modifications: \n \ntable {\n max-width: 95%;\n border: 1px solid #ccc;\n}\n\nth {\n background-color: #000000;\n color: #ffffff;\n}\n\ntd {\n background-color: #dcdcdc;\n}\n \n \n Open a new R script file in RStudio and add the following content: \n \noptions(rstudio.markdownToHTML = \n function(inputFile, outputFile) {  \n require(markdown)\n markdownToHTML(inputFile, outputFile, stylesheet='custom.css') \n }\n)\n \nSave it as e.g. style.R in the same directory as the markdown and CSS files. Note: you will need to specify the full path to the CSS file if it is not saved in the same directory. \n \n The important part : style.R has to be sourced in RStudio before the custom styles will be applied. With style.R as the active tab in RStudio, just click the \u201cSource\u201d button. \n \n  The table with custom CSS applied Finally, make tables.Rmd the active RStudio tab and click \u201cKnit HTML\u201d again. You should now see that the custom CSS has been applied to the document as shown on the right. \n Filed under: programming , R , research diary , statistics Tagged: css , how to , html , rstudio"], "link": "http://nsaunders.wordpress.com/2012/08/27/custom-css-for-html-generated-using-rstudio/", "bloglinks": {}, "links": {"http://jeromyanglim.co.nz/": 1, "https://github.com/": 1, "http://www.lmu.de/": 1, "http://yihui.name/": 1, "http://rpubs.com/": 1, "http://rstudio.org/": 2, "http://nsaunders.wordpress.com/": 10, "http://www.rstudio.org/": 1, "http://feeds.wordpress.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["This week in Retraction Watch: Hypertension retracts paper over data glitch . \n The retraction notice describes the \u201cdata glitch\u201d in question (bold emphasis added by me): \n \n\u2026the authors discovered an error in the code for analyzing the data . The National Health and Nutrition \nExamination Survey (NHANES) medication data file had multiple observations per participant and \nwas merged incorrectly with the demographic and other data files. Consequently, the sample size was \ntwice as large as it should have been (24989 instead of 10198). Therefore, the corrected estimates of \nthe total number of US adults with hypertension, uncontrolled hypertension, and so on, are significantly \ndifferent and the percentages are slightly different.\n \n Let\u2019s leave aside the observation that 24989 is not 2 x 10198. I tweeted: \n \"an error in the code for analyzing the data\" - retractionwatch.wordpress.com/2012/08/16/hyp\u2026 . Entirely avoidable if methods were published in full. \u2014 Neil Saunders (@neilfws) August 17, 2012 \n Not that simple though, is it? Read on for the Twitter discussion. \n \n Bosco replied: \n @ neilfws do you really think one can do a complete code review in the average time it takes to review a paper? \u2014 Bosco Ho (@boscoh) August 17, 2012 \n Good point. The best response I could manage at the time was that errors may not be detected during review, but stand a better chance of eventual detection if code is available. Or as Bosco says, this is not really what review is for: \n @ boscoh 1/2 No. We need multiple changes in how things are done, of which publishing reproducible code is just one. \u2014 Neil Saunders (@neilfws) August 17, 2012 \n @ boscoh 2/2 But if the code were \"out there\", there's a chance someone may have spotted the problem. \u2014 Neil Saunders (@neilfws) August 17, 2012 \n @ neilfws agreed. what needs to happen is outside of the actual review process itself. it's more of a meta-requirement to deposit code. \u2014 Bosco Ho (@boscoh) August 17, 2012 \n Chris made a similar point: \n @ neilfws you will be able to find errors in published code, yes. But people will still make them and they are not found automatically. \u2014 Chris Evelo (@Chris_Evelo) August 17, 2012 \n Summary \nIt\u2019s easy to make the mistake of thinking that making code available is the solution to reproducibility and error detection. In fact, it\u2019s just a small part of the solution, since someone (or something) then has to (1) look at the code and (2) notice the errors. \n While we wait for that change to sweep aside traditional scientific publishing, I suggest that if you want to publish research that uses code to generate the results \u2013 at least get your colleagues to take a look before submission. \n Filed under: open science , publications Tagged: reproducibility , retraction"], "link": "http://nsaunders.wordpress.com/2012/08/21/reproducibility-releasing-code-is-just-part-of-the-solution/", "bloglinks": {}, "links": {"http://twitter.com/": 6, "http://feeds.wordpress.com/": 1, "https://twitter.com/": 5, "http://nsaunders.wordpress.com/": 4, "http://retractionwatch.wordpress.com/": 2}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["OK, let\u2019s do this: some statistics and visualization of the tweets for ISMB 2012 . \n \n First, thanks to Stephen Turner who got things started in this post at his excellent blog, Getting Genetics Done . Subscribe to his feed if you don\u2019t already do so. \n I\u2019ve created a Github repository for this project (and future Twitter-related work). If you\u2019d like to reproduce the analyses (or skip reading this post), there\u2019s a Sweave file and the resulting PDF . The Sweave file contains a relative path to the data file, so to run the analysis you\u2019ll need to clone the repository, cd to ismb/code/sweave and run R CMD Sweave ismb.Rnw from there. \n 1. Getting the data from Twitter \nMuch as described by Stephen, I grabbed the relevant tweets like this: \n \nismb1 <- searchTwitter(\"#ISMB\", n = 1500, since = \"2012-07-13\", until = \"2012-07-13\")\n \n creating lists ismb1, ismb2\u2026ismb5 for days 1-5 (July 13-17) of the meeting, by editing the since/until dates as appropriate. \n Next, I wrote this ugly hack which loads the 5 saved lists, then uses the twListToDF() function from the twitteR package to merge them into one data frame, called ismb . That\u2019s saved here as ismb.RData or if you prefer, as CSV . \n Note: I assume that all tweets in this data are from public timelines and as such, the authors do not object to mining and archiving. If that\u2019s not the case and you don\u2019t want me to store your Twitter handle at Github, let me know. \n A quick check that all tweet IDs are unique\u2026yes, they are and there we go: 3 162 tweets with the #ISMB hashtag, ready for analysis. \n Note that the following code snippets require these libraries: ggplot2, xtable, RColorBrewer, tm, wordcloud and sentiment . \n 2. Tweets per day \n \nismb$date <- as.Date(ismb$created)\nbyDay <- as.data.frame(table(ismb$date))\ncolnames(byDay) <- c(\"date\", \"tweets\")\nprint(ggplot(byDay) + geom_bar(aes(date, tweets), fill = \"salmon\") + theme_bw() + opts(title = \"ISMB 2012 tweets per day\"))\n \n \n \n \nThis is pretty straightforward, since we can use as.Date() to convert tweet timestamp to date, then sum tweets by date using table() : \n Several hundred tweets per day; quite a reasonable rate, especially on days 3-5 which is when the main event (ISMB \u201cproper\u201d) took place. \n \n \n  ISMB 2012 tweets per day \n \n \n \n 3. Tweets per hour by day \n \nismb$hour <- as.POSIXlt(ismb$created)$hour\nbyDayHour <- as.data.frame(table(ismb$date, ismb$hour))\ncolnames(byDayHour) <- c(\"date\", \"hour\", \"tweets\")\nbyDayHour$hour <- as.numeric(as.character(byDayHour$hour))\nprint(ggplot(byDayHour) + geom_bar(aes(hour, tweets), fill = \"salmon\", binwidth = 1) + facet_grid(date ~ hour) + opts(axis.text.x = theme_blank(), axis.ticks = theme_blank(), panel.background = theme_blank(), title = \"ISMB 2012 tweets per hour by day\"))\n \n \n \n \nI\u2019m not convinced that this code or the resulting plot are exactly what I was trying to achieve, but it\u2019s probably close enough. \n This code uses a ggplot facet_grid to display hourly tweets (horizontally) across 5 days (vertically). Twitter uses UTC for timestamps, whilst ISMB took place in Long Beach, California, which is UTC \u2013 8 (or currently, UTC \u2013 7 due to daylight saving). This explains why the majority of tweets occur from late afternoon to evening; local time is actually morning through afternoon. \n We could change the time zone before plotting but (1) not everyone who tweets is in the same time zone and (2) not all tweets occur at the same time as the event they describe. So I figure it\u2019s best to leave times in UTC. \n For days 3-5, two \u201cwaves\u201d are discernible which correspond to the morning and afternoon sessions; in particular, one imagines, the keynote talks.\n \n \n  ISMB 2012 tweets per hour by day \n \n \n \n 4. Popular talks \nISMB 2012 used a Twitter hashtag for each talk based on category: keynote (KN), published paper (PP), special session (SS), technology track (TT) and workshop (WK). So for example, tweets might refer to the first keynote as #KN1. We can use this as some measure of talk popularity. \n First we extract hashtags from the tweets, count them and sort: \n \nwords <- strsplit(ismb$text, \" \")\nhashtags <- lapply(words, function(x) x[grep(\"^#\", x)])\nhashtags <- unlist(hashtags)\nhashtags <- tolower(hashtags)\nhashtags <- gsub(\"[^A-Za-z0-9]\", \"\", hashtags)\nht <- as.data.frame(table(hashtags))\nht <- ht[sort.list(ht$Freq, decreasing=F),]\n \n Now we can extract, for example, just the keynotes and plot the count: \n \nkn <- ht[grep(\"^kn\", ht$hashtags),]\nkn$hashtags <- factor(kn$hashtags, levels = as.character(kn$hashtags))\nprint(ggplot(tail(kn)) + geom_bar(aes(hashtags, Freq), fill = \"salmon\") + coord_flip() + theme_bw() + opts(title = \"ISMB 2012 tweets - keynotes\"))\n \n \n \n \nWe have a winner: keynote 3, which was Analysis of transcriptome structure and chromatin landscapes by Barbara Wold. Perhaps the poor coverage of later keynotes can be partially explained by attendees leaving early? \n The plots for other categories of talk look similar so I\u2019ve omitted them here in the interests of not being tedious \u2013 see the PDF if you\u2019re interested. For the record, most-tweeted talks in the other categories were: \n - PP44 : Toward interoperable bioscience data (Susanna-Assunta Sansone) \n- SS4 : Bioinformatic Integration of Diverse Experimental Data Sources (Kyle Ellrott, David Haussler, Artem \nSokolov, Josh Stuart) \n- TT06 : The Taverna Server \u2013 Executing Scientific Workflows Remotely (Katy Wolstencroft) \n- WK3 : Bioinformatics Core Facilities (Simon Andrews, Fran Lewitter, Brent Richter, David Sexton) \n \n \n  ISMB 2012 keynote tweets \n \n \n \n 5. Users \nFirst, let\u2019s examine the \u201clong tail\u201d: \n \nusers <- as.data.frame(table(ismb$screenName))\ncolnames(users) <- c(\"user\", \"tweets\")\nusers <- users[sort.list(users$tweets, decreasing = T),]\nprint(ggplot(users) + geom_point(aes(1:nrow(users), tweets), color = \"salmon\") + theme_bw() + opts(title = \"ISMB 2012 tweets per user\") + xlab(\"User\"))\n \n \n \n \n393 individuals contributed tweets. However, the median tweets per user = 2 and only 11 individuals contributed > 50 tweets. For posterity, let\u2019s name names: \n \n   user Freq\n Chris_Evelo 293\n genetics_blog 221\n  tladeras 162\n  iGenomics 125\n WonderMixTape 124\n  alexishkin 91\n   bffo 90\n  spitshine 69\n Albertagael 62\n  andrewsu 61\n bioontology 53\n \n \n \n  ISMB 2012 tweets per user \n \n \n \n 6. Text mining \n 6.1 Word frequency \nWhat was everyone talking about? I decided to extract words from tweets by splitting on space, extracting only those elements which contain letters and numbers, removing stop words and then removing \u201cRT\u201d and \u201cMT\u201d. This approach loses words in hashtags and words concatenated with punctuation characters, but it\u2019s quicker than trying to clean up words case-by-case and the losses are very small. \n \nsw <- stopwords(\"en\")\nwords <- lapply(words, function(x) x[grep(\"^[A-Za-z0-9]+$\", x)])\nwords <- unlist(words)\nwords <- tolower(words)\nwords <- words[-grep(\"^[rm]t$\", words)]\nwords <- words[!words %in% sw]\nwords.t <- as.data.frame(table(words))\nwords.t <- words.t[sort.list(words.t$Freq, decreasing = T),]\nprint(xtable(head(words.t, 10), caption = \"Top 10 words in tweets\"), include.rownames = FALSE)\n\npal2 <- brewer.pal(8, \"Dark2\")\nwordcloud(words.t$words, words.t$Freq, scale = c(8, .2), min.freq = 3, max.words = 200, random.order = FALSE, rot.per = .15, colors = pal2)\n \n \n \n \nFor those who like a wordcloud \u2013 see image, right. For those who like a top 10 list: \n \ndata talk using gene protein slides network people analysis ismb\n 251 238 132 117  80  73  71  69  68 66\n \n The \u201cbig\u201d words in the wordcloud are pretty obvious; it\u2019s the orange group which caught my attention. In particular: integration, networks, wikipedia and galaxy . Your response may be different which, of course, is all part of the subjective fun with wordclouds.\n \n \n  ISMB 2012 tweet wordcloud (top 200 words) \n \n \n \n 6.2 Sentiment analysis \nI wondered whether to even include this section but\u2026I thought it might be fun to try out the sentiment package. Even if (1) I think sentiment analysis is, on the whole, complete rubbish and (2) tweets from scientific meetings are hardly likely to be very emotional. Bear in mind that at this point, I no longer know what I\u2019m doing. Anyway, let\u2019s start with classification into positive, negative or neutral: \n \npo <- classify_polarity(ismb$text, algorithm = \"bayes\")\nprint(xtable(table(po[, \"BEST_FIT\"]), caption = \"Tweet polarity\"))\n\n# negative neutral positive \n#  573  197  2392\n \n Wow \u2013 tweets from ISMB were largely (76%) \u201cpositive\u201d! What does that mean, exactly? Well, here\u2019s a positive tweet: \n \n\u201cOften forgotten: CS Optimizing (network) models only makes sense when you keep the model simple to not overfit #ISMB #netbiosiig\u201d\n \n What\u2019s positive about that? Optimizing? Simple? Here\u2019s a negative tweet: \n \n\u201cChris Sander in Network Biology SIG and how translational medicine: tries to explain how complicated it all is in cancer biology #ISMB\u201d\n \n Complicated? Cancer? Who knows. How about classification into one of 6 emotions: \n \nem <- classify_emotion(ismb$text, algorithm = \"bayes\")\nprint(xtable(table(em[, \"BEST_FIT\"]), caption = \"Tweet emotion\"))\n\n# anger disgust  fear  joy sadness surprise \n#  24  4  11  149  44  37\n \n ISMB was, clearly, a joyous occasion. Let\u2019s look at a joyous tweet: \n \n\u201cHooray! First #comicsans talk of the conference. Didn\u2019t take long. It\u2019s like an old friend that refuses to die. #ismb\u201d\n \n That\u2019s my issue with sentiment analysis, right there. What people say and what they mean are often quite different things. I assume that people are applying machine learning to irony and sarcasm. \n Summary \nIt\u2019s difficult to compare coverage with previous ISMB meetings (2008-2011), since those meetings used FriendFeed for microblogging and I have not looked at Twitter coverage in previous years. My personal opinion is that a FriendFeed-like system is better suited to conference coverage due to (1) its less \u201chaphazard\u201d nature (what happens when people use hashtags incorrectly?); (2) longer-form comments and (3) threaded discussion. Unfortunately, FriendFeed has become equally non-viable as an archive due to lack of ongoing support and gradual decline in performance * . \n As I mentioned in a previous post, it\u2019s fortunate that I ran searchTwitter() on the day after ISMB closed, because ISMB-related tweets have since, effectively, vanished. This is a concern when using Twitter for conference coverage; it\u2019s not an effective archive. \n I\u2019d say that Twitter coverage of ISMB 2012 was quite extensive, with at least 393 contributors and > 3000 tweets. There are undoubtedly more of both than were captured in this study, which focuses on days 1-5 of the meeting itself. The system of hashtags for each talk was reasonably successful and allowed capture of sufficient tweets for some analysis of talk popularity. \n Perhaps, at the end of the day, Twitter coverage of conferences is as much about \u201ccapturing the moment\u201d as it is about describing the content of talks. \n * Not to mention idiots like myself who might destroy much of the archive by deleting their accounts. I remind you that I did, at least, archive all 2008-2011 ISMB data here . \n Filed under: bioinformatics , meetings , R , statistics Tagged: ismb , ismb2012"], "link": "http://nsaunders.wordpress.com/2012/08/16/twitter-coverage-of-the-ismb-2012-meeting-some-statistics/", "bloglinks": {}, "links": {"http://gettinggeneticsdone.com.au/": 1, "http://www.iscb.org/": 1, "https://github.com/": 7, "http://nsaunders.wordpress.com/": 11, "http://en.wikipedia.org/": 1, "http://feeds.wordpress.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["In previous years, when FriendFeed was used as the micro-blogging platform for the annual ISMB meeting , I\u2019ve written a post describing some statistical analysis of the conference coverage. Here\u2019s my post from last year. \n This year, it appears that the majority of the conference coverage happened at Twitter, using the #ISMB hashtag. Here\u2019s what happened on July 18th when I used the R package twitteR to retrieve ISMB-related tweets for July 13/14: \n \nlibrary(twitteR)\nismb1 <- searchTwitter(\"#ISMB\", since = \"2012-07-13\", until = \"2012-07-14\")\nlength(ismb1)\n# [1] 383\n \n 383 tweets. Here\u2019s what happened when I ran the same query today: \n \nlibrary(twitteR)\nismb1 <- searchTwitter(\"#ISMB\", since = \"2012-07-13\", until = \"2012-07-14\")\nlength(ismb1)\n# [1] 0\n \n Zero tweets. Indeed, run the same query via the Twitter web interface and you\u2019ll see only a very few tweets with the message \u201cOlder Tweet results for #ismb are unavailable.\u201d \n So far as Twitter is concerned, ISMB 2012 never happened. Or if it did, the data are buried away in a data centre, inaccessible to the likes of you and I. Did you ever hear anything more about that plan to archive every Tweet at the Library of Congress? Neither did I. I very much doubt that it\u2019s going to happen. \n I think Twitter is great \u2013 for broadcasting short pieces of information, such as useful URLs, in near real-time. For conference coverage which benefits from threaded conversation, longer comments and archiving, I think it\u2019s rubbish. \n On July 18 I did manage to retrieve 3162 Tweets for ISMB 2012, created between July 13 and July 17. I\u2019ll write about them in a forthcoming post. All I\u2019ll say for now is \u2013 lucky I was able to grab them when I did. \n Filed under: bioinformatics , meetings Tagged: ismb , ismb2012 , twitter"], "link": "http://nsaunders.wordpress.com/2012/08/13/ismb-2012-on-twitter-here-today-gone-tomorrow/", "bloglinks": {}, "links": {"http://www.iscb.org/": 1, "http://nsaunders.wordpress.com/": 6, "https://github.com/": 1, "http://feeds.wordpress.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["Update : as pointed out in the comments, the amusing error in this article has been \u201ccorrected\u201d (or at least, \u201cedited away\u201d). Thanks for your interest. \n Update : I note that this article is now \u201cHighly Accessed\u201d ;) \n \n An integrative analysis of DNA methylation and RNA-Seq data for human heart, kidney and liver \n BMC Systems Biology 2011, 5(Suppl 3):S4\n \n  (insert statistical method here). No, really. \n With thanks to Simon J Greenhill and Dave Winter . \n Filed under: bioinformatics , publications , statistics Tagged: bmc , errors , journals"], "link": "http://nsaunders.wordpress.com/2012/07/23/we-really-dont-care-what-statistical-method-you-used/", "bloglinks": {}, "links": {"http://www.biomedcentral.com/": 1, "https://twitter.com/": 1, "http://nsaunders.wordpress.com/": 7, "http://feeds.wordpress.com/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["A comprehensive framework for prioritizing variants in exome sequencing studies of Mendelian diseases . \n  Why not also highlight the key genes by colouring cells in red? \n You\u2019re welcome . \n Filed under: bioinformatics , publications Tagged: best practice , errors , reproducibility"], "link": "http://nsaunders.wordpress.com/2012/07/18/fixed-that-for-you/", "bloglinks": {}, "links": {"http://www.biomedcentral.com/": 1, "http://nar.oxfordjournals.org/": 1, "http://nsaunders.wordpress.com/": 6, "http://feeds.wordpress.com/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["A couple of years ago, I noted that some journals were not making the process of commenting on articles especially easy. My latest experience suggests that little has changed. \n \nOn April 26 this year, I read A comparison of feature selection and classification methods in DNA methylation studies using the Illumina Infinium platform . It\u2019s an interesting comparative study, completely devoid of the code used to reach the conclusions. Nothing unusual about that \u2013 most published articles are similarly deficient. Still, I thought it was worth highlighting the issue. My comment: \n \nThis type of comparative study is potentially very useful. However, how are we supposed to reproduce the results or try the methods for ourselves without the code used by the authors? I find it baffling that journals publish statistical analyses without sufficient detail for others to reproduce.\n \n Eleven weeks later\u2026I receive a response: \n \nMany thanks for writing to BMC Bioinformatics with a comment for one of our articles. We are in principle always willing to publish a variety of points of view. However, in this instance I should be most grateful if you could slightly rephrase your comments with a rather more positive slant. \n We are an open access series and encourage authors to provide code and data to the readers of our BMC series journals.\n \n Well OK\u2026my comment could have been more constructive and I could have focused on the article, rather than drifting off into a more general rant about \u201cthe state of things.\u201d And sure, site owners should have their own rules regarding comments \u2013 I do. That said: 11 weeks, to request a slight rewording of something not particularly offensive? \n I\u2019ll stick to discussing articles in near real-time on Twitter, thanks. Judging by comparison of my Twitter stream to journal websites, so will everyone else. Perhaps it\u2019s time for journals to admit that in general, comments on the article page itself don\u2019t work and just turn them off. \n Filed under: uncategorized Tagged: comment , interaction , journals , publishing"], "link": "http://nsaunders.wordpress.com/2012/07/13/comments-at-journals-websites-just-turn-them-off/", "bloglinks": {}, "links": {"http://www.biomedcentral.com/": 1, "http://nsaunders.wordpress.com/": 6, "http://feeds.wordpress.com/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}, {"content": ["Bioinformaticians (and anyone else who programs) love effective automation of mundane tasks. So it may amuse you to learn that I used to update PMRetract , my PubMed retraction notice monitoring application, by manually running the following steps in order: \n \n Run query at PubMed website with term \u201cRetraction of Publication[Publication Type]\u201c \n Send results to XML file \n Run script to update database with retraction and total publication counts for years 1977 \u2013 present \n Run script to update database with retraction notices \n Run script to update database with retraction timeline \n Commit changes to git \n Push changes to Github \n Dump local database to file \n Restore remote database from file \n Restart Heroku application \n \n I\u2019ve been meaning to wrap all of that up in a Rakefile for some time. Finally, I have . Along the way, I learned something about using efetch from BioRuby and re-read one of my all-time favourite tutorials , on how to write rake tasks. So now, when I receive an update via RSS, updating should be as simple as: \n \nrake pmretract\n \n In other news: it\u2019s been quiet here, hasn\u2019t it? I recently returned from 4 weeks overseas, packed up my office and moved to a new building. Hope to get back to semi-regular posts before too long. \n Filed under: programming , ruby Tagged: automation , rake , retraction"], "link": "http://nsaunders.wordpress.com/2012/07/05/pmretract-now-with-rake-tasks/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://pmretract.heroku.com/": 1, "http://www.biostars.org/": 1, "http://nsaunders.wordpress.com/": 5, "http://jasonseifer.com/": 1, "https://github.com/": 1}, "blogtitle": "What You're Doing Is Rather Desperate"}]