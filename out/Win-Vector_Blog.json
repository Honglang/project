[{"blogurl": "http://www.win-vector.com/blog\n", "blogroll": [], "title": "Win-Vector Blog"}, {"content": ["We have added a worked example to the README of our experimental logistic regression code . \n The Logistic codebase is designed to support experimentation on variations of logistic regression including: \n \n A pure Java implementation (thus directly usable in Java server environments). \n A simple multinomial implementation (that allows more than two possible result categories). \n The ability to work with too large for memory data-sets and directly from files or database tables. \n A demonstration of the steps needed to use standard Newton-Raphson in Hadoop. \n Ability to work with arbitrarily large categorical inputs. \n Provide explicit L2 model regularization. \n Implement safe optimization methods (like conjugate gradient, line-search and majorization ) for situations where the standard Iteratively-re-Weighted-Least-Squares/Newton-Raphson fails. \n Provide an overall framework to quickly try implementation experiments (as opposed to novel usage experiments). \n \n What we mean by this code being \u201cexperimental\u201d is that it has capabilities that many standard implementations do not. In fact most of the items in the above list are not usually made available to the logistic regression user. But our project is also stand-alone and not as well integrated into existing workflows as standard production systems. Before trying our code you may want to try R or Mahout . In principle running the code is easy: all you do is supply a training file as a TSV (tab separated file) or CSV (comma separated file), write down the column you want to predict as a schematic formula of the columns you wish to use in your model. In practice it is a bit harder: you have to have already set up your Java or Hadoop environment to bring in all required dependencies. \n Setting up a Hadoop configuration can range from simple (like the single machine tests in our projects JUnit test suite) to complicated. Also, for non-trivial clusters you often do not control the configuration (i.e. somebody else supplies the cluster environment). So we really can\u2019t tell you how to set up your non-trivial Hadoop environment (just too many variables). Some time ago we supplied a complete example of how to run an example on Amazon\u2019s Elastic Map Reduce (but that was in 2010, so the environment may have changed a bit. However the current code runs at least on Hadoop versions 0.20.0, 1.0.0 and 1.0.3 without modification). \n Our intent was never to depend on Hadoop except in the case of very large data (and even then there are other options like sub-sampling and Mahout). So we supplied direct Java command line options. It is time to re-work a simple example of using these options (this example is now in the project README file). We repeat this example here. We are assuming a command-line environment (for example the Bash shell on OSX or Linux, but this can be done on Windows using either CMD or Cygwin). \n We show what works for us in our environment, you will have to adapt to your environment as it differs. \n Example of running at the command line (using some Apache support classes, but not running under Hadoop): \n \n \n Get a data file. For our example we took the data file from the UCI Iris data example saved it and added a header line of the form \u201cSepalLength,SepalWidth,PetalLength,PetalWidth,TrainingClass\u201d to make the file machine readable. The edited file is available here: iris.data.txt . \n \n \n Get all the supporting code you need and set your Java CLASSPATH. To run this you need all of the classes from: \n \n \n https://github.com/WinVector/Logistic \n \n \n https://github.com/WinVector/SQLScrewdriver \n \n \n http://acs.lbl.gov/software/colt/ \n \n \n Some of the Apache commons files (command line parsing and logging). We got these from the Hadoop-1.0.3 lib directory: http://hadoop.apache.org/releases.html#Download . It turns out we only need hadoop-1.0.3/lib/commons-logging-1.1.1.jar , hadoop-1.0.3/lib/commons-logging-api-1.0.4.jar and hadoop-1.0.3/lib/commons-cli-1.2.jar from the Hadoop package if we are not running a Map Reduce.\n \n \n This is complicated- but it is one time set up cost. In practice you would not manipulate classes directly at the command line by use an IDE like Eclipse or a build manager like Maven to do all of the work. But not everybody uses the same tools and tools bring in even more dependencies and complications; so we show how to set up the class paths directly. \n In our shell (bash on OSX) we set our class variable as follows: \n \n \n CLASSES=\"/Users/johnmount/project/hadoop-1.0.3/lib/commons-logging-1.1.1.jar:/Users/johnmount/project/hadoop-1.0.3/lib/commons-logging-api-1.0.4.jar:/Users/johnmount/project/hadoop-1.0.3/lib/commons-cli-1.2.jar:/Users/johnmount/project/Logistic/bin:/Users/johnmount/project/Colt-1.2.0/bin:/Users/johnmount/project/SQLScrewdriver/bin\" \n \n \n We are using where we put the files \u201c/Users/johnmount/project/\u201d and the path separator \u201c:\u201d (separator is \u201c;\u201d on Windows). The path you would use would depend on where you put the files you downloaded. \n \n \n In the directory you saved the training data file run the logistic training procedure: \n \n \n java -cp $CLASSES com.winvector.logistic.LogisticTrain -trainURI file:iris.data.txt -sep , -formula \"TrainingClass ~ SepalLength + SepalWidth + PetalLength + PetalWidth\" -resultSer iris_model.ser \n \n \n This produces iris_model.ser , the trained model. The diagnostic printouts show the confusion matrix (tabulation of training class versus predicted class) and show a high degree of training accuracy. \n \nINFO: Consfusion matrix:\nprediction\tactual\tactual\tactual\nindex:outcome\t0:Iris-setosa\t1:Iris-versicolor\t2:Iris-virginica\n0:Iris-setosa\t50\t0\t0\n1:Iris-versicolor\t0\t47\t1\n2:Iris-virginica\t0\t3\t49\n \n Notice that there are only 4 training errors (1 Iris-verginica classified as Iris-versicolor and 3 iris-versicolor classified as Irs-virginica). \n The model coefficients are also printed as part of the diagnostics. \n \nINFO: soln details:\noutcome\toutcomegroup\tvariable\tkind\tlevel\tvalue\nIris-setosa\t0\t\tConst\t\t0.774522294889561\nIris-setosa\t0\tPetalLength\tNumeric\t\t-5.192578560594749\nIris-setosa\t0\tPetalWidth\tNumeric\t\t-2.357410090972314\nIris-setosa\t0\tSepalLength\tNumeric\t\t1.69382466234698\nIris-setosa\t0\tSepalWidth\tNumeric\t\t3.9224697382723903\nIris-versicolor\t1\t\tConst\t\t1.8730846627542541\nIris-versicolor\t1\tPetalLength\tNumeric\t\t-0.3747220505092903\nIris-versicolor\t1\tPetalWidth\tNumeric\t\t-2.839314336523609\nIris-versicolor\t1\tSepalLength\tNumeric\t\t1.1786843497402208\nIris-versicolor\t1\tSepalWidth\tNumeric\t\t0.2589801610139257\nIris-virginica\t2\t\tConst\t\t-2.6476069576668615\nIris-virginica\t2\tPetalLength\tNumeric\t\t5.567416774143793\nIris-virginica\t2\tPetalWidth\tNumeric\t\t5.196786825681218\nIris-virginica\t2\tSepalLength\tNumeric\t\t-2.8725550015586947\nIris-virginica\t2\tSepalWidth\tNumeric\t\t-4.1815354814927215\n \n \n \n In the same directory run the logistic scoring procedure: \n \n \n java -cp $CLASSES com.winvector.logistic.LogisticScore -dataURI file:iris.data.txt -sep , -modelFile iris_model.ser -resultFile iris.scored.tsv \n \n \n This produces iris.scored.tsv , the final result. The scored file is essentially the input file (in this case iris.data.txt) copied over with a few prediction columns (predicted category, confidence in predicted category and probability for each possible outcome category) prepended.\n \n \n For the Hadoop demonstrations of training and scoring the commands are as follows (though obviously some of the details depend on your Hadoop set-up): \n \n Get or build a jar containing the Logistic code, SQLScrewdriver code and free portions of the COLT library (pre built: WinVectorLogistic.Hadoop0.20.2.jar ).\n \n Make sure the training file is tab-separated (instead of comma separated). For example the iris data in such a format is here: iris.data.tsv \n \n \nRun the Hadoop version of the trainer: \n \n \n/Users/johnmount/project/hadoop-1.0.3/bin/hadoop jar /Users/johnmount/project/Logistic/WinVectorLogistic.Hadoop0.20.2.jar logistictrain iris.data.tsv \"TrainingClass ~ SepalLength + SepalWidth + PetalLength + PetalWidth\" iris_model.ser \n \n \n \n \nRun the Hadoop version of the scorring function: \n \n \n/Users/johnmount/project/hadoop-1.0.3/bin/hadoop jar /Users/johnmount/project/Logistic/WinVectorLogistic.Hadoop0.20.2.jar logisticscore iris_model.ser iris.data.tsv scoreDir \n \n \n \n \n Scored output is left in Hadoop format in the user specified scoreDir (slightly different format than the stand-alone programs). The passes take quite a long time due to the overhead of setting up and tearing down a Hadoop environment for such a small problem. Also if you are running in a serious Hadoop environment (like elastic map-reduce) you will have to change certain file names to the type of URI type the system is using. In our elastic map reduce example we used S3 containers which had forms like: \u201cs3n://bigModel.ser\u201d and so on. \n The Hadoop code can also be entered using the main() s found in com.winvector.logistic.demo.MapReduceLogisticTrain and com.winvector.logistic.demo.MapReduceScore . This allows interactive debugging through an IDE (like Eclispe) without having go use the \u201chadoop\u201d command. \n Again: this is experimental code. It can do things other code bases can not. If you need one of its features or capabilities it is very much worth the trouble. But if you can make do with a standard package like R you have less trouble and are more able to interact with others work. \n Related posts: \n Large Data Logistic Regression (with example Hadoop code) \n How robust is logistic regression? \n The equivalence of logistic regression and maximum entropy models"], "link": "http://www.win-vector.com/blog/2012/10/added-worked-example-to-logistic-regression-project/?utm_source=rss&utm_medium=rss&utm_campaign=added-worked-example-to-logistic-regression-project", "bloglinks": {}, "links": {"http://archive.uci.edu/": 1, "https://github.com/": 8, "http://acs.lbl.gov/": 1, "http://www.win-vector.com/blog": 9, "https://cwiki.apache.org/": 1, "http://cran.r-project.org": 1, "http://hadoop.apache.org/": 1}, "blogtitle": "Win-Vector Blog"}, {"content": ["Check out: I Write, Therefore I Think \n Related posts: \n Congratulations to both Dr. Nina Zumel and EMC- great job \n An Appreciation of Locality Sensitive Hashing"], "link": "http://www.win-vector.com/blog/2012/10/win-vectors-nina-zumel-i-write-therefore-i-think/?utm_source=rss&utm_medium=rss&utm_campaign=win-vectors-nina-zumel-i-write-therefore-i-think", "bloglinks": {}, "links": {"http://ninazumel.com/": 1, "http://www.win-vector.com/blog": 2}, "blogtitle": "Win-Vector Blog"}, {"content": ["It\u2019s often the case that I want to write an R script that loops over multiple datasets, or different subsets of a large dataset, running the same procedure over them: generating plots, or fitting a model, perhaps. I set the script running and turn to another task, only to come back later and find the loop has crashed partway through, on an unanticipated error. Here\u2019s a toy example: \n \n > inputs = list(1, 2, 4, -5, 'oops', 0, 10)\n\n> for(input in inputs) {\n+ print(paste(\"log of\", input, \"=\", log(input)))\n+ }\n\n[1] \"log of 1 = 0\"\n[1] \"log of 2 = 0.693147180559945\"\n[1] \"log of 4 = 1.38629436111989\"\n[1] \"log of -5 = NaN\"\nError in log(input) : Non-numeric argument to mathematical function\nIn addition: Warning message:\nIn log(input) : NaNs produced\n \n \n The loop handled the negative arguments more or less gracefully (depending on how you feel about NaN), but crashed on the non-numeric argument, and didn\u2019t finish the list of inputs. \n How are we going to handle this? \n \n The try block \n The most straightforward way is to wrap our problematic call in a try block: \n \n \n> for(input in inputs) {\n+ try(print(paste(\"log of\", input, \"=\", log(input))))\n+ }\n\n[[1] \"log of 1 = 0\"\n[1] \"log of 2 = 0.693147180559945\"\n[1] \"log of 4 = 1.38629436111989\"\n[1] \"log of -5 = NaN\"\nError in log(input) : Non-numeric argument to mathematical function\nIn addition: Warning message:\nIn log(input) : NaNs produced\n[1] \"log of 0 = -Inf\"\n[1] \"log of 10 = 2.30258509299405\"\n \n \n This skips over the error-causing non-numeric input with an error message (you can suppress the error message with the silent=T argument to try ), and continues on with the rest of the input. Generally, this is what you would like. \n The tryCatch block \n Sometimes, however, you might want substitute your own return value when errors (or warnings) are returned. We can do this with tryCatch , which allows you to write your own error and warning handlers. Let\u2019s set our loop to return log(-x) when x is negative (negative arguments throw a warning) and return a NaN for non-numeric arguments (which throw an error). We\u2019ll print out an advisory message, too. \n \n \n> for(input in inputs) {\n+  tryCatch(print(paste(\"log of\", input, \"=\", log(input))),\n+    warning = function(w) {print(paste(\"negative argument\", input)); \n          log(-input)},\n+    error = function(e) {print(paste(\"non-numeric argument\", input));\n         NaN})\n+ }\n\n[1] \"log of 1 = 0\"\n[1] \"log of 2 = 0.693147180559945\"\n[1] \"log of 4 = 1.38629436111989\"\n[1] \"negative argument -5\"\n[1] \"non-numeric argument oops\"\n[1] \"log of 0 = -Inf\"\n[1] \"log of 10 = 2.30258509299405\" \n \n Whoops \u2014 not quite! We are correctly catching and messaging warnings and errors, but we are not printing out our desired corrected value. This is because the warning and error handlers are altering the execution order and throwing out of the print statement. If we want to return and print out the appropriate value when warnings and errors are thrown, we have to wrap our tryCatch into a function. We\u2019ll leave the advisory message in. \n \n \n> robustLog = function(x) {\n+ tryCatch(log(x),\n+   warning = function(w) {print(paste(\"negative argument\", x)); \n         log(-x)},\n+   error = function(e) {print(paste(\"non-numeric argument\", x)); \n         NaN}) \n+ }\n> \n> for(input in inputs) {\n+ print(paste(\"robust log of\", input, \"=\", robustLog(input)))\n+ }\n\n[1] \"robust log of 1 = 0\"\n[1] \"robust log of 2 = 0.693147180559945\"\n[1] \"robust log of 4 = 1.38629436111989\"\n[1] \"negative argument -5\"\n[1] \"robust log of -5 = 1.6094379124341\"\n[1] \"non-numeric argument oops\"\n[1] \"robust log of oops = NaN\"\n[1] \"robust log of 0 = -Inf\"\n[1] \"robust log of 10 = 2.30258509299405\"\n \n \n Now we return and print out a valid numeric value for numeric inputs to robustLog , and a NaN only for non-numeric input. Notice also that log(0) still returns -Inf, with no warning or error. \n Of course, now that we are writing a new function, it would make more sense to check the arguments before calling log , to avoid the recalculation. This example is only to demonstrate tryCatch , which is useful for defending against unexpected errors. \n Advanced Exception Handling \n The above is about as much about exception and error handling in R as you will usually need to know, but there are a few more nuances. The documentation for tryCatch claims that it works like Java or C++ exceptions: this would mean that when the interpreter generates an exceptional condition and throws, execution then returns to the level of the catch block and all state below the try block is forgotten. In practice, tryCatch is a bit more powerful than that, because you have the ability to insert custom warning and exception handlers. There is another exception handling routine called withCallingHandlers that similarly allows you to insert custom warning and exception handlers. There may be some difference in semantics or in environment context between tryCatch and withCallingHandlers ; but we couldn\u2019t find it. \n The final concept in R\u2019s error handling is withRestarts , which is not really an error handling mechanism but rather a general control flow structure. The withRestarts structure can return to a saved execution state, rather like a co-routine or long-jump. It can be used with withCallingHandlers or with tryCatch to design either interactive or automated \u201cretry on failure\u201d mechanisms, where the retry logic is outside of the failing function. Although obviously a function that checks for potential errors and alters its behavior before signaling a failure is much easier to maintain. \n Here\u2019s as simple an example of using restarts as we could come up with. The idea is that there is some big expensive computation that you want to do with the function input before you get to the potentially error-causing code. You want the exception handlers to mitigate the failure and continue running the code without having to redo the expensive calculation. Imagine this function as being part of a library of routines that you wish to call regularly. \n By default, our example routine will enter R\u2019s debugging environment upon exception. The user then has to select the appropriate restart function to continue the operation. \n \n \n> # argument x: item to take logarithm of\n> # argument warning: warning handler\n> # argument error: error handler\n> # invokeRestart(\"flipArg\"): re-runs function on -x if x \n> # (appropriate fix for negative numeric arguments)\n> # invokeRestart(\"zapOutArg\"): re-runs function on x=1 \n> # (appropriate fix for non-numeric arguments)\n> expensiveBigLibraryFunction \n \n Here's the code working on valid input. \n \n \n> # normal operation\n> expensiveBigLibraryFunction(2)\n[1] \"big expensive step we don't want to repeat for x: 2\"\n[1] \"attempt cheap operation for z: 2\"\n[1] 0.6931472\n \n \n Here's what happens when you call the code with a negative argument, and then invoke the correct restart. \n \n \n> # bad numeric argument (negative)\n> # user must restart with flipArg\n> expensiveBigLibraryFunction(-2)\n[1] \"big expensive step we don't want to repeat for x: -2\"\n[1] \"attempt cheap operation for z: -2\"\n[1] \"warning: simpleWarning in log(z): NaNs produced\\n\"\nCalled from: function (w) \n{\n print(paste(\"warning:\", w))\n browser()\n}(list(message = \"NaNs produced\", call = log(z)))\nBrowse[1]> invokeRestart(\"flipArg\")\n[1] \"attempt cheap operation for z: 2\"\n[1] 0.6931472\n \n \n Here's what happens when you call the code with a non-numeric argument, and then invoke the inappropriate restart. \n \n \n> # bad non-numeric argument\n> # flipArg is the wrong restart function\n> expensiveBigLibraryFunction('a')\n[1] \"big expensive step we don't want to repeat for x: a\"\n[1] \"attempt cheap operation for z: a\"\n[1] \"e: Error in log(z): Non-numeric argument to mathematical function\\n\"\nCalled from: h(simpleError(msg, call))\nBrowse[1]> invokeRestart(\"flipArg\")\nError in -z : invalid argument to unary operator\n \n \n Here's what happens when you call the code with a non-numeric argument, and then invoke the correct restart. \n \n \n> # bad non-numeric argument\n> # zapOutArg is the right restart function\n> expensiveBigLibraryFunction('a')\n[1] \"big expensive step we don't want to repeat for x: a\"\n[1] \"attempt cheap operation for z: a\"\n[1] \"e: Error in log(z): Non-numeric argument to mathematical function\\n\"\nCalled from: h(simpleError(msg, call))\nBrowse[1]> invokeRestart(\"zapOutArg\")\n[1] \"attempt cheap operation for z: 1\"\n[1] 0\n \n \n Of course, you probably don't want to have invoke the restart manually. so we will rewrite the exception handlers to invoke the appropriate restart automatically. \n \n > autoBigLibraryFunction = function(x) {\n+ expensiveBigLibraryFunction(x,\n+        warning=function(w) {invokeRestart(\"flipArg\")},\n+        error=function(e) {invokeRestart(\"zapOutArg\")})\n+ }\n\n> autoBigLibraryFunction(2)\n[1] \"big expensive step we don't want to repeat for x: 2\"\n[1] \"attempt cheap operation for z: 2\"\n[1] 0.6931472\n\n> autoBigLibraryFunction(-2)\n[1] \"big expensive step we don't want to repeat for x: -2\"\n[1] \"attempt cheap operation for z: -2\"\n[1] \"attempt cheap operation for z: 2\"\n[1] 0.6931472\n\n> autoBigLibraryFunction('a')\n[1] \"big expensive step we don't want to repeat for x: a\"\n[1] \"attempt cheap operation for z: a\"\n[1] \"attempt cheap operation for z: 1\"\n[1] 0\n \n \n Using withRestart is a bit complex, as you can see. Fortunately try and tryCatch will most likely be good enough for the vast majority of your exception handling needs. \n Related posts: \n R annoyances \n Your Data is Never the Right Shape \n Survive R"], "link": "http://www.win-vector.com/blog/2012/10/error-handling-in-r/?utm_source=rss&utm_medium=rss&utm_campaign=error-handling-in-r", "bloglinks": {}, "links": {"http://www.win-vector.com/blog": 3, "http://cran.r-project.org/": 1}, "blogtitle": "Win-Vector Blog"}, {"content": ["We have been writing for a while about the convergence of Newton steps applied to a logistic regression (See: What does a generalized linear model do? , How robust is logistic regression? and Newton-Raphson can compute an average ). This is all based on our principle of working examples for understanding. This eventually progressed to some writing on the nature of problem solving (a nice complement to our earlier writing on calculation ). In the course of research we were directed to a very powerful technique called the MM algorithm (see: \u201cThe MM Algorithm\u201d Kenneth Lang, 2007 ; \u201cA Tutorial on MM Algorithms\u201d, David R. Hunter, Kenneth Lange, Amer. Statistician 58:30\u201337, 2004; and \u201cMonotonicity of Quadratic-Approximation Algorithms\u201d, Dankmar Bohning, Bruce G. Lindsay, Ann. Inst. Statist. Math, Vol. 40, No. 4, pp 641-664, 1988). The MM algorithm introduces an essential idea: majorized functions (not to be confused with the majorized order on R^d ). Majorization it is an interesting way to modify Newton methods to be reliable contractions (and therefore converge in a manner similar to EM algorithms). \n Here we will work an example of the MM method. We will not work it in its most general form, but in a form that quickly reveals much of the beauty of the method. We also introduce a \u201ccollared Newton step\u201d which guarantees convergence without resorting to line-search (essentially resolving the issues in solving a logistic regression by Newton style methods ). To find an optimal logistic regression model parameters we try to find a B minimizing a function of the form: \n \n \nf(B) = - sum_i log(P(x(i),y(i),B)) \n \n \n where P(x(i),y(i),B) is the probability the model assigns to an outcome of y(i) given an input of x(i) (see The Simpler Derivation of Logistic Regression ). To do this it is traditional to find a stationary point (a point where the vector f\u2019(B) is zero) by taking Newton-Rapshon steps of the form: \n \n \nB(k+1) = B(k) - inverse(f''(B(k))) f'(B(k)) \n \n \n where B(k) is our k\u2019th estimate of the coefficients (starting with B(1) = 0), f\u2019() is the gradient vector of f() and f\u201d() is the Hessian matrix of f() (which is also the Jacobian matrix of the vector f\u2019()). We terminate if f\u2019(B(k)) is sufficiently close to zero. When f() is a quadratic function with a unique minimum the method is exact and B(2) is the optimal simultaneous setting of parameters. By analogy if f() is locally well approximated by a quadratic function then the method converges very fast to the optimal point. \n But the method is not always monotone in f(), sometimes f() is larger after an update step instead of getting smaller. With some starts the updates diverge and fail to solve the problem. Many implementations ignore this possibility (and in fact fail). Some implementations perform a line search and use the update: \n \n \nB(k+1) = B(k) - s(k) inverse(f''(B(k))) f'(B(k)) \n \n \n where s(k) is some near optimal step size between zero and one. This fixes the problem, but is a little unsatisfying: we still need to argue that s(k) isn\u2019t forced to zero (preventing progress). Also, one of the virtues of the Newton-Raphson method is it tells you exactly what to do without requiring additional search. The simplicity of the original Newton-Raphson \u201citerate this formula until done\u201d is much preferred to the search and bookkeeping required when using methods such as those found in \u201cAlgorithms for Minimization Without Derivatives\u201d, Richard P. Brent ,1973. \n So the question is: can we do a bit more math to avoid having to do a bit more programming? \n One candidate idea is: introducing a majorizing function g(,). Such a g(,) is a function that takes two vector arguments and obeys the following relations: \n \n \nf(B) = g(B,B) for all B\n \n \nf(z) \u2264 g(z,B) for all z in N(B) (the \u201cneighborhood of B\u201d).\n \n \n Note that the standard definition takes N(B) to be all space (i.e. their is no neighborhood restriction). For our application we will take N(B) to be all z such that ||z-B|| \u2264 r(k) for some r(k) > 0 (perhaps taking r(k)=1, for example). g(,) becomes very valuable when we insist on one more property: \n \n \nWe can find a vector C(g,B) in N(B) such that g(C(g,B),B) < g(B,B).\n \n \n Suppose we could produce such a g(,). The MM algorithm it to compute a series of parameter B(k) as follows: \n \nB(k+1) = C(g,B(k)).\n \n By our assumptions we have: \n \nf(B(k+1)) = f(C(g,B(k))) \u2264 g(C(g,B(k)),B(k)) < g(B(k),B(k)) = f(B(k))\n \n or the sequence of B(k)\u2019s is driving f() to lower and lower values as desired. \n The following diagram illustrates the desired situation: \n \n That is well and nice. But how do we know such a g(,) with all of these nice properties even exists? The more nice things we insist on the less likely there is such a g(,). Fortunately in this case we have not yet asked too much. Using the Lagrange form of Taylor\u2019s remainder theorem can quickly write-down a suitable g(,). \n The first few terms of the Taylor series of our particular f() (expanded around B) are: \n \n \nf(x) ~ f(B) + f'(B)(x-B) + transpose(x-B) f''(B) (x-B)/(2!) + ... \n \n \n The remainder form says we exactly have: \n \n \nf(x) = f(B) + f'(B)(x-B) + transpose(x-B) f''(Z) (x-B)/(2!) \n \n \n where Z is some point between x and B (in our case we will weaken this to just some point in N(B)). Further suppose we found a quadratic form G(B) such that: \n \n \ntranspose(z-B) G(B) (z-B) \u2265 transpose(z-B) f''(Z) (z-B) \n \n \n for all z,Z in N(B). Then: a great majorizing function g(,) would just be the quadratic form: \n \n \ng(x,B) = f(B) + f'(B)(x-B) + transpose(x-B) G(B) (x-B)/(2!). \n \n \n This g(,) satisfies: g(B,B) = f(B), f(C) \u2264 g(C,B) for all C in N(B). And because g(x,B) is a quadratic form in x we know the Newton-Raphson method minimizes it in exactly one step so we can set C(g,B) to be this solution: \n \n \nC(g,B) = B - inverse(G(B)) f'(B). \n \n \n We know g(C(g,B),B) < g(B,B) (unless f\u2019(B) = 0 or G(B) is rank-deficient). \n So we have the majorizing g(,) we need if we can just produce a G(B) with the properties mentioned above. From The Simpler Derivation of Logistic Regression we know f\u201d(B) itself has the form: \n \n \nf''(B) = sum_i P_i(1-P_i) x(i) transpose(x(i)) \n \n \n where x(i) is the vector known values for the i-th example and P_i is shorthand for P(B,x(i)): the model\u2019s current estimated probability of an example with known values x(i) is a positive or true example. Notice since P_i is between 0 and 1 we have P_i(1 \u2013 P_i) is never more than 1/4. So define G(B) as the sum: \n \n \nG(B) = sum_i (1/4) x(i) transpose(x(i)). \n \n \n We can show we have for all Z in N(B): G(B) \u2265 f\u201d(Z) under the usual order of positive semi-definite matrices . This means that for all z,Z in N(B) we have transpose(z-B) G(B) (z-B) \u2265 transpose(z-B) f\u201d(Z) (z-B). And we have the G(B) we needed. Thus our g(,) constructed from G() has all of the specified properties. \n Also notice for the first step standard Newton Step the P_i are all equal to 1/2 (given B(1) = 0) so the original 1/4 bound is both correct and tight. We now have shown that the first Newton-Raphson step can not increase f() when solving a logistic regression (starting from zero) unless we have a rank-deficiency. A direct Newton-Raphson implementation may still diverge (as we showed in our earlier writing), but it won\u2019t show an increase on the first step. It is not that f() is exactly a quadratic function at the first step (it is not). It is that f() is majorized by its own truncated Taylor series g(,0) at the first step. \n When we take N(B) as the whole space the above is the standard majorizing construction. That standard construction has the additional advantage that G(B) is independent of B; so we can form the inverse(G(B)) just once and re-use it in every iteration. \n Our non-standard suggestions (base on using smaller N(B)) are as follows. Instead of using the global relation 1/4 \u2265 P_i(1-P_i) we can instead pick potentially smaller b(i) such that b(i) \u2265 P_i(1-P_i) for all points in N(B). The point being if we only need the bound to be true over all of N(B) (instead of over all space) we may be able to pick b(i) much closer to the actual P_i(1-P_i). We can try b(i) = sup P(z,x(i)) (1-P(z,x(i)) for all z in N(B) (which in later iterations may be much less that 1/4 for many data points). A closer bound would potentially drive faster convergence (allowing the MM steps to behave more like the often more beneficial Newton-Raphson steps). Our point is: the standard global bound may not be as useful later in the optimization (hence the introduction of the N(B) allowing us to use possibly tighter b(i) as long as we limit steps to stay in the N(B)). \n Another non-standard use of the majorizing g(,) function would be to not take the majorizing update, but instead take the Newton-Raphson step if g(B(k+1),B(k)) < g(B(k),B(k)) (which ensures the Newton-Raphson step is productive: f(B(k+1)) < f(B(k))). If this relation fails we can take the MM step (B(k+1) = B(k) \u2013 inverse(G(B(k))) f\u2019(B(k))). Or we can perform a partial Newton-Raphson step (B(k+1) = B(k) \u2013 s(k) inverse(f\u201d(B(k))) f\u2019(B(k))) but use g(,B(k)) to pick the s(k) instead more expensive line-search on f(). \n The point is the more complicated function g(,) allows us to encode more useful facts and guarantees about the properties of f(). The richer structure and additional choice in constructing a g(,) (already given on f()) allows us to come up with reliable optimization procedures that do not require line search. \n Related posts: \n How robust is logistic regression? \n Newton-Raphson can compute an average \n The equivalence of logistic regression and maximum entropy models"], "link": "http://www.win-vector.com/blog/2012/10/rudie-cant-fail-if-majorized/?utm_source=rss&utm_medium=rss&utm_campaign=rudie-cant-fail-if-majorized", "bloglinks": {}, "links": {"http://www.berkeley.edu/": 1, "http://en.wikipedia.org/": 3, "http://www.win-vector.com/blog": 11}, "blogtitle": "Win-Vector Blog"}, {"content": ["Model level fit summaries can be tricky in R . A quick read of model fit summary data for factor levels can be misleading. We describe the issue and demonstrate techniques for dealing with them. When modeling you often encounter what are commonly called categorical variables, which are called factors in R. Possible values of categorical variables or factors are called levels. We will demonstrate some of the needed in interpreting level based summaries, using R as an example. \n Consider the following data frame: \n \n \nd <- data.frame(x=c('a','a',\n 'b','b','b','b','b','b','b','b','b','b', 'b', 'b',\n 'c','c','c','c','c','c','c','c','c','c'),\n y=c(-5,5,\n  1,1,1,1,1,1,1,1,1,1,1,0.8,\n  -1,-1,-1,-1,-1,-1,-1,-1,-1,-1))\n \n \n When we use such a frame in a model (such as a lm() ) the categorical variable x needs to be converted into a numerical representation to be usable. The common way to do this is to introduce each observed value or level into a new zero/one indicator variable. For our input x we would expect three new numeric indicator varaibles: xa , xb and xc . Then the values of x are encoded in these three variables as given in the following table: \n \n \n \n \n   xa  xb  xc  \n  x==\u2019a\u2019  1.00  0.00  0.00  \n  x==\u2019b\u2019  0.00  1.00  0.00  \n  x==\u2019c\u2019  0.00  0.00  1.00  \n  \n \n This sort of bookkeeping rapidly gets painful, so R has a lot of code deep down in its implementation of factors and contrasts to perform this encoding for you. However no matter how much you mess with contrasts() , C() or other commands (setting treatment to things like contr.sum and so on) you can not get over the fact that R really really wants to use 2 variables to encode 3 levels (and it will fight mightily to prevent you from fixing this). Largely this is because R is trying to prevent a linear dependence from making it to the fitter. Notice in our above table all encodings obey the linear relation xa + xb + xc = 1 . This means one of these variables is redundant and something like the reasonable linear model xa + xb + xc is equivalent to the unreasonable model 1000000 xa + 1000000 xb + 1000000 xc - 999999 . Now my strong preference would be to use modeling code that can deal with this by using simple methods like ridge regression . But R, like most traditional stat programs, doesn\u2019t trust its linear algebra system to deal with linear dependencies- so they don\u2019t allow the above encoding and force us to use variations of the encoding seen in the next table: \n \n \n \n \n   xb  xc  \n  x==\u2019a\u2019  0.00  0.00  \n  x==\u2019b\u2019  1.00  0.00 \n  x==\u2019c\u2019  0.00  1.00  \n  \n \n In this encoding both b and c are written as difference from the \u201chidden value\u201d a . We no longer have any redundancy as a \u2018s encoding ([0,0]) no longer totals to 1. However, this leads to a new subtle problem in fit reporting. Consider fitting a linear model for y ~ a x + b on the example data: \n \n \nm <- lm(y~x,data=d)\nsummary(m)\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n Min  1Q Median  3Q  Max \n-5.0000 0.0000 0.0083 0.0167 5.0000 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)\n(Intercept) 1.541e-15 1.091e+00 0.000 1.000\nxb   9.833e-01 1.179e+00 0.834 0.414\nxc   -1.000e+00 1.196e+00 -0.836 0.412\n\nResidual standard error: 1.544 on 21 degrees of freedom\nMultiple R-squared: 0.3002,\tAdjusted R-squared: 0.2336 \nF-statistic: 4.505 on 2 and 21 DF, p-value: 0.02355 \n \n \n The default factor encoding chose a as the reference or hidden level. The coefficients on remaining two levels ( xb and xc ) now represent how much boost you get relative to the hidden level. In this case there are large error bars around the hidden level (it only occurred twice and with wildly different y values). So we have horrid t-values and no reported evidence that the effects of b and c are significantly different than a . But this is misleading. This report does not tell us if there is a significant difference between xb and xc , the only reason we can\u2019t tell if they are far from the hidden estimate of xa is uncertainty in estimating the (hidden) coefficient of xa itself. Uncertainties like this are not distances and don\u2019t obey the triangle inequality or transitivity- not being able to show either of xb or xc is far from xa does not mean we can not show xb is far from xc . It just wasn\u2019t in this report. \n To fix this we apply a function to \u201cre-level\u201d the factor x so the hidden level to the most popular value (i.e. choosing which level is hidden and choosing it in such a way we expect the error bars to be small): \n \n \nsetRefToMostCommonLevel <- function(f) { \n f <- as.factor(f) \n t <- table(f)\n relevel(f,ref=as.integer(which(t>=max(t))[[1]]))\n}\nd$x <- setRefToMostCommonLevel(d$x)\nd$x\nm <- lm(y~x,data=d)\nsummary(m)\n \n \n With the most popular level ( b ) as the reference we get the following result: \n \n \nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n Min  1Q Median  3Q  Max \n-5.0000 0.0000 0.0083 0.0167 5.0000 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|) \n(Intercept) 0.9833  0.4456 2.207 0.03860 * \nxa   -0.9833  1.1789 -0.834 0.41362 \nxc   -1.9833  0.6609 -3.001 0.00681 **\n---\nSignif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n\nResidual standard error: 1.544 on 21 degrees of freedom\nMultiple R-squared: 0.3002,\tAdjusted R-squared: 0.2336 \nF-statistic: 4.505 on 2 and 21 DF, p-value: 0.02355 \n \n \n We now have p \u2264 0.00681 significance (which is good, smaller is better for reported significances) that the outcome ( y ) difference between the level c the hidden level b is unlikely under the null-hypothesis. \n Of course what was hiding the significance report were large error bars on the reference level. So we could try running the following code to select the level with the least variance in outcome to minimize the size of the reference error bars. So here is code that picks the level with minimum sample variance (a trade-off between number of observations and variation on associated y values): \n \n \nsetRefToLeastVarLevel <- function(f,y) { \n f <- as.factor(f) \n t <- sapply(levels(f),function(fi) var(y[f==fi]))\n if(all(sapply(t,is.na))) {\n  setRefToMostCommonLevel(f)\n } else {\n  relevel(f,\n  ref=as.integer(which((!sapply(t,is.na))&(t<=min(t,na.rm=T)))[[1]]))\n }\n}\nd$x <- setRefToLeastVarLevel(d$x,d$y)\nd$x\nm <- lm(y~x,data=d)\nsummary(m)\n \n \n This picks c as the reference level of minimum variance resulting in the following: \n \n \nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n Min  1Q Median  3Q  Max \n-5.0000 0.0000 0.0083 0.0167 5.0000 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|) \n(Intercept) -1.0000  0.4881 -2.049 0.05320 . \nxb   1.9833  0.6609 3.001 0.00681 **\nxa   1.0000  1.1957 0.836 0.41237 \n---\nSignif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n\nResidual standard error: 1.544 on 21 degrees of freedom\nMultiple R-squared: 0.3002,\tAdjusted R-squared: 0.2336 \nF-statistic: 4.505 on 2 and 21 DF, p-value: 0.02355 \n \n \n Another approach is to look for summaries that are invariant under the choice of reference level. One such summary, though it is an clinical effect size not a significance, is the standard deviation of the levels (with the missing level added into the vector). This statistic is the root mean square coefficient size and is invariant over level choice (due to our appending a zero to represent the coefficient of the reference level): \n \n \nsd(c(m$coefficients[grep('^x',names(m$coefficients))],0))\n \n \n \n \n[1] 0.9916783\n \n \n Unfortunately there is no easy relevel() invariant significance term, as when the reference level estimate is itself uncertain all reported differences are also uncertain and when the references level estimate is certain we can have many other levels return significant or insignificant. \n The lesson is you have to be a bit careful in reading the significance summaries on factor levels. Model summary reports are only on how each level relates to the hidden level (not the value of the level relative to zero). So the items in the report could be facts about the levels you see or could be facts about the hidden level- you can\u2019t tell which. This is fairly traditional in statistics. \n We feel a better approach would be for the fitter to not use hidden levels (add the redundant level back in) and then enforce a bias for small coefficients by an explicit regularization term. In fact many forms of regularization don\u2019t really make sense until you get more explicit control of the level encoding. Another fix would be to add a linear constraint that states the expected impact of the fit level coefficients is zero over the training data (which would eat one degree of freedom and make handing missing values easier, this would be written as ca xa + cb xb + cc xc = 0 ; where ca,cb and cc are the number of training rows with each of the given levels). But this thinking in term of effects is at odds with the traditional design of factors and contrasts in R. So it would be a lot of visible effort to try and trick R into consistently working in this way (and all of these approaches would complicate computation of significance levels a bit). \n We advise: take just a little care and experiment with relevel() when appropriate. Always be wary when reading reported model coefficient significance tables. \n Related posts: \n Modeling Trick: Impact Coding of Categorical Variables with Many Levels \n Learn Logistic Regression (and beyond) \n What does a generalized linear model do?"], "link": "http://www.win-vector.com/blog/2012/10/level-fit-summaries-can-be-tricky-in-r/?utm_source=rss&utm_medium=rss&utm_campaign=level-fit-summaries-can-be-tricky-in-r", "bloglinks": {}, "links": {"http://www.win-vector.com/blog": 3, "http://en.wikipedia.org/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Win-Vector Blog"}, {"content": ["I am going to come-out and say it: I am emotionally done with 32 bit machines and operating systems. My sympathy for them is at an end. \n I know that ARM is still 32 bit, but in that case you get something big back in exchange: the ability to deploy on smartphones and tablets. For PCs and servers 32 bit addressing\u2019s time is long past, yet we still have to code for and regularly run into these machines and operating systems. The time/space savings of 32 bit representations is nothing compared to the loss of capability in sticking with that architecture and the wasted effort in coding around it. My work is largely data analysis in a server environment, and it is just getting ridiculous to not be able to always assume at least a 64 bit machine. Some dust-ups are associated with on 32 bit addressing and word-size include: \n \n The failure of Go\u2019s heuristic garbage collector (32 bit values are too subject to collision to allow the heuristic \u201clooks like a pointer\u201d determination to be reliable.) \n Mongo DB quietly losing data after 2GB . \n GNU Hurd\u2019s 2GB file partition limit . \n Limited arithmetic and small memory limits in tagged native-work Lisp implementations . \n (Notice that the emacs-lisp variant this article is talking about has a native machine integer that tops out at under 300 million giving it only 24 bit addressing.)\n \n I say \u201cassociated with\u201d because none of these failures was really due to 32 bit addressing/word-size alone. They are all due to 32 bit addressing combined with one more shortcut (heuristic instead of correct garbage collector, not checking error codes, memory-mapping partitions, pointer mangling and on on). So the problem is 32 bit addressing has already spent a lot of your luck. \n And we do still encounter these machines. Some common hold-outs of 32 bits in a server environment include: \n \n Amazon EC small/medium instances \n Cheap Hadoop clusters. \n Cheap virtual machine strategies (lots of small 32 bit virtual instances, often hosted on a 64 bit machine). \n \n That sad part is all of these environments are remote/virtual and therefor only a simple configuration away from being 64 bit. \n Don\u2019t you feel your software deserves access to more than $40 worth of memory? \n \n \n Or from a computational point of view: my aging laptop only takes around 1.2 seconds to count up from zero and overflow into negative numbers in Java: \n \n \npublic class Count {\n\tpublic static void main(String[] args) {\n\t\tfinal long t0MS = System.currentTimeMillis();\n\t\tfor(int i=0;i>=0;++i) {\t\t\t\n\t\t}\n\t\tfinal long tFMS = System.currentTimeMillis();\n\t\tSystem.out.println(\"MS elapapsed: \" + (tFMS-t0MS));\n\t}\n}\n \n \n We all know 32 bits represents trade-off of space for expressiveness. But I don\u2019t think enough people remember they are settling for the expressiveness of about $40 of memory and 1.2 seconds of computation. That is how far Moore\u2019s law has moved what we should settle for. The sweet spot in trading code complexity versus machine effort move rapidly, so compromises that made sense in the past rapidly become hindrances when not re-evaluated. \n I will end with a personal example: a lot of our clients are in what I call the region of \u201cmedium data\u201d (many gigabytes, but not terabytes). In this range you can, on a 64 bit architecture, perform very rapid and agile analyses using relational DBs and standard in-memory analysis suites (like vanilla R ). However, at this scale on a 32 bit machine (or cluster of 32 bit machines) you tend to resort to the big data techniques designed for terabyte situations: map reduce and out-of core extensions (like Revolution Analytics or ff ). These methods can limit your expressiveness, take longer to code for and take longer to run (using network and disk more often). And, you still are only in the \u201cmedium data regime\u201d (you may not yet have enough data to hit \u201cThe Unreasonable Effectiveness of Data\u201d effect, so you still need the agility in trying models to make progress as you don\u2019t yet have enough data for the data to simply construct a dominant model). For many analysis tasks delaying the switch from small/medium data techniques to \u201cbig data methods\u201d has significant advantages. \n It all comes down to how much your time is worth. \n Related posts: \n Kernel Methods and Support Vector Machines de-Mystified \n \u201cThe Mythical Man Month\u201d is still a good read \n What to do when you run out of memory"], "link": "http://www.win-vector.com/blog/2012/09/i-am-done-with-32-bit-machines/?utm_source=rss&utm_medium=rss&utm_campaign=i-am-done-with-32-bit-machines", "bloglinks": {}, "links": {"http://cran.r-project.org": 1, "http://aws.amazon.com/": 1, "http://www.gnu.org/": 1, "http://www.win-vector.com/blog": 3, "http://www.revolutionanalytics.com": 1, "http://www.wvu.edu/": 1, "http://diegobasch.com/": 1, "http://www.emacswiki.org/": 1, "http://www.newegg.com/": 1, "http://ff.r-project.org": 1, "http://www.abtinforouzandeh.com/": 1, "http://www.1060.org/blog": 1}, "blogtitle": "Win-Vector Blog"}, {"content": ["When people ask me what it means to be a data scientist, I used to answer, \u201cit means you don\u2019t have to hold my hand.\u201d By which I meant that as a data scientist (a consulting data scientist), I can handle the data collection, the data cleaning and wrangling, the analysis, and the final presentation of results (both technical and for the business audience) with a minimal amount of assistance from my clients or their people. Not no assistance, of course, but little enough that I\u2019m not interfering too much with their day-to-day job. \n This used to be a key selling point, because people with all the necessary skills used to be relatively rare. This is less true now; data science is a hot new career track. Training courses and academic tracks are popping up all over the place. So there is the question: what should such courses teach? Or more to the heart of the question \u2014 what does a data scientist do, and what do they need to know? \n \n Hilary Mason and Chris Wiggins took a crack at answering that a couple of years ago . They break down data science, the process, into 5 steps: \n \n Obtain the data: in their case from Web APIs. \n Scrub the data: Look for missing data, bad data, outlier. Regularize text data (for instance locations: is \u201cCA\u201d California, or Canada? What about \u201cCal.\u201d, \u201cCa\u201d, \u201cCalifornia\u201d, \u201cSan Francisco\u201d, etc..).\n  \n Explore. And visualize. Here and during the scrub step is where I might start thinking about the best representations of the data, for modeling. Here is where I begin variable selection. \n Model. And evaluate. This is where the statistics and machine learning knowledge comes in. \n Interpret. And disseminate. \n \n That\u2019s basically the breakdown most of us would give. Their focus is on web analysis and on data collection over the web; generally my focus has been on clients who have the data (albeit in some completely cryptic form); it\u2019s our job to wring some insight from it \u2014 somehow. So in addition to the emphasis that Mason and Wiggins place on scripting languages and unix tools, I would also add knowledge of SQL, and a tool like R that can access data directly from the database for analysis. My colleague John Mount would also add that version-control is a must . As with software engineering, data science is a process where \u201cthat one last tweak\u201d to the model or to the data handling can turn out to be a tweak too many\u2026 \n Beyond the tools, and the technical details, though \u2014 what would I add? \n I would add that the process is a loop; more than that, it\u2019s loops within loops. Obtain-Scrub-Explore is often one loop. Scrub (Represent)-Explore-Model can be another loop. It always depends. \n I would add that a healthy understanding of the business processes that generate the data is essential \u2014 otherwise you are apt to \u201cdiscover\u201d things in the data that everyone (that is, your client) already knows, because they are known artifacts of the business process. The insights in data are like degrees of freedom. Don\u2019t eat up your degrees of freedom on known phenomena. If you don\u2019t have that domain knowledge yourself, make sure your client partners you with a contact who does. \n I would add that a solid understanding of statistics fundamentals is essential (and the whole Win-Vector blog attests to how much time we spend thinking about fundamentals), but stat and machine learning are not the core of the job. The real science, in my opinion \u2014 the part where you form hypotheses, test them, revise them \u2014 comes less in the modeling and more in the scrub and explore steps. Why does this branch of the bank report recoveries where they never reported losses? What is that \u201cprofit\u201d column reporting, really? Does gross national product really predict mortgage defaults, or is it just a proxy variable for time (and in the recent economy, time predicts mortgage default rate pretty well)? \n And there is more science after the modeling, during the evaluation phase. Or more prosaically: the debug phase. Why does the model report absolute nonsense on this one subset of the data? Is the error in the modeling? The data handling? The programming? The \u201cmodeling\u201d step itself is actually a very small, and relatively straightforward, part of the overall process. \n No one ever wants to hear this. We all come into the job hoping to wield support vector machines or neural nets like Wonder Woman wields her magic lasso: we capture the data, and then wrest the truth out of it, willy-nilly. I wish. I\u2019d love to wear those cool bullet-deflecting bracelets, too. \n \n Art: Alex Ross \n My point here is that answering the original question is more a discussion of process than a checklist of skills to have and technologies to be familiar with. \n What would you add? What do you think a data scientist needs to know? \n \n This was originally posted at ninazumel.com ; reblogged here. \n Related posts: \n Book Review: Ensemble Methods in Data Mining (Seni & Elder) \n Setting expectations in data science projects \n Your Data is Never the Right Shape"], "link": "http://www.win-vector.com/blog/2012/09/on-being-a-data-scientist/?utm_source=rss&utm_medium=rss&utm_campaign=on-being-a-data-scientist", "bloglinks": {}, "links": {"http://ninazumel.com/": 1, "http://www.dataists.com/": 1, "http://www.win-vector.com/blog": 4}, "blogtitle": "Win-Vector Blog"}, {"content": ["This was originally posted at ninazumel.com . I\u2019m re-blogging it here. \n \n \n Photo: John Mount \n I came across a post from Emily Willingham the other day: \u201cIs a PhD required for Good Science Writing?\u201d . As a science writer with a science PhD, her answer is: is it not required, and it can often be an impediment. I saw a similar sentiment echoed once by Lee Gutkind, the founder and editor of the journal Creative Nonfiction . I don\u2019t remember exactly what he wrote, but it was something to the effect that scientists are exactly the wrong people to produce literary, accessible writing about matters scientific. \n I don\u2019t agree with Gutkind\u2019s point, but I can see where it comes from. Academic writing has a reputation for being deliberately obscure and prolix, jargonistic. Very few people read journal papers for fun (well, except me, but I\u2019m weird). On the other hand, a science writer with a PhD has been trained for critical thinking, and should have a nose for bullpucky, even outside their field of expertise. This can come in handy when writing about medical research or controversial new scientific findings. Any scientist \u2014 any person \u2014 is going to hype up their work. It\u2019s the writer\u2019s job to see through that hype. \n I\u2019m not a science writer in the sense that Dr. Willingham is. I write statistics and data science articles (blog posts) for non-statisticians. Generally, the audience that I write for is professionally interested in the topic, but aren\u2019t necessarily experts at it. And as a writer, many of my concerns are the same as those of a popular science writer. \n I want to cut through the bullpucky. I want you, the reader, to come away understanding something you thought you didn\u2019t \u2014 or even couldn\u2019t \u2014 understand. I want you, the analyst or data science practitioner, to understand your tools well enough to innovate, not just use them blindly. And if I\u2019m writing about one of my innovations, I want you to understand it well enough to possibly use it, not just be awed at my supposed brilliance. \n I don\u2019t do these things perfectly; but in the process of trying, and of reading other writers with similar objectives, I\u2019ve figured out a few things. \n It\u2019s not a research paper; don\u2019t write it like a research paper. \n To the outsider (and sometimes, to the insider), academic writing is a giant exercise in gamesmanship . The goals are to demonstrate that you are smarter than your peers, and your research is innovative and novel. Those goals impede clarity; after all, if it\u2019s hard to read, it must be difficult material, and you must be one smart cookie. \n When you are writing for the public, or for the practitioner, you usually aren\u2019t writing about something you invented. This frees you from the pressure of appearing novel, and the topic doesn\u2019t have to seem profound. You don\u2019t have to try to look smart, either. By definition, you know something the reader doesn\u2019t \u2014 and if you can explain it so that it seems obvious in hindsight, you actually look that much more clever. \n Channel your inner undergrad. \n This is related to the first point. Your readers are not your research peers, but they aren\u2019t stupid, either. They just have a different set of expertise. They won\u2019t know all the acronyms or the standard symbology, they won\u2019t know all the terminology or all the logic shortcuts (and occasional sloppy misuse of terminology) that people in the field are accustomed to. They are, however, motivated to learn the subject matter. It\u2019s your job to make it all make sense. \n This is where I have an advantage in writing about statistics: my PhD isn\u2019t in statistics. I learned it after I started working, because I had to. I remember a good deal of frustration with the general presentations of standard material given in textbooks. And I can remember even more frustrations with the \u201cinsider\u201d style of writing in research papers \u2014 often a lot of hard work for not a lot of reward. The Statistics to English series of blog posts arose from that experience. It doesn\u2019t have to be that hard. \n Explain everything. If you are going to talk about p-values, for example, then define what they are; define significance, or at least point to a readable reference. Don\u2019t expect your readers to know that \u03c0 means \u201cvector of probabilities\u201d and not \u201cthe area of a unit radius circle.\u201d In any technical field, a clear explanation of definitions \u2014 especially terms and symbols that are used differently in different fields, like \u201centropy\u201d or \u201clikelihood\u201d \u2014 is worth a lot. It may not seem like much, but believe me, your readers will thank you. \n If an explanation helps make sense of a concept to you , it will probably make sense to someone else. \n \u2026 and therefore, it will be helpful to someone, if not everyone. A related point to this is that parroting the textbook or research paper explanations is not always helpful, no matter how official or technical it sounds. \n An example: the definition of likelihood (or likelihood function) from The Cambridge Dictionary of Statistics is as follows: \n The probability of a set of observations given the value of some parameter or set of parameters. \n The dictionary goes on to give an example, the likelihood of a random sample of n observations, x1,..x2,\u2026xn with probability distribution f(x,\u03b8) : \n \n This makes perfect sense, from a 50,000 foot point of view, especially if you are in the habit of thinking statistically. But what if you aren\u2019t? Or if you want to actually calculate a specific likelihood for a specific purpose? Like the person who asked a question about likelihood versus probability here . This person received several excellent answers, most of which were elaborations of the Cambridge definition, with variations on how the likelihood formula was written. This seems to have helped the original questioner; but I can tell you that back in the days when I was struggling to learn these concepts, it wouldn\u2019t have helped me. I learn by concrete examples. Like this (borrowed from one of the answers at the link): \n I have an unfair coin, and I think that the probability that it will land heads is p . I flip the coin n times, and I record whether it lands heads (x_i=1), or whether it lands tails (x_i=0). The likelihood of observing my series of coin flips, given that I have guessed probability p correctly is: \n \n Here, I\u2019m using the notation L(x|p) to emphasize that the likelihood of the set of observations X is dependent on my \u201cmodel\u201d \u2014 the estimate for p . If the likelihood is high, I have guessed p correctly; if it is low, I need to re-estimate p . \n We can generalize this to any set of observations X that lead to a binary outcome, and any model for the probability, P(x). \n \n Again, if the likelihood isn\u2019t high, then we need to adjust our model P(x), probably by tweaking some parameter or parameters \u03b8 within the model. This leads us back to the original Cambridge definition. \n My formula is not as general as the Cambridge formula (or the formulas given by the people who answered the question in my link above), and it may still not be clear enough for everyone. Still, it helped me; this is the formula I use to explain the use of likelihood in deriving logistic regression . Judging by Win-Vector\u2019s blog stats, it helps other people out there, too. \n Few things are \u201cobvious.\u201d \n But after all, the most successful device in mathmanship is to leave out one or two pages of calculations and for them substitute the word \u201chence,\u201d followed by a colon. This is guaranteed to hold the reader for a couple of days figuring out how you got hither from hence. Even more effective is to use \u201cobviously\u201d instead of \u201chence,\u201d since no reader is likely to show his ignorance by seeking help in elucidating anything that is obvious. This succeeds not only in frustrating him but also in bringing him down with an inferiority complex, one of the prime desiderata of the art. \n \u2013 Nicolas Vanserg, \u201cMathmanship\u201d (1958). Originally published in American Scientist , collected in A Stress Analysis of a Strapless Evening Gown: Essays for a Scientific Age. Sorry, couldn\u2019t find the essay online. \n \n If it is \u201cobvious\u201d enough to be explained concisely, then explain it. If the calculation or derivation is short, then spell it out. Otherwise, use another phrase like \u201cit happens to be the case\u201d, or \u201cit can be shown,\u201d with references \u2014 or at least cop to the fact that it\u2019s a messy calculation. Some people will complain about how long your posts are; but there will also be an army of students and other novices who will be grateful to you, because they finally \u201cget it.\u201d \n And finally: \n No matter how hard you try, someone will think you stink. Let it go. \n Even if you don\u2019t write your article like a research paper, someone out there will read it (or criticize it) as if it were. If you try to define every concept and explain every chain of reasoning, you will get snarky \u201cTLDR\u201d (too long, didn\u2019t read) comments, or comments to the effect that it was a lot of reading for a point that was \u201cobvious.\u201d If you express an opinion, no matter how mildly or with any number of qualifications (for example, my colleague John Mount\u2019s article on why he dislikes dynamic typing ), you will get sarcastic comments from those who hold the opposite opinion, on why you are just wrong, or why the issues you point out are your fault, not (e.g.) dynamic typing\u2019s fault. You will get people hurling back at you points that you already addressed in the post. \n So fine, some people don\u2019t read (or are too impatient or closed-minded to read); they aren\u2019t your readers. Just let it go and move on. \n Being the concrete-minded person that I am, these points are addressed to my specific situation; but I hope that they are useful to other science writers and technical writers as well. I\u2019m sure that I\u2019ve violated many of these points in my own writing; hopefully the more I keep at it, the better my writing gets. \n Related posts: \n What does a generalized linear model do? \n Kernel Methods and Support Vector Machines de-Mystified \n Public Service Article: JSTOR and other Useful Research Archives"], "link": "http://www.win-vector.com/blog/2012/09/on-writing-technical-articles-for-the-nonspecialist/?utm_source=rss&utm_medium=rss&utm_campaign=on-writing-technical-articles-for-the-nonspecialist", "bloglinks": {}, "links": {"http://www.emilywillinghamphd.com/": 1, "http://www.win-vector.com/blog": 6, "http://ninazumel.com/": 1, "http://stats.stackexchange.com/": 1, "http://en.wikipedia.org/": 1, "http://www.amazon.com/": 1}, "blogtitle": "Win-Vector Blog"}, {"content": ["A recent run of too many articles on the same topic (exhibits: A , B and C ) puts me in a position where I feel the need to explain my motivation. Which itself becomes yet another article related to the original topic. The explanation I offer is: this is the way mathematicians think. To us mathematicians the tension is that there are far too many observable patterns in the world to be attributed to mere chance. So our dilemma is: for which patterns/regularities should we derive some underlying law and which ones are not worth worrying about. Or which conjectures should try to work all the way to proof or counter-example? Mathematicians are famously abstract and concerned with notations and distinctions that do not exist outside of mathematics. However, I choose \u201cpatterns in nature\u201d (instead of mathematics itself) as the guiding dilemma. This is because I do not (and I feel most mathematicians also do not) in fact worry much about the foundations mathematics itself. A lot has been written about reliability of the foundations of mathematics (Whitehead and Russell\u2019s Principia Mathematica , foundational set theory , constructivism and the Association des collaborateurs de Nicolas Bourbaki \u2018s foundational work). And even though G\u00f6del\u2019s incompleteness theorems indicate there can not be an axiomitization of mathematics itself that is both complete and useful (as hoped for in Whitehead and Russell or in Hilbert\u2019s research program) mathematics is in-fact much more stable and amenable to axiomatic analysis than any other field of study. So most mathematicians are comfortable with the state of mathematics itself. \n It is regularities in calculations that disturb us. Are they really there? Is there a counter-example? Is there a proof? How much can we depend on them? \n \n For a specific calculation we return to our recent fixation: common implementations of logistic regression tools such as R \u2018s glm() package. This is an important and useful statistical implementation. But what can we count on from the algorithm and its implementation? \n \n \nDoes it minimize traditional square error/loss? \n No: minimizing loss is different than maximizing likelihood in this case.\n \n \nDoes it always converge from the standard start? \n No: there are easy to construct examples that fail to converge from the standard all-zero start .\n \n \nCan we prove anything? \n Yes: constant problems always converge correctly from the zero start .\n \n \n This situation is in fact close the theorist/mathematician\u2019s view of mathematical knowledge. The view being mathematics is thin web of certainty with great reach and scope. The filaments of mathematical certainty reach almost everywhere, but are thin and do not cover. You are always a small move away from unsupported empty space (as we saw above in being able to generate easy examples that caused popular software to fail). \n \n In fact this is why you even bother with proofs: you suspect even small changes are not safe. You want to establish what is safe, what is unsafe and how to discern the difference. In trying to prove the standard full-step Newton-Raphson method safe we have already found easy examples that break R\u2019s glm() implementation. That is something you want to examine in your experimental lab, not happening quietly in production. So it is something you want to characterize and axiomatize. \n Each of the articles I mentioned was written was trying either to prove or dispute possible regularities noticed in practice or while working on the previous article. In fact we used an ugly annealing technique to find the bad examples for our article. This is compatible with the \u201ccalculate and conjecture\u201d style often used in mathematics (and heavily automated by Metropolis and Ulam). But during the search for bad examples we noticed a new regularity: no problem ever got worse on the first optimization step (even those problems that actually diverged). The question is: is this regularity a fact about the world or did we just do a poor job implementing the annealing/genetic-algorithm search we used to find counter-examples? Annealing has its own deep theory and can be implemented poorly or done well. The pure form of the dilemma: prove the conjecture or find a counter-example. \n In some cases we use a principle like taste to walk away from a problem (the problem may not be worth solving). But there is always a nagging doubt that somebody else will solve the problem. And that in the work of solving it they will find another regularity to conjecture about. It could be that the next regularity they see can be generalized into something important. \n But any time spent on search is time not spent on theory: \n \n\u201cIf Edison had a needle to find in a haystack, he would proceed at once with the diligence of the bee to examine straw after straw until he found the object of his search. \u2026 I was a sorry witness of such doings, knowing that a little theory and calculation would have saved him ninety per cent of his labor.\u201d \nNikola Tesla, New York Times (19 October 1931) \n(source: wikiquote.org )\n \n And ego/taste is also a problem. I have spent a lot of time trying to prove problems are in fact difficult to justify having worked on them. And sometimes the proof of difficulty is itself in now way obvious. \n But enough philosophy: the point is the regularity is like a stone in our shoe. We need to prove it or get rid of it. In our case: does the full-step Newton-Raphson method of solving logistic regression always improve on the first step from the origin? It is actually hard to walk away from this question because we have a lot of empirical evidence this is the case- but can\u2019t count this as a guarantee (as we have no measure of the quality and comprehensiveness of our empirical evidence). What if right after we quit working on the problem somebody exhibits a simple counter-example or a simple proof? \n So we work a bit more. If we can\u2019t completely work the problem we try to at least eliminate much of the uncertainty and its attendant tension and excitement. Can we find a method to put the question to bed? \n Suppose we have m data items that are pairs (x(i),y(i)) where each x(i) is a column vector in R^n with first entry equal to 1 and each y(i) is either 1 or 0. The first step of full step Newton-Raphson method of solving a logistic regression is to compute Equation 1: \n \n \nw = inverse(sum(i=1...m :(1/4) x(i) transpose(x(i)))) *\n    (sum(i=1...m : (y(i)-1/2) transpose(x(i)))).\n \n \n This is a simplification of the update step from the simpler derivation of logistic regression . It is specialized to the first step where the initial weight estimate is zero and all probability estimates are 1/2. \n What we are trying to optimize is what we (through some abuse of terminology) we will call the unscaled perplexity (or perplexity, if we forget to include the \u201cunscaled\u201d qualifier). If p is our vector of current model estimates the unscaled perplexity is defined as Equation 2: \n \n \nunscaledPerplexity(p) = sum(i=1...m | y(i)=1 : -log(p(i)) ) \n + sum(i=1...m | y(i)=0 : -log(1-p(i)) ).\n \n \n This equation is penalizing the model for claiming events than actually happened are modeled as rare. For example if for the i-th datum we have y(i)=1 then the sum includes a term of -log(p(i))) which is large when p(i) is near zero. Similarly if the correct answer is y(i)=0 the penalty is -log(1-p(i)) which is large with p(i) is near 1. The p estimates themselves are defined as p(i) = s(w.x(i)) where s(z) = exp(z)/(1+exp(z)) . \n The unscaled perplexity of the standard zero start is Equation 2 evaluated the p estimates you would get at w=0. This assigns all p(i)=1/2 (independent of x(i)). So the initial un-scaled perplexity is exactly m*log(2). We want to show the unscaled perplexity after the first step is less than this quantity. We want to show (at least informally) that Equation 2 is no more than m*log(2) when we plug in the \u201cp\u201d derived from Equation 1\u2032s \u201cw\u201d. \n The argument is as follows: Equation 1 is recognizable as a standard regression picking w such that w.x(i) is as close as possible to 4(y(i)-1/2). Formally w is such that the following square error is minimized in Equation 3: \n \n \nsquareError(w) = sum(i=1..m: ( w.x(i) - 4(y(i)-1/2) )^2 ).\n \n \n Because we insisted all x(i) have the first entry 1 (representing the constant traditionally present in these fitting problems) we know our solution is at least as good as picking a w such w.x(i) = 4(q \u2013 1/2) where q = sum(i = 1\u2026m : y(i))/m. So the square error of the best solution should be able to place w.x(i) at least as close to 4(y(i) \u2013 1/2) as 4(q \u2013 1/2) is on average. \n To actually prove a solid result we would relate how the vector p in R^m such that p(i) = s(w.x(i)) (s(z) as in Equation 2) being close to the vector 4(y-1/2) in the sense of the square norm ||v - 4(y-1/2)||^2 is small implies what we need to know. But this would take some tedious math, so we will use only a heuristic or hand-waving argument. Suppose additionally that not only is the square norm ||p - 4(y-1/2)||^2 small but for each \u201ci\u201d p(i) is near 4(y(i)-1/2) (at least as close as q is and sometimes closer). This additional condition will not always be true; there can be \u201ci\u201d such that p(i) is far from 4(y(i)-1/2), but they have to be rare since on average the relation is true. The additional (unsupported) assumption lets us think at example data-points (specific i\u2019s) which can be easier than thinking in terms of norms (though when we work in norms we bring in theorems like the Cauchy\u2013Schwarz inequality , so the norm based proofs do tend to be quite short and clear). \n To characterize even the single term of our sum, and then our whole problem, we need try a number of techniques. So we must know or invent methods. To experience the flavor of the work let us first look at a single term of our unscaled perplexity sum. Such a term is always: -log(s(z)) or -log(1-s(z)) (depending on y(i)) for some z. One technique to analyze functions is to look at the Maclaurin series series (Taylor series around zero). The Maclaurin series of a function is the approximation of a function as a series of powers, so using it we can often see some structure. The Maxima symbolic algebra system shows the Maclaurin series of s(z) is: \n \n \n(%i11) taylor(exp(z)/(1+exp(z)),z,0,4);\n          3\n        1 z z\n(%o11)/T/      - + - - -- + . . .\n        2 4 48\n\n \n \n Paydirt! The function g(z) = 1/2+z/4 is the inverse of the function f(z) = 4(z - 1/2) . (I feel it is a remarkable regularity of the mathematics of functions that I don\u2019t have to say if I mean left or right inverse as they are the same : f(g(z))=z and g(f(z))=z .) So our strange least-squares estimate given in Equation 1 parks w.x(i) pretty much at the exact right place to have s(w.x(i)) near q . That is if w.x(i) is nearly 4(q-1/2) then s(w.x(i)) is nearly q (which is the best constant estimate of all the y(i), being their average). \n In fact the Maclaurin series of our loss term -log(s(w.x(i))) or -log(1-s(w.x(i))) (depending if y(i) is 1 or 0 respectively) looks like this: \n \n \n(%i14) taylor(-log(exp(z)/(1+exp(z))),z,0,4);\n          2 4\n         z z z\n(%o14)/T/    log(2) - - + -- - --- + . . .\n         2 8 192\n(%i15) taylor(-log(1-exp(z)/(1+exp(z))),z,0,4);\n          2 4\n         z z z\n(%o15)/T/    log(2) + - + -- - --- + . . .\n         2 8 192\n \n \n So an estimate that was at least as good as 4(q-1/2) would sum over all m data points for a total unscaled perplexity of: \n \n \n(%i18) expand(-m*(q*subst(4*(q-1/2),x,taylor(log(exp(x)/(1+exp(x))),x,0,2)) +\n    (1-q)*subst(4*(q-1/2),x,taylor(log(1-exp(x)/(1+exp(x))),x,0,2))));\n        2      m\n(%o18)     - 2 m q + 2 m q + log(2) m - -\n              2\n \n \n Or: the perplexity after one step is just the m*log(2) perplexity (as seen at the starting zero point) plus roughly a term of: m*(-2q^2 + 2q - 1/2) . Notice this term is never positive for any q in the interval [0,1] (and is negative for all points except q=1/2). This means unless q is near 1/2 we expect a large drop in perplexity after the first optimization step (which is what we are trying to prove). Thus we could (with some more care) rigorously prove for q sufficiently far away from 1/2 (likely only a small increment) that the first optimization step does improve. So we have a partial result, or a greater understanding of the problem. We can say: the dc term can usefully dominate error in the first step unless q is near 1/2. You are not going to find a counter-example with q very far from 1/2. \n At this point we can either strengthen our code that searches for counter-examples to restrict itself to the q=1/2 case (brining to mind Tesla\u2019s quote: we have proven most of our previous counter-example search was wasted as it did not concentrate on q near 1/2) or work more rigorously on the q=1/2 case to try and prove the proposition that the first optimization step decreases perplexity. \n We have learned some things. But the dilemma remains: do we believe there is a counter-example or proof just around the corner? Do we want to work through two pages of this kind of argument? Are we willing to slog through tens of pages of this kind of argument? Can we find new techniques to argue? And we are also back to taste. If we are working on this problem (which may not be important) it means we are not working on something else. We can\u2019t really know the value of the solution until we see it. Conceivably the solution method developed to solve an unimportant problem can itself have deep and important consequences. So if we drop the work because it seems unimportant we could miss something. However as Rota wrote: \n \nThere is a ratio by which you can measure how good a mathematician is, and that how many crackpot ideas he must have in order to have one good one. If this ratio equals ten to one then he is a genius. For the average mathematician, it may be one hundred to one. This is a feature of creative thinking that the man in the street fails to appreciate.\n \n (Gian-Carlo Rota, \u201cIndiscrete Thoughts\u201d, Birkhauser, 1997; sorry about the single-gendered pronouns representing any mathematician, they are in the source). \n On the negative (fixated) side Underwood Dudley goes on to characterize those who can not give up a crackpot idea as in fact being crackpots in \u201cA Budget of Trisections\u201d, Springer, 1987. And there is the extreme example of mathematician Paul Erd\u00f6s\u2019s amphetamine use to maintain concentration: \n \n\u2026 Graham bet Erd\u00f6s $500 that he couldn\u2019t stop taking amphetamines for a month. Erd\u00f6s accepted the challenge, and went cold turkey for thirty days. After Graham paid up\u2013and wrote the $500 off as a business expense\u2013Erd\u00f6s said, \u201cYou\u2019ve showed me I\u2019m not an addict. But I didn\u2019t get any work done. I\u2019d get up in the morning and stare at a blank piece of paper. I\u2019d have no ideas, just like an ordinary person. You\u2019ve set mathematics back a month.\u201d\n \n (from \u201cThe Man Who Loved Only Numbers\u201d , Paul Hoffman). \n The matter of mathematical taste, or when to walk away from a problem, is of central importance to mathematical thought. It is definitely the institutional bias to stay on a problem a bit longer than would seem natural. And there is the issue of ego: moving on from a result you wanted makes means admitting you didn\u2019t win. This is the nature of the motivations and dilemmas we wished to exhibit here. \n Related posts: \n The Joy of Calculation \n What is Mathematics, Really? \n Kernel Methods and Support Vector Machines de-Mystified"], "link": "http://www.win-vector.com/blog/2012/09/the-mathematicians-dilemma/?utm_source=rss&utm_medium=rss&utm_campaign=the-mathematicians-dilemma", "bloglinks": {}, "links": {"https://github.com/": 1, "http://mathworld.wolfram.com/": 1, "http://plato.stanford.edu/": 1, "http://www.win-vector.com/blog": 11, "http://en.wikipedia.org/": 8, "http://en.wikiquote.org/": 1, "http://cran.r-project.org": 1, "http://www.nytimes.com/": 1, "http://maxima.sourceforge.net": 1}, "blogtitle": "Win-Vector Blog"}, {"content": ["In our article How robust is logistic regression? we pointed out some basic yet deep limitations of the traditional full-step Newton-Raphson or Iteratively Reweighted Least Squares methods of solving logistic regression problems (such as in R \u2018s standard glm() implementation). In fact in the comments we exhibit a well posed data fitting problem that can not be fit using the traditional methods starting at the traditional (0,0) start point. And we cited an example where the traditional methods fail to compute the average from a non-zero start. The question remained: can we prove the standard methods always compute the average correctly if started at zero? It turns out they can, and the proof isn\u2019t as messy as I anticipated. Throughout this article let q be a real number strictly between 1/2 and 1. Consider a data set of n-items (n a large integer) with floor(n*q) of the items marked positive and n-floor(n*q) marked negative. Fitting a logistic model as a function of only a constant is essentially using logistic regression to compute an average in a very roundabout manner. \n The R code to do this is as follows: \n \n \n> n <- 100\n> q <- 0.9\n> d <- data.frame(y=rep(c(T,F),c(floor(q*n),n-floor(q*n))))\n> glm(y~1,data=d,family=binomial(link='logit'))\n\nCall: glm(formula = y ~ 1, family = binomial(link = \"logit\"), data = d)\n\nCoefficients:\n(Intercept) \n  2.197 \n\nDegrees of Freedom: 99 Total (i.e. Null); 99 Residual\nNull Deviance:\t 65.02 \nResidual Deviance: 65.02 \tAIC: 67.02 \n> exp(2.197)/(1+exp(2.197))\n[1] 0.8999798\n \n \n And we see we correctly compute the logit() of the desired density. logit(p) is defined as log(p/(1-p)) and we have exp(logit(p))/(1+exp(logit(p))) = p for p in the open interval (0.1). Hence our check that exp(2.197)/(1+exp(2.197)) = 0.9 (to 4 decimal places). We can force this fit to fail by giving a start away from zero (but closer to zero than you would think!): \n \n \n> glm(y~1,data=d,family=binomial(link='logit'),start=-5)\n\nCall: glm(formula = y ~ 1, family = binomial(link = \"logit\"), data = d, \n start = -5)\n\nCoefficients:\n(Intercept) \n 3.153e+15 \n\nDegrees of Freedom: 99 Total (i.e. Null); 99 Residual\nNull Deviance:\t 65.02 \nResidual Deviance: 720.9 \tAIC: 722.9 \nWarning message:\nglm.fit: fitted probabilities numerically 0 or 1 occurred \n \n \n And we see a fitting failure (warning messages, residual deviance greater than null deviance and astronomical coefficients). \n As we showed in the comments of How robust is logistic regression? there are bounded data problems that fail from a start of zero (which is bad news, as starting at zero is standard practice). So there are definitely easy examples the standard logistic regression solvers (Newton-Raphson or Iteratively Reweighted Least Squares) will fail on (for reasons of their own; not due to rounding, linear dependencies, overflow or unbounded situations). \n The starting from zero failing example is as follows (showing bad start (0,0) and good start (-4,-5)): \n \n \nd <- data.frame(x=c(0,0,0.001,100,-1,-1),\n y=c(F,T,F,F,F,T),\n wt=c(50,1,50,1,5,10))\nglm(y~x,data=d,family=binomial(link='logit'),\n weights=d$wt)\nglm(y~x,data=d,family=binomial(link='logit'),\n weights=d$wt,start=c(-4,-5))\n \n \n This example is counter to common experience that: for a bounded problem the start (0,0) should work (it in fact does not work here). \n Empirically practitioners don\u2019t run into this problem as much as they run into problems due to separated or quasi-separated data (which can not be fixed by merely changing the starting point). However, the question remains: is there a non-trivial class of problems we can prove the standard methods always succeed for? For example can the standard methods always compute the average when started from zero (the traditional start)? This is an important question for two reasons. First: we want to know can we prove anything at all after so much bad news. Second: being able to compute averages has some useful consequences we will indicate later. \n Let\u2019s prove we can in fact compute an average. We take q as defined earlier and adapt the math from The Simpler Derivation of Logistic Regression to define the following function f(x): \n \n \nf(x) = x + (q-p)/(p*(1-p)) \n where p = sigmoid(x) = exp(x)/(1+exp(x))\n \n \n f(x) is in fact the Newton-Raphson algorithm adapted to solve for only the constant term of a logistic regression model. We start with our first coefficient guess as zero and our improved estimates of the logit of the average are: f(0), f(f(0)), f(f(f(0))) and so on. For k=0,1,2\u2026 define f(k,x) = f(x) if k=0 and f(f(k-1,x)) otherwise. We would like to show f(k,0) converges to logit(q) as k goes to infinity. \n We call out a few facts about f() (using the fact 1/2 < q < 1): \n \n f(x) > x for all x in the half open interval [0,logit(q)). \n f(0) = 4*(q-1/2). \n f(logit(q)) = logit(q). \n f'(x) = (2p-1)*(q-p)/(p*(1-p)) > 0 where p = exp(x)/(1+exp(x)) for all x in the half-open interval [0,logit(q)). \n f(x) - x \u2265 4 (q - exp(x)/(1+exp(x))) for all x in the half-open interval [0,logit(q)). \n \n Facts 1 through 3 are useful, but not enough to prove much. Facts 4 and 5 not only give us enough tools to show f(k,0) converges to logit(q) (for 1/2 < q < 1), but tools enough to show it does this monotonely: f(k,0) < f(k+1,0) \u2264 logit(q). The proof is now simple: consider the original function f(x) as x varies from 0 to logit(q) (remember, q > 1/2 implies logit(q) > 0). f'(x) is positive for all points in the interior of the interval [0,logit(q)]. Therefore any maximum value of f(x) on the interval [0,logit(x)] must occur at one of the endpoints (since an interior maximum would have f\u2019(x) = 0, which is not the case). The two candidates for max(f(x)) on the inteval are therefore f(0) and f(logit(q)). f(logit(q)) = logit(q) and we just have to check that f(0) \u2264 logit(q) (or 4*(q-1/2) \u2264 logit(q) for all q in the open interval (1/2,1)- an exercise left to the reader) and we are done. So 0 \u2264 f(x) \u2264 logit(q) for all x in [0,logit(q)]. We now know inductively: f(k,0) < f(k+1,0) \u2264 logit(q). Fact 5 implies f(k,0) approaches logit(q) as k goes to infinity at a reasonable rate (the rate of iteration motion isn\u2019t too small before we get to x=logit(q)). Also, once we have x near logit(q) we would expect the usual very fast quadratic convergence that Newton-Raphson methods are chosen for. \n Now we seem to have worked hard to prove very little (as is often the case with iterated systems). However, I think we can not speculate why the standard optimizers (full-step Newton-Raphson and Iteratively Reweighted Least Squares) tend to work so well in practice. We can prove that these methods can always compute averages for q > 1/2 when starting at zero (the cases q=1/2 and q < 1/2 follow by inspection and symmetry). Thus these methods can solve any one dimensional problem over indicators (variables that are always zero and one). We can then imagine that any problem that can be approximated as a system of nearly conditionally independent indicators (so many variables, but all zero one and all nearly independent of each other when conditioned on the outcome) can also be solved (as the Newton Raphson iterations will update model coefficients nearly independently). So the intuition is that the standard implementation logistic regression will be unlikely to run into difficulties for problems that have bounded range nearly conditionally independent inputs. Or in other terms: for problems that Naive Bayes does well (which it often does, even though it depends only on independently computed averages) the Newton-Raphson optimizer should also perform well (and not spoil the logistic regression). The logistic regression model can in many cases be preferred over the Naive Bayes model due to its better handling of common dependencies. \n Though, it must be emphasized it would make sense for more of the common implementations to be toughened up to defend against Newton-Raphson failures without user intervention. \n Related posts: \n How robust is logistic regression? \n The equivalence of logistic regression and maximum entropy models \n What does a generalized linear model do?"], "link": "http://www.win-vector.com/blog/2012/08/newton-raphson-can-compute-an-average/?utm_source=rss&utm_medium=rss&utm_campaign=newton-raphson-can-compute-an-average", "bloglinks": {}, "links": {"http://cran.r-project.org": 2, "http://www.win-vector.com/blog": 6}, "blogtitle": "Win-Vector Blog"}]