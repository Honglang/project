[{"blogurl": "http://citizen-statistician.org\n", "blogroll": [], "title": "Citizen-Statistician"}, {"content": ["I don\u2019t understand the website Pinterest, but it looks pretty (especially on the iPad), and an undergraduate student said it was the greatest thing since Facebook, so I thought I would give it a shot. The idea is that Pinterest \u201clets you organize and share all the beautiful things you find on the web.\u201d You organize beautiful things by creating a \u201cboard\u201d (a page), and then adding \u201cpins\u201d (links to websites). \n My thought was\u2026plots in ggplot2 are beautiful\u2026I will create a board with useful links/tutorials for creating ggplot2 plots! \n I already have three followers. Now you can follow too. \n http://pinterest.com/zief0002/ggplot2/"], "link": "http://citizen-statistician.org/2012/10/29/ggplot2-pinterest/", "bloglinks": {}, "links": {"http://pinterest.com/": 1}, "blogtitle": "Citizen-Statistician"}, {"content": ["With fewer than two weeks left till the US presidential elections, motivating class discussion with data related to the candidates, elections, or politics in general is quite easy. So for yesterday\u2019s lab we used data\u00a0released by The Federal Election Commission on contributions made to 2012 presidential campaigns. I came across the data last week, via a post \u00a0on\u00a0The Guardian Datablog. The post has a nice interactive feature for analyzing data from all contributions. The students started the lab by exploring the data using this applet, and then moved on to analyzing the data in R. \n The original dataset can be found here . You can download data for all contributions (~620 MB csv file), or contributions\u00a0by state (~16 MB for North Carolina, for example). The complete dataset has information on over 3.3 million contributions.\u00a0The students worked with a random sample of 10,000 observations from this dataset. I chose to not use the entire population data because (1) it\u2019s too large to efficiently work with in an introductory stats course, and (2) we\u2019re currently\u00a0covering inference so a setting where you start with random sample data to infer something about the population felt more natural. \n While the data come in csv format, loading the data into R is slightly problematic. For some reason, all rows except the header row end with a comma, and hence naively loading the data into R using the read.csv function results in an error as R sees the extra comma as indicating an additional column and complains the header row does not have the same length as the rest of the dataset. Below are a couple ways to resolve this problem: \n \n One solution is to simply open the csv file in Excel, and resave. This eliminates the spurious commas at the end of each line, making it possible to load the data using the read.csv function. However this solution is not ideal for the large dataset of all contributions. \n Another solution for loading the population data (somewhat quickly) and taking a random sample is presented below: \n \n \nx = readLines(\"P00000001-ALL.csv\")\nn = 10000 # desired sample size\ns = sample(2:length(x), n)\nheader = strsplit(x[1],\",\")[[1]]\nd = read.csv(textConnection(x[s]), header = FALSE)\nd = d[,-ncol(d)]\ncolnames(d) = header\n \n Our lab focused on comparing average contribution amounts among elections and candidates. But these data could also be used to compare contributions from different geographies (city, state, zip code), or to explore characteristics of contributions from individuals of various occupations, individuals vs. PACs etc. \n If you\u2019re interested, there should still be enough time for you to squeeze an analysis/discussion of these data in your class before the elections. But even if not, the data should still be interesting after November 6."], "link": "http://citizen-statistician.org/2012/10/26/contributions-to-the-2012-presidential-election-campaigns/", "bloglinks": {}, "links": {"http://www.fec.gov/": 1, "http://public.tableausoftware.com/": 1, "http://www.co.uk/": 1}, "blogtitle": "Citizen-Statistician"}, {"content": ["http://www.dcactionforchildren.org/kids-count/dc-kids-count-data-tools \n Thanks, Frauke."], "link": "http://citizen-statistician.org/2012/10/24/dc-children-statisticians/", "bloglinks": {}, "links": {"http://www.dcactionforchildren.org/": 1}, "blogtitle": "Citizen-Statistician"}, {"content": ["The L.A. Times had an interesting article about how a pair of \u2018citizen scientists\u2019 discovered a planet with four suns.\u00a0 I would say that a more accurate term for the pair would be \u2018citizen data miners\u2019, because essentially the astronomy community crowd sources data mining by providing reams of data for anyone to examine. \n It seemed timely for me, following a seminar at the UCLA Center for Applied Statistics by Kiri Wagstaff on automated procedures for discovering interesting features in large data sets. Kiri\u2019s working on algorithms that use the pixels of photos (such as those produced by the Mars Rover) as data, and flag unusual features so that scientists can examine.\u00a0 Basically, the algorithms weed out things scientists already know and try to present them with the unknown. \n The data behind the \u201cfour suns\u201d discovery comes from Planet Hunters- -definitely worth a visit.\u00a0\u00a0 A parent website, zooniverse , offers opportunities to collect data, for example by transcribing historical ship logs for use in climate modeling. Who knows, you might make a discovery! \n What\u00a0 crowd-sourcing projects would you like to see to benefit statistics?"], "link": "http://citizen-statistician.org/2012/10/21/citizen-scientists-in-space/", "bloglinks": {}, "links": {"http://www.latimes.com/": 1, "http://statistics.ucla.edu/": 2, "https://www.zooniverse.org/": 1}, "blogtitle": "Citizen-Statistician"}, {"content": ["I was creating a dataset this last week in which I had to partition the observed responses to show how the ANOVA model partitions the variability. I had the observed Y (in this case prices for 113 bottles of wine), and a categorical predictor X \u00a0(the region of France that each bottle of wine came from). I was going to add three columns to this data, the first showing the marginal mean, the second showing the effect, and the third showing the residual. To create the variable indicating the effect, I essentially wanted to recode a particular region to a particular effect: \n \n Bordeaux ==> 9.11 \n Burgundy ==> 4.20 \n Languedoc ==> \u20139.30 \n Rhone ==> \u20130.75 \n \n As I was considering how to do this, it struck me that several options were available to me. Here are two solutions that come up when Googling how to do this. \n Use the recode() function from the car package. \n \nlibrary(car)\nwine$Effect <- recode(wine$Region,\n \" 'Bordeaux' = 9.11;\n 'Bordeaux' = 4.20;\n 'Languedoc' = -9.30;\n 'Rhone' = -0.75 \" )\n \n This is a commonly suggested solution. The strings inside quotation marks, however, make it likely students (and teachers) will commit a syntax error. This is especially true when recoding a categorical variable into another categorical variable. R-wise (it\u2019s a technical term) it also produces a factor, even though it is clear that the intent was to produce numerical values. This is of course, easily fixable using\u00a0 as.numeric() , but it can lead to confusion. \n \n \n Another solution is to use indexing. \n \nwine$Effect <- 9.11\nwine$Effect[wine$Region == \"Burgundy\"] <- 4.20\nwine$Effect[wine$Region == \"Languedoc\"] <- -9.30\nwine$Effect[wine$Region == \"Rhone\"] <- -0.75\n \n \n This solution is canonical in that it is clean and the R code is concise. (Note: This is what I ended up using to create this re-coded variable.) In my experience, however, this also means that students without a programming background don\u2019t initially understand it. This alone makes it unattractive pedagogically. \n \n A better solution pedagogically seems to be to create a new data frame of key-value pairs (in computer science this is called a hash table) and then use the join() function from the plyr package to `join\u2019 the original data frame and the new data frame. \n \nkey <- data.frame(\n Region = c(\"Bordeaux\", \"Burgundy\", \"Languedoc\", \"Rhone\"),\n Effect = c(9.11, 4.20, -9.33, -0.75)\n )\njoin(wine, key, by = Region)\n \n For me this is a useful way to teach how to recode variables. It has a direct link to the Excel VLOOKUP function, and also to ideas of relational databases. It also allows more generalizability in terms of being able to merge data sets using a common variable. \n R-wise, it is not difficult syntax, since almost every student has successfully used the data.frame() function to create a data frame. The join() function is also easily explained."], "link": "http://citizen-statistician.org/2012/10/20/recoding-variables-in-r-pedagogic-considerations/", "bloglinks": {}, "links": {}, "blogtitle": "Citizen-Statistician"}, {"content": ["I\u2019ve been reading and greatly enjoying Nate Silver\u2019s book, The Signal and the Noise: Why So Many Predictions Fail\u2014and Some Don\u2019t. \u00a0 I\u2019d recommend the book based on the introduction and first chapter alone. (And, no, that\u2019s not because that\u2019s all I\u2019ve read so far.\u00a0 It\u2019s\u00a0 because they\u2019re that good.)\u00a0 If you\u2019re the sort who skips introductions, I strongly suggest you become a new sort and read this one. It\u2019s a wonderful essay about the dangers of too much information, and the need to make sense of it.\u00a0 Silver makes the point that, historically, when we\u2019ve been faced with more information than we can handle, we tend to pick-and-choose which \u2018facts\u2019 we wish to believe.\u00a0 Sounds like a presidential debate, no? \n Another thing to like about the book is for the argument it provides\u00a0 against the Wired Magazine view that Big Data means the end of scientific theory.\u00a0 Chapter by chapter, Silver describes the very important role that theory and modeling play in making (successful) predictions.\u00a0 In fact, a theme of the book is that prediction is a human endeavor, despite the attention data scientists pay to automated algorithmic procedures.\u00a0 \u201cBefore we can demand more of our data, we need to demand more of ourselves.\u201d\u00a0 In other words, the Data Deluge requires us to find useful information, not just any old information. (Which is where we educators come in!) \n The first chapter makes a strong argument that the financial crisis was, to a great extent, a failure to understand fundamentals of statistical modeling, in particular to realize that the models are not the thing they model.\u00a0 Models are shaped by data but run on assumptions, and when the assumptions are wrong, the predictions fail.\u00a0 Chillingly, Silver points out that recoveries from financial crises tend to be much, much slower than recoveries from economic crises and, in fact, some economies never recover. \n Other chapters talk about baseball, weather, earthquakes, poker and more.\u00a0 I particularly enjoyed the weather chapter because, well, who doesn\u2019t enjoy talking about the weather? For me, perhaps because we are in the midst of elections, it also raised questions about the role of the U.S. federal government in supporting the economy.\u00a0 Weather prediction plays a big role in our economic infrastructure, even though many people tend to be dismissive of our ability to predict the weather.\u00a0 So it was interesting to see that, in fact, the government agencies do predict weather better than the private prediction firms (such as The Weather Channel), and are much better than local news channels\u2019 predictions.\u00a0 In fact, as Silver explains, the marketplace rewards poor predictions (at least when it comes to predicting rain).\u00a0 For me, this underlines the importance of a \u2018neutral\u2019 party. \n As I think about preparing students for the Deluge, I think that teaching prediction should take priority over teaching inference.\u00a0 Inference is important, but it is a specialized skill, and so is not needed by all.\u00a0 Prediction, on the other hand, is inherently important, and has been for millennia.Yes, prediction is a type of inference, but prediction and inference are not the same thing.\u00a0 As Silver points out, estimating a candidate\u2019s support for president is different from predicting whether or not the candidate will win. (Which leads me to propose a new slogan: \u201cPrediction: Inference for Tomorrow!\u201d\u00a0 Or \u201cPrediction: Inference for Procrastinators!\u201d) \n Much of this may be beyond the realm of introductory statistics, since some of the predictive models are complex.\u00a0 But the basics are important for intro stats students.\u00a0 All students\u00a0 should understand what a statistical model is and what it is not.\u00a0 Equally importantly, they should understand how to evaluate a model.\u00a0 And I don\u2019t mean that they should learn about r-squared (or only about r-squared.)\u00a0 They should learn about the philosophy of measuring model performance.\u00a0 In other words, intro stats students should understand why many predictions fail, but some don\u2019t, and how to tell the difference. \n So let\u2019s talk specifics.\u00a0 Post your comments on how you teach your students about prediction and modeling."], "link": "http://citizen-statistician.org/2012/10/19/nate-silvers-new-book/", "bloglinks": {}, "links": {}, "blogtitle": "Citizen-Statistician"}, {"content": ["Yesterday (October 14, 2012),\u00a0Felix Baumgartner made history by becoming the first person to break the speed of sound during a free fall. He also set some other records (e.g., longest free fall, etc.) during the Red Bull Stratos Mission \u2013which was broadcast live on the internet. Kind of cool, but imagine the conversation that took place daydreaming this one\u2026 \n Red Bull Creative Person:\u00a0 What if we got some idiot to float up into the stratosphere in a space capsule and then had him step out of it and free fall four minutes breaking the sound barrier? \n Another Red Bull Creative Person:\u00a0 Great idea! Lets\u2019 also broadcast it live on the internet. \n Well anyway, after the craziness ensued, It was suggested on Facebook that, \u201cI think this data should be on someone\u2019s blog!\u201d. Rising to the bait, I immediately looked at the mission page, \u00a0but the data was no longer there. Thank goodness for Wikipedia [ Red Bull Stratos Mission Data ]. The data can be copied and pasted into an Excel sheet, or read in to R using the\u00a0 readHTMLTable() function from the XML package. \n \nmission <- readHTMLTable(\n doc = \"http://en.wikipedia.org/wiki/Red_Bull_Stratos/Mission_data\",\n header = TRUE\n )\n \n We can then write it to an external file, I called it Mission.csv and put it on my desktop, using the read.csv() function. \n \nwrite.csv(mission,\n file = \"/Users/andrewz/Desktop/Mission.csv\",\n row.names = FALSE,\n quote = FALSE\n )\n \n Opening the new file in a text editor, we see some issues to deal with (these are also apparent from looking at the data on the Wikipedia page). \n \n The first line is the first table header,\u00a0 Elevation Data , which spanned three columns in the Wikipedia page. Delete it. \n The last row are the re-printed variable names. Delete it. \n Change the variable names in the current first row to be statistical software compliant (e.g., remove the commas and spaces from each variable). My first row looks like the following: \n \n Time,Elevation,DeltaTime,Speed \n \n \n Remove the commas from the values in the last column. With a comma separated value (CSV) file, they are trouble. \n There are nine rows which have parentheses around their value in the last column. I don\u2019t know what this means. For now, I will remove those values. \n \n The file can be downloaded here . \n Then you can plot (or analyze) away to your heart\u2019s content. \n \n# read in data to R\nmission <- read.csv(file = \"/Users/andrewz/Desktop/Mission.csv\")\n\n# Load ggplot2 library\nlibrary(ggplot2)\n\n# Plot speed vs. time\nggplot(data = mission, aes(x = Time, y = Speed)) +\n geom_line()\n\n# Plot elevation vs. time\nggplot(data = mission, aes(x = Time, y = Elevation)) +\n geom_line()\n \n  \n  \n Since I have no idea what these really represent other than what the variable names tell me, I cannot interpret these very well. Perhaps someone else can."], "link": "http://citizen-statistician.org/2012/10/15/red-bull-stratos-mission-data/", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1, "http://citizen-statistician.org/": 4}, "blogtitle": "Citizen-Statistician"}, {"content": ["The Current Population Survey (CPS) is a statistical survey conducted by the United States Census Bureau for the Bureau of Labor Statistics. The data collected is used to provide a monthly report on employment \u00a0in the United States. \n Although the CPS data are available, to this point it has really only been easy to deal with for SPSS, Stata, or SAS users. A new blog is also making it easy for R users to obtain and analyses these data. From their About/FAQ page: \n This blog announces obsessively-detailed instructions to analyze us government survey data with free tools \u2013 the r language and the survey package. \n \n They provide commented R scripts describing\u00a0how to load, clean, configure, and analyze many current data sets available. Each script contains information on how to automatically\u00a0download every microdata file from every survey year as an R data file onto your local disk. In addition, they detail how to use R to\u00a0match the published results from other statistical languages. They also provide video showing how to do much of what they cover in the scripts. \n Begin you foray into this and other government data sources here."], "link": "http://citizen-statistician.org/2012/10/15/current-population-survey-data-using-r/", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1, "http://usgsd.blogspot.com/": 2}, "blogtitle": "Citizen-Statistician"}, {"content": ["Simply Statistics lists some data analysis projects. Skewing towards the intermediate rather than novice student.\u00a0 But still useful in many ways.\u00a0 And\u2014some FitBit ideas! \n http://simplystatistics.org/post/32881133740/statistics-project-ideas-for-students-part-2"], "link": "http://citizen-statistician.org/2012/10/13/more-fitbit/", "bloglinks": {}, "links": {"http://simplystatistics.org/": 1}, "blogtitle": "Citizen-Statistician"}, {"content": ["A little bit ago [July 19, 2012 --- so I'm a little behind], the L.A. Times ran an article about whether TV hosts are pulling their own weight, salary wise. ( What is the real value of TV stars and personalities? )\u00a0 I took their data table and put it in a CSV format, and added a column called \u201cepynomious\u201d, which indicates whether the show is named after the host.\u00a0 (This apparently doesn\u2019t explain the salary variation.)\u00a0 A later letter to the editor pointed out that the analysis doesn\u2019t take into account how frequently the show must be recorded, and hence how often the host must come to work.\u00a0 Your students might enjoy adding this variable and analyzing the data to see if it explains anything. Maybe this is a good candidate for \u2018enrichment\u2019 via Google Refine?\u00a0 TV salaries from LA Times"], "link": "http://citizen-statistician.org/2012/10/13/tv-show-hosts/", "bloglinks": {}, "links": {"http://www.latimes.com/": 1, "http://citizen-statistician.org/": 1}, "blogtitle": "Citizen-Statistician"}]