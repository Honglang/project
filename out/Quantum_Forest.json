[{"blogurl": "http://www.quantumforest.com\n", "blogroll": [], "title": "Quantum Forest"}, {"content": ["(This post continues discussing issues I described back in January in Academic publication boycott ) \n Some weeks ago I received a couple of emails the same day: one asking me to submit a paper to an open access journal, while the other one was inviting me to be the editor of an \u2018special issue\u2019 of my choice for another journal. I haven\u2019t heard before about any of the two publications, which follow pretty much the same model: submit a paper for $600 and\u2014if they like it\u2014it will be published. However, the special issue email had this \u2018buy your way in\u2019 feeling: find ten contributors (i.e. $6,000) and you get to be an editor. Now, there is nothing wrong per-se with open access journals, some of my favorite ones (e.g. PLoS ONE ) follow that model. However, I was surprised by the increasing number of new journals that look at filling the gap for \u2018I need to publish soon, somewhere\u2019. Surprised until one remembers the incentives at play in academic environments. \n If I, or most academics for that matter, want to apply for academic promotion I have to show that I\u2019m a good guy that has a \u2018teaching philosophy\u2019 and that my work is good enough to get published in journals; hopefully in lots of them. The first part is a pain, but most people can write something along the lines \u2018I\u2019m passionate about teaching and enjoy creating a challenging environment for students\u2026\u2019 without puking \u2020 . The second part is trickier because one has to really have the papers in actual journals. \n Personally, I would be happier with only having the odd \u2018formal\u2019 publication. The first time (OK, few times) I saw my name in a properly typeset paper was very exciting, but it gets old after a while. These days, however, I would prefer to just upload my work to a website, saying here you have some ideas and code, play with it. If you like it great, if not well, next time I hope it\u2019ll be better. Nevertheless, this doesn\u2019t count as proper publication, because it isn\u2019t peer reviewed, independently of the number of comments the post may get. PLoS ONE counts, but it\u2019s still a journal and I (and many other researchers) work in many things that are too small for a paper, but cool enough to share. The problem: there is little or no credit for sharing so Quantum Forest is mostly a \u2018labor of love\u2019, which counts bugger all for anything else. \n These days as a researcher I often learn more from other people\u2019s blogs and quick idea exchanges (for example through Twitter) than via formal publication. I enjoy sharing analysis, ideas and code in this blog. So what\u2019s the point of so many papers in so many journals? I guess that many times we are just \u2018ticking the box\u2019 for promotions purposes. In addition, the idea of facing referees\u2019 or editors\u2019 comments like \u2018it would be a good idea that you cite the following papers\u2026\u2019 puts me off \u2021 . And what about authorship arrangements? We have moved from papers with 2-3 authors to enough authors to have a football team (with reserves and everything). Some research groups also run arrangements where \u2018I scratch your back (include you as a coauthor) and you scratch mine (include me in your papers)\u2019. We break ideas into little pieces that count for many papers, etc. \n Another related issue is the cost of publication (and the barriers it imposes on readership). You see, we referee papers for journals for free (as in for zero money) and tell ourselves that we are doing a professional service to uphold the high standards of whatever research branch we belong to. Then we spend a fortune from our library budget to subscribe to the same journals for which we reviewed the papers (for free, remember?). It is not a great deal, as many reasonable people have pointed out; I added a few comments in academic publication boycott . \n So, what do we need? We need promotion committees to reduce the weight on publication. We need to move away from impact factor. We can and need to communicate in other ways: scientific papers will not go away, but their importance should be reduced. \n  Some times the way forward is unclear. Incense doesn\u2019t hurt (Photo: Luis). \n \u2020 Making an effort to prepare interesting lectures doesn\u2019t hurt either. \n \u2021 These days it is fairly common editors \u2018suggesting\u2019 to include additional references in our manuscripts, which just happen to be to papers in the same journal, hoping to inflate the impact factor of the journal. Referees tend to suggest their own papers (some times useful, many times not). Lame, isn\u2019t it? \n PS. 2012-10-19 15:27 NZST. You also have to remember that not because something was published it is actually correct: outrageously funny example (via Arthur Charpentier ). Yep, through Twitter."], "link": "http://www.quantumforest.com/2012/10/publication-incentives/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www.quantumforest.com/": 3, "http://marginalrevolution.com/": 1, "http://www.plosone.org": 1}, "blogtitle": "Quantum Forest"}, {"content": ["I\u2019ve ignored my quantitative geneticist side of things for a while (at least in this blog) so this time I\u2019ll cover some code I was exchanging with a couple of colleagues who work for other organizations. \n It is common to use diallel mating designs in plant and tree breeding, where a small number of parents acts as both males and females. For example, with 5 parents we can have 25 crosses, including reciprocals and selfing (crossing an individual with itself). Decades ago this mating design was tricky to fit and, considering an experimental layout with randomized complete blocks, one would have something like y = mu + blocks + dads + mums + cross + error . In this model dads and mums were estimating a fraction of the additive genetic variance. With the advent of animal model BLUP, was possible to fit something like y = mu + blocks + individual (using a pedigree) + cross + error . Another less computationally demanding alternative (at least with unrelated parents) is to fit a parental model, overlaying the design matrices for parents with something like this y = mu + blocks + (dad + mum) + cross + error . \n The following code simulate data for a a diallel trial with three replicates and runs a parental model with ASReml. Later I expand the analysis building the matrices by hand. \n \n# Defining diallel cross\nn.blocks <- 3\ndiallel <- matrix(0, nrow = 5, ncol = 5)\nmales <- 1:dim(diallel)[1]\nfemales <- 1:dim(diallel)[2]\ncross <- factor(outer(males, females, paste, sep ='x'))\ncross\n#[1] 1x1 2x1 3x1 4x1 5x1 1x2 2x2 3x2 4x2 5x2 1x3 2x3 3x3 4x3 5x3 1x4 2x4\n#[18] 3x4 4x4 5x4 1x5 2x5 3x5 4x5 5x5\n#25 Levels: 1x1 1x2 1x3 1x4 1x5 2x1 2x2 2x3 2x4 2x5 3x1 3x2 3x3 ... 5x5\n\n\n#### Generating random values for the trial\n# Variance components\nsig2.a <- 40 # additive genetic variance\nsig2.sca <- 10 # specific combining ability (SCA)\nsig2.b <- 10 # blocks\nsig2.e <- 30 # residuals\n\n# Numbers of everything\nn.parents <- length(males)\nn.crosses <- length(cross)\nn.trees <- n.crosses*n.blocks\n\n# Random values for everything\nu.g <- rnorm(n.parents)*sqrt(sig2.a)\nu.sca <- rnorm(n.crosses)*sqrt(sig2.sca)\nu.block <- rnorm(n.blocks)*sqrt(sig2.b)\nu.e <- rnorm(n.trees)*sqrt(sig2.e)\n\n# Whole trial\ntrial <- data.frame(block = factor(rep(1:n.blocks, each = n.crosses)),\n     mum = factor(rep(females, n.blocks)),\n     dad = factor(rep(males, each = length(females))),\n     mate = factor(rep(cross, n.blocks)))\n\ntrial$yield <- 0.5*(u.g[trial$mum] + u.g[trial$dad]) +\n    u.sca[trial$mate] + u.block[trial$block] + u.e\n\nhead(trial) \n#block mum dad mate  yield\n#1  1 1 1 1x1 -0.02185486\n#2  1 2 1 2x1 10.79760712\n#3  1 3 1 3x1 16.45186037\n#4  1 4 1 4x1 8.15026291\n#5  1 5 1 5x1 5.57707180\n#6  1 1 2 1x2 -7.30675148\n\n# Fitting the model with ASReml\nlibrary(asreml)\nm1 <- asreml(yield ~ 1, \n    random = ~ block + dad + and(mum) + mate, \n    data = trial)\nsummary(m1)\n#      gamma component std.error z.ratio\n#block!block.var 1.299110e-02 3.861892e-01 1.588423e+00 0.2431274\n#dad!dad.var  2.101417e-01 6.246930e+00 5.120745e+00 1.2199259\n#mate!mate.var 4.589938e-07 1.364461e-05 2.340032e-06 5.8309519\n#R!variance  1.000000e+00 2.972722e+01 5.098177e+00 5.8309519\n\n# Obtaining the predicted breeding values for the parents\ncoef(m1, pattern = 'dad')\n#   effect\n#dad_1 1.780683\n#dad_2 -2.121174\n#dad_3 3.151991\n#dad_4 -1.473620\n#dad_5 -1.337879\n \n How is the matrix overlay working? We can replicate the calculations used by ASReml by building the matrices from scratch and reusing the variance components, so we avoid the nastiness of writing code for residual maximum likelihood. Once I build the basic matrices I use the code from my How does a linear mixed model look like? post. \n \n# Building incidence matrices for males and females\nZmales <- model.matrix(~ dad - 1, data = trial)\nZfemales <- model.matrix(~ mum - 1, data = trial)\n\n# and() in ASReml overlays (i.e. sums) the matrices\n# (Notice that this creates 0s, 1 and 2s in the design matrix)\nZparents <- Zmales + Zfemales\nZparents[1:6,]\n# dad1 dad2 dad3 dad4 dad5\n#1  2 0 0 0 0\n#2  1 1 0 0 0\n#3  1 0 1 0 0\n#4  1 0 0 1 0\n#5  1 0 0 0 1\n#6  1 1 0 0 0\n\n# Design matrices from other factors\nZblocks <- model.matrix(~ block - 1, data = trial)\nZmates <- model.matrix(~ mate - 1, data = trial)\n\n# Creating y, X and big Z matrices to solve mixed model equations\ny <- trial$yield\nX <- matrix(1, nrow = 75, ncol = 1)\nZ <- cbind(Zblocks, Zparents, Zmates)\n\n# Building covariance matrices for random effects\n# Using the variance components from the ASReml model\nG = diag(c(rep(3.861892e-01, 3), \n   rep(6.246930e+00, 5), \n   rep(1.364461e-05, 25)))\nR = diag(2.972722e+01, 75, 75)\nRinv = solve(R)\n\n# Components of C\nXpX = t(X) %*% Rinv %*% X\nZpZ = t(Z) %*% Rinv %*% Z\n\nXpZ = t(X) %*% Rinv %*% Z\nZpX = t(Z) %*% Rinv %*% X\n\nXpy = t(X) %*% Rinv %*% y\nZpy = t(Z) %*% Rinv %*% y\n\n# Building C * [b a] = RHS\nC = rbind(cbind(XpX, XpZ),\n   cbind(ZpX, ZpZ + solve(G)))\nRHS = rbind(Xpy, Zpy)\n\nblup = solve(C, RHS)\nblup\n# These results are identical to the ones\n# produced by ASReml\n#dad1  1.780683e+00\n#dad2 -2.121174e+00\n#dad3  3.151991e+00\n#dad4 -1.473620e+00\n#dad5 -1.337879e+00\n \n The overlay matrix Zparents is double the actual value we should use: \n \nZparents2 <- 0.5*(Zmales + Zfemales)\nZparents2[1:6,]\n# dad1 dad2 dad3 dad4 dad5\n#1 1.0 0.0 0.0 0.0 0.0\n#2 0.5 0.5 0.0 0.0 0.0\n#3 0.5 0.0 0.5 0.0 0.0\n#4 0.5 0.0 0.0 0.5 0.0\n#5 0.5 0.0 0.0 0.0 0.5\n#6 0.5 0.5 0.0 0.0 0.0\n \n Repeating the analyses \u2018by hand\u2019 using Zparents2 to build the Z matrix results in the same overall mean and block effects, but the predicted breeding values for parents when using Zparents are 0.7 of the predicted breeding values for parents when using Zparents2 . I may need to refit the model and obtain new variance components for parents when working with the correct overlaid matrix. \n  Gratuitous picture: walking in Quail Island (Photo: Luis)."], "link": "http://www.quantumforest.com/2012/10/overlay-of-design-matrices-in-genetic-analysis/", "bloglinks": {}, "links": {"http://www.quantumforest.com/": 2}, "blogtitle": "Quantum Forest"}, {"content": ["I have written a few posts discussing descriptive analyses of evaluation of National Standards for New Zealand primary schools.The data for roughly half of the schools was made available by the media, but the full version of the dataset is provided in a single-school basis. In the page for a given school there may be link to a PDF file with the information on standards sent by the school to the Ministry of Education. \n I\u2019d like to keep a copy of the PDF reports for all the schools for which I do not have performance information, so I decided to write an R script to download just over 1,000 PDF files. Once I can identify all the schools with missing information I just loop over the list, using the fact that all URL for the school pages start with the same prefix. I download the page, look for the name of the PDF file and then download the PDF file, which is named school_schoolnumber.pdf . And that\u2019s it. \n Of course life would be a lot simpler if the Ministry of Education made the information available in a usable form for analysis. \n \nlibrary(XML) # HTML processing\noptions(stringsAsFactors = FALSE)\n\n# Base URL\nbase.url = 'http://www.educationcounts.govt.nz/find-a-school/school/national?school='\ndownload.folder = '~/Downloads/schools/'\n\n# Schools directory\ndirectory <- read.csv('Directory-Schools-Current.csv')\ndirectory <- subset(directory, \n     !(school.type %in% c(\"Secondary (Year 9-15)\", \"Secondary (Year 11-15)\")))\n\n# Reading file obtained from stuff.co.nz obtained from here:\n# http://schoolreport.stuff.co.nz/index.html\nfairfax <- read.csv('SchoolReport_data_distributable.csv')\nfairfax <- subset(fairfax, !is.na(reading.WB)) \n\n# Defining schools with missing information\nto.get <- merge(directory, fairfax, by = 'school.id', all.x = TRUE)\nto.get <- subset(to.get, is.na(reading.WB))\n\n# Looping over schools, to find name of PDF file\n# with information and download it\n\nfor(school in to.get$school.id){\n \n # Read HTML file, extract PDF link name\n cat('Processing school ', school, '\\n')\n doc.html <- htmlParse(paste(base.url, school, sep = ''))\n doc.links <- xpathSApply(doc.html, \"//a/@href\")\n pdf.url <- as.character(doc.links[grep('pdf', doc.links)])\n if(length(pdf.url) > 0) {\n pdf.name <- paste(download.folder, 'school_', school, '.pdf', sep = '')\n download.file(pdf.url, pdf.name, method = 'auto', quiet = FALSE, mode = \"w\",\n     cacheOK = TRUE, extra = getOption(\"download.file.extra\"))\n }\n}\n \n Can you help? \n It would be great if you can help me to get the information from the reports. The following link randomly chooses a school, click on the \u201cNational Standards\u201d tab and open the PDF file. \n \n Then type the achievement numbers for reading, writing and mathematics in this Google Spreadsheet . No need to worry about different values per sex or ethnicity; the total values will do. \n  Gratuitous picture: a simple summer lunch (Photo: Luis)."], "link": "http://www.quantumforest.com/2012/10/scraping-pages-and-downloading-files-using-r/", "bloglinks": {}, "links": {"https://docs.google.com/": 1, "http://www.quantumforest.com/": 3, "http://www.govt.nz/": 1}, "blogtitle": "Quantum Forest"}, {"content": ["This week I\u2019ve tried to i-stay mostly in the descriptive statistics realm and ii-surround any simple(istic) models with caveats and pointing that they are very preliminary. We are working with a sample of ~1,000 schools that did reply to Fairfax\u2019s request, while there is a number of schools that either ignored the request or told Fairfax to go and F themselves. Why am I saying this? If one goes and gets a simple table of the number of schools by type and decile there is something quite interesting: we have different percentages for different types of schools represented in the sample and the possibility of bias on the reporting to Fairfax, due to potential low performance (references to datasets correspond to the ones I used in this post ): \n \nsummary(standards$school.type)\n#   Composite (Year 1-10)   Composite (Year 1-15)  Contributing (Year 1-6) \n#        1        29       403 \n#  Full Primary (Year 1-8) Intermediate (year 7 and 8) Restricted Composite (Yr 7-10) \n#       458        62        1 \n#   Secondary (Year 7-15) \n#       56 \n \n Now let\u2019s compare this number with the school directory: \n \nsummary(factor(directory$school.type))\n#   Composite (Year 1-10)   Composite (Year 1-15)  Contributing (Year 1-6) \n#        4       149       775 \n#   Correspondence School  Full Primary (Year 1-8) Intermediate (year 7 and 8) \n#        1       1101       122 \n#Restricted Composite (Yr 7-10)   Secondary (Year 11-15)   Secondary (Year 7-10) \n#        4        2        2 \n#   Secondary (Year 7-15)   Secondary (Year 9-15)     Special School \n#       100       238        39 \n#    Teen Parent Unit \n#       20 \n \n As a proportion we are missing more secondary schools . We can use the following code to get an idea of how similar are school types, because the small number of different composite schools is a pain. If \n \n# Performance of Contributing (Year 1-6) and\n# Full Primary (Year 1-8) looks pretty much the\n# same. Composites could be safely merged\nqplot(school.type, reading.OK, \n  data = standards, geom = 'jitter')\n\nqplot(school.type, writing.OK, \n  data = standards, geom = 'jitter')\n\nqplot(school.type, math.OK, \n  data = standards, geom = 'jitter')\n\n# Merging school types and plotting them colored\n# by decile\nstandards$school.type.4 <- standards$school.type\nlevels(standards$school.type.4) <- c('Composite', 'Composite', 'Primary',\n          'Primary', 'Intermediate',\n          'Composite', 'Secondary')\n\nqplot(school.type.4, reading.OK, colour = decile,\n  data = standards, geom = 'jitter')\n \n  Representation of different schools types and deciles is uneven. \n  Different participations in the sample for school types. This type is performance in mathematics. \nI\u2019m using jittering rather than box and whisker plots to i- depict all the schools and ii- get an idea of the different participation of school types in the dataset. Sigh. Another caveat to add in the discussion. \n P.S. 2012-09-27 16:15. Originally I mentioned in this post the lack of secondary schools (Year 9-15) but, well, they are not supposed to be here, because National Standards apply to years 1 to 8 (Thanks to Michael MacAskill for pointing out my error.)"], "link": "http://www.quantumforest.com/2012/09/a-word-of-caution-the-sample-may-have-an-effect/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www.quantumforest.com/": 3}, "blogtitle": "Quantum Forest"}, {"content": ["Eric and I have been exchanging emails about potential analyses for the school data and he published a first draft model in Offsetting Behaviour. I have kept on doing mostly data exploration while we get a definitive full dataset, and looking at some of the pictures I thought we could present a model with fewer predictors. \n The starting point is the standards dataset I created in the previous post : \n \n# Make authority a factor\nstandards$authority <- factor(standards$authority)\n\n# Plot relationship between school size and number of FTTE\n# There seems to be a separate trend for secondary schools\nqplot(total.roll, teachers.ftte, \n  data = standards, color = school.type)\n \n  There seems to be a different trend for secondary vs non-secondary schools concerning the relationship between number of full time teacher equivalent and total roll. The presence of a small number of large schools suggests that log transforming the variables could be a good idea. \n \n# Create a factor for secondary schools versus the rest\nstandards$secondary <- factor(ifelse(standards$school.type == 'Secondary (Year 7-15)', \n        'Yes', 'No'))\n\n# Plot the relationship between number of students per FTTE\n# and type of school\n qplot(secondary, total.roll/teachers.ftte, \n  data = standards, geom = 'boxplot',\n  ylab = 'Number of students per FTTE')\n \n  Difference on the number of students per FTTE between secondary and non-secondary schools. \n Now we fit a model where we are trying to predict reading standards achievement per school accounting for decile, authority , proportion of non-european students, secondary schools versus the rest, and a different effect of number of students per FTTE \nfor secondary and non-secondary schools. \n \nstandards$students.per.ftte <- with(standards, total.roll/teachers.ftte)\n\n# Decile2 is just a numeric version of decile (rather than a factor)\nm1e <- lm(reading.OK ~ decile2 + I(decile2^2) + \n   authority + I(1 - prop.euro) + secondary*students.per.ftte, \n   data = standards, weights = total.roll)\nsummary(m1e)\n\n#Coefficients:\n#           Estimate Std. Error t value Pr(>|t|) \n#(Intercept)        0.6164089 0.0305197 20.197 < 2e-16 ***\n#decile2         0.0422733 0.0060756 6.958 6.24e-12 ***\n#I(decile2^2)       -0.0015441 0.0004532 -3.407 0.000682 ***\n#authorityState: Not integrated   -0.0440261 0.0089038 -4.945 8.94e-07 ***\n#I(1 - prop.euro)      -0.0834869 0.0181453 -4.601 4.74e-06 ***\n#secondaryYes       -0.2847035 0.0587674 -4.845 1.47e-06 ***\n#students.per.ftte      0.0023512 0.0011706 2.009 0.044854 * \n#secondaryYes:students.per.ftte   0.0167085 0.0040847 4.091 4.65e-05 ***\n---\n\n#Residual standard error: 1.535 on 999 degrees of freedom\n# (3 observations deleted due to missingness)\n#Multiple R-squared: 0.4942,\tAdjusted R-squared: 0.4906 \n#F-statistic: 139.4 on 7 and 999 DF, p-value: < 2.2e-16 \n \n The residuals are still a bit of a mess: \n  Residuals for this linear model: still a bit of a mess. \n If we remember my previous post decile accounted for 45% of variation and we explain 4% more through the additional predictors. Non-integrated schools have lower performance, a higher proportion of non-European students reduce performance, secondary schools have lower performance and larger classes tend to perform better (Eric suggests reverse causality, I\u2019m agnostic at this stage), although the rate of improvement changes between secondary and non-secondary schools. In contrast with Eric, I didn\u2019t fit separate ethnicities as those predictors are related to each other and constrained to add up to one. \n Of course this model is very preliminary, and a quick look at the coefficients will show that changes on any predictors besides decile will move the response by a very small amount (despite the tiny p-values and numerous stars next to them). The distribution of residuals is still heavy-tailed and there are plenty of questions about data quality; I\u2019ll quote Eric here: \n But differences in performance among schools of the same decile by definition have to be about something other than decile. I can\u2019t tell from this data whether it\u2019s differences in stat-juking, differences in unobserved characteristics of entering students, differences in school pedagogy, or something else. But there\u2019s something here that bears explaining."], "link": "http://www.quantumforest.com/2012/09/some-regressions-on-school-data/", "bloglinks": {}, "links": {"http://www.quantumforest.com/": 4, "http://offsettingbehaviour.co.nz/": 1}, "blogtitle": "Quantum Forest"}, {"content": ["In two previous posts I put together a data set and presented some exploratory data analysis on school achievement for national standards. After those posts I exchanged emails with a few people about the sources of data and Jeremy Greenbrook-Held pointed out Education Counts as a good source of additional variables, including number of teachers per school and proportions for different ethnic groups. \n The code below call three files: Directory-Schools-Current.csv , teacher-numbers.csv and SchoolReport_data_distributable.csv , which you can download from the links. \n \noptions(stringsAsFactors = FALSE)\n\nlibrary(ggplot2)\n\n# Reading School directory and dropping some address information\n# for 2012 obtained from here:\n# http://www.educationcounts.govt.nz/directories/list-of-nz-schools\ndirectory <- read.csv('Directory-Schools-Current.csv')\ndirectory <- subset(directory, select = -1*c(3:9, 11:13))\n\n# Reading teacher numbers for 2011 obtained from here:\n# http://www.educationcounts.govt.nz/statistics/schooling/teaching_staff\nteachers <- read.csv('teacher-numbers.csv')\n\n# Reading file obtained from stuff.co.nz obtained from here:\n# http://schoolreport.stuff.co.nz/index.html\nfairfax <- read.csv('SchoolReport_data_distributable.csv')\n\n# Merging directory and teachers info\nstandards <- merge(directory, teachers, by = 'school.id', all.x = TRUE)\n\n# Checking that the school names match\n# This shows ten cases, some of them obviously the same school\n# e.g. Epsom Girls Grammar School vs Epsom Girls' Grammar School\ntocheck <- subset(standards, school.name.x != school.name.y)\ntocheck[, c(1:2, 29)]\n#school.id       school.name.x     school.name.y\n#61   64    Epsom Girls Grammar School Epsom Girls' Grammar School\n#125  135      Fraser High School Hamilton's Fraser High School\n#362  402      Waiau Area School Tuatapere Community College\n#442  559      Te Wainui a Rua   Whanganui Awa School\n#920  1506 St Michael's Catholic School (Remuera) St Michael's School (Remuera)\n#929  1515      Sunnyhills School    Sunny Hills School\n#985  1573      Willow Park School    Willowpark School\n#1212  1865    Te Wharekura o Maniapoto   Te Wharekura o Oparure\n#1926  2985   Sacred Heart Cathedral School Sacred Heart School (Thorndon)\n#2266  3554       Waitaha School  Waitaha Learning Centre\n\n# Merging now with fairfax data\nstandards <- merge(standards, fairfax, \n     by = 'school.id', all.x = TRUE)\n\n# Four schools have a different name\ntocheck2 <- subset(standards, school.name.x != school.name)\ntocheck2[, c(1:2, 32)]\n#school.id       school.name.x     school.name\n#920  1506 St Michael's Catholic School (Remuera) St Michael's School (Remuera)\n#929  1515      Sunnyhills School   Sunny Hills School\n#985  1573      Willow Park School    Willowpark School\n#2266  3554       Waitaha School  Waitaha Learning Centre\n\n# Checking the original spreadsheets it seems that, despite \n# the different name they are the same school (at least same\n# city)\n\n# Now start dropping a few observations that are of no use\n# for any analysis; e.g. schools without data\nstandards <- subset(standards, !is.na(reading.WB))\n\n# Dropping school 498 Te Aho o Te Kura Pounamu Wellington\n# http://www.tekura.school.nz/ \n# It is a correspondence school without decile information\nstandards <- subset(standards, school.id != 498)\n\n# Converting variables to factors\nstandards$decile <- factor(standards$decile)\nstandards$school.type <- factor(standards$school.type)\nstandards$education.region <- factor(standards$education.region,\n          levels = c('Northern', 'Central North', 'Central South', 'Southern'))\n\n# Removing 8 special schools\nstandards <- subset(standards, as.character(school.type) != 'Special School')\nstandards$school.type <- standards$school.type[drop = TRUE]\n\n# Saving file for Eric\n# write.csv(standards, 'standardsUpdated.csv', \n#   row.names = FALSE, quote = FALSE)\n\n# Create performance groups\nstandards$math.OK <- with(standards, math.At + math.A)\nstandards$reading.OK <- with(standards, reading.At + reading.A)\nstandards$writing.OK <- with(standards, writing.At + writing.A)\n\n# Creating a few proportions\nstandards$prop.euro <- with(standards, european/total.roll)\nstandards$prop.maori <- with(standards, maori/total.roll)\nstandards$prop.pacific <- with(standards, pacific.island/total.roll)\nstandards$prop.asian <- with(standards, asian/total.roll)\n\n \n This updated data set is more comprehensive but it doesn\u2019t change the general picture presented in my previous post beyond the headlines . Now we can get some cool graphs to point out the obvious, for example the large proportion of Maori and Pacific Island students in low decile schools: \n \nqplot(prop.maori, prop.pacific,\n  data = standards, color = decile,\n  xlab = 'Proportion of Maori',\n  ylab = 'Proportion of Pacific Island')\n \n  Proportion of Pacific Island (vertical axis) and Maori students (horizontal axis) in schools with points colored by decile. Higher proportions for both are observed in low decile schools. \n I have avoided \u2018proper\u2019 statistical modeling because i- there is substantial uncertainty in the data and ii- the national standards for all schools (as opposed to only 1,000 schools) will be released soon; we do\u2019t know if the published data are a random sample. In any case, a quick linear model fitting the proportion of students that meet reading standards (reading.OK) as a function of decile and weighted by total school roll\u2014to account for the varying school sizes\u2014will explain roughly 45% of the observed variability on reading achievement. \n \nm1 <- lm(reading.OK ~ decile, \n   data = standards, \n   weights = total.roll)\nsummary(m1)\n\n# Coefficients:\n# Estimate Std. Error t value Pr(>|t|) \n# (Intercept) 0.56164 0.01240 45.278 < 2e-16 ***\n# decile2  0.06238 0.01769 3.527 0.000439 ***\n# decile3  0.10890 0.01749 6.225 7.07e-10 ***\n# decile4  0.16434 0.01681 9.774 < 2e-16 ***\n# decile5  0.18579 0.01588 11.697 < 2e-16 ***\n# decile6  0.22849 0.01557 14.675 < 2e-16 ***\n# decile7  0.25138 0.01578 15.931 < 2e-16 ***\n# decile8  0.24849 0.01488 16.701 < 2e-16 ***\n# decile9  0.28861 0.01492 19.347 < 2e-16 ***\n# decile10  0.31074 0.01439 21.597 < 2e-16 ***\n# \n# Residual standard error: 1.594 on 999 degrees of freedom\n# (1 observation deleted due to missingness)\n# Multiple R-squared: 0.4548, Adjusted R-squared: 0.4499 \n# F-statistic: 92.58 on 9 and 999 DF, p-value: < 2.2e-16 \n \n Model fit has a few issues with distribution of residuals, we should probably use a power transformation for the response variable, but I wouldn\u2019t spend much more time before getting the full data for national standards. \n \npar(mfrow = c(2, 2))\nplot(m1)\npar(mfrow = c(1, 1))\n \n  Residuals of a quick weighted linear model. The residuals show some heterogeneity of variance (top-left) and deviation from normality (top-right) with heavy tails. \n Bonus plot: map of New Zealand based on school locations, colors depicting proportion of students meeting reading national standards. \n \nqplot(longitude, latitude, \n  data = standards, color = reading.OK)\n \n  New Zealand drawn using school locations; color coding is for proportion of students meeting reading national standards. \n P.S. 2012-09-26 16:01 . The simple model above could be fitted taking into account the order of the decile factor (using ordered() ) or just fitting linear and quadratic terms for a numeric expression of decile. Anyway, that would account for 45% of the observed variability. \n \nm1d <- lm(reading.OK ~ as.numeric(decile) + I(as.numeric(decile)^2), \n   data = standards, weights = total.roll)\nsummary(m1d)\n\n#Coefficients:\n#       Estimate Std. Error t value Pr(>|t|) \n#(Intercept)    0.5154954 0.0136715 37.706 < 2e-16 ***\n#as.numeric(decile)  0.0583659 0.0051233 11.392 < 2e-16 ***\n#I(as.numeric(decile)^2) -0.0023453 0.0004251 -5.517 4.39e-08 ***\n#---\n#Residual standard error: 1.598 on 1006 degrees of freedom\n# (1 observation deleted due to missingness)\n#Multiple R-squared: 0.4483,\tAdjusted R-squared: 0.4472 \n#F-statistic: 408.8 on 2 and 1006 DF, p-value: < 2.2e-16 \n \n P.S. 2012-09-26 18:17 . Eric Crampton has posted preliminary analyses based on this dataset in Offsetting Behaviour ."], "link": "http://www.quantumforest.com/2012/09/updating-and-expanding-new-zealand-school-data/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www.quantumforest.com/": 9, "http://offsettingbehaviour.co.nz/": 1, "http://www.govt.nz/": 1}, "blogtitle": "Quantum Forest"}, {"content": ["Music and creativity relax me: Diego Stocco\u2019s Experibass."], "link": "http://www.quantumforest.com/2012/09/a-little-break-from-data-policy-experibass/", "bloglinks": {}, "links": {"http://www.quantumforest.com/": 1}, "blogtitle": "Quantum Forest"}, {"content": ["I like the idea of having data on school performance, not to directly rank schools\u2014hard, to say the least, at this stage\u2014but because we can start having a look at the factors influencing test results. I imagine the opportunity in the not so distant future to run hierarchical models combining Ministry of Education data with Census/Statistics New Zealand data. \n At the same time, there is the temptation to come up with very simple analyses that would make appealing newspaper headlines. I\u2019ll read the data and create a headline and then I\u2019ll move to something that, personally, seems more important. In my previous post I combined the national standards for around 1,000 schools with decile information to create the standards.csv file. \n \nlibrary(ggplot2)\n\n# Reading data, building factors\nstandards <- read.csv('standards.csv')\nstandards$decile.2008 <- factor(standards$decile.2008)\nstandards$electorate <- factor(standards$electorate)\nstandards$school.type <- factor(standards$school.type)\n\n# Removing special schools (all 8 of them) because their\n# performance is not representative of the overall school\n# system\nstandards <- subset(standards, \n     as.character(school.type) != 'Special School')\nstandards$school.type <- standards$school.type[drop = TRUE]\n\n# Create performance variables. Proportion of the students that\n# at least meet the standards (i.e. meet + above)\n# Only using all students, because subsets are not reported\n# by many schools\nstandards$math.OK <- with(standards, math.At + math.A)\nstandards$reading.OK <- with(standards, reading.At + reading.A)\nstandards$writing.OK <- with(standards, writing.At + writing.A)\n \n Up to this point we have read the data, removed special schools and created variables that represent the proportion of students that al least meet standards. Now I\u2019ll do something very misleading: calculate the average for reading.OK for each school decile and plot it, showing the direct relationship between socio-economic decile and school performance. \n \nmislead <- aggregate(reading.OK ~ decile.2008, \n      data = standards, mean)\nqplot(decile.2008, reading.OK , data = mislead)\n \n  Scatterplot of average proportion of students at least meeting the reading national standards for each socioeconomic decile. \n Using only the average strengthens the relationship, but it hides something extremely important: the variability within each decile. The following code will display box and whisker plots for each decile: the horizontal line is the median, the box contains the middle 50% of the schools and the vertical lines 1.5 times the interquantile range (pretty much the range in most cases): \n \nqplot(decile.2008, reading.OK, \n  data = standards, geom = 'boxplot',\n  xlab = 'Decile in 2008',\n  ylab = 'Reading at or above National Standard')\n \n  Box and whisker plot for proportion of students at least meeting the reading national standard for each decile. Notice the variability for each decile. \n A quick look at the previous graph shows a few interesting things: \n \n the lower the decile the more variable school performance, \n there is pretty much no difference between deciles 6, 7, 8 and 9, and a minor increase for decile 10. \n there is a trend for decreasing performance for lower deciles; however, there is also a huge variability within those deciles. \n \n We can repeat the same process for writing.OK and math.OK with similar trends, although the level of standard achievement is lower than for reading: \n \nqplot(decile.2008, writing.OK, \n  data = standards, geom = 'boxplot',\n  xlab = 'Decile in 2008',\n  ylab = 'Writing at or above National Standard')\n\nqplot(decile.2008, math.OK, \n  data = standards, geom = 'boxplot',\n  xlab = 'Decile in 2008',\n  ylab = 'Mathematics at or above National Standard')\n \n  Box and whisker plot for proportion of students at least meeting the writing national standard for each decile. \n  Box and whisker plot for proportion of students at least meeting the mathematics national standard for each decile. \n Achievement in different areas (reading, writing, mathematics) is highly correlated: \n \ncor(standards[, c('reading.OK', 'writing.OK', 'math.OK')], \n use = 'pairwise.complete.obs')\n\n#   reading.OK writing.OK math.OK\n#reading.OK 1.0000000 0.7886292 0.7749094\n#writing.OK 0.7886292 1.0000000 0.7522446\n#math.OK  0.7749094 0.7522446 1.0000000\n\n# We can plot an example and highlight decile groups\n# 1-5 vs 6-10\nqplot(reading.OK, math.OK,\n  data = standards, color = ifelse(as.numeric(decile.2008) > 5, 1, 0)) + \n  opts(legend.position = 'none')\n \n  Scatterplot for proportion meeting mathematics and reading national standards. Dark points are deciles 1 to 5, while light points are deciles 6 to 10. Notice the large overlap for performance between the two groups. \n All these graphs are only descriptive/exploratory; however, once we had more data we could move to hierarchical models to start adjusting performance by socioeconomic aspects, geographic location, type of school, urban or rural setting, school size, ethnicity, etc. Potentially, this would let us target resources on schools that could be struggling to perform; nevertheless, any attempt at creating a \u2018quick & dirty\u2019 ranking ignoring the previously mentioned covariates would be, at the very least, misleading. \n Note 2012-09-26: I have updated data and added a few plots in this post ."], "link": "http://www.quantumforest.com/2012/09/new-zealand-school-performance-beyond-the-headlines/", "bloglinks": {}, "links": {"http://www.quantumforest.com/": 8}, "blogtitle": "Quantum Forest"}, {"content": ["Some people have the naive idea that the national standards data and the decile data will not be put together. Given that at some point in time all data will be available, there is no technical reason to not have merged the data, which I have done in this post for an early release. \n Stuff made data for the National Standards for a bit over 1,000 schools available here as an Excel File . I cleaned it up a bit to read it in R, saved it to csv format, of which you can get a copy here . Decile data is available from the Ministry of Education , which I transformed to csv and uploaded here . We can now merge the datasets using R: \n \noptions(stringsAsFactors = FALSE)\n\ndeciles <- read.csv('DecileChanges20072008.csv')\nstandards <- read.csv('SchoolReport_data_distributable.csv')\n\nstandards <- merge(standards, deciles, by = 'school.id')\n \n But there are some name discrepancies in the merge, which we can check using: \n \nstandards$name.discrepancy <- factor(ifelse(standards$school.name.x != standards$school.name.y, 1, 0))\n\ntocheck <- subset(standards[, c(1, 2, 64, 77)], name.discrepancy == 1)\n# school.id       school.name.x       school.name.y name.discrepancy\n#9   82    Aidanfield Christian School   Canterbury Christian College    1\n#33  266     Waipa Christian School    Bethel Christian School    1\n#73  429      Excellere College     Kamo Christian College    1\n#143  1245 Christ the King Catholic School (Owairak Christ The King School (Mt Roskill)    1\n#150  1256   Cornwall Park District School     Cornwall Park School    1\n#211  1360  Marist Catholic School (Herne Bay)    Marist School (Herne Bay)    1\n#270  1500  St Leo's Catholic School (Devonport)    St Leos School (Devonport)    1\n#297  1588 St Francis Xavier Catholic School (Whang St Francis Xavier School (Whangarei)    1\n#305  1650     Drummond Primary School Central Southland Rural Primary School    1\n#314  1678     Te Kura o Waikaremoana     Te Kura O Waikaremoana    1\n#335  1744    Horahora School (Cambridge)      Horahora School    1\n#389  1991     Tauranga Primary School      Tauranga School    1\n#511  2424    Parkland School (P North)      Parkland School    1\n#587  2663     Reignier Catholic School    Reignier School (Taradale)    1\n#627  2841  Fergusson Intermediate (Trentham)     Fergusson Intermediate    1\n#771  3213    Parklands School (Motueka)      Parklands School    1\n#956  3801    Pine Hill School (Dunedin)      Pine Hill School    1\n \n There are four schools that have very different names in the dataset: 82, 266, 429 and 1650. They may just have changed names or something could be wrong. At this point one may choose to drop them from any analysis; up to you. We then just rename the second variable to school.name and save the file to csv (available here standards.csv for your enjoyment). \n \nnames(standards)[2] <- 'school.name'\nwrite.csv(standards, 'standards.csv', \n   row.names = FALSE, quote = FALSE)\n \n Now we should try to merge any other economic and social information available in Statistics New Zealand. \n Note 2012-09-26: I have updated data and added a few plots in this post ."], "link": "http://www.quantumforest.com/2012/09/new-zealand-school-data/", "bloglinks": {}, "links": {"http://www.quantumforest.com/": 4, "http://schoolreport.co.nz/": 1, "http://www.govt.nz/": 1}, "blogtitle": "Quantum Forest"}, {"content": ["Oddities tend to jump out when one uses software in a daily basis. The situation is even clearer when using software for teaching: many more people looking at it with fresh eyes. \n Let\u2019s say that we are fitting a simple linear model and we use the summary function, then POW! i- one gets all sorts of stars next to each of the coefficients and ii- some tiny p-values with lots of digits. Since immemorial times (childcare, at least) we got star stickers when doing a good job and here we have R doing the same. It is possible to remove the stars, I know, but the default is the subject of this post. \n \nx <- 1:20\ny <- 5 + 2*x + rnorm(20) *5\n\nm <- lm(y ~ x)\nsummary(m)\n\n#Call:\n#lm(formula = y ~ x)\n\n#Residuals:\n# Min  1Q Median  3Q  Max \n#-9.1030 -0.8772 0.1168 2.3375 9.5846 \n\n#Coefficients:\n#   Estimate Std. Error t value Pr(>|t|) \n#(Intercept) 5.0193  2.0507 2.448 0.0249 * \n#x    2.0670  0.1712 12.074 4.57e-10 ***\n#---\n#Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n\n#Residual standard error: 4.415 on 18 degrees of freedom\n#Multiple R-squared: 0.8901,\tAdjusted R-squared: 0.884 \n#F-statistic: 145.8 on 1 and 18 DF, p-value: 4.571e-10 \n \n In contrast, I like the simplicity of the display() function in the arm package. If we use it with the same model m we get: \n \nlibrary(arm)\ndisplay(m)\n\n#lm(formula = y ~ x)\n#   coef.est coef.se\n#(Intercept) 5.02  2.05 \n#x   2.07  0.17 \n#---\n#n = 20, k = 2\n#residual sd = 4.41, R-Squared = 0.89\n \n There is an emphasis on understanding and awareness of limitations. Rather than a false sense of accuracy and precision, we get coefficients rounded to two decimal places, no stars and no p-values. Did you really believed that the p-value was 0.0249 instead of 0.0243? \n Defaults also require consistency, because consistency begets \u2018intuitiveness\u2019. Every function that deals with data should take a data argument and avoid being schizophrenic: \n \ndf <- data.frame(x = rnorm(100), y = rnorm(100))\n\n# This works fine\nplot(y ~ x, data = df)\n\n# This craps out\nplot(x, y, data = df)\n#Error in plot(x, y, data = df) : object 'x' not found\n \n I\u2019ve mentioned in previous posts the apply() family of functions. Yes, I hate them with passion and that\u2019s why I think we should unite and rely on Talk like a Pirate Day (Rrrrstats, matey!) to lead a mutiny to get plyr as the new default. \n However, even some of my favorite packages have strange defaults. In ggplot2 , for example, the size (too small) and the color (too light) of the text in the axes is extremely hard to read. Maybe not for you young reader, but for a long-sighted, receding-hairline-guy with salt & pepper beard is a struggle. This is magnified when pasting said graph in a Keynote or PowerPoint presentation and then using a crappy projector. Again, thanks Hadley, I can define my own theme but there are thousands of difficult to read graphs doing the rounds because, hey, those are the defaults. \n Every time we define a default we have to think that, even when we allow change, most users will probably stick to it. Of course I\u2019m the first one to acknowledge that my favorite defaults may not coincide with yours, as shown below. \n Sticker in a University of Canterbury toilet cubicle. There are cultural differences on the default correct position."], "link": "http://www.quantumforest.com/2012/09/unsurprisingly-users-default-to-the-defaults/", "bloglinks": {}, "links": {"http://www.quantumforest.com/": 1, "http://www.google.com/": 1}, "blogtitle": "Quantum Forest"}]