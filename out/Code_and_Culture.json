[{"blogurl": "http://codeandculture.wordpress.com\n", "blogroll": [], "title": "Code and Culture"}, {"content": ["It\u2019s election season and that means I have ample opportunities to be annoyed by people misunderstanding how sampling error works. Let\u2019s put aside the popular canard that n/N is a meaningful ratio (a complaint found in innumerable letters to the editor furious that a sample of 1500 is being used to draw inferences about a population of 300 million). Let\u2019s also put aside questions that are about validity rather than sampling error (Bradley effect, cell phones vs landlines, likely voter screens, etc) as in principle these are valid issues even if they are sometimes the objects of motivated reasoning and/or bizarre conspiracy theories as with the whole \u201cunskew\u201d trope. \n What I have in mind is misunderstanding about \u201cmargin of error\u201d that treats all points within the confidence interval as equally likely, as if the central limit theorem implied a bounded uniform distribution instead of a t distribution. For instance, let\u2019s imagine if a poll showed the president up by 2 points in a poll with a 4 point margin of error and the Romney campaign said \u201cwe\u2019re not worried as that\u2019s within the poll\u2019s margin of error.\u201d A Google search for the phrase \u201cwithin the margin of error\u201d gives me 863,000 hits, 14,500 of which are from the last month. Well, sure, but the smart money would still be on the president. Indeed, we can quantify by exactly how much. \n Anyway, I\u2019m going too fast for those of you in the back of the class. Let\u2019s back up to the beginning. For starters, the term \u201cmargin of error\u201d is just a heuristic for explaining to people who don\u2019t really understand statistics that you have to take the point estimate (i.e., the headline figure) with a grain of salt. In real statistics we generally speak of \u201cstandard error\u201d and margin of error is just double the standard error. It\u2019s doubled because that gives you enough wiggle room that the correct answer will be in that range 95% of the time and by convention statistics usually sets 5% as the acceptable rate of error from statistical inference. (This is also what we use in most scientific journals). So if you want to interpret poll results like a pro, the first thing you do is cut the margin of error in half and that\u2019s your standard error. \n The way you interpret standard error is by realizing that sampling error follows a t distribution, which with the exception of very small datasets is the same thing as a normal distribution (i.e., \u201cthe bell curve\u201d). (Thanks to the Central Limit Theorem it doesn\u2019t matter if the underlying thing you\u2019re measuring follows a normal distribution or not, an infinite number of repeated estimates of its mean will still follow a normal distribution.). Standard error is the standard deviation (or \u201csigma\u201d) of this bell curve of repeated estimates. Your point estimate is the center of the curve and you measure different alternate possibilities by their difference from the point estimate divided by the standard error. The thing that talk about \u201cmargin of error\u201d misses is that possibilities that are close to the point estimate are much more likely than possibilities that are at the edge of the margin. In a normal distribution, 68% of the density is within one standard deviation of the mean, 95% within two sigmas, and 99.5% within three sigmas. As you may have noticed, 68% is a lot bigger than 27% (i.e., 95%-68%). So if a poll says 52% of people favor the president and the margin of error is 4 points, there\u2019s a 68% chance that the actual number favoring the president is between 50% and 54% and a 95% chance that the number favoring the president is between 48% and 56%. \n You may have noticed that in a country where elections are decided by majorities, the change from 54% to 56% is much less interesting than the shift from 50% to 48%. Indeed, the most meaningful way to interpret a poll is probably to ask what are the chances that a candidate\u2019s actual level of support is lower than 50%. This is a special case of a \u201cone-tailed test,\u201d which means we don\u2019t care about plus and minus, but only plus or minus. In this case since our point estimate is above the interesting threshold, we only care about the minus or left tail. Take our point estimate of 52% and subtract 50% for majority and our estimate is 2 percentage points above a majority. This equals our standard error, so the one-tailed test is at one standard deviation out. If you remember that normal is symmetrical, you know how many tails you want, and you\u2019ve memorized the 68/95/99.5 densities for a standard normal distribution, then you can calculate it in your head. If not (or if you\u2019re not dealing with integer sigmas) you can use the NORM.S.DIST() function in Excel or the normal() function in Stata. With our example of a point estimate of 52% and a margin of error of 4 points, you find there\u2019s an 84% chance that the true answer is a bare majority or higher. This is technically within the \u201cmargin of error,\u201d but it\u2019s also 5 to 1 odds, which would be great odds to have if you were playing blackjack. Bottom line, there\u2019s nothing magical about being just inside versus just outside the margin of error. If you\u2019re down, you\u2019re down."], "link": "http://codeandculture.wordpress.com/2012/10/01/within-the-margin-of-error/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n A few months ago Stanford\u2019s sociology department was nice enough to invite me up to give a talk on chapter four of Climbing the Charts . This chapter argues that the opinion leadership hypothesis cannot be supported in radio and in the talk I show a simulation of why we should be skeptical of this hypothesis in general. There\u2019s no video, but here\u2019s an enhanced audio file with slideshow . Also a separate PDF of the slides in case you have problems with the integrated version.\u00a0(A caveat, I knew I was speaking to a technically sophisticated audience so I let the jargon flow freely, the chapter itself is much easier to follow for people without a networks background). \n Also in shameless plugging news, Fabio\u2019s review at OrgTheory ."], "link": "http://codeandculture.wordpress.com/2012/09/19/climbing-the-charts-ch-4/", "bloglinks": {}, "links": {"https://dl.dropbox.com/": 2, "http://www.amazon.com/": 1, "http://feeds.wordpress.com/": 1, "http://orgtheory.wordpress.com/": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n [Update 1: From skimming Al Jazeera's (conveniently date-stamped) blog on this issue for Saturday and Sunday it looks like the protests have slowed considerably, which would imply an s-curve.] \n [Update 2: It looks like the prime mover political entrepreneur was the shock artist, who actively tried to get a reaction out of people . That is, this is more similar to Jones threatening to burn Korans than to the Danish imans going on tour with (forged versions of) the Danish cartoons. Of course as in purely domestic culture wars issues, there can be a strange symbiosis between partisans on both sides who disagree on the merits but mutually benefit from discord.] \n I took the Atlantic Wire\u2019s map (also see the KML file ) of \u201cInnocence of Muslims\u201d protest, did my best to add dates, and graphed it as a (cumulative) diffusion curve. Pretty much as you\u2019d expect, it shows exponential growth indicating a process of imitation. Note that the curve rises a bit above trend on Friday, but on the other hand it\u2019s not entirely a Friday thing since you do see growth on Wednesday and Thursday too. I\u2019m gonna split the difference and say it\u2019s about half garden variety imitation and half the fact that Friday is the Islamic Sabbath. \n Let\u2019s hope the curve starts bumping up against the asymptote soon and goes from exponential to s-curve. On a more pessimistic note, even after this particular issue burns out, the tactic itself of drumming up outrage against an obscure blasphemy will be imitated at some point in the future by some political entrepreneur, just as this itself was almost certainly inspired by earlier similar efforts by other policy entrepreneurs . That is, there is a logic of imitation at both micro and macro, for protests within each scandal and for scandals imitating each other. \n  \n A caveat, I did my best to get the dates right but it often isn\u2019t clear, even in the original news story linked in the KML file. Also, thanks to Neal Caren and Matt Frost for pointing to and showing me how to download the file."], "link": "http://codeandculture.wordpress.com/2012/09/15/the-diffusion-of-innocence/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.theatlanticwire.com/": 1, "http://m.mcclatchydc.com/": 1, "http://codeandculture.wordpress.com/": 1, "http://en.wikipedia.org/": 1, "https://maps.google.com/": 1, "http://blogs.aljazeera.com/": 1}, "blogtitle": "Code and Culture"}, {"content": ["The following is a guest post from\u00a0 Trey Causey , a long-time reader of codeandculture and a grad student at Washington who does a lot of work with web scraping. This is his second guest post here, his first was on natural language processing SNAFUs . \n | Trey | \n \u201cWe all know that survey methods are becoming increasingly outdated and clunky.\u201d While at the ASA Annual Meeting in Denver, I attended a methods panel where a graduate student opened with this remark; I\u2019m paraphrasing here (but not by much). This remark was met with sympathetic laughter, mock(?) indignation, and some groans. Subsequent presenters, whose papers were mostly based on survey research, duly referred back to this comment. Survey research has been the workhorse of social scientists for generations and large portions of our methods training are usually survey-oriented in one way or another. \n While the phrasing may have been indelicate, the graduate student is on to something important. I later spoke to a faculty member from the same department as the aforementioned graduate student. We were discussing the rise of \u201cbig data\u201d and non-survey-based methods in the social sciences and he commented that he felt that the discipline was inevitably headed towards these methods, as survey response rates continue to fall. \n I was reminded of both of these anecdotes earlier this week when Claude Fischer blogged about the crisis of survey response rates. He writes that Pew is averaging an astonishing 9% completion rate these days. He closes by speculating on what we can use instead of surveys to measure public opinion: \u201c\u2026letters to the editor, Facebook \u201clikes,\u201d calls to congressional offices, tweet vocabulary, street demonstrations \u2014 or the most common way we do it, assuming that what we and our friends think is typical. We might track Americans\u2019 health by admissions to the hospitals, death rates, consumption of Lipitor. We could estimate changes in poverty by counting beggars on the street, malnourished kids at school. We could try to figure out the \u201cdark\u201d crime number by\u2026 I don\u2019t know.\u201d \n Fischer is right to be pessimistic about the use of surveys. It\u2019s not clear that the dwindling numbers of participants in many surveys are representative of the larger population (for many reasons). I think the writing is on the wall for all but the most well-funded and well-staffed survey organizations to provide large, representative samples of the American public. \n However, I\u2019m optimistic that new forms of data and methods are waiting to pick up the slack. As readers of this blog are no doubt aware, I am a proponent of using data scraped from the web; this could include Facebook likes or Twitter vocabulary, as Fischer points out, or newspaper articles, message board posts, or music sales data. Although debates inevitably (and rightly) arise when studies using these kinds of data are discussed amongst social scientists, the representativeness argument wielded (mostly) by survey researchers is overblown and perhaps deserves to be redirected back at survey research. \n Are data scraped from the web representative of a larger population? That depends on which population we\u2019re talking about. Questions that take the form of \u201cdoes subgroup A differ significantly on average from subgroup B or population P on some attitudinal measure Y\u201d usually require some kind of representative sample of a general population. However, many questions don\u2019t take this form and social scientists are often interested in studying specific subpopulations, often underrepresented in probability samples. \n Data scraped from the web have some real benefits. They are often naturalistic and intentional \u2014 individuals decided to write something of their own accord, unprompted by any researcher, posted it online for others to read, hoping to communicate something. They are generated in real-time. Reading message board threads like this one from Metafilter on September 11, 2001 gives us real-time insight into how people were making sense of an unfolding tragedy. No retrospective bias, no need to get a survey team on the ground a day or two after things happen. We can combine what individuals say with behavioral data. Scraping is one of the more unobtrusive forms of data collection \u2014 the act of observation by the researcher does not alter the individual\u2019s behavior. It\u2019s cheap and fast\u2013even by sociologists\u2019 standards. \n Obviously, as people become more savvy about what companies and governments track about them from their online activities, this will probably change. However, it\u2019s still more naturalistic than answers to closed-form survey questions over the phone. Rather than trying to figure out if survey responses represent a stable attitude or if they are epiphenomenal, the individual generates the data independently. While presentation management will always be an underlying concern when trying to figure out what people \u201creally\u201d mean or \u201creally\u201d want, it is not obvious to me that this problem was ever solved in survey research. Amazingly, people offer up copious information about sensitive topics\u2013their sex lives, their drug habits, and their personal prejudices\u2013without being asked and without the questions being cleared by an IRB (although analyzing this information will still require IRB approval). Combined with new methods to analyze unstructured text, advances in network analysis, and increases in computing power, the possibilities are really quite amazing. \n It\u2019s also important to note that data scraped from the web are not limited to the activities of computer users themselves \u2014 governments store loads of information online, media outlets often have full transcripts online, corporations publish earnings reports, and universities post course schedules and enrollment figures. They aren\u2019t posted on the ICPSR as \u201cdatasets\u201d, but they\u2019re easily obtainable. \n Arguing that data from the internet are somehow less real, less valid, or less reliable does nothing to further the cause of social science. We all know about the Literary Digest moments for survey research. But we didn\u2019t give up on surveys right away, we figured out how to make them better. Surveys allowed us to peer into a section of the public\u2019s mind and figure out what people were thinking on an unprecedented scale. New forms of data and new ways to analyze those data offer some of the same promise. Note that I\u2019m not a \u201cbig data\u201d polyanna; bigger data don\u2019t spell the end of science . But updating our methods of inference to deal with big data and new kinds of data will allow us to study social interaction on a new scale yet again. \n Is Twitter representative of the overall American or world population? Of course not. Will it ever be? Who knows. The key is to figure out the sources of bias and correct for them \u2014 the return on investment to doing so would most likely exceed that of trying to figure out how to salvage an ailing survey research patient."], "link": "http://codeandculture.wordpress.com/2012/09/12/after-surveys/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.metafilter.com/": 1, "http://www.wired.com/": 1, "http://students.washington.edu/": 1, "http://codeandculture.wordpress.com/": 1, "http://madeinamericathebook.wordpress.com/": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n All metrics and models are assumption-laden, but some are more assumption-laden than others. Among the worst offenders are smoothers, which as the name implies, assume that the underlying reality is smooth. If the underlying reality has discontinuity then the smoother will obfuscate this in the course of trying to smooth out \u201cnoise.\u201d This can actually have big theoretical implications. Most notably, there was always a lot of zig-zag in the fossil record but traditionally people assumed it was just noise and so they smoothed it out. Then Gould came up with the theory of punctuated equilibrium and said that evolution substantively works through bursts, which at a data level is equivalent to saying that the zig zags are signal, not noise. \n Here\u2019s an illustration. Let\u2019s simulate a dataset that, by assumption, follows a step function. To keep it simple we\u2019ll have no noise at all, just the underlying step function. Now, let\u2019s apply a LOWESS smooth on the time-series. As you can see, the smoothed trend is basically an s-curve even though we know by assumption that the underlying structure of causation is a step. \n set obs 50\ngen t=[_n]\ngen x=0 in 1/30\nreplace x=1 in 31/50\ntwoway (lowess x t) (scatter x t, msymbol(circle_hollow)), legend(off) \n  \n Moral of the story, think carefully about whether the smoother is theoretically appropriate. If there are substantive reasons to expect discontinuities then it probably ain\u2019t. For a similar reason you may want to not just assume a linear effect or even a polynomial specification in regression but compare various transformations (e.g., linear splines vs quadratics) and see what fits best."], "link": "http://codeandculture.wordpress.com/2012/09/06/caveat-smoother/", "bloglinks": {}, "links": {"http://codeandculture.wordpress.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n Yglesias has a good post on Samsung but he kind of buried the lede with this passage: \n A second reason [that supporting incumbents through patents is problematic] is that we may not be able to recombine ideas. What if pinch-to-zoom (iPhone) is a good idea, but so are Android-style widgets that display live information? Well if nobody can copy each other\u2019s ideas, then nobody will ever be able to buy a phone that has them both. \n That was interesting to me because it\u2019s exactly the same argument that biologists give as to why sexual reproduction is important at the macro level (the explanations at the micro level are different and mostly have to do with avoiding a monoculture for parasites). This is literally the textbook explanation, as seen in this passage from Futuyama\u2019s Evolution 2nd ed \u201cRecombination breaks down linkage disequilibrium, so that combinations of deleterious alleles on the one hand, and of advantageous alleles on the other, arise and thus increase the variance in fitness, so that selection can increase fitness more effectively\u201d (p. 391). In plain English, Futuyama is saying that with asexual reproduction advantageous mutations have to occur in the same lineage whereas with sexual reproduction they can occur in different lineages then combine through sex. Over the course of a few generations this radically increases the odds of getting more fit organisms. Another way to think of it is to imagine how much easier it would be to build a winning poker hand if you could pair up with another player, xerox their cards, and build several hands out of the copies. It is noteworthy that asexual lineages tend to be very recent lineages descended from sexual lineages which in turn suggests that over the long-run asexual lineages tend to go extinct at much higher rates than sexual lineages. \n Patents are actually even worse than asexual reproduction since unlike US patent law, nature allows for the (unlikely) possibility of \u201cindependent invention\u201d in two distinct asexual lineages. Let\u2019s put that aside though and assume that patents are no worse for technical innovation than parthenogenesis is for biological innovation \u2014 it\u2019s still pretty bad. That nature is dominated by organisms that recombined advantageous traits from independent lineages and that parthenogenetic lineages are so prone to extinction is at least suggestive that we want an intellectual property regime that facilitates borrowing through relatively short patent/copyright terms, narrow patents, and a more robust (and low transaction cost) fair use doctrine to facilitate \u201cderived works.\u201d As is though, we are letting our IP regime drift its way towards the amoeba. \n [Update: My colleague Bill Roy observed that my argument assumes a lack of patent licensing. This is true, but it may or may not matter depending on how efficiently licensing works. It is obviously true that patent licensing does not follow perfect Coasean bargaining but equally obvious that there is some licensing. I tend to be a skeptic of the power of licensing to implement anything like perfect Coasean bargaining for three reasons: lack of indexability , transaction costs to negotiations, and prisoner's dilemma among licensors. However it is worth noting that the recombination issue loses its power to the extent that we assume a relatively efficient licensing regime.]"], "link": "http://codeandculture.wordpress.com/2012/08/28/patents-and-parthenogenesis/", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://papers.ssrn.com/": 1, "http://feeds.wordpress.com/": 1, "http://www.slate.com/blog": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n Last time we talked about Social Security names data and how to query particular names. Today I want to talk about the big picture of the variety of names and the question of whether names are getting more diverse over time. Well, this is basically a question of entropy and so let\u2019s do a time-trend of the gini by cohort. \n  \n To me, the main thing the graph demonstrates is actually how sensitive these things are to measurement to the extent that I\u2019m unwilling to make substantive claims without better understanding the history of how the data were collected. As you can see, there\u2019s a huge jump in the gini starting with birth cohorts dating to WWI. This predates the enactment of Social Security (which I\u2019ve drawn as a black vertical line) by about twenty years and so my best guess it corresponds to cohorts that were aging into the labor force around the time the act passed. Alternately it could be something about WWI accelerating assimilation in naming practices, but when I see a sharp discontinuity like that my instincts tell me it\u2019s a measurement artifact not a real social change. \n Putting that aside, let\u2019s think about the index itself. The Gini coefficient was developed to study social inequality and as such it\u2019s sensitive to both the top and the bottom. Gini is basically a better version of taking the ratio of a high percentile and a low percentile. If you have exactly two people with exactly equal wealth (or exactly two names with equal numbers of babies) then you\u2019d have a very low Gini. \n Two names sounds ridiculous but not as much as you\u2019d think. Consider Republican era Rome. We have a pretty good idea of Roman names, at least in the upper classes, because they kept lists called \u201cfasti consulares\u201d of every man who served as consul. I previously did a post showing how a few clans dominated, but for today I want to just use these lists to show how few first names there were. These lists show only 29 male first names, of which only 17 were popular.* (In contrast, the Social Security data lists thousands of male names in circulation in any given year.) The Gini coefficient for praenomen on Republican fasti consulares is .72, which is not that far below the pre-1910 Social Security data. If you\u2019re wondering, the most popular praenomen were Lucius, Gaius, Marcus, and Quintus. Here\u2019s a kernel density plot for praenomen frequency. \n  \n As you can see, Roman names follow a count but it\u2019s not ridiculously steep like American names in any arbitrary year (like this graph of 1920 ). The fact that 29 names following a fairly shallow count could show a comparable gini to thousands of names following an extremely steep count suggests to me that there is something unsatisfying about the metric for our purposes. \n Another entropy index we can use is the Herfindahl Hirschman Index (HHI). HHI is meant to measure the potential for monopolies and cartels and as such it\u2019s only really sensitive to the top. HHI is basically a better version of taking the share held by the top-4 (or top-8 or top- k ) actors in the system. If you have exactly two people with exactly equal wealth (or exactly two names with equal numbers of babies) then you\u2019d have a very high HHI. \n A thought experiment that reveals the difference between Gini and HHI is that if the United States were to suddenly add a few million desperately poor people, for instance by annexing Haiti, this wouldn\u2019t change our income HHI at all but it would drive our income Gini up appreciably. Nonetheless, under a wide range of circumstances the Gini and HHI will be correlated as both measure inequality, they just have different emphases. \n In the case of names, HHI will capture the dominance of stock names like \u201cJake\u201d and \u201cMary\u201d whereas Gini is better at capturing how common weird names are. So that said, let\u2019s do the time trend again, but this time with HHI. \n  \n It\u2019s very interesting that we now don\u2019t see a precipitous change in the late teens but rather a gradual shift leading up to that time. For comparison, the HHI of consular praenomen is 1152, which is off the charts compared to the Social Security data. Finally, let\u2019s note that HHI and Gini agree that girls names show more entropy than boy\u2019s names. \n \u2014\u2014\u2014- \n* I say male names because there were no female consuls. Roman women took the feminized form of their father\u2019s clan name. Hence, most of the women of the Julio-Claudian dynasty were (by adoption) descendants of Gaius Julius Caesar and were named \u201cJulia,\u201d which is the feminine form of \u201cJulius.\u201d"], "link": "http://codeandculture.wordpress.com/2012/08/23/now-these-are-the-names-pt-2/", "bloglinks": {}, "links": {"http://codeandculture.wordpress.com/": 4, "http://feeds.wordpress.com/": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n There\u2019s a lot of great research on names and I\u2019ve been a big fan of it for years, although it\u2019s hard to commensurate with my own favorite diffusion models since names are a flow whereas the stuff I\u2019m interested in generally concern diffusion across a finite population. \n Anyway, I was inspired to play with this data by two things in conversation. The one I\u2019ll discuss today is somebody repeated a story about a girl named \u201cLah-d,\u201d which is pronounced \u201cLa dash da\u201d since \u201cthe dash is not silent.\u201d \n This appears to be a slight variation on an existing apocryphal story , but it reflects three real social facts that are well documented in the name literature. First, black girls have the most eclectic names of any demographic group, with a high premium put on on creativity and about 30% having unique names . Second, even when their names are unique coinages they still follow systematic rules , as with the characteristic prefix \u201cLa\u201d and consonant pair \u201csh.\u201d Third, these distinctly black names are an object of bewildered mockery (and a basis for exclusion ) by others, which is the appeal in retelling this and other urban legends on the same theme.* \n To tell if there was any evidence for this story I checked the Social Security data , but the web searchable interface only includes the top 1000 names per year. Thus checking on very rare names requires downloading the raw text files . There\u2019s one file per year, but you can efficiently search all of them from the command line by going to the directory where you unzipped the archive and grepping. \n cd ~/Downloads/names\ngrep '^Lah-d' *.txt\ngrep '^Lahd' *.txt \n As you can see, this name does not appear anywhere in the data. Case closed? Well, there\u2019s a slight caveat in that for privacy reasons the data only include names that occur at least five times in a given birth year. So while it includes rare names, it misses extremely rare names. For instance, you also get a big fat nothing if you do this search: \n grep '^Reihan' *.txt \n This despite the fact that I personally know an American named Reihan. (Actually I\u2019ve never asked him to show me a photo ID so I should remain open to the possibility that \u201cReihan Salam\u201d is just a memorable nom de plume and his birth certificate really says \u201cJason Miller\u201d or \u201cBrian Davis\u201d). \n For names that do meet the minimal threshold though you can use grep as the basis for a quick and dirty time series. To automate this I wrote a little Stata script to do this called grepnames. To call it, you give it two arguments, the (case-sensitive) name you\u2019re looking for and the directory where you put the name files. It gives you back a time-series for how many births had that name. \n capture program drop grepnames\nprogram define grepnames\n\tlocal name \"`1'\"\n\tlocal directory \"`2'\"\n\n\ttempfile namequery\n\tshell grep -r '^`name'' \"`directory'\" > `namequery'\n\n\tinsheet using `namequery', clear\n\tgen year=real(regexs(1)) if regexm(v1,\"`directory'yob([12][0-9][0-9][0-9])\\.txt\")\n\tgen name=regexs(1) if regexm(v1,\"`directory'yob[12][0-9][0-9][0-9]\\.txt:(.+)\")\n\tkeep if name==\"`name'\"\n\tren v3 frequency\n\tren v2 sex\n\tfillin sex year\n\trecode frequency .=0\n\tsort year sex\n\ttwoway (line frequency year if sex==\"M\") (line frequency year if sex==\"F\"), legend(order(1 \"Male\" 2 \"Female\")) title(`\"Time Series for \"`name'\" by Birth Cohort\"')\nend \n For instance: \n grepnames Gabriel \"/Users/rossman/Documents/codeandculture/names/\" \n  \n Note that these numbers are not scaled for the size of the cohorts, either in reality or as observed by the Social Security administration. (Their data is noticeably worse for cohorts prior to about 1920). Still, it\u2019s pretty obvious that my first name has grown more popular over time. \n We can also replicate a classic example from Lieberson of a name that became less popular over time, for rather obvious reasons. \n grepnames Adolph \"/Users/rossman/Documents/codeandculture/names/\" \n  \n Next time, how diverse are names over time with thoughts on entropy indices. \n (Also see\u00a0 Jay\u2019s thoughts \u00a0on names, as well as taking inspiration from my book to apply Bass models to film box office). \n * Yes, I know that one of those stories is true but the interesting thing is that people like to retell it (and do so with mocking commentary), not that the underlying incident is true. It is also true that yesterday I had eggs and coffee for breakfast, but nobody is likely to forward an e-mail to their friends repeating that particular banal but accurate nugget."], "link": "http://codeandculture.wordpress.com/2012/08/10/now-these-are-the-names-pt-1/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.ssa.gov/": 2, "http://www.snopes.com/": 4, "http://economics.harvard.edu/": 1, "http://codeandculture.wordpress.com/": 2, "http://montclairsoci.blogspot.com/": 1, "http://www.amazon.com/": 1, "http://karlan.yale.edu/": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n Kal Penn gave a really good commencement address to my department a few weeks ago and it\u2019s now available on YouTube. I loved the speech (I\u2019m the guy directly behind him in black and orange who is frequently laughing) and generally found him to be a very nice and thoughtful guy."], "link": "http://codeandculture.wordpress.com/2012/07/10/kal-penns-commencement-address-for-ucla-sociology/", "bloglinks": {}, "links": {"http://codeandculture.wordpress.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Code and Culture"}, {"content": ["| Gabriel | \n As someone who mostly works with text files with a lot of strings, I often run into trouble with the Stata insheet command being extremely finicky about how it takes data. Frequently it ends up throwing out half the rows because at some point in the file there\u2019s a stray character and Stata not only throws out that row but everything thereafter. In my recent work on IMDb I\u2019ve gotten into the habit of first reading text files into Excel, then having Stata read the xlsx files. This is tolerable if you\u2019re dealing with a relatively small number of files that you\u2019re only importing once, but it won\u2019t scale to repeated imports or a large number of files. \n More recently , I\u2019ve been dealing with data collected with the R library twitteR. Since tweets sometimes contain literal quotes, twitteR escapes them with a backslash. However Stata does not recognize this convention and it chokes on this when the quote characters are unbalanced. I realized this the other night when I was trying to test for left-censorship and fixed it using the batch find/replace in TextWrangler. Of course this is not scriptable and so I was contemplating taking the plunge into Perl when the Modeled Behavior hive mind suggested the Stata command filefilter. Using this command I can replace the escaped literal quotes (which chokes Stata\u2019s insheet) with literal apostrophes (which Stata\u2019s insheet can handle). \n filefilter foo.txt foo2.txt, from(\\BS\\Q) to(\\RQ) \n Problem solved, natively in Stata, and I have about 25% more observations. Thanks guys."], "link": "http://codeandculture.wordpress.com/2012/06/08/filefilter/", "bloglinks": {}, "links": {"http://codeandculture.wordpress.com/": 1, "http://feeds.wordpress.com/": 1, "feed://blogs.forbes.com/modeledbehavior/feed/": 1}, "blogtitle": "Code and Culture"}]