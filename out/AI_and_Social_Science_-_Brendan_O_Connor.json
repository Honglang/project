[{"blogurl": "http://brenocon.com/blog\n", "blogroll": [], "title": "AI and Social Science - Brendan O'Connor"}, {"content": ["There\u2019s a lot to say about Powerset , the short-lived natural language search company (2005-2008) where I worked after college. AI overhype, flying too close to the sun, the psychology of tech journalism and venture capitalism, etc. A year or two ago I wrote the following bit about Powerset\u2019s technology in response to a question on Quora . I\u2019m posting a revised version here. \n Question: What was Powerset\u2019s core innovation in search? As far as I can tell, they licensed an NLP engine. They did not have a question answering system or any system for information extraction. How was Powerset\u2019s search engine different than Google\u2019s? \n My answer: Powerset built a system vaguely like a question-answering system on top of Xerox PARC\u2019s NLP engine. The output is better described as query-focused summarization rather than question answering; primarily, it matched semantic fragments of the user query against indexed semantic relations, with lots of keyword/ngram-matching fallback for when that didn\u2019t work, and tried to highlight matching answers in the result snippets. \n The Powerset system indexed semantic relations and entities (the latter often being wordnet/freebase nodes), did a similar analysis on the user query, then formed a database query against that index of semantic relations, synonym/hypernym expansions, and other textual information (e.g. word positions or gender identification). Then with all the rich (complicated) index information, you have neat features for ranking and snippet generation (i.e. query-focused summarization), but it\u2019s so complicated it\u2019s easy to screw up. (And don\u2019t get me started on trying to run a segfault-prone Tcl/Prolog/C parser under an unstable 2006-era Hadoop\u2026) \n Here is a diagram I wrote in July 2007 to try to communicate internally what the entire system was doing. As you might imagine, it was difficult to keep everyone on the same page. This diagram only depicts the indexing pipeline; the query-time system would have required another diagram. NLP folks will note some rather surprising technology choices in some places. (Unweighted FST for NER? Yes. In fairness, it was eventually replaced by a statistical tagger. But the company did have >$12 million in funding at this point.) \n  \n As to whether this was \u201cdifferent than Google,\u201d sure, I suppose. Certainly no serious search engine was crazy enough to do constituent parses (and unification parses, lexical lookups, coreference, etc.) of all sentences at index time \u2014 raising indexing costs, compared to keyword indexing, by perhaps 100x \u2014 but Powerset sure did. \n It\u2019s worth noting that since then, Google has added much more question-answering and structured information search, presumably using related but different techniques than Powerset used. (And Google even had some simple question-answering back then, as I recall; and, these days it\u2019s said they parse the web all the time, at least for experimental purposes. They now have excellent groups of highly-regarded specialists in parsing, unsupervised lexical semantics, machine translation, etc., which Powerset never did.) And IBM\u2019s Watson project more recently managed to produce a nice factoid question-answering system. In principle, deep semantic analysis of web text could be useful for search (and shallow NLP, like morphology and chunking, perhaps more so); but as the primary thing for a search startup to focus on, it seemed a little extreme. \n As to what the \u201ccore innovation\u201d was, that\u2019s a loaded question. Was all this stuff useful? Usually I am cynical and say Powerset had no serious innovation for search. But that is just an opinion. Powerset developed some other things that were more user-visible, including a browser of the extracted semantic relations (\u201cFactz\u201d or \u201cPowermouse\u201d), a mostly separate freebase-specific query system (somewhat similar to Google\u2019s recently released Knowledge Graph results), and completely separately, an open-source BigTable clone for index-time infrastructure (HBase, which has been developed quite a bit since then). In general, I found that design/UI engineering people respected Powerset for the frontends, scalability engineers respected Powerset for the HBase contributions, but NLP and IR experts were highly cynical about Powerset\u2019s technology claims. If you get a chance, try asking researchers who were at ACL 2007 in Prague about Barney Pell\u2019s keynote; I am told a number walked out while it was underway. \n For good commentary on the situation at the time, see these Fernando Pereira blog posts from 2007: Powerset in PARC Deal , and Powerset in the NYT . \n After the acquisition, Microsoft filed patent applications for all the Powerset-specific proprietary tech. You can read all of them on the USPTO website or wherever; for example, this page seems to list them. \n \n Quora stuff: 21 votes by Ian Wong, Joseph Reisinger, William Morgan, Marc Bodnick, Cameron Ellis, Kartik Ayyar, Can Duruk, Brandon Smietana, Ronen Amit, Amit Chaudhary, Dare Obasanjo, Joseph Quattrocchi, Siqi Chen, Tim Converse, Zoltan Varju, Sundeep Yedida, Elliot Turner, Nenshad Bardoliwalla, Mike Mintz, Abhimanyu Gupta, and Nick Kaluzhenkoff"], "link": "http://brenocon.com/blog/2012/10/powersets-natural-language-search-system/", "bloglinks": {}, "links": {"http://brenocon.com/": 2, "http://www.faqs.org/": 1, "http://en.wikipedia.org/": 1, "http://www.quora.com/": 1, "http://earningmyturns.blogspot.com/": 2}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["We\u2019re pleased to announce a new release of the CMU ARK Twitter Part-of-Speech \nTagger, version 0.3. \n \n The new version is much faster (40x) and more accurate (89.2 -> 92.8) than \n before. \n We also have released new POS-annotated data, including a dataset of one \n tweet for each of 547 days. \n We have made available large-scale word clusters from unlabeled Twitter data \n (217k words, 56m tweets, 847m tokens).\n \n Tools, data, and a new technical report describing the release are available at: \n www.ark.cs.cmu.edu/TweetNLP . \n 0100100 a 1111100101110 111100000011 , Brendan"], "link": "http://brenocon.com/blog/2012/09/cmu-ark-twitter-part-of-speech-tagger-v0-3-released/", "bloglinks": {}, "links": {"http://www.cmu.edu/": 4}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["It is worth contemplating how grand the General Social Survey is. When playing around with the Statwing YC demo (which is very cool!) I was reminded of the very old-school SDA web tool for exploratory cross-tabulation analyses\u2026 They have the GSS loaded and running here . The GSS is so large you can analyze really weird combinations of variables. For example, here is one I just did: How much good versus evil is there in the world (on a 7 point scale, of course!), versus age."], "link": "http://brenocon.com/blog/2012/08/berkeley-sda-and-the-general-social-survey/", "bloglinks": {}, "links": {"http://sda.berkeley.edu/": 2, "http://brenocon.com/blog": 1, "http://blog.statwing.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["Re \n So I just wrote this hierarchical kernelized Boltzmann process in Prolog using ed on my iPhone. I can send you the RCS repository. \n \u2014 ML Hipster (@ML_Hipster) July 19, 2012 \n \n The best I can do is: I once programmed Prolog, wrapped inside Tcl and Ruby, to generate a query against a C++ IR engine, which generated features for a boosted decision tree trained in R, to rank documents indexed with that segfault-prone Tcl/Prolog/C system wrapped in Java/Hadoop. Then I wrote a Javascript/Ruby web interface on top. \n Switching between these languages is too much for one days\u2019 work. Educational, but not the best way to do things."], "link": "http://brenocon.com/blog/2012/07/1739/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www2.parc.com/": 1}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["Update Aug 10: THIS IS NOT A SUMMARY OF THE WHOLE PAPER! it\u2019s whining about one particular method of analysis before talking about other things further down \n \n A quick note on Berg-Kirkpatrick et al EMNLP-2012, \u201cAn Empirical Investigation of Statistical Signi\ufb01cance in NLP\u201d . They make lots of graphs of p-values against observed magnitudes and talk about \u201ccurves\u201d, e.g. \n We see the same curve-shaped trend we saw for summarization and dependency parsing. Different group comparisons, same group comparisons, and system combination comparisons form distinct curves. \n For example, Figure 2. \n  \n I fear they made 10 graphs to rediscover a basic statistical fact: a p-value comes from a null hypothesis CDF. That\u2019s what these \u201ccurve-shaped trends\u201d are in all their graphs. They are CDFs. \n To back up, the statistical significance testing question is whether, in their notation, the observed dataset performance difference \\(\\delta(x)\\) is \u201creal\u201d or not: if you were to resample the data, how much would \\(\\delta(x)\\) vary? One way to think about significance testing in this setting is, it asks if the difference you got has the same sign as the true difference. (not quite the Bayesian posterior of what Gelman et al call a \u201csign error\u201d , but rather a kind of pessimistic worst-case probability of it; this is how classic hypothesis testing works.) Getting the sign right is an awfully minimal requirement of whether your results are important, but seems useful to do. \n Strip away all the complications of bootstrap tests and stupidly overengineered NLP metrics, and consider the simple case where, in their notation, the observed dataset difference \\( \\delta(x) \\) is a simple average of per-unit differences \\(\\delta(one unit)\\); i.e. per-sentence, per-token, per-document, whatever. The standard error is the standard deviation of the dataset difference, a measure of how much it were to vary if the units were resampled, and unit-level differences were i.i.d. This standard error is, according to bog-standard Stat 101 theory, \n \\[ \\sqrt{Var(\\delta(x))} = \\frac{\\sqrt{Var(\\delta(\\text{one unit}))} }{ \\sqrt{n} } \\] \n And you get 5% significance (in a z-test .. is this a \u201cpaired z-test\u201d?) if your observed difference \\(\\delta(x)\\) is more than 1.96 times its standard error. The number 1.96 comes from the normal CDF. In particular, you can directly evaluate your p-value via, in their notation again, \n \\[ pvalue(x) = 1-NormCDF\\left( \n\\delta(x);\\ \\ \nmean=0,\\ sd=\\frac{\\sqrt{Var(\\delta(\\text{one unit}))} }{ \\sqrt{n} } \n\\right) \n\\] \n (I think I wrote a one-sided test there.) The point is you can read off, from the standard error equation, the important determinants of significance: (numerator) the variability of system performance, variability between systems, and difficulty of the task; and (denominator) the size of the dataset. You shouldn\u2019t need to read this paper to know the answer is \u201cno\u201d to the qustion Is it true that for each task there is a gain which roughly implies significance? \n In fact, lots of their graphs look an awful lot like a normal CDF. For example, here is Figure 2 again, where I\u2019ve plotted a normal CDF on top. (Yes, copy and pasted from R.) They\u2019re plotting \\(1-pvalue\\), which is the null hypothesis CDF. Indeed, for the red points they look pretty similar: \n  \n Now, they\u2019re using bootstrap tests, which are great and in principle better than the z-test because they can handle all sorts of nasty, non-analyzable metrics like BLEU, and there are some subtleties with comparing all pairs of systems I admit I didn\u2019t totally follow. (There\u2019s another point that BLEU should have been designed to be mathematically simpler to analyze, instead of all that crap with global penalties, though the stupid overengineering of NLP metrics is a topic for another blog post.) But in nonparametric tests there\u2019s still a null hypothesis CDF \u2014 it\u2019s the ECDF of the statistic over simulation outcomes \u2014 and your pvalue comes from it! I think this is what all those curves are they claim to have discovered. Every dataset has its own null hypothesis CDF; if it was the normal approximation case, those factors go into the numerator and denominator of the standard error. (If you want to read more about the empirical CDF plug-in estimator explanation of these nonparametric tests, see the wonderful chapters 6 to 7 to 10 or so of Larry Wasserman\u2019s All of Statistics .) \n In fact, if their recovered null hypothesis CDFs really are the same as an unpaired z-test\u2019s null hypothesis CDF, that could be telling is that these NLP metrics are truly overengineered: their statistical variability behaves similarly to an arithmetic mean. That\u2019s a not a for-sure conclusion based on the data they show, but perhaps worth thinking about. \n The more interesting thing about Figure 2 are those points in green off the null hypo CDF curve. At first when I saw it I got excited because I thought they were going to claim evidence of self-serving reporting biases, like some people have done for various classes of experimental papers (via p-value histograms across papers and such, e.g. Vul et al\u2019s \u201cVoodoo Correlations\u201d paper.) But actually, they say it\u2019s just an example of paired testing at work: the same research group will produce a series of systems that have very highly correlated outputs; this is a textbook example of where paired tests have greater statistical power than unpaired ones, since they have the ability to only pay attention to pairs where things changed, and not let those differences be washed out by all the pairs that haven\u2019t changed. Well, maybe. Someone should investigate potential \u201cvoodoo performance differences\u201d. Now that would be an exciting NLP methodology paper. \n The last section, basically a demonstration that \u201cstatistical significance != substantive significance\u201d in the context of parsing (where practically you might care about domains other than the late-1980\u2032s Wall Street Journal, which has been presumably over-optimized via 15 years and zillions of PhD theses\u2019 worth of effort), was nice. Since statistical signicance testing is basically the question \u201cDo I have the correct sign of the performance difference?\u201d, the question \u201cIs my performance difference important?\u201d is obviously a whole other thing. You probably need to get the former right before confidently answering the latter, but anyone who runs a significance test and mindlessly declares victory is hiding behind the former to dodge the latter. \n This paper illustrates the point in an odd fashion, looking at the out-of-domain p-value versus in-domain p-value \u2014 why not just directly map between domain effect sizes instead of doing everything roundabout with significance tests, which hide what\u2019s truly going on? For example, just plot Brown performance versus WSJ performance, and throw in standard error bars if you like. (I guess if you\u2019re a true NLP engineer, you always care about paired tests since it makes it easier to see incremental system improvements, thus the general focus on hypothesis tests instead of confidence regions? I like confidence regions and standard errors much more as a thing to be reported, you can trivially read off unpaired tests from them yourself, but you don\u2019t get the increased power of paired testing then.) \n But it\u2019s still a good point. I feel like anyone who\u2019s done real-world NLP is used to the fact that academic NLP research is often horribly insular and narrowly focused, and would not be surprised at all about results that WSJ performance is more weakly correlated with out-of-domain performance than, well, WSJ performance; but maybe it\u2019s worth communicating this to the academic NLP audience. \n p.s. sorry to harp on this but the unfortunately failed-to-be-cited Clark et al. ACL 2011 is really relevant. In that paper they actually break out different sources of variability for the specific problem of variability due to optimizer instability."], "link": "http://brenocon.com/blog/2012/07/p-values-cdfs-nlp-etc/", "bloglinks": {}, "links": {"http://www.edvul.com/": 1, "http://www.cmu.edu/": 2, "http://brenocon.com/blog": 2, "http://www.columbia.edu/": 1, "http://en.wikipedia.org/": 2, "http://www.berkeley.edu/": 1}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["There was an interesting ICML paper this year about very large-scale training of deep belief networks (a.k.a. neural networks) for unsupervised concept extraction from images. They ( Quoc V. Le and colleagues at Google/Stanford) have a cute example of learning very high-level features that are evoked by images of cats (from YouTube still-image training data); one is shown below. \n For those of us who work on machine learning and text, the question always comes up, why not DBN\u2019s for language? Many shallow latent-space text models have been quite successful (LSI, LDA, HMM, LPCFG\u2026); there is hope that some sort of \u201cdeeper\u201d concepts could be learned. I think this is one of the most interesting areas for unsupervised language modeling right now. \n But note it\u2019s a bad idea to directly analogize results from image analysis to language analysis. The problems have radically different levels of conceptual abstraction baked-in. Consider the problem of detecting the concept of a cat; i.e. those animals that meow, can be our pets, etc. I hereby propose a system that can detect this concept in text, and compare it to the image analysis DBN as follows. \n \n \n Problem \n Concept representation \n Concept detector \n Cost to create concept detector \n \n \nImage analysis \n \n \n \n \n \n1,152,000 CPU-hours to train neural network \n $61,056 at current GCE prices \n \n \nLanguage analysis \n \n \n cat \n a.k.a. \n1100011 \n1100001 \n1110100 \n \n \"cat\" in\nre.split('[^a-z]', text.lower()) \n \n147 CPU-microseconds to compile finite-state network /[^a-z]/ \n $0.000078 at GCE prices \n \n I mean: you can identify the concept \u201ccat\u201d by tokenizing a text, i.e. breaking it up into words, and looking for the word \u201ccat\u201d. To identify the \u201ccat\u201d concept from a vector of pixel intensities, you have to run through a cascade of filters, edge detectors, shape detectors and more. This paper creates the image analyzer with tons of unsupervised learning; in other approaches you still have to train all the components in your cascade. [Note.] \n In text, the concept of \u201ccat\u201d is immediately available on the surface \u2014 there\u2019s a whole word for it. Think of all the different shapes and types of cats which you could call a \u201ccat\u201d and be successfully understood. Words are already a massive dimension reduction of the space of human experiences. Pixel intensity vectors are not, and it\u2019s a lot of work to reduce that dimensionality. Our vision systems are computational devices that do this dimension reduction, and they took many millions of years of evolution to construct. \n In comparison, the point of language is communication, so it\u2019s designed, at least a little bit, to be comprehensible \u2014 pixel intensity vectors do not seem to be have such a design goal. [\"Designed.\"] The fact that it\u2019s easy to write a rule-based word extractor with /[^a-zA-Z0-9]/ doesn\u2019t mean bag-of-words or n-grams are \u201clow-level\u201d; it just means that concept extraction is easy with text. In particular, English has whitespace conventions and simple enough morphology that you can write a tokenizer by hand, and we\u2019ve designed character encoding standards let computers unambiguously map between word forms and binary representations. \n Unsupervised cross-lingual phonetic and morphological learning is closer, cognitive-level-of-abstraction-wise, to what the deep belief networks people are trying to do with images. To make a fairer table above, you might want to compare to the training time of an unsupervised word segmenter / cross-lingual lexicon learner. \n [Another aside: The topic modeling community, in particular, seems to often mistakenly presume you need dimension reduction to do anything with text. Every time you run a topic model you're building off of your rule-based concept extractor -- your tokenizer -- which might very well be doing all the important work. Don't forget you can sometimes get great results with just the words (and phrases!), for both predictive and exploratory tasks . Getting topics can also be great, but it would be nice to have a better understanding exactly when or how they're useful.] \n This isn\u2019t to say that lexicalized models (be they document or sequence-level) aren\u2019t overly simple or crude. Just within lexical semantics, it\u2019s easy to come up with examples of concepts that \u201ccat\u201d might refer to, but you want other words as well. You could have synonyms {cat, feline} , or refinements {cat, housecat, tabby} or generalizations {cat, mammal, animal} or things that seem related somehow but get tricky the more you think about it {cat, tiger, lion} . Or maybe the word \u201ccat\u201d is a part of a broad topical constellation of words {cat, pets, yard, home} or with an affective aspect twist {cat, predator, cruel, hunter} or maybe a pretty specific narrative frame {cat, tree, rescue, fireman} . (I love how ridiculous this last example is, but we all instantly recognize the scenario it evokes. Is this an America-specific cultural thing?) \n If you want to represent and differentiate between the concepts evoked by these wordsets, then yes, the bare symbol \u201ccat\u201d is too narrow (or too broad), and maybe we want something \u201cdeeper\u201d. So what does \u201cdeep learning\u201d mean? There\u2019s a mathematical definition in the largeness of the class of functions these models can learn; but practically when you\u2019re running these things, you need a criterion for how good of concepts you\u2019re learning, which I think the rhetoric of \u201cdeep learning\u201d is implicitly appealing to. \n In the images case, \u201cdeep\u201d seems to mean \u201crecognizable concepts that look cool\u201d. (There\u2019s room to be cynical about this, but I think it\u2019s fine when you\u2019re comparing to things that are not recognizable.) In the text case, if you let yourself use word-and-ngram extraction, then you\u2019ve already started with recognizable concepts \u2014 where are you going next? (And how do you evaluate?) One interesting answer is, let\u2019s depart lexical semantics and go compositional ; but perhaps there are many possibilities. \n \n \nNote on table: Timing of regex compilation was via IPython %timeit re.purge();re.compile(\u2018[^a-z]\u2018) . Also I\u2019m excluding human costs \u2014 hundreds (thousands?) of hours from 8 CS researcher coauthors (and imagine how much Ng and Dean cost in dollars!), versus whatever skill level it is to write a regex. The former costs are justified given it is, after all, research; the regex works well because someone invented all the regular expression finite-state algorithms we now take for granted. But there are awfully good reasons there was so much finite-state research decades ago: they\u2019re really, really useful for processing symbolic systems created by humans; most obviously artificial programming languages designed that way, but also less strict quasi-languages like telephone number formats, and certain natural language analysis tasks like tokenization and morphology\u2026 \n \u201cDesign:\u201d \u2026where we can define \u201cdesign\u201d and \u201cintention\u201d in a Herbert Simon sort of way to mean \u201cpart of the optimization objective of either cultural or biological evolution\u201d; i.e. aspects of language that don\u2019t have good communicative utility might be go away over time, but the processes that that give us light patterns hitting our retina are quite exogenous, modulo debates about God or anthropic principles etc."], "link": "http://brenocon.com/blog/2012/07/the-60000-cat-deep-belief-networks-make-less-sense-for-language-than-vision/", "bloglinks": {}, "links": {"http://www.socher.org/": 1, "http://ai.stanford.edu/": 1, "http://brenocon.com/": 1, "http://brenocon.com/blog": 2, "http://research.google.com/": 2, "http://docs.oracle.com/": 1, "http://en.wikipedia.org/": 1, "http://cloud.google.com/": 1, "http://www.amazon.com/": 1}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["The Dice similarity is the same as F1-score ; and they are monotonic in Jaccard similarity . I worked this out recently but couldn\u2019t find anything about it online so here\u2019s a writeup. \n Let \\(A\\) be the set of found items, and \\(B\\) the set of wanted items. \\(Prec=|AB|/|A|\\), \\(Rec=|AB|/|B|\\). Their harmonic mean, the \\(F1\\)-measure, is the same as the Dice coefficient: \n\\begin{align*} \nF1(A,B) \n&= \\frac{2}{1/P+ 1/R} \n = \\frac{2}{|A|/|AB| + |B|/|AB|} \\\\ \nDice(A,B) \n&= \\frac{2|AB|}{ |A| + |B| } \\\\ \n&= \\frac{2 |AB|}{ (|AB| + |A \\setminus B|) + (|AB| + |B \\setminus A|)} \\\\ \n&= \\frac{|AB|}{|AB| + \\frac{1}{2}|A \\setminus B| + \\frac{1}{2} |B \\setminus A|} \n\\end{align*} \n It\u2019s nice to characterize the set comparison into the three mutually exclusive partitions \\(AB\\), \\(A \\setminus B\\), and \\(B \\setminus A\\). This illustrates Dice\u2019s close relationship to the Jaccard metric, \n\\begin{align*} \nJacc(A,B) \n&= \\frac{|AB|}{|A \\cup B|} \\\\ \n&= \\frac{|AB|}{|AB| + |A \\setminus B| + |B \\setminus A|} \n\\end{align*} \nAnd in fact \\(J = D/(2-D)\\) and \\(D=2J/(1+J)\\) for any input, so they are monotonic in one another. \nThe Tversky index (1977) generalizes them both, \n\\begin{align*} \nTversky(A,B;\\alpha,\\beta) \n&= \\frac{|AB|}{|AB| + \\alpha|A\\setminus B| + \\beta|B \\setminus A|} \n\\end{align*} \nwhere \\(\\alpha\\) and \\(\\beta\\) control the magnitude of penalties of false positive versus false negative errors. It\u2019s easy to work out that all weighted F-measures correspond to when \\(\\alpha+\\beta=1\\). The Tversky index just gives a spectrum of ways to normalize the size of a two-way set intersection. (I always thought Tversky\u2019s more mathematical earlier work (before the famous T & K heuristics-and-biases stuff) was pretty cool. In the 1977 paper he actually does an axiomatic derivation of set similarity measures, though as far as I can tell this index doesn\u2019t strictly derive from them. Then there\u2019s a whole debate in cognitive psych whether similarity is a good way to characterize reasoning about objects but that\u2019s another story.) \n So you could use either Jaccard or Dice/F1 to measure retrieval/classifier performance, since they\u2019re completely monotonic in one another. Jaccard might be a little unintuitive though, because it\u2019s always less than or equal min(Prec,Rec); Dice/F is always in-between."], "link": "http://brenocon.com/blog/2012/04/f-scores-dice-and-jaccard-set-similarity/", "bloglinks": {}, "links": {"http://homepage.utexas.edu/": 1, "http://en.wikipedia.org/": 6}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["Cosine similarity, Pearson correlations, and OLS coefficients can all be viewed as variants on the inner product \u2014 tweaked in different ways for centering and magnitude (i.e. location and scale, or something like that). \n Details: \n You have two vectors \\(x\\) and \\(y\\) and want to measure similarity between them. A basic similarity function is the inner product \n \\[ Inner(x,y) = \\sum_i x_i y_i = \\langle x, y \\rangle \\] \n If x tends to be high where y is also high, and low where y is low, the inner product will be high \u2014 the vectors are more similar. \n The inner product is unbounded. One way to make it bounded between -1 and 1 is to divide by the vectors\u2019 L2 norms, giving the cosine similarity \n \\[ CosSim(x,y) = \\frac{\\sum_i x_i y_i}{ \\sqrt{ \\sum_i x_i^2} \\sqrt{ \\sum_i y_i^2 } } \n= \\frac{ \\langle x,y \\rangle }{ ||x||\\ ||y|| } \n\\] \n This is actually bounded between 0 and 1 if x and y are non-negative. Cosine similarity has an interpretation as the cosine of the angle between the two vectors; you can illustrate this for vectors in \\(\\mathbb{R}^2\\) (e.g. here ). \n Cosine similarity is not invariant to shifts. If x was shifted to x+1, the cosine similarity would change. What is invariant, though, is the Pearson correlation . Let \\(\\bar{x}\\) and \\(\\bar{y}\\) be the respective means: \n \\begin{align} \nCorr(x,y) &= \\frac{ \\sum_i (x_i-\\bar{x}) (y_i-\\bar{y}) }{ \n\\sqrt{\\sum (x_i-\\bar{x})^2} \\sqrt{ \\sum (y_i-\\bar{y})^2 } } \n\\\\ \n& = \\frac{\\langle x-\\bar{x},\\ y-\\bar{y} \\rangle}{ \n||x-\\bar{x}||\\ ||y-\\bar{y}||} \\\\ \n& = CosSim(x-\\bar{x}, y-\\bar{y}) \n\\end{align} \n Correlation is the cosine similarity between centered versions of x and y, again bounded between -1 and 1. People usually talk about cosine similarity in terms of vector angles, but it can be loosely thought of as a correlation, if you think of the vectors as paired samples. Unlike the cosine, the correlation is invariant to both scale and location changes of x and y. \n This isn\u2019t the usual way to derive the Pearson correlation; usually it\u2019s presented as a normalized form of the covariance , which is a centered average inner product (no normalization) \n \\[ Cov(x,y) = \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y}) }{n} \n= \\frac{ \\langle x-\\bar{x},\\ y-\\bar{y} \\rangle }{n} \\] \n Finally, these are all related to the coefficient in a one-variable linear regression . For the OLS model \\(y_i \\approx ax_i\\) with Gaussian noise, whose MLE is the least-squares problem \\(\\arg\\min_a \\sum (y_i \u2013 ax_i)^2\\), a few lines of calculus shows \\(a\\) is \n \\begin{align} \n OLSCoef(x,y) &= \\frac{ \\sum x_i y_i }{ \\sum x_i^2 } \n= \\frac{ \\langle x, y \\rangle}{ ||x||^2 } \n\\end{align} \n This looks like another normalized inner product. But unlike cosine similarity, we aren\u2019t normalizing by \\(y\\)\u2019s norm \u2014 instead we only use \\(x\\)\u2019s norm (and use it twice): denominator of \\(||x||\\ ||y||\\) versus \\(||x||^2\\). \n Not normalizing for \\(y\\) is what you want for the linear regression: if \\(y\\) was stretched to span a larger range, you would need to increase \\(a\\) to match, to get your predictions spread out too. \n Often it\u2019s desirable to do the OLS model with an intercept term: \\(\\min_{a,b} \\sum (y \u2013 ax_i \u2013 b)^2\\). Then \\(a\\) is \n \\begin{align} \nOLSCoefWithIntercept(x,y) &= \\frac \n{ \\sum (x_i \u2013 \\bar{x}) y_i } \n{ \\sum (x_i \u2013 \\bar{x})^2 } \n= \\frac{\\langle x-\\bar{x},\\ y \\rangle}{||x-\\bar{x}||^2} \n \\\\ \n&= OLSCoef(x-\\bar{x}, y) \n\\end{align} \n It\u2019s different because the intercept term picks up the slack associated with where x\u2019s center is. So OLSCoefWithIntercept is invariant to shifts of x. It\u2019s still different than cosine similarity since it\u2019s still not normalizing at all for y. Though, subtly, it does actually control for shifts of y. This isn\u2019t obvious in the equation, but with a little arithmetic it\u2019s easy to derive that \\( \n\\langle x-\\bar{x},\\ y \\rangle = \\langle x-\\bar{x},\\ y+c \\rangle \\) for any constant \\(c\\). (There must be a nice geometric interpretation of this.) \n Finally, what if x and y are standardized: both centered and normalized to unit standard deviation? The OLS coefficient for that is the same as the Pearson correlation between the original vectors. I\u2019m not sure what this means or if it\u2019s a useful fact, but: \n \\[ OLSCoef\\left( \n\\sqrt{n}\\frac{x-\\bar{x}}{||x-\\bar{x}||}, \n\\sqrt{n}\\frac{y-\\bar{y}}{||y-\\bar{y}||} \\right) = Corr(x,y) \\] \n Summarizing: Cosine similarity is normalized inner product. Pearson correlation is centered cosine similarity. A one-variable OLS coefficient is like cosine but with one-sided normalization. With an intercept, it\u2019s centered. \n Of course we need a summary table. \u201cSymmetric\u201d means, if you swap the inputs, do you get the same answer. \u201cInvariant to shift in input\u201d means, if you add an arbitrary constant to either input, do you get the same answer. \n \n \n Function\n Equation\n Symmetric?\n Output range\n Invariant to shift in input? \n Pithy explanation in terms of something else \n \n \n Inner(x,y) \n \\[ \\langle x, y\\rangle\\] \n Yes\n \\(\\mathbb{R}\\)\n No \n \n \n CosSim(x,y)\n \\[ \\frac{\\langle x,y \\rangle}{||x||\\ ||y||} \\] \n Yes \n [-1,1] or [0,1] if inputs non-neg \n No \n normalized inner product \n \n Corr(x,y)\n \\[ \\frac{\\langle x-\\bar{x},\\ y-\\bar{y} \\rangle }{||x-\\bar{x}||\\ ||y-\\bar{y}||} \\] \n Yes\n [-1,1]\n Yes \n centered cosine; or normalized covariance \n \n Cov(x,y)\n \\[ \\frac{\\langle x-\\bar{x},\\ y-\\bar{y} \\rangle}{n} \\] \n Yes\n \\(\\mathbb{R}\\)\n Yes \n centered inner product \n \n OLSCoefNoIntcpt(x,y) \n \\[\\frac{ \\langle x, y \\rangle}{ ||x||^2 }\\] \n No\n \\(\\mathbb{R}\\)\n No \n (compare to CosSim) \n \n OLSCoefWithIntcpt(x,y) \n \\[ \\frac{\\langle x-\\bar{x},\\ y \\rangle}{||x-\\bar{x}||^2} \\] \n No\n \\(\\mathbb{R}\\)\n Yes \n \n \n Are there any implications? I\u2019ve been wondering for a while why cosine similarity tends to be so useful for natural language processing applications. Maybe this has something to do with it. Or not. One implication of all the inner product stuff is computational strategies to make it faster when there\u2019s high-dimensional sparse data \u2014 the Friedman et al. 2010 glmnet paper talks about this in the context of coordinate descent text regression. I\u2019ve heard Dhillon et al., NIPS 2011 applies LSH in a similar setting (but haven\u2019t read it yet). And there\u2019s lots of work using LSH for cosine similarity; e.g. van Durme and Lall 2010 [slides] . \n Any other cool identities? Any corrections to the above? \n References: I use Hastie et al 2009, chapter 3 to look up linear regression, but it\u2019s covered in zillions of other places. I linked to a nice chapter in Tufte\u2019s little 1974 book that he wrote before he went off and did all that visualization stuff. (He calls it \u201ctwo-variable regression\u201d, but I think \u201cone-variable regression\u201d is a better term. \u201cone-feature\u201d or \u201cone-covariate\u201d might be most accurate.) In my experience, cosine similarity is talked about more often in text processing or machine learning contexts."], "link": "http://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/", "bloglinks": {}, "links": {"http://cs.jhu.edu/": 1, "http://www.jstatsoft.org/": 1, "http://www.edwardtufte.com/": 2, "http://nlp.stanford.edu/": 1, "http://en.wikipedia.org/": 5, "http://www-stat.stanford.edu/": 1, "http://www.utexas.edu/": 1}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["The idea for a shared task on web parsing is really cool. But I don\u2019t get this one: \n Shared Task \u2013 SANCL 2012 (First Workshop on Syntactic Analysis of Non-Canonical Language) \n They\u2019re explicitly banning \n \n Manually annotating in-domain (web) sentences\n Creating new word clusters, or anything, from as much text data as possible\n \n \u2026 instead restricting participants to the data sets they release. \n Isn\u2019t a cycle of annotation, error analysis, and new annotations (a self-training + active-learning loop, with smarter decisions through error analysis) the hands-down best way to make an NLP tool for a new domain? Are people scared of this reality? Am I off-base? \n I am, of course, just advocating for our Twitter POS tagger approach, where we annotated some data, made a supervised tagger, and iterated on features. The biggest weakness in that paper is we didn\u2019t have additional iterations of error analysis. Our lack of semi-supervised learning was not a weakness."], "link": "http://brenocon.com/blog/2012/03/i-dont-get-this-web-parsing-shared-task/", "bloglinks": {}, "links": {"http://www.cmu.edu/": 1, "https://sites.google.com/": 1}, "blogtitle": "AI and Social Science - Brendan O'Connor"}, {"content": ["To the delight of those of us enjoying the ride on the anti-power-law bandwagon (bandwagons are ok if it\u2019s a backlash to another bandwagon), Cosma links to a new article in Science, \u201cCritical Truths About Power Laws,\u201d by Stumpf and Porter . Since it\u2019s behind a paywall you might as well go read the Clauset/Shalizi/Newman paper on the topic, and since you won\u2019t be bothered to read the paper, see the blogpost entitled \u201cSo You Think You Have a Power Law \u2014 Well Isn\u2019t That Special?\u201d \n Anyway, the Science article is nice \u2014 it amusingly refers to certain statistical tests as \u201cepically fail[ing]\u201d \u2014 and it\u2019s on the side of truth and goodness so it should be supported, BUT, it has one horrendous figure. I just love that, in this of all articles that should be harping on deeply flawed uses of (log-log) plots, they use one of those MBA-style bozo plots with unlabeled axes, one of which is viciously, unapologetically subjective: \n  \n If there is one power law I may single out for mercy in this delightful but verging-on-scary witch hunt, it would be Zipf\u2019s law , cruelly put a bit low on that \u201cmechanistic sophistication\u201d axis. Zipf\u2019s law has a wonderful explanation as the outcome of a Pitman-Yor process (going back to Simon 1955 !), and Clauset/Shalizi/Newman found it was the only purported power law that actually checked out: \n There is only one case\u2014the distribution of the frequencies of occurrence of words \nin English text\u2014in which the power law appears to be truly convincing, in the sense \nthat it is an excellent \ufb01t to the data and none of the alternatives carries any weight. \n Now, it is the case that the CRP/PYP/Yule-Simon stuff is still more of a statistical generative explanation than a deeper mechanistic one; but no one knows how cognition works, there are no satisfying causal stories for linguistic production, and it\u2019s probably fundamentally unknowable anyways, so that\u2019s the best science you can get. yay."], "link": "http://brenocon.com/blog/2012/02/save-zipfs-law/", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://cscs.umich.edu/": 2, "http://jmlr.mit.edu/": 1, "http://www.sciencemag.org/": 1, "http://brenocon.com/blog": 1, "http://www.unc.edu/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "AI and Social Science - Brendan O'Connor"}]