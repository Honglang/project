[{"blogurl": "http://quantombone.blogspot.com\n", "blogroll": [], "title": "tombone's blog"}, {"content": ["Bagpipes and International Conference of Machine Learning (ICML) in Edinburgh  Two weeks ago, I attended the ICML 2012 Conference in Edinburgh, UK . First of all, Edinburgh is a great place for a conference! The scenery is marvelous, the weather is comfortable, and most notably, the sound of bagpipes adds an inimitable charm to the city. I attended the conference because I was invited to give an invited applications talk during the invited talks session. In case you\u2019re wondering, I did not have a plenary session (a plenary session is a session attended by all conference members) which is preserved for titans such as Yann Lecun, David MacKay, and Andrew Ng. My presentation was on the last day of ICML and was titled \u201c Exemplar-SVMs for Visual Object Detection, Label Transfer and Image Retrieval ,\u201d during which I gave an overview of my ICCV 2011 paper on visual object detection as well as the SIGGRAPH ASIA 2011 paper on cross-domain image retrieval. As part of the invited talk, we submitted a 2 page extended abstract which summarizes some key ideas behind the exemplar-svm project: you can check out the abstract as well as the presentation slides online. I believe the talk was recorded, so I will post the video link once it becomes available. It was a great opportunity to convey some of my ideas to a non-vision audience. I think I got a handful of new people excited about single example SVMs (i.e., Exemplar-SVMs)!   Tomasz Malisiewicz, Abhinav Shrivastava, Abhinav Gupta, and Alexei A. Efros. Exemplar-SVMs for Visual Object Detection, Label Transfer and Image Retrieval. To be presented as an invited applications talk at ICML, 2012. PDF | Talk Slides    Getting Ready for Edinburgh with David Hume  To get ready for my first visit to Edinburgh (pronounced Ed-in-bur-ah which does not rhyme with Pittsburgh), I bought a Kindle Touch and proceeded to read David Hume \u2019s An Enquiry Concerning Human Understanding. David Hume is one of the great British Empiricists (together with John Locke and George Berkeley) who stood by the empiricist motto: impressions are the source of all ideas . Empiricists can be contrasted to rationalists who appeal to reason as the source of knowledge. [Of course, I am neither an empiricist nor a rationalist. Such polarizing extremes are a thing of the past. I am a pragmatists and my world-view combines elements from many different philosophies.] I choose Hume\u2019s treatise because he is the one whom Kant credits for awakening him from his dogmatic slumber. I found Hume\u2019s words rejuvenating, full of gedankenexperiments which show the limits of radical empiricism, and most notably is free on the Kindle store! In your attempts to build intelligent machines, maybe you will also words of inspiration in the classics. It was a great book to get into the Edinburgh mindset (although the ICML crowd is probably more familiar with a different University of Edinburgh figure, namely Reverend Bayes).     Impressions of ICML  I would first like to first say that the ICML website is well-organized and serves as a great tool during the conference! Good job ICML! There is a great mobile version of the ICML website which is excellent for visiting on your iPhone when figuring out which talk to go to next. The ICML website also provides a forum for discussing papers and every paper gets a presentation and a poster. The discussion boards do not seem heavily utilized but it would be great to use a moderator-style system to have the actual after-presentation questions come from this forum. I\u2019m sure something like this will actually arise in the upcoming years. ICML is much smaller than CVPR (compare ~700 attendees with ~2000 attendees) which makes for a much more intimate environment. I was amazed by the number of people proving bounds and doing \u201ctheoretical\u201d non-applied machine learning. Its like some people really don't care about anything other than analysis. However, this is not my style, and I personally prefer to build \u201creal\u201d systems and combine insights from disparate disciplines such as mathematics, cognitive science, philosophy, physics, and computer science. There is a bit of ICML and Machine Learning conferences which I think of as nothing more than mathturbation . I understand there's merit to doing analysis of this sort -- somebody\u2019s gotta do it, but if you\u2019re gonna do it, please at least try to understand the implications of the real-world problem your dataset and task are trying to address.   Machine Learning doesn\u2019t Matter?  The highlight of the conference by far was Kiri Wagstaff \u2019s plenary talk \u201cMachine Learning that Matters.\u201d Kiri gave an enchanting 30 minute presentation regarding what is rotten in the state of Edinburgh (aka what is wrong with the style of machine learning conferences). Her words were gentle, yet harsh, while simultaneously enlightening, yet morbid. She showed us, machine learning researchers, just how useless much of machine learning research is today. Let\u2019s not forget that Machine Learning is one of the most revolutionary ideas if the modern computer science classroom. Trying to get a PhD in Computer Science and avoiding Machine Learning is like avoiding Calculus while getting and undergraduate degree in Engineering. There is nothing wrong with machine learning as a discipline, but there is something wrong with researchers making the field overly academic. Making a discipline overly academic means creating a self-contained, overly-mathematical, self-citing, and jargon-filled discipline which doesn\u2019t care about world-impact but only cares to propagate a small community\u2019s citation count. Note that much of these arguments also apply to the CVPR world. But do not take my words for granted, read Kiri\u2019s treatise yourself. Abstract Below:    \"Machine Learning that Matters\" Abstract: Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field\u2019s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.   Kiri Wagstaff, \" Machine Learning that Matters ,\" ICML 2012.   PDF Link: http://icml.cc/2012/papers/298.pdf  If you have something to say in response to Kiri's treatise, check out her Machine Learning Impact Forum on http://mlimpact.com/ ."], "link": "http://quantombone.blogspot.com/feeds/3615170954131080635/comments/default", "bloglinks": {}, "links": {"http://mlimpact.com/": 2, "http://www.wkiri.com/": 1, "http://www.cmu.edu/": 5, "http://graphics.cmu.edu/": 1, "http://icml.cc/": 4, "http://en.wikipedia.org/": 3}, "blogtitle": "tombone's blog"}, {"content": ["Intelligence is all about making inferences given observations, but somewhere in the history of Computer Vision, we (as a community) have put too much emphasis on classification tasks. What many researchers in the field (unfortunately this includes myself) focus on is extracting semantic meaning from images, image collections, and videos. Whether the output is a scene category label, an object identity and location, or an action category, the way we proceed is relatively straightforward: Extract some measurements from the image (we call them \"features\", and SIFT and HOG are two very popular such features) Feed those features into a machine learning algorithm which predicts the category these features belong to. Some popular choices of algorithms are Neural Networks, SVMs, decision trees, boosted decision stumps, etc. Evaluate our features on a standard dataset (such as Caltech-256, PASCAL VOC, ImageNet, LabelMe , etc) Publish (or as is commonly know in academic circles: publish-or-perish ) While only in the last 5 years has action recognition become popular, it still adheres to the generic machine vision pipeline. But let's consider a scenario where adhering to this template can hav disastrous consequences. Let's ask ourselves the following question:  Q: Why did the robot cross the road?   Image courtesy of napkinville.com  A: The robot didn't cross the road -- he was obliterated by a car. This is because in order to make decisions in the world you can't just wait until all observations happened. To build a robot that can cross the road, you need to be able to predict things before they happen! (Alternate answer: The robot died because he wasn't using Minh's early-event detection framework, the topic of today's blog post.)  This year's Best Student Paper winner at CVPR has given us a flavor of something more, something beyond the traditional action recognition pipeline , aka \"early event detection.\" Simply put, the goal is to detect an action before it completes. Minh's research is rather exciting, which opens up room for a new paradigm in recognition. If we want intelligent machines roaming the world around us (and every CMU Robotics PhD student knows that this is really what vision is all about), then recognition after an action has happened will not enable our robots to do much beyond passive observation. Prediction (and not classification) is the killer app of computer vision because classification assumes you are given the data and prediction assumes there is an intent to act on and interpret the future.    While Minh's work focused on simpler actions such as facial recognition, gesture recognition, and human activity recognition, I believe these ideas will help make machines more intelligent and more suitable for performing actions in the real world. Disgust detection example from CVPR 2012 paper  To give the vision hackers a few more details, this framework uses Structural SVMs (NOTE: trending topic at CVPR) and is able to estimate the probability of an action happening before it actually finishes. This is something which we, humans, seem to do all the time but has been somehow neglected by machine vision researchers.   Max-Margin Early Event Detectors.  Hoai, Minh & De la Torre, Fernando CVPR 2012 Abstract: The need for early detection of temporal events from sequential data arises in a wide spectrum of applications ranging from human-robot interaction to video security. While temporal event detection has been extensively studied, early detection is a relatively unexplored problem. This paper proposes a maximum-margin framework for training temporal event detectors to recognize partial events, enabling early detection. Our method is based on Structured Output SVM, but extends it to accommodate sequential data. Experiments on datasets of varying complexity, for detecting facial expressions, hand gestures, and human activities, demonstrate the benefits of our approach. To the best of our knowledge, this is the first paper in the literature of computer vision that proposes a learning formulation for early event detection. Early Event Detector Project Page (code available on website) Minh gave an excellent, enthusiastic, and entertaining presentation during day 3 of CVPR 2012 and was definitely one of the highlights of that day. He received his PhD from CMU's Robotics Institute (like me, yipee!) and is currently a Postdoctoral research scholar in Andrew Zissermann's group in Oxford . Let's all congratulate Minh for all his hard work."], "link": "http://quantombone.blogspot.com/feeds/7986965971848583835/comments/default", "bloglinks": {}, "links": {"http://labelme.mit.edu/": 1, "http://www.ac.uk/": 6, "http://en.wikipedia.org/": 3, "http://www.napkinville.com/": 2}, "blogtitle": "tombone's blog"}, {"content": ["Due to popular request, here is my overview of some of the coolest stuff from Day 2 of CVPR 2012 in Providence, RI . While the Lobster dinner was the highlight for many of us, there were also some serious learning/optimization-based papers presented during Day 2 worthy of sharing. Here are some of the papers which left me with a very positive impression. Dennis Strelow of Google Research in Mountain View presented a general framework for Wiberg minimization. This is a strategy for minimizing objective functions with multiple variables -- objectives which are typically tackled in an EM-style fashion. The idea is to express one of the variables as a linear function of the other variable, effectively making the problem depend on only one set of variables. The technique is quite general and has been shown to produce state-of-the-art results on a bundle adjustment problem. I know Dennis from my second internship at Google where we worked on some sparse-coding problems. If you perform lots of matrix decomposition problems, check out his paper!  Dennis Strelow General and Nested Wiberg Minimization CVPR 2012 Another cool paper which is all about learning is Hossein Mobahi's algorithm for optimizing objectives by smoothing them to avoiding getting stuck in local minima. This paper is not about blurry images, but about applying Gaussians to objective functions. In fact, for the problem of image alignment, Hossein provides closed form versions of image operators. Now when you apply these operators to images, you efficiently smooth the underlying cross-correlation alignment objective. You decrease the blur, while following the optimum path, and get much nicer answers that doing naive image alignment.   Hossein Mobahi , C. Lawrence Zitnick, Yi Ma Seeing through the Blur CVPR 2012 Ira Kemelmacher-Shlizerman, of Photobios fame, showed a really cool algorithm for computing optical flow between two different faces based on learning a subspace (using a large database of faces). The ideas is quite simple and allows for flowing between two very different faces where the underlying operation produces a sequence of intermediate faces in an interpolation-like manner. She shared this video with us during her presentation, but it is on Youtube, so now you can enjoy it for yourself. Ira Kemelmacher-Shlizerman, Steven M. Seitz Collection Flow CVPR 2012 Now talk about cool ideas! Pyry, of CMU fame, presented a recommendation engine for classifiers. The idea is to take techniques from collaborative filtering (think Netflix!) and apply then to the classifier selection problem. Pyry has been working on action recognition and the ideas presented in this work are not only quite general, but have are quite intuitive and likely to benefit anybody working with large collections of classifiers.  Pyry Matikainen, Rahul Sukthankar, Martial Hebert Model Recommendation for Action Recognition CVPR 2012 And finally, a super-easy algorithm presented for metric learning by Martin K\u00f6stinger had me intrigued! This a Mahalanobis distance metric learning paper which uses equivalence relationships. This means that you are given pairs of similar items and pairs of dissimilar items. The underlying algorithm is really not much more than fitting two covariance matrices, one to the positive equivalence relations, and another to the non-equivalence relations. They have lots of code online, and if you don't believe that such a simple algorithm can beat LMNN (Large-Margin Nearest Neighbor from Killian Weinberger), then get their code and hack away! Martin K\u00f6stinger, Martin Hirzer, Paul Wohlhart, Peter M. Roth, Horst Bischof Large Scale Metric Learning from Equivalence Constraints CVPR 2012   CVPR 2012 gave us many very math-oriented papers, and while I cannot list of all of them, I hope you found my short list useful."], "link": "http://quantombone.blogspot.com/feeds/4583677327795210506/comments/default", "bloglinks": {}, "links": {"http://www.illinois.edu/": 1, "http://perception.illinois.edu/": 2, "http://grail.washington.edu/": 2, "http://research.google.com/": 3, "http://lrs.tugraz.at/": 2, "http://4.blogspot.com/": 1, "http://www.cvpr2012.org/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Today ended the first day of CVPR 2012 in Providence, RI. And here's a quick recap: On the administrative end of things, Deva Ramanan received an award for his contributions to the field as a new young CVPR researcher. This is a new nomination-based award so be sure to vote for your favorite vision scientists next year! Deva's work has truly influenced the field and he is well-known for being a co-author of the Felzenszwalb et al. DPM object detector , but since then he has pushed his ideas on part-based models to the next level. Congratulations Deva , you are the type of researcher we should all strive to be. Secondly, it looks like CVPR 2015 will be in Boston. Here are some noteworthy papers from the oral sessions of Day 1: During the first oral session, Antonio Torralba gave an intriguing talk where he showed the world how accidental anti-pinhole and pin-speck cameras are \"all around us.\" In his presentation, he showed how a person walking in front of a window can be used to image the world outside of a window. Additionally he showed a variant of image-based Van-Eck phreaking , where his technique could be used to view what is on a person's computer screen without having to look at the screen directly.  Accidental pinhole and pinspeck cameras: revealing the scene outside the picture Antonio Torralba and William T. Freeman CVPR 2012 Andrew Gallagher gave a really great presentation on using computer vision to solve jigsaw puzzles, where not only are the pieces jumbled, but their orientation is unknown. His algorithm was used to solve really really large puzzles, ones which are much larger than could be tackled by a human.  Jigsaw Puzzles with Pieces of Unknown Orientation Andrew Gallagher CVPR 2012 Gunhee Kim presented his newest work on co-segmentation. He has been working on this for quite some time and if you are interested in segmentation in image collections, you should definitely check it out.  On Multiple Foreground Cosegmentation Gunhee Kim and Eric P. Xing CVPR 2012"], "link": "http://quantombone.blogspot.com/feeds/5734332173353572095/comments/default", "bloglinks": {}, "links": {"http://chenlab.cornell.edu/": 1, "http://www.brown.edu/": 1, "http://draft.blogger.com/": 1, "http://people.mit.edu/": 2, "http://www.cmu.edu/": 4, "http://web.mit.edu/": 2, "http://en.wikipedia.org/": 1, "http://www.uci.edu/": 1, "http://www.cvpr2012.org/": 1, "http://amp.cornell.edu/": 2}, "blogtitle": "tombone's blog"}, {"content": ["Today (Sunday 6/17/2012) is the second day of CVPR 2012 workshops and I'll be going to the Egocentric Vision workshop . The workshop kicks off at 8:50am (come earlier for some CVPR breakfast) and will start with a keynote talk by Takeo Kanade . There will also be a talk by Hartmut Neven of Neven-vision and now a part of Google. Also during the poser session, my fellow colleague, Abhinav Shrivastava , will be presenting his work on applying ExemplarSVMs to detection from a first-person point of view --- yet another super-cool application of ExemplarSVMs .   Object detection from first person's view using exemplar SVMs There are lots of other plenty of cool talks during this workshop including: action recognition from a first-person point of view, experience classification, as well as a study of the obtrusiveness of wearable computing platforms by some fellow MIT vision hackers.   The accuracy-obtrusiveness tradeoff for wearable vision platforms You might be thinking, \"What is egocentric vision?\" but nothing explains it better than the following video from Google about its super exciting research project codename Project Glass . I'm really hoping Hartmut talks about this... If you're looking for me, you know where I'll be tomorrow. Happy computing."], "link": "http://quantombone.blogspot.com/feeds/3762418663607799219/comments/default", "bloglinks": {}, "links": {"http://www.abhinav-shrivastava.info/": 2, "http://mit.edu/": 1, "http://www.cmu.edu/": 2, "http://en.wikipedia.org/": 2, "http://www.cvpr2012.org/": 1, "http://egovision12.gatech.edu/": 2}, "blogtitle": "tombone's blog"}, {"content": ["I have a certain attitude when it comes to computer vision research -- don't do it in isolation. Reading vision papers on your own is not enough. Learning how your peers analyze computer vision ideas will only strengthen your own understanding of the field and help you become a more critical thinker. And that is why at places like CMU and MIT we have computer vision reading groups. The computer vision reading group at CMU (also known as MISC-read to the CMU vision hackers) has a long tradition, and Martial Hebert has made sure it is a strong part of the CMU vision culture. Others ex-CMU hackers such as Sanjiv Kumar have continued the vision reading group tradition onto places such as Google Research in NY (correct me if this is no longer the case). I have continued the reading group tradition to MIT (where I'm currently a postdoc) because I was surprised there wasn't one already! In reality, we spend so much time talking about papers in an informal setting, that I felt it was a shame to not do so in a more organized fashion.  Image courtesy of Platypus My personal philosophy is that as a vision researcher, the way towards the goal of creating novel long-lasting ideas is learning how others think about the field. There's a lot of value in being able to analyze, criticize, and re-synthesize other researchers' ideas. Believe me when I say that a lot of new vision papers come out of top tier vision conferences every year. You should be reading them! But not just reading, also criticizing them among your peers. Because once you learn to criticize others' ideas, you will become better at promulgating your own. Do not equate criticism with nasty words for the sake of being nasty -- good criticism stems from a keen understanding of what must be done in science to convince a broad audience of your ideas.  In case you want to start your own computer vision research group, I've collected some tips, tricks, and advice:  1. You don't need faculty. If you can't find a season vision veteran to help you organize the event, do not worry. You just need 3+ people interested in vision and the motivation to maintain weekly meetings. Who cares if you don't understand every detail of every paper! Nobody besides the authors will ever understand every detail.  2. Be fearless. Ask dumb questions. Alyosha Efros taught me that if you're reading a paper or listening to a presentation, if you don't understand something then there's a good chance you're not the only one in the audience with the same questions. Sometimes younger PhD students are afraid of \"asking a dumb question\" in front of audience. But if you love knowledge, then it is your duty to ask. Silence will not get you far. Be bold, be curious, and grow wise.  3. Choose your own papers to present. Do not present papers that others want you to present -- that is better left for a seminar course led by a faculty member. In a reading group it is very important that you care about the problems you will be discussing with your peers. If you keep up with this trend then when it comes to \"paper writing time\" you should be up to date on many relevant papers in your field and you will know about your other lab mates' research interests.  4. It is better to show a paper PDF up on a projector than cancel a meeting. Even if everybody is busy, and the presenter didn't have time to create slides, it is important to keep the momentum going .  5. After a major conference, have all of the people who attended the conference present their \"top K paper.\" The week after CVPR it will be valuable to have such a massive vision brain dump onto your peers because it is unlikely that everybody got to attend.  6. Book a room every week and try to have the meeting at the same time and place. Have either the presenter or the reading group organizer send out an announcement with the paper they will be presenting ahead of time. At MIT we share a google doc with the information about interesting papers and the upcoming presenter usually chooses the paper one week in advance so that the following week's presenter doesn't choose the same paper. If somebody already presents your paper, don't do it a second time! Choose another paper. cvpapers.com is a great resource to find upcoming papers.  At CMU, there is a long rotating schedule which includes every vision student and faculty member. Once it is your time to present, you can only get off the hook if you swap your slot with somebody else. Being on a schedule months in advance means you'll have lots of time to prepare your slides. At MIT, we are currently following the object recognition / scene understanding / object detection theme where we (Prof. Torralba, his students, his postdocs, his visiting students, etc) choose a paper highly relevant to our interests. By keeping such a focus, we can really jump into the relevant details without having to explain fundamental concepts such as SVMs, features, etc. However, at CMU the reading group is much broader because on the queue are students/profs interested in all aspects of vision and related fields such as graphics, illumination, geometry, learning, etc."], "link": "http://quantombone.blogspot.com/feeds/2493002447194563130/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 4, "http://www.sanjivk.com/": 1, "http://newyork.platypus1917.org/": 2, "http://www.cvpapers.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Last week, some of us vision hackers at MIT started an Object Recognition Reading Group. The group is currently in stealth-mode, but our goal is to analyze, criticize, and re-synthesize ideas from the object detection/recognition community. To inaugurate the group, I covered Hamed Pirsiavash 's Steerable Part Models paper from the upcoming CVPR 2012 conference. As background reading, I had to go over the mathematical basics of learning with tensors (i.e., multidimensional arrays) which were outlined in their earlier NIPS 2009 paper, Bilinear Classifiers for Visual Recognition . After reading up on their work, I have a better grasp of what the trace operator actually does. It is nothing more than a Hermitian inner product defined between the space of linear operators from C^N to C^M (see post here for geometric interpretations of the trace ).  Hamed Pirsiavash , Deva Ramanan , \" Steerable part models \", CVPR 2012  \"Our representation can be seen as an approach to sharing parts.\" -- H. Pirisiavash and D. Ramanan  The idea behind this paper is relatively simple -- instead of learning category-specific part-models, learn a part-basis from which all category-specific part models come from. Consider the different parts learned from a deformable part model (see Felzenszwalb's DPM page for more info about DPMs) and their depiction below. If you take a close look you see that the parts are quite general, and it makes sense to assume that there is a finite basis from which these parts come from. Parts from a Part-model  The model learns a steerable basis by factoring the matrix of all part models into the product of two low rank matrices, and because the basis is shared across categories, this performs both dimensionality reduction (like to help prevent over-fitting as well as speed up the final detectors) and sharing (likely to boost performance). The learned steerable basis  While the objective function is not convex, it can be tackled via a simple alternating optimization algorithm where the resulting sub-objectives are convex and can be optimized using off-the-shelf Linear SVM solvers. They call this property bi-convexity, and it doesn't guarantee finding the global optimum, just makes using standard tools easy. While the results on PASCAL VOC2007, do not show an improvement in performance (VOC2007 is not a very good dataset for sharing as there are only a few category combinations which should in theory benefit significantly from sharing (e.g., bicycle and motorbike)), they show a significant computational speed up. Below is a picture of the part-based car model from Felzenszwalb et al, as well as the one from their steerable basis approach. Note that the HOG visualizations look very similar.  In conclusion, this is one paper worthy of checking out if you are serious about object recognition research. The simplicity of the approach is a strong point, and if you are a HOG-hacker (like many of us these days) then you will be able to understand the paper without a problem."], "link": "http://quantombone.blogspot.com/feeds/8144480098588221164/comments/default", "bloglinks": {}, "links": {"http://www.uci.edu/": 6, "http://2.blogspot.com/": 2, "http://mathoverflow.net/": 1, "http://3.blogspot.com/": 1, "http://www.brown.edu/": 1}, "blogtitle": "tombone's blog"}, {"content": ["There's a lot more to automated object interpretation than merely predicting the correct category label. If we want machines to be able to one day interact with objects in the physical world, then predicting additional properties of objects such as their attributes, segmentations, and poses is of utmost importance. This has been one of the key motivations in my own research behind exemplar-based models of object recognition.  The same argument holds for scenes. If we want to build machines which understand environments around them, then they will have to do much more than predict some sloppy \"scene category.\" Consider what happens when a machine automatically analyzes a picture and says that it from the \"theatre\" category. Well, the picture could be of the stage, the emergency exit, or just about anything else within a theater -- in each of these cases, the \"theatre\" category would be deemed correct, but would fall short of explaining the content of the image. Most scene understanding papers either focus getting the scene category right, or strive to obtain a pixel-wise semantic segmentation map. However, there's more to scene categories than meets the eye.  Well, there is an interesting paper which will be presented this summer at the CVPR2012 Conference in Rhode Island which tries to bring the concept of \" pose \" into scene understanding. Pose-estimation has already been well established in the object recognition literature, but this is one of the first serious attempts to bring this new way of thinking into scene understanding.  J. Xiao, K. A. Ehinger, A. Oliva and A. Torralba. Recognizing Scene Viewpoint using Panoramic Place Representation. Proceedings of 25th IEEE Conference on Computer Vision and Pattern Recognition, 2012.  The SUN360 panorama project page also has links to code, etc.   The basic representation unit of places in their paper is that of a panorama . If you've ever taken a vision course, then you probably stitched some of your own. Below are some examples of cool looking panoramas from their online gallery. A panorama roughly covers the space of all images you could take while centered within a place. Car interior panoramas from SUN360 page  Building interior panoramas from SUN360 page  What the proposed algorithm accomplishes is twofold. First it acts like an ordinary scene categorization system, but in addition to producing a meaningful semantic label, it also predicts the likely view within a place . This is very much like predicting that there is a car in an image, and then providing an estimate of the car's orientation. Below are some pictures of inputs (left column), a compass-like visualization which shows the orientation of the picture (with respect to a cylindrical panorama), as well as a depiction of the likely image content to fall outside of the image boundary. The middle column shows per-place mean panoramas (in the style of TorralbaArt), as well as the input image aligned with the mean panorama.  I think panoramas are a very natural representation for places, perhaps not as rich as a full 3D reconstruction of places, but definitely much richer than static photos. If we want to build better image understanding systems, then we should seriously start looking at using richer sources of information as compared to static images. There is only so much you can do with static images and MTurk, thus videos, 3D models, panoramas, etc are likely to be big players in the upcoming years."], "link": "http://quantombone.blogspot.com/feeds/3652243384897777179/comments/default", "bloglinks": {}, "links": {"http://people.mit.edu/": 3, "http://1.blogspot.com/": 2, "http://4.blogspot.com/": 1, "http://en.wikipedia.org/": 2, "http://web.mit.edu/": 1, "http://www.cvpr2012.org/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["I often listen to lectures and audiobooks when I drive more than 2 hours because I don't always have the privilege of enjoying a good conversation with a passenger. Recently I was listening to some philosophy of science podcasts on my iPhone while driving from Boston to New York when the following sentence popped into my head:  \"I shot the cat with my proton gun.\"     I had just listened to three separate Podcasts (one about Kant, one about Wittgenstein and one about Popper) when the sentence came to my mind. What is so interesting about this sentence is that while it is effortless to grasp, it uses two different types of concepts in a single sentence, a \"proton gun\" and a \"cat.\" It is a perfectly normal sentence, and the above illustration describes the sentence fairly well (photo credits to http://afashionloaf.blogspot.com/2010/03/cat-nap-mares.html for the kitty, and http://www.colemanzone.com/ for the proton gun).  Cat == an \"everyday\" empirical concept \"Cat\" is an everyday \"empirical\" concept, a concept with which most people have first hand experience (i.e., empirical knowledge). It is commonly believed that such everyday concepts are acquired by children at a young age -- it is an exemple of a basic level concept which people like Immanuel Kant and Ludwig Wittgenstein discuss at great length. We do not need a theory of cats for the idea of a cat to stick.   Image from shadowpaw99   Proton Gun == a \"scientific\" theoretical concept On the other extreme is the \"proton gun.\" It is an example of a theoretical concept -- a type of concept which rests upon classroom (i.e., \"scientific\") knowledge. The idea of a proton gun is akin to the idea of Pluto, an esophagus or cancer -- we do not directly observe such entities, we learn about them from books and by seeing illustrations such as the one below. Such theoretical constructs are the the entities which Karl Popper and the Logical Positivists would often discuss.  While many of us have never seen a proton (nor a proton gun), it is a perfectly valid concept to invoke in my sentence. If you have a scientific background, then you have probably seen so many artistic renditions of protons (see Figure below) and spent so many endless nights studying for chemistry and physics exams, that the word proton conjures a mental image. It is hard for me to thing of entities which trigger mental imagery as non-empirical.    Image from Traveling the Heart of Matter   How do we learn such concepts? The proton gun comes from scientific education! The cat comes from experience! But since the origins of the concept \"proton\" and the concept \"cat\" are so disjoint, our (human) mind/brain must be more-amazing-than-previously-thought because we have no problem mixing such concepts in a single clause. It does not feel like these two different types of concepts are stored in different parts of the brain.  The idea which I would like you, the reader, to entertain over the next minute or so is the following:  Perhaps the line between ordinary \"empirical\" concepts and complex \"theoretical\" concepts is an imaginary boundary -- a boundary which has done more harm than good.   One useful thing I learned from Philosophy of Science, is that it is worthwhile to doubt the existence of theoretical entities. Not for iconoclastic ideals, but for the advancement of science! Descartes' hyperbolic doubt is not dead. Another useful thing to keep in mind is Wittgenstein's Philosophical Investigations and his account of the acquisition of knowledge. Wittgenstein argued elegantly that \"everyday\" concepts are far from \"easy-to-define.\" (see his family resemblances argument and the argument on defining a \"game.\") Kant, with his transcendental aesthetic, has taught me to question a hardcore empiricist account of knowledge.  So then, as good cognitive scientists, researchers, and pioneers in artificial intelligence, we must also doubt the rigidity of those everyday concepts which appear to us so ordinary. If we want to build intelligent machines, then we must be ready to break down own understanding of reality, and not be afraid to questions things which appear unquestionable.  In conclusion, if you find popular culture reference more palatable than my philosophical pseudo-science mumbo-jumbo, then let me leave you with two inspirational quotes. First, let's not forget Pink Floyd's lyrics which argued against the rigidity of formal education: \" We don't need no education, We don't need no thought control.\" And the second, a misunderstood, yet witty aphorism which comes to us from Dr. Timothy Leary reminds us that there is a time for education and there is a time for reflection. In his own words: \" Turn on, tune in, drop out.\""], "link": "http://quantombone.blogspot.com/feeds/2587850981936414920/comments/default", "bloglinks": {}, "links": {"http://www.desy.de/": 2, "http://2.blogspot.com/": 1, "http://shadowpaw909.deviantart.com/": 2, "http://afashionloaf.blogspot.com/": 1, "http://www.colemanzone.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Everybody who loves computer sciences loves graphs. But the fat 'n juicy graphs, the ones with complex structure you just gotta visualize. To enjoy these beautiful data structures, the hackers at AT&T gave us, the world, Graphviz as a powerful tool for visualizing complex graphs in two dimensions. I do a lot of stuff in Matlab, so I've put my simple graphviz matlab wrappers up on Github so everybody can enjoy them. I do a lot of stuff with graphs... My repository, which I'm already using as a submodule in many of my projects, can be found here: https://github.com/quantombone/graphviz_matlab_magic Here is a matlab script (included as a Github gist), which should be ran in an empty directory, and it will download a nice mat file plus clone my repo and show the following nice graph. I perform two graphviz passes where the first one is used to read the graphviz coordinates (from the sfdp embedding) and use Matlab's jet colormap to color the edges based on distances in this space. In other words, nearby nodes which are connected will be connected by red (hot) edges and faraway nodes will be connected by blue (cold) edges.   The matrix visualized comes from an electromagnetic model, the details can be found here: http://www.cise.ufl.edu/research/sparse/matrices/Bai/qc324.html The original picture generated by Yifan Hu is here for comparison:  Enjoy --Tomasz"], "link": "http://quantombone.blogspot.com/feeds/771583058768352637/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://www.att.com/": 1, "https://github.com/": 1, "http://www2.att.com/": 1, "http://www.ufl.edu/": 1}, "blogtitle": "tombone's blog"}, {"content": ["I like high-risk / high-reward activity. While some say that this is my temperament (perhaps a vestige of youth?) I simply say: \"that's how I roll.\" Maybe I was too young when I read Kuhn's Structure of Scientific Revolutions , or maybe I was born with iconoclastic ideals, but I earnestly believe that life is too short to always do what you've been told. One of my favorite maxims is the following: \"The only limits we have are the ones we impose upon ourselves.\" I took a gamble when I started this blog, blurring the line between all things related to computer vision, philosophy, artificial intelligence, machine learning, and other fun things which constitute my intellectual life. During my PhD I was even discouraged from blogging , because \"my superiors\" incessantly reminded me that \"you get famous by writing CVPR papers\" and not by wasting time maintaining a \"cute\" blog. Today I'd like to argue that my adventure in blogging has not been a failure at all! I had multiple reasons for wanting to blog, several of which I list below: I wanted to practice my writing, and what better way to practice writing than by writing! I wanted an outlet to discuss certain ideas which I find invaluable in my pursuit of building intelligence, but which aren't necessarily publishable. On my blog I am the sole contributor, the sole editor. If you don't like what I have to say, start your own blog. I don't need anonymous reviews, the CVPR submission process stresses me out enough for one lifetime. I wanted a medium to advertise my own work as well other works which I find important for graduate students in Computer Vision to know about. I wanted to expose the field of Computer Vision to a broader audience and hopefully get others excited about this amazing research field.    Today I'm glad to announce that according to statcounter , my computer vision blog has reached over 100,000 views. In an absolute sense, this really is nothing to be excited about. By since my CMU homepage has approximately 30,000 views, this means that my blog is 3x as popular as my academic homepage! Next goal: 1,000,000 page views!  I actually meet more people that know me through my blog than through my research papers, even though I put in 100x the effort in doing the research behind those papers. I don't plan on taking up blogging full time anytime soon, but it feels good to know that my blogging adventure has paid off.  Here are some of the top keywords which have been used to find my blog: computer vision blog tombone cmu computer vision blog newton fractal matlab tombone's blog  Here are some of my most popular blog posts of all time: Computer Vision is Artificial Intelligence The vision hacker culture at Google graph visualizations as sexy as fractals Simple Newton's Method Fractal code in MATLAB Kinect Object Datasets: Berkeley's B3DO, UW's RGB-D, and NYU's Depth Dataset  I encourage anybody who reads my blog to shoot me a quick \"yo what's up!\" at a local conference or where ever else our paths might cross. I also encourage everybody to suggest the types of things they would like to read about on my blog."], "link": "http://quantombone.blogspot.com/feeds/8876228578345539116/comments/default", "bloglinks": {}, "links": {"https://www.google.com/": 5, "http://3.blogspot.com/": 1, "http://quantombone.blogspot.com/": 6, "http://en.wikipedia.org/": 1, "http://statcounter.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Let's say you want to train a cat detector... If you're anything like me, then you probably have a few labeled cats (~100), as well as a source of non-cat images (~1000). So what do you do when you can't get any more labeled cats? (Maybe Amazon's Mechanical Turk service was shut down by the feds, you've got a paper deadline in 48 hours, and money can't get you out of this dilemma.) Answer: 1) Realize that there are some labeled dogs/cows/sheep in your dataset! 2) Transform some of the dogs/cows/sheep in your dataset to make them look more like cats. Maybe some dogs are already sufficiently similar to cats! (see cheezburger.com image below) 3) Use a subset of those transformed dogs/cows/sheep examples as additional positives in your cat detector!   Some dogs just look like cats! (and vice-versa) Image courtesy of cheezburger.com Using my own internal language, I view this phenomenon as \" exemplar theft .\" But not the kind of theft which sends you to prison, 'tis the kind of theft which gives you best-paper prizes at your local conference. Note that this was the answer provided by the vision hackers at MIT in their most recent paper, \" Transfer Learning by Borrowing Examples for Multiclass Object Detection ,\" which was just presented at this year's big machine learning-oriented NIPS conference, NIPS 2011 . See the illustration from the paper below, which depicts this type of \"example borrowing\"-sharing for some objects in the SUN09 dataset.  The paper empirically demonstrates that instead of doing transfer learning (also known as multi-task learning ) the typical way (regularizing weight vectors towards each other), it is beneficial to simply borrow a subset of (transformed) examples from a related class. Of course the problem is that we do not know apriori which categories to borrow from, nor which instances from those categories will give us a gain in object detection performance. The goal of the algorithm is to learn which categories to borrow from, and which examples to borrow. Not all dogs will help the cat detector. Here are some examples of popular object categories, the categories from which examples are borrowed, and the categories from which examples are shared once we allow transformations to happen. Notice the improvement in AP (the higher the average precision the better) when you allow sharing.   They also looked at what happens if you want to improve a single category badass detector on one particular dataset, such as the PASCAL VOC . Note that these days just about everybody is using the one-and-only \"badass detector\" and trying to beat it in its own game. These are the different ways you'll hear people talk about the Latent-SVM-based Deformable Part Model baseline . \"badass detector\"=\"state-of-the-art detector\"=\"Felzenszwalb et al. detector\"=\"Pedro's detector\"=\"Deva's detector\",\"Pedro/Deva detector\",\"LDPM detector\",\"DPM detector\" Even if you only care about your favourite dataset, such as PASCAL VOC, you're probably willing to use additional positive data points from another dataset. In their NIPS paper, the MIT hackers show that simply concatenating datasets is inferior to their clever example borrowing algorithm (mathematical details are found in the paper, but feel free to ask me detailed questions in the comments). In the figure below, the top row shows cars from one dataset (SUN09), the middle row shows PASCAL VOC 2007 cars, and the bottom row shows which example the SUN09-car detector wants to borrow from PASCAL VOC.  Here the the cross-dataset generalization performance on the SUN09/PASCAL duo. These results were inspired by the dataset bias work of Torralba and Efros .   In case you're interested, here is the full citation for this excellent NIPS2011 paper: Joseph J. Lim , Ruslan Salakhutdinov , and Antonio Torralba . \" Transfer Learning by Borrowing Examples for Multiclass Object Detection ,\" in NIPS 2011. [ pdf ]  To get a better understanding of Lim et al's paper, it is worthwhile going back in time to CVPR2011 and taking a quick look the following paper, also from MIT: Ruslan Salakhutdinov , Antonio Torralba , Josh Tenenbaum . \" Learning to Share Visual Appearance for Multiclass Object Detection ,\" in CVPR 2011. [ pdf ] Of course, these authors need no introduction (they are all professors at big-time institutions). Ruslan just recently became a Professor and is now back on home turf (where he got his PhD) in Toronto, where he is likely to become the next Hinton. In my opinion, this \"Learning to share\" paper was one of the best papers of CVPR 2011. In this paper they introduced the idea of sharing across rigid classifier templates, and more importantly learning a tree to organize hundreds of object categories. The tree defines how the sharing is supposed to happen. The root note is global and shared across all categories, the mid-level nodes can be interpreted as super-categories (i.e., animal, vehicle), and the leaves are the actual object categories (e.g., dog, chair, person, truck).  The coolest thing about the paper is that they use a CRP (chinese restaurant process) to learn a tree without having to specify the number of super-categories!   Finally, we can see some learned weights for three distinct object categories: truck, van, and bucket. Please see the paper if you want to learn more about sharing -- the clarity of Ruslan's paper is exceptional.    In conclusion, it is pretty clear everybody wants some sort of visual memex. (It is easy to think of the visual memex as a graph where the nodes are individual instances and the edges are relationships between these entities) Sharing, borrowing, multi-task regularization, exemplar-svms, and a host of other approaches are hinting at the breakdown of the traditional category-based way of approaching the problem of object recognition. However, our machine learning tools were designed for supervised machine learning with explicit class information. So what we, the researchers do, is try to break down those classical tools so that we can more effectively exploit the blurry line between not-so-different object categories. At the end of the day, rigid categories can only get us so far. Intelligence requires interpretation at multiple and potentially disparate levels. When it comes to intelligence, the world is not black and white, there are many flavours of meaningful image interpretation."], "link": "http://quantombone.blogspot.com/feeds/3775278291392261459/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "https://www.mturk.com/": 1, "http://www.toronto.edu/": 2, "http://draft.blogger.com/": 1, "http://people.mit.edu/": 7, "http://1.blogspot.com/": 3, "http://cheezburger.com/": 1, "http://4.blogspot.com/": 1, "http://en.wikipedia.org/": 2, "http://web.mit.edu/": 3, "http://2.blogspot.com/": 1, "http://pascallin.ac.uk/": 1, "http://nips.cc/": 1, "http://www.brown.edu/": 1}, "blogtitle": "tombone's blog"}, {"content": ["We've all played Where's Waldo as children, and at least for me it was quite a fun game. So today let's play an image-based Big Data version of Where's Waldo. I will give you a picture, and you have to find it in a large collection of images! This is a form of image retrieval , and this particular formulation is also commonly called \"image matching.\"  The only catch is that you are only given one picture , and I am free to replace the picture with a painting or a sketch. Any two-dimensional pattern is a valid query image, but the key thing to note is that there is only a single input image . Life would be awesome if Google's Picasa had this feature built in!  The classical way of solving this problem is via a brute-force nearest neighbor algorithm, an algorithm which won't match pixel pattern directly, but an algorithm which will also use a state-of-the-art image descriptor such as GIST for comparison. Back in 2007, at SIGGRAPH, James Hays and Alexei Efros have shown this to work quite well once you have a very large database of images! But the reason why the database had to be so large is because a naive Nearest Neighbor algorithm is actually quite dumb. The descriptor might be cleverer than matching raw pixel intensities, but for a machine, an image is nothing but a matrix of numbers, and nobody told the machine which patterns in the matrix are meaningful and which ones aren't. In short, the brute-force algorithm works if there are similar enough images such that all parts of the input image will match a retrieved image. But ideally we would like the algorithm to get better matches by automatically figuring out which parts of the query image are meaningful (e.g., the fountain in the painting) and which parts aren't (e.g., the reflections in the water). A modern approach to solve this issue is to collect a large set of related \"positive images\" and a large set of un-related \"negative images\" and then train a powerful classifier which can hopefully figure out the meaningful bits of the image. But in this approach the problem is twofold. First, working with a single input image it is not clear whether standard machine learning tools will have a chance of learning anything meaningful. The second issue, a significantly worse problem, is that without a category label or tag, how are we supposed to create a negative set?!? Exemplar-SVMs to the rescue! We can use a large collection of images from the target domain (the domain we want to find matches from) as the negative set -- as long as the \"negative set\" contains only a small fraction of potentially related images, learning a linear SVM with a single positive still works.  Here is an excerpt from a Techcrunch article which summarizes the project concisely: \"Instead of comparing a given image head to head with other images and trying to determine a degree of similarity, they turned the problem around. They compared the target image with a great number of random images and recorded the ways in which it differed the most from them. If another image differs in similar ways, chances are it\u2019s similar to the first image. \" -- Techcrunch  Abhinav Shrivastava , Tomasz Malisiewicz , Abhinav Gupta , Alexei A. Efros . Data-driven Visual Similarity for Cross-domain Image Matching. In SIGGRAPH ASIA, December 2011. Project Page   Here is a short listing of some articles which mention our research (thank Abhinav !).  http://techcrunch.com/2011/12/06/cmu-researchers-one-up-google-image-search-and-photosynth-with-visual-similarity-engine/   http://news.cs.cmu.edu/article.php?a=2858   http://futureoftech.msnbc.msn.com/_news/2011/12/06/9252228-computer-mimics-human-ability-to-match-images   http://www.physorg.com/news/2011-12-team-computerized-method-images-photos.html   http://nanopatentsandinnovations.blogspot.com/2011/12/carnegie-mellon-creates-computerized.html   http://www.sciencecodex.com/read/carnegie_mellon_creates_computerized_method_for_matching_images_in_photos_paintings_sketches-82595   http://www.cra.org/ccc/rh-imatch.php"], "link": "http://quantombone.blogspot.com/feeds/5214763284029006409/comments/default", "bloglinks": {}, "links": {"http://www.abhinav-shrivastava.info/": 2, "http://www.mckinsey.com/": 1, "http://3.blogspot.com/": 1, "http://www.physorg.com/": 1, "http://www.brown.edu/": 1, "http://techcrunch.com/": 2, "http://futureoftech.msn.com/": 1, "http://www.cmu.edu/": 5, "http://www.sciencecodex.com/": 1, "http://en.wikipedia.org/": 2, "http://graphics.cmu.edu/": 3, "http://www.cra.org/": 1, "http://news.cmu.edu/": 1, "http://nanopatentsandinnovations.blogspot.com/": 1, "http://googlephotos.blogspot.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Disclaimer #1: I don't specialize in faces. When it comes to learning, I like my objectives to be convex. When it comes to hacking on vision systems, I like to tackle entry-level object categories.  Fun fact #1: Faces are probably the easiest objects in the world for a machine to localize/detect/recognize.  Note #1: I supplied the images, my algorithm supplied the red boxes.  Note #2: Sorry to all my friends who failed to get detected by my accidental face detector! (see below)  So I was hackplaying with some of my PhD thesis code over Thanksgiving, and I accidentally made a face detector. oops! I immediately ran to my screenshot capture tool and ran my code on my Mac desktop while browsing Google Images and Facebook. It seems to work pretty well on real faces as well as sketches/paintings of faces (see below)! I even caught two Berkeleyites (an Alyosha and a Jianbo ), but you gotta find them for yourself. The detector is definitely tuned to frontal faces, but runs pretty fast and produces few false positives. Not too shabby for some midnight hackerdom.                Yes, I'm doing dense multiscale sliding windows here. Yes, I'm HoGGing the hell outta these images. Yes, I'm using a single frontal-face tuned template. And yes, I only used faces of myself to train this accidental face detector.  Note: If I've used one of your pictures without permission, and you would like a link back to your home on the interwebs, please leave a comment indicating the image and link to original."], "link": "http://quantombone.blogspot.com/feeds/7959575846991504818/comments/default", "bloglinks": {}, "links": {"https://github.com/": 1, "http://1.blogspot.com/": 2, "http://www.cmu.edu/": 1, "http://www.upenn.edu/": 1, "http://3.blogspot.com/": 3, "http://4.blogspot.com/": 2, "http://2.blogspot.com/": 2}, "blogtitle": "tombone's blog"}, {"content": ["I have been finding great computer vision research papers by using Google Scholar for the past 2+ years. My recipe is straightforward and has two key ingredients. First, by f inding new papers that cite one of my published papers , I automatically get to read papers which will be relevant to my own research interests. The best bit is that by using Google Scholar, I'm not limiting my search to a single conference -- Google finds papers from the raw web.  Second, I have a short list of superstar vision researchers ( Jitendra Malik , among others) and I basically read anything and everything these gurus publish. Regularly visiting academic homepages is the best way to do this, but Google Scholar also lets me search by name. In addition, nobody lists on their homepage their papers' citation counts. This means if I visit a researcher's personal website, I have to make a decision as to what paper to read based on (title, co-authors, publication venue). But highly-cited papers are likely to be more important to read first. I believe that this is a good rule of thumb, and very important if you are new to the field.  I am really glad that Google finally let researchers make public profiles to view their papers and see their citations, etc. See Google Scholar blog for more information. I've been using statcounter to monitor my blog's visitors, and now I can use Google Scholar to monitor who is citing my research papers! I'm not claiming that the only way for me to read one of your papers is to cite one of my papers, but believe me, even if we never met at a vision conference, if you cited one one my papers there's a good chance I already know about your research :-) I would love to see Google Scholar Citations pages one day replace \"my publications\" sections on academic homepages...    My Citations screenshot    My only complaint with Google Scholar is that I can't seem to get it to recognize my two most recent papers. I have these papers listed on my homepage, so do my co-authors, but Google isn't picking them up!!! I manually added them to my Google My Citations page, and using Google Scholar I was able to find at least one other paper which cites on of these two papers.  I read the inclusion guidelines , and I'm still baffled. The PDFs are definitely over 5MB, but my older papers which were indexed by Google were also over 5MB. Dear Google, are you seriously not indexing my recent papers because they are over 5MB? It takes us, researchers, months of hard work to get our work out the door. We see the sun rise for weeks straight when we are in deadline-mode, and the conferences/journals give us size limitations -- we work hard to make our stuff fit within these limits (something like 20MB per PDF). And we, researchers, are crazy about Google and what it means for organizing the world's information -- naturally we are jumping on the Google Scholar bandwagon. I really hope there's some silly reason why I can't find my own papers using Google Scholar, but if I can't find my own work, that means others can't find my own work, and until I can be confident that Google Scholar is bug-free, I cannot give it my full recommendation.  Problematic papers for Google Scholar:  Abhinav Shrivastava, Tomasz Malisiewicz, Abhinav Gupta, Alexei A. Efros. Data-driven Visual Similarity for Cross-domain Image Matching. In SIGGRAPH ASIA, December 2011.  Tomasz Malisiewicz, Abhinav Gupta, Alexei A. Efros. Ensemble of Exemplar-SVMs for Object Detection and Beyond. In ICCV, November 2011.  If anybody has any suggestions (there's a chance I'm doing something wrong), or an explanation as to why my papers haven't been indexed, I would love to hear from you."], "link": "http://quantombone.blogspot.com/feeds/8288006801574455514/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 2, "http://scholar.google.com/": 3, "http://googlescholar.blogspot.com/": 1, "http://statcounter.com/": 1, "http://www.berkeley.edu/": 1}, "blogtitle": "tombone's blog"}, {"content": ["My thesis experiments on Exemplar-SVMs (my PhD thesis link : Note, 33MB) would have taken approximately 20 CPU years to finish. But not on a fat CMU cluster! Here is some simple code which helped make things possible in ~1month of 200+ cores of crunching. That scale of computation is not quite Google-scale computing, but it was a unforgettable experience as a CMU PhD student. I've recently had to go back to the SSH / GNU Screen method of starting scripts at MIT, since we do not have torque/pbs there, but I definitely use these scripts. Fork it, use it, change it, hack it, improve it, break it, learn from it, etc. https://github.com/quantombone/warp_scripts I used these scripts to drive the experiments in my Exemplar-SVM framework (also on Github).  The basic take home message is \"do not throw away old code\" which you found useful at some time. C'mon ex-phd students, I know you wrote a lot of code, you graduated and now you feel embarrassed to share your code. Who cares if you never had a chance to clean it up, if the world never gets to see it then it will die a silent death from lack of use. Just put it on Github , and let others take a look. Git is the world's best source control/versioning system. Its distributed nature makes it perfect for large-scale collaboration. Now with github sharing is super easy! Sharing is caring. Let's make the world a better place for hackerdom, one repository at a time. I've met some great hackers at MIT, such as the great cvondrick , who is still teaching me how to branch like a champ. Mathematicians share proofs. Hackers share code. Embrace technology, embrace Github. If you ever want to hack with me, it is probably as important for you to know the basics of git as it is for you to be a master of linear algebra. Additional Reading:  Distributed Version Control: The Future of History , an article about Git by some Kitware software engineers"], "link": "http://quantombone.blogspot.com/feeds/1949086628856957509/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 1, "http://www.kitware.com/": 2, "https://github.com/": 5}, "blogtitle": "tombone's blog"}, {"content": ["Today, I wanted to point everyone's attention to a super-cool paper from day 1 of this year's ICCV 2011 Conference . Megha Pandey is the lead on this, and Lana Lazebnik (of spatial pyramid fame) is the seasoned vision community member supervising this research. The idea is really simple (and simplicity is a plus!): train a latent deformable part-based model for scenes. Some of the scene models look really cool, and I encourage everybody interested in scene recognition to take a look.    A Part-based Scene Model  One of the reasons why I like this paper is because just like our SIGGRAPH ASIA 2011 paper on cross-domain image matching , they are using HOG features to represent scenes and applying these models in a sliding-window fashion. This is much different than the traditional image-to-feature-vector mapping used in systems based on the GIST descriptor. These types of approaches allow the detection of a scene inside another image! Framing issues are elegantly handled by allowing the model to slide.  Scene Recognition and Weakly Supervised Object Localization with Deformable Part-Based Models.  Megha Pandey and Svetlana Lazebnik . Proceedings of the IEEE International Conference on Computer Vision , 2011. Project Page [pdf]   Abstract: Weakly supervised discovery of common visual structure in highly variable, cluttered images is a key problem in recognition. We address this problem using deformable part-based models (DPM\u2019s) with latent SVM training . These models have been introduced for fully supervised training of object detectors, but we demonstrate that they are also capable of more open-ended learning of latent structure for such tasks as scene recognition and weakly supervised object localization. For scene recognition, DPM\u2019s can capture recurring visual elements and salient objects; in combination with standard global image features, they obtain state-of-the-art results on the MIT 67-category indoor scene dataset. For weakly supervised object localization, optimization over latent DPM parameters can discover the spatial extent of objects in cluttered training images without ground-truth bounding boxes. The resulting method outperforms a recent state-of-the-art weakly supervised object localization approach on the PASCAL-07 dataset.     Weakly Supervised Object Localization (see paper for details)"], "link": "http://quantombone.blogspot.com/feeds/6098771917716620389/comments/default", "bloglinks": {}, "links": {"http://www.unc.edu/": 7, "http://www.iccv2011.org/": 1, "http://graphics.cmu.edu/": 1}, "blogtitle": "tombone's blog"}, {"content": ["All the cool vision kids are going, so why aren't you? http://www.iccv2011.org/ This will be my first ICCV ever! and my first trip to Spain! Seriously though, if you need to find me over the next week, come to Barcelona. There are lots of great papers out this year and I'll be sure the write about the few which I find interesting (and haven't already blogged about). If you want to learn more about the craziness behind ExemplarSVMs, or just to say 'Hi' don't hesitate to find me walking around the conference. I'll be there during all the workshop days too. If anybody has a favourite ICCV2001 paper they want me to look and perhaps write something about (hardcore object recognition please -- I don't care about illumination models), please send me your requests (in the comments below)."], "link": "http://quantombone.blogspot.com/feeds/2516032852816853183/comments/default", "bloglinks": {}, "links": {"http://www.iccv2011.org/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Disclaimer: the following post is cross-posted from Yaroslav's \"Machine Learning, etc\" blog . Since I always rave about my experiences at Google as an intern (did it twice!), I thought some of my fellow readers would find this information useful. If you are a vision PhD student at CMU or MIT, feel free to ask me more about life at Google. If you have questions regarding the following internship offer, you'll have to ask Yaroslav. Original post at: http://yaroslavvb.blogspot.com/2011/10/google-internship-in-visionml.html  My group has intern openings for winter and summer. Winter may be too late (but if you really want winter, ping me and I'll find out feasibility). We use OCR for Google Books, frames from YouTube videos, spam images, unreadable PDFs encountered by the crawler, images from Google's StreetView cameras, Android and few other areas. Recognizing individual character candidates is a key step in OCR system. One that machines are not very good at. Even with 0 context, humans are better. This shall not stand! For example, when I showed the picture below to my Taiwanese coworker he immediately said that these were multiple instance of Chinese \"one\".  Here are 4 of those images close-up. Classical OCR approaches, have trouble with these characters.  This is a common problem for high-noise domain like camera pictures and digital text rasterized at low resolution. Some results suggest that techniques from Machine Vision can help. For low-noise domains like Google Books and broken PDF indexing, shortcomings of traditional OCR systems are due to 1) Large number of classes (100k letters in Unicode 6.0) 2) Non-trivial variation within classes Example of \"non-trivial variation\" I found over 100k distinct instances of digital letter 'A' from just one day's crawl worth of documents from the web. Some more examples are here Chances are that the ideas for human-level classifier are out there. They just haven't been implemented and tested in realistic conditions. We need someone with ML/Vision background to come to Google and implement a great character classifier. You'd have a large impact if your ideas become part of Tesseract. Through books alone, your code will be run on books from 42 libraries. And since Tesseract is open-source, you'd be contributing to the main OCR effort in the open-source community. You will get a ton of data, resources and smart people around you. It's a very low bureocracy place. You could run Matlab code on 10k cores if you really wanted, and I know someone who has launched 200k core jobs for a personal project. The infrastructure also makes things easier. Google's MapReduce can sort a petabyte of data (10 trillion strings) with 8000 machines in just 30 mins . Some of the work in our team used features coming from distributed deep belief infrastructure. In order to get an internship position, you must pass general technical screen that I have no control of. If you are interested in more details, you could contact me directly . -- Yaroslav (the link to apply is usually here , but now it's down, will update when it's fixed)"], "link": "http://quantombone.blogspot.com/feeds/1476206257316623231/comments/default", "bloglinks": {}, "links": {"http://research.microsoft.com/": 1, "http://www.google.com/": 1, "http://yaroslavvb.blogspot.com/": 4, "http://googleresearch.blogspot.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Ross Girshick , Pedro Felzenszwalb , David McAllester Object Detection with Grammar Models To appear in NIPS 2011 pdf Today, I want to point out two upcoming NIPS papers which might be of interest to the Computer Vision community. First, we have a person detection paper from the hackers who brought you Latent Discriminatively Trained Part-based Models (aka voc-release-3.1 and voc-release-4.0). I personally don't care for grammars (I think exemplars are a much more data-driven and computation-friendly way of modeling visual concepts), but I think any paper with Pedro on the author list is really worth checking out. Maybe after I digest all the details, I'll jump on the grammar bandwagon (but I doubt it). Also of note, is the fact that Pedro Felzenszwalb has relocated to Brown University. The second paper, is by Carl Vondrick and Deva Ramanan (also of latent-svm fame). Carl is the author of vatic and a fellow vision@github hacker . Carl, like myself, has joined Antonio Torralba 's group at MIT this fall. He just started his PhD, so you can only expect the quality of his work to increase without bound over the next ~5 years. vatic is an online, interactive video annotation tool for computer vision research that crowdsources work to Amazon's Mechanical Turk. Vatic makes it easy to build massive, affordable video data sets and can be deployed on a cloud. Written in Python + C + Javascript, vatic is free and open-source software. The video below showcases the power of vatic. In this paper, Vondrick et al. use active learning to select the frames which require human annotation. Rather than simply doing linear interpolation between frames, they are truly putting the \"machine-in-the-loop.\" When doing large-scale video annotation, this approach can supposedly save you tens of thousands of dollars.   Carl Vondrick and Deva Ramanan . \" Video Annotation and Tracking with Active Learning \" Neural Information Processing Systems (NIPS) Granada, Spain, December 2011. [paper] [slides]"], "link": "http://quantombone.blogspot.com/feeds/7916320402173051128/comments/default", "bloglinks": {}, "links": {"http://www.cmu.edu/": 1, "https://github.com/": 1, "http://mit.edu/": 5, "http://www.brown.edu/": 3, "http://1.blogspot.com/": 1, "http://ttic.uchicago.edu/": 1, "http://people.uchicago.edu/": 1, "http://www.uci.edu/": 1, "http://en.wikipedia.org/": 1, "http://web.mit.edu/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Why Kinect?   www.pirobot.org The Kinect, made by Microsoft, is starting to become quite a common item in Robotics and Computer Vision research. While the Robotics community has been using the Kinect as a cheap laser sensor which can be used for obstacle avoidance, the vision community has been excited about using the 2.5D data associated with the Kinect for object detection and recognition. The possibility of building object recognition systems which have access to pixel features as well as 2.5D features is truly exciting for the vision hacker community!  Berkeley's B3DO First of all, I would like to mention that it looks like the Berkeley Vision Group jumped on the Kinect bandwagon. But the data collection effort will be crowdsourced -- they need your help! They need you to use your Kinect to capture your own home/office environments and upload it to their servers This way, a very large dataset will be collected, and we, the vision hackers, can use machine learning techniques to learn what sofas, desks, chairs, monitors, and paintings look like. They Berkeley hackers have a paper on this at one of the ICCV 2011 workshops in Barcelona, here is the paper information:   kinectdata.com  A Category-Level 3-D Object Dataset: Putting the Kinect to Work Allison Janoch , Sergey Karayev , Yangqing Jia , Jonathan T. Barron , Mario Fritz , Kate Saenko , Trevor Darrell ICCV-W 2011 [ pdf ] [ bibtex ]  UW's RGB-D Object Dataset On another note, if you want to use 3D for your own object recognition experiments then you might want to check out the following dataset: University of Washington's RGB-D Object Dataset . With this dataset you'll be able to compare against UW's current state-of-the-art.    In this dataset you will find RGB+Kinect3D data for many household items taken from different views. Here is the really cool paper which got me excited about the RGB-D Dataset: A Scalable Tree-based Approach for Joint Object and Pose Recognition Kevin Lai , Liefeng Bo , Xiaofeng Ren , and Dieter Fox In the Twenty-Fifth Conference on Artificial Intelligence (AAAI) , August 2011.   NYU's Depth Dataset I have to admit that I did not know about this dataset (created by by Nathan Silberman of NYU), until after I blogged about the other two datasets. Check out the NYU Depth Dataset homepage . However the internet is great, and only a few hours after posted this short blog post, somebody let me know that I left out this really cool NYU dataset. In fact, it looks like this particular dataset might be at the LabelMe-level regarding dense object annotations, but with accompanying Kinect data. Rob Fergus & Co strike again!    Nathan Silberman , Rob Fergus . Indoor Scene Segmentation using a Structured Light Sensor. To Appear: ICCV 2011 Workshop on 3D Representation and Recognition"], "link": "http://quantombone.blogspot.com/feeds/964805211522199627/comments/default", "bloglinks": {}, "links": {"http://www.iccv2011.org/": 1, "http://cs.nyu.edu/": 6, "http://www.washington.edu/": 7, "http://kinectdata.com/": 2, "http://sergeykarayev.com/": 4, "http://www.pirobot.org/": 2, "http://www.berkeley.edu/": 7, "http://en.wikipedia.org/": 1}, "blogtitle": "tombone's blog"}, {"content": ["\"All bodies together, and each by itself, give off to the surrounding air an infinite number of images which are all-pervading and each complete, each conveying the nature, colour and form of the body which produces it.\" --Leonardo da Vinci Yesterday, Edward H. Adelson (\"Ted Adelson\") gave a lecture at MIT on the plenoptic function and its role in understanding (and unifying) early vision. Ted has been at MIT for quite some time. He is sometimes described as being (1/3 human vision, 1/3 computer vision, and 1/3 computer graphics) and was Bill Freeman's advisor. What is the plenoptic function? Etymology: Plenoptic comes from plenus+optic. plenus : full, filled optic : relating to eye or vision  Ted Adelson imagined a sort of unified field theory for vision -- instead of proposing a jungle of atoms such as edges, corners, and peaks, the plenoptic function offers a unifying principle under which color, texture, motion, etc. can all be viewed as gradients of the plenoptic function. The plenoptic function is a complete representation which contains, implicitly, a description of every possible photograph that could be taken of a particular space-time chunk of the world. Omniscience is to knowing as the plenoptic function is to seeing. Ted remarked that if you asked him 20 years ago what he was working on in vision, you might have gotten a confusing answer. \"Do you work on texture, motion, stereo, or illumination?\" you might ask. \"All of them. Aren't they the all the same thing?\" he might reply. Ted argues that vision scientists in the 80s and early 90s tried to cut up the world of vision into neat little \"particles\" and would develop theories with their favorite particle -- here the particles are early vision concepts such as color, texture, and motion. In their seminal paper on the plenoptic function, The Plenoptic Function and the Elements of Early Vision , Adelson and Bergen state that \"the elemental operations of early vision involve the measurement of local change long various directions within the plenoptic function.\" As a theoretical device, the plenoptic function has left a long-standing impression on me. I first came across Ted's ideas back in 2006 -- thanks to Alyosha Efros' course on vision. Having just completed a BS in Physics, I was well aware of unified field theories in physics, and the plenoptic function seemed too cool to forget. What the plenoptic function means to me However, if the plenoptic function is the Maxwell's equations equivalent for early (low-level) vision, then what I'm ultimately after is the Schrodinger's equation of late (high-level) vision. In his lecture, Ted Adelson acknowledged that vision scientists have a sort of Atom Envy -- they envy the physicists who are able to understand the world in terms of a few fundamental ontologically meaningful entities. First of all, I like particles, but I have no apriori reason to be in the particle camp all of my life. Secondly, the plenoptic function was all about early vision, but my research in vision is all about high-level vision such as object recognition. I might be young and foolish, but the search for a \"mind mechanics\" has been a part of my research life (at least partially) since ~2003. Right now, my best shot at an answer is that exemplars and associations are the basic building blocks of high-level vision -- but unlike the British Empricisits (the champions of associationism), I would argue that the atomic building blocks of associations are object instances, and not ideas such as \"roundness\" and \"blueness\". Complex ideas are then the object categories which arise out of the interactions between these concrete elements of experience. Conclusion The Adelson and Bergen paper is a must read for anybody serious about vision. While it might not offer much in terms of \"what next\" in vision research, it is nevertheless a useful construct in thinking about vision. I get excited when it comes down to unifying principles and I wish there were more papers like this in vision, especially for high-level vision."], "link": "http://quantombone.blogspot.com/feeds/671220490728351870/comments/default", "bloglinks": {}, "links": {"http://en.wiktionary.org/": 2, "http://persci.mit.edu/": 2, "http://people.mit.edu/": 1, "http://1.blogspot.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "tombone's blog"}, {"content": ["Immanuel Kant (1724-1804) , Daniel Dennett (1942-), Josh Tenenbaum (~1971-)  \u201cThoughts without content are empty, intuitions without concepts are blind. The understanding can intuit nothing, the senses can think nothing. Only through their unison can knowledge arise.\u201d -- Immanuel Kant \u201cWe live in a world that is subjectively open. And we are designed by evolution to be \"informavores\", epistemically hungry seekers of information, in an endless quest to improve our purchase on the world, the better to make decisions about our subjectively open future.\u201d -- Daniel Dennett \"For scientists studying how humans cometo understand their world, the central challenge is this: How do our minds get so much from so little? We build rich causal models,make strong generalizations, and construct powerful abstractions, whereas the input data are sparse, noisy, and ambiguous\u2014in every way far too limited. A massive mismatch looms between the information coming in through our senses and the outputs of cognition.\" -- Josh Tenenbaum  Organizing by space (space, time, and physics) There are two faculties of understanding which it is unlikely we have acquired from experience. The first is that of understanding objects as extended bodies in a 3D space and thus occupying some volume. I believe it is Kant argued best against the hardcore British Empiricists, who proclaimed that experience is the sole originator of knowledge. Experiences are the pen strokes, which fill the Empricisit\u2019s tabula rasa. Kant argued (against Hume) that the concept of a spatially extended object is not acquired from experience \u2013 the very notion of experience requires that we already possess the notion of an object in order to have a meaningful percept. It is as if the Empiricists failed to acknowledge that to make strokes on a sheet of paper, we need to already have a pen. Kant\u2019s intuitions are the pens of experience . The requirement of having suitable intuitions for grouping percepts into experiences is what Kant described as a form of transcendental idealism. \u201cObjectness\u201d is a faculty of human understanding, not something acquired from experience. If you are a vision researcher, being aware of this can have drastic implications on your research programme.  It has also been argued that there are some primitive notions of object dynamics, aka folk-physics, which can are possessed by very young children. Given the uniformity of human experience (at least I have no ostensible reason to double that my colleague\u2019s experiences significantly differ from my own), and the diversity in our individual upbringing, it is also unlikely that folk-physics is learned from experience. However, I don't want to make any strong claims regarding folk-physics. I feel safe to say that Quantum Mechanics is another story -- it requires years of mathematics and thousands hours of deliberate problem solving to grasp.  Organizing by mind (psychology, mind, and intent) The second faculty of understanding, which can be found in many aspects of human intelligence, is that of understanding the world in terms of cognitive agents. Humans have an amazing capability when it comes to attributing stuff with having a mind. This way of thinking about the world is so common and uniform among children all over the world, that the differences in their upbringing cannot be reconciled with the uniformity of their capability to project humanness onto objects. Consider the following video (thanks J. Tenenbaum's videos/lectures for pointing this out).      We cannot just view this video os triangles, dots, and lines. Each one of understands the story in terms of a narrative based on agents and their intent. We are stimulated by the external world, we take as input sense-data, and the brain helps us make sense of it -- it turns the hodgepodge of data into experience. But the brain is a mold, it conforms percepts to some shape defined by the mold. These molds are the faculties of understanding which let us understand things, it is like the faculties of understanding are basis vectors onto which we project all input sense data . The data is weak and noisy, the priors are strong, and understanding is the result of their union. An experience without a proper basis is blind, it is just a ball of percepts. These faculties allow us to have experience. The experiences, coupled with memory, allow us to obtain understanding \u2013 where understanding is the relationship between a given experience and past experiences, either in the form of direct associations between currently-experienced-objects and previously-experienced-objects, or rules abstracted away from previously-experienced-objects being directly applied to the current sense data.  What I am talking about is what philosopher Daniel Dennett refers to by the \u201c intentional stance .\u201d Given my background in AI and philosophy of mind, it is very likely that Dennett and I have had the same influences. I like to juxtapose my ideas with those of the classical philosophers such as Descartes, Locke, Kant, Wittgenstein and Pinker -- I\u2019m not sure how Dennett motivates his philosophy nor do I know against whose ideas he juxtaposes his own stance.    At MIT, J. Tenenbaum is pushing these ideas to the next level. I only wish there was more perception in his work -- toy worlds just don't do it for me. I want to build intelligent machines, and really cannot afford to sidestep the issue of perception. Here is a great talk by Josh Tenenbaum on reverse engineering the mind from NIPS 2010 . Video is on videolectures.net, just click the link.  Implications for Artificial Intelligence and Machine Vision Following Josh Tenenbaum, I think that a criticism of classical machine learning is long overdue. Machine Learning, as a field, has been spewing out hardcore empiricists. \u201cLet me download your features, my machine learning algorithm will take care of the rest,\u201d they say. It is like the glory is in the mathematics, which manipulates N-D vectors. But I argue that intelligence isn\u2019t \u201cin the calculus,\u201d it is what the primitives in the calculus actually represent. As an undergraduate I proclaimed, \u201cI am not a mathematician, I am a physicists. I care about the structure of the world, not the structure of proofs. \u201c As a graduate student I proclaimed, \u201cThe glory isn\u2019t in the manipulation of vectors, the glory is understanding the what/why of encoding information about the world into vectors. I am a computer vision researcher, not a machine learning researcher.\u201d That is why the view of the world as coming from K different classes is wrong \u2013 this is merely a convenient view if the statistician\u2019s toolbox is at your disposal. It is all about structuring the input to match a researcher\u2019s high-level intuitions about the world."], "link": "http://quantombone.blogspot.com/feeds/2363371848647449369/comments/default", "bloglinks": {}, "links": {"http://web.mit.edu/": 1, "http://videolectures.net/": 2, "http://ase.tufts.edu/": 2, "http://en.wikipedia.org/": 1, "http://www.philosophypages.com/": 1}, "blogtitle": "tombone's blog"}, {"content": ["In case anybody hasn't heard the news, I am no longer a PhD student at CMU. After I handed in my camera-ready dissertation, it didn't take long for my CMU advisor to promote me from his 'current students' to 'former students' list on his webpage. Even though I doubt there is anyplace in the world which can rival CMU when it comes to computer vision, I've decided to give MIT a shot. I had wanted to come to MIT for a long time, but 6 years ago I decided to choose CMU's RI over MIT's CSAIL for my computer vision PhD. Life is funny because the paths we take in life aren't dead-ends -- I'm glad I had a second chance to come to MIT.    In case you haven't heard, MIT is a little tech school somewhere in Boston. Lots of undergrads can be caught wearing math Tshirts and posters like the following can be found on the walls of MIT:   A cool (undergrad targeted) poster I saw at MIT  As of last week I'm officially a postdoc in CSAIL and I'll be working with Antonio Torralba and Aude Oliva . I've been closely following both Antonio's and Aude's work over the last several years and getting to work with these giants of vision will surely be a treat. In case you don't know what a postdoc is , it is a generic term used to describe post-PhD researchers with generally short term (1-3 year) appointments. People generally use the term Postdocotral Fellow or Postdoctoral Associate to describe their position in a university. I guess 3 years working on vision as an undergrad and 6 years of working on vision as a grad student just wasn't enough for me... I've been getting adjusted to my daily commute through scenic Boston, learning about all the cool vision projects in the lab, as well as meeting all the PhD students working with Antonio. Today was the first day of a course which I'm sitting-in on, titled \"What is intelligence?\". When I saw a course offered by two computer vision titans ( Shimon Ullman and Tomaso Poggio ), I couldn't resist. Here is the information below: What is intelligence?   \"What is intelligence?\" course homepage : http://web.mit.edu/9.s915/www/   Class Times: Friday 11:00-2:00 pm  Units: 3-0-9  Location: 46-5193 (NOTE: we had to choose a bigger room)  Instructors: Shimon Ullman and Tomaso Poggio  The class was packed -- we had to relocate to a bigger room. Much of today's lecture was given by Lorenzo Rosasco . Lorenzo is the Team Leader of IIT@MIT . Here is a blurb from IIT@MIT's website describe what this 'center' is all about: The IIT@MIT lab was founded from an agreement between the Massachusetts Institute of Technology (MIT) and the Istituto Italiano di Tecnologia (IIT). The scientific objective is to develop novel learning and perception technologies \u2013 algorithms for learning, especially in the visual perception domain, that are inspired by the neuroscience of sensory systems and are developed within the rapidly growing theory of computational learning. The ultimate goal of this research is to design artificial systems that mimic the remarkable ability of the primate brain to learn from experience and to interpret visual scenes. Another cool class offered this semester at MIT is Antonio Torralba's Grounding Object Recognition and Scene Understanding ."], "link": "http://quantombone.blogspot.com/feeds/3719213786975493873/comments/default", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://cbcl.mit.edu/": 1, "http://cvcl.mit.edu/": 1, "http://people.mit.edu/": 2, "http://www.iit.it/": 1, "http://www.cmu.edu/": 1, "http://4.blogspot.com/": 1, "http://libraries.mit.edu/": 1, "http://en.wikipedia.org/": 1, "http://mcgovern.mit.edu/": 1, "http://web.mit.edu/": 4, "http://www.mit.edu/": 2, "http://upload.wikimedia.org/": 1}, "blogtitle": "tombone's blog"}, {"content": ["I sometimes get frustrated when developing machine learning algorithms in C++. And since working in object recognition basically means you have to be a machine learning expert, trying something new and exciting in C++ can be extremely painful. I don't miss the C++ heavy workflow for vision projects at Google. C++ is great for building large-scale systems, but not for pioneering object recognition representations. I like to play with pixels and I like to think of everything as matrices. But programming languages, software engineering philosophies, and other coding issues aren't going to be today's topic. Today I want to talk about the one thing that is more valuable that is computers, and that is people . Not just people, but a community of people, and in particular the culture at Google -- in particular, vision@Google.  I miss being around the hacker culture at Google .  The people at Google aren't just hackers, they are Jedis when it comes to building great stuff -- and that is why I recommend a Google internship to many of my fellow CMU vision Robograds (fyi, Robograds are CMU Robotics Graduate Students). CMU-ers, like Googlers, like to build stuff. However, CMU-ers are typically younger.   Image from http://www.rabittooth.com/  What is a software engineering Jedi, you might ask? Tis' one who is not afraid of million cores, one who is not afraid of building something great. While little boys get hurt by the guns 'n knives of C++, Jedi use their tools like ninjas use their swords. You go into Google as a boy, you come out a man. NOTE: I do not recommend going to Google and just toying around in Matlab for 3 months. Build something great, find a Yoda-esque mentor, or at least strive to be a Jedi. There's plenty of time in grad school for Matlab and writing papers. If you get a chance to go to Google, take the opportunity to go large-scale and learn to MapReduce like the pros.  Every day I learn about more and more people I respect in vision and learning going to Google, or at least interning there (e.g., Andrej Karpathy who is starting his PhD@Stanford and Santosh Divvala who is a well-known CMU PhD student and vision hacker). And I really can't blame them for choosing Google over places like Microsoft for the summer. I can't think of many better places to be -- the culture is inimitable. I spent two summers at Jay Yagnik 's group some of the great people I interned with are already full-time Googlers (e.g. Luca Bertelli and Mehmet Emre Sargin ). And what is really great about vision@google is that these guys get to publish surprisingly often ! Not just throw-away-code kind of publish, but stuff that fits inside large-scale systems -- stuff which is already inside Google products. The technology is often inside the Google product before the paper goes public! Of course it's not easy to publish at a place like Google because there is just way too much exciting large-scale stuff going on. Here is a short list of some cool 2010/2011 vision papers (from vision conferences) with significant Googler contributions.   Kernelized Structural SVM Learning \u201cKernelized Structural SVM Learning for Supervised Object Segmentation\u201d, Luca Bertelli , Tianli Yu , Diem Vu, Burak Gokturk, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition 2011 . [ abstract ] [ pdf ]   Finding Meaning on YouTube  \u201cFinding Meaning on YouTube: Tag Recommendation and Category Discovery\u201d, George Toderici , Hrishikesh Aradhye , Marius Pasca , Luciano Sbaiz, Jay Yagnik , Computer Vision and Pattern Recognition , 2010. [ abstract ] [ pdf ]  Here is a very exciting and new paper from SIGGRAPH 2011. It is a sort of Visual Memex for faces -- congratulations on this paper, guys! Check out the video below.    Exploring Photobios Movie   Exploring Photobios from Ira Kemelmacher on Vimeo    Ira Kemelmacher-Shlizerman , Eli Shechtman , Rahul Garg , Steven M. Seitz . \"Exploring Photobios.\" ACM Transactions on Graphics (SIGGRAPH), Aug 2011. [pdf]   Finally, here is a very mathematical paper with a sexy title from the vision@google team. It will be presented at the upcoming ICCV 2011 Conference in Barcelona -- the same conference where I'll be presenting my Exemplar-SVM paper .    The Power of Comparative Reasoning Jay Yagnik , Dennis Strelow, David Ross, Ruei-Sung Lin. ICCV 2011. [PDF]    P.S. If you're a fellow vision blogger, then come find me in Barcelona@iccv2011 -- we'll go brag a beer."], "link": "http://quantombone.blogspot.com/feeds/4528857719726665397/comments/default", "bloglinks": {}, "links": {"http://www.adobe.com/": 1, "http://www.iccv2011.org/": 1, "http://www.toronto.edu/": 1, "http://1.blogspot.com/": 2, "http://grail.washington.edu/": 2, "http://www.cmu.edu/": 2, "http://research.google.com/": 14, "http://www.rabittooth.com/": 2, "http://www.washington.edu/": 3, "http://www.ubc.ca/": 1, "http://vimeo.com/": 3}, "blogtitle": "tombone's blog"}]