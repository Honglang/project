[{"blogurl": "http://flxlexblog.wordpress.com\n", "blogroll": [], "title": "In between lines of code"}, {"content": ["After having given an overview of the PacBio error-correction (PacBioToCA) pipeline of Koren et al (see previous blog post ), it was interesting to see another paper coming out describing combining PacBio and Illumina for assembling bacterial genomes: Ribeiro et al, \u201c Finished bacterial genomes from shotgun sequence data \u201c, Genome research accepted preprint. The authors are all from the Broad Institute. David Jaffe was so kind as to provide me with the supplementary material file, which so far is not yet available online. In this post, I will summarise the ALPATHS_LG paper, and contrast the approach with the PacBioToCA pipeline. \n  Picture from zazzle.com (http://bit.ly/RMGgZR) \n The Broad Institute of MIT/Harvard is an impressive genome centre. Sorry, \u2018Genomic Medicine Center\u2019. One of their achievements in recent years is an optimised pipeline for assembly of (small and) large genomes based on short read (Illumina) data. The software program they developed, ALLPATHS_LG , combined with a kind of special type of \u2018recipe\u2019, has proven to be extremely succesful. The Broad keeps churning out very respectable assemblies of, among other groups, large complex eukaryotes. In order to be able to handle large amounts of genome projcts, I get the impression that the Broad Institute aims for standardisation and optimisation of protocols and, what they call \u2018recipes\u2019. The idea being that, if you follow their recipes and use their programs, you are more or less guaranteed an optimal result. In the case of ALLPATHS_LG, it also means, however, that one is required to have at least one Illumina jumping (\u2018mate pair\u2019) library and a short-insert (\u2018paired-end\u2019) Illumina library for which the insert size is shorter than twice the read length. Paired end libraries with slightly larger insert sizes are standard, making the ALLPATHS_LG requirements kind of uncommon. It also means not many projects are only able to compare other programs with ALLPATHS without generating an extra read dataset (I speak from experience here\u2026). \n The Ribeiro et al paper is entirely focussing on microbial size genomes, a big difference with the Koren et al (PacBioToCA) paper. The short-read recipe for ALLPATHS_LG remains unchanged: 50x coverage short-insert (up to 220bp) paired reads and 50x coverage jumping (2-10kb) reads. This is then supplemented with 50x PacBio reads, with library insert sizes between 1 and 3 kb (although for the paper, they also created two larger insert libraries for two of the strains, at 6 and 10kb respectively). In total, there are three size ranges represented this way: short \u2013 overlapping paired reads, intermediate \u2013 PacBio reads, long \u2013 illumina mate pairs. Finally, the short reads have high quality relative to the PacBio reads. \n The ALLPATHS_LG assembly algorithm was adjusted to take the PacBio reads into consideration. Without going too much into the details (described in detail in the supplementary and some of it a bit over my head), ALLPATHS_LG first builds a unipath graph using K=64. An interesing, and surprising, point the authors mention here is that \n \u201c\u2026 because [the reads from the Illumina jumping libraries] can be less biased than [the reads from the short-insert paired-end libraries] we next fill some gaps in the unipath graph using the jumping reads, ignoring their pairing.\u201d \n This is the first time I hear of this apparently lower GC bias for jumping reads compared to paired-end reads. The authors have this to say about it: \n \u201cThese differences in bias are not well understood but might be attributable to protocol differences between data types or sample-to-sample variability in protocol instantiation.\u201d \n Next, rather than correcting the long, error-rich PacBio reads, as Koren et al chose to do, these long reads are used directly to fill gaps in the unipath graph. Then the PacBio reads are overlaid on the unipath graph to help solve repeats, and consensus sequences are built. Using these consensus sequences, a new unipath graph is built, this time using the unusually large K value of 640 (approximately half the average PacBio read length). Finally, the long jump Illumina reads are used to build the assembly graph. The end result is this graph, representing the maximum amount of information that can be obtained from the data. The authors present these graphs in the paper. From them, fasta sequences are extracted and reported as the familiar contigs and scaffolds. A big advantage of the graph representation is that the relation between the contigs/scaffolds is reported, which makes finishing efforts, and comparative analyses, much easier. \n The paper describes 16 datasets and assemblies, all bacterial genomes ranging from 1.6 to 6Mbp, with GC percentages ranging from 27 to 69%. For three species, reference genomes were available (although the authors went to great lengths to correct these based on their read datasets \u2013 in itself interesting reading). Samples were oversequenced, then datasets reduced to the 50x coverage levels of the recipe. For Illumina, libraries were pooled and sequenced on the HiSeq (using V3 TruSeq chemistry). For the PacBio datasets, C1 chemistry (the first version as per April 2011) was used for the 3kb libraries, but notably, for two strains, C2 chemistry (released February 2012, with a doubling of the readlength, and increased per-base read qualities) was used. For these strains, a 6kb and 10kb insert library were prepared. Between 8 and 30 SMRTCells (\u2018chips\u2019) were used (with the exception of one strain, for which only 4 SMRTCells were used \u2013 incidentally, this was one of the strains for which C2 chemistry was used). \n The authors assembled the data using ALLPATHS_LG with default settings, and describe, using up to 48 CPUS \u2013 runtimes of up to 2.5 hours and peak memory up to 46 GB. The exception to this was one strain that took 6.5 hours and needed 83 GB of RAM. This means that a modest server \u2013 by today\u2019s standard \u2013 with, for example 96 GB of RAM, can handle these assemblies (but one would come quite far with only 48 GB RAM). \n For the three strains for which a reference genome was available, the assemblies could be compared to these. All three assemblies showed the main, large chromosome assembled into one gapless (!) circular contig. A very large repeat shared between two plasmids casued a break in the contigs representing these. One genome assembled without errors, the others had very few (2 and 4 respectively, substitutions and one single-base deletion). For the other 13 strains, three had the main chromosome represented as a closed circle, many others were largely resolved except for some very long repeats. This is a quite amazing result! In the words of the authors: \n \u201cThe resulting assemblies appear at least in some cases to be better than the finished genome reference sequences produced in the past, and yet can be produced rapidly at a cost that is about an order of magnitude lower.\u201d \n In the supplementary material, the authors give an overview of the costs of an assembly using their recipe (direct costs, with amortization of instruments). They arrive at $3,133 for sequencing and assembly (excluding finishing) based on some assumptions. I dare to say this is quite affordable! However, it remains to be seen whether other labs than the Broad \u2013 which is focussed entirely on these kinds of projects \u2013 can get this done for the same price. \n On the ALLPATHS_LG blog \u00a0it is mentioned that: \n \u201cThe Broad Institute is preparing to offer a service that takes as input a bacterial DNA sample and provides as output a genome assembly based on the new laboratory and computational methods.\u201d \n Before I round off, I have a two comments on the libraries presented in the paper. In the paper, the authors mention \u201cthat because of the wide distribution of the jumping pair fragment sizes (2-10 kb), it was necessary to devise a new statistical model to compute these distances.\u201d The explanation ofr this wide distribution is that \u201cjumping libraries were size selected using magnetic beads rather than agarose gel\u201d. This I think has to do with the strategy of the Braod to simplify and streamline protocols \u2013 getting rid of the gel step is an improtant factor, but consequenctly yields jumping reads with wider distributions. \n Second, the authors mention that coverage in PacBio reads over 2 or 3kb is much lower then \u2018raw\u2019 coverage of reads coming off the instrument. This has to do with the fact that the distribution of the PacBio read lengths has a peak much lower than the average, and a long tail towards long read lengths. Also, reads need to be split into shorter subreads if the polymerase seauenced the same insert multiple times. In the Supplementary, tables are presented with absolute coverage at 1, 2 and 3 kb. I used the data to calculate relative coverage (taking the coverage by all reads as 100%), and plotted the results: \n  Data from Ribeiro et al, 2012 Supplementary Table 3 normalised \u00a0as percentage of coverage at 0kb (all reads). The same strain identifiers (1-16) as in the article are used. \n Note the two strains standing out with significantly more relative coverage at 1, 2 and 3 kb: numbers 4 and 8, something not really commented on by the Ribeiro et al. Strains 4 and 8 are also the strains that were sequenced using the newer C2 chemistry, and longer insert libraries (6 and 10 kb instead of 3kb for the C1 libraries). As the inserts for these libraries are longer, the subreads will be longer, resulting in more coverage of longer reads than the C1 chemistry sequenced 3kb libraries. See below for why this is significant. \n How does this method compare to the PacBioToCA pipeline presented by Koren et al? \n \n Error-correcting the PacBio reads as described in the Nature Biotech paper did not result in single-contig bacterial genome assemblies. In that respect, the ALLPATHS_LG approach outperforms PacBioToCA. However, no jumping libraries were used for the bacterial genomes presented in Koren et al paper. \n The PacBioToCA pipeline is computationally very intensive (more or less doubling assembly times), requiring a compute grid or big server, while the ALLPATHS_LG assemblies were done in a few hours, and with a modest server. \n Around 10x uncorrected PacBio reads were sufficient for the error-correction + assembly approach, while ALLPATHS_LG needs 50x PacBio coverage. The higher coverage requirement for ALLPATHS_LG has probably to do with the need to obtain enough long reads to be able to resolve reepats and fill gap in the unipath graph. Error-corrected long PacBio reads have by themselves enough information to yield good enough overlaps for assembly, thereby reducing the coverage needed. Note that the increased coverage using longer insert libraries and C2 chemistry might bring the ALLPATHS_LG coverage requirement down. \n Ribeiro et al report that based on 4 months using the C2 chemistry, 3 PacBio SMRTCells would be needed for 50x coverage. Compared to the 10x needed for PacBioToCA, this translates to 1 SMRTCell for this program. Not a huge difference, but still. \n It needs to be said that the Koren et al paper was entirely based on C1 chemistry, so improvements may be expected with C2 chemistry-based datasets \n For the ALLPATHS_LG approach to work, one has to tailor the Illumina libraries to it, requiring the short-insert library to yield partially overlapping reads, and the sequencing of a jumping library. The PacBioToCA error-correction approach only needs one short-insert read dataset (although assemblies do improve if jumping libraries are used). \n The ALLPATHS_LG approach is currently only available for bacterially sized genomes. In principle, as the authors notice, the program can be adapted to work for larger, and even complex eukaryotic, genomes. However, we are not there yet. As of today, the PacBioToCA error-correction pipeline is already available for large genomes, and long error-corrected reads have been shown to significantly aid assembly. The 50x coverage requirement with ALLPATHS_LG would make the approach even more prohibitively costly for large genomes than the 10x requirement for PacBioToCA. \n \n My conclusion : for bacterially sized genomes, ALLPATHS_LG is preferred \u2013 if you don\u2019t mind using their recipe. For larger genomes, stick with PacBioToCA for now, but keep an eye on what the Broad Institute is doing! \n Feel free to let me know in the comments if you (dis)agree! \n P.S. Laudably, all datasets used for the Ribeiro et al paper have been made available for us to try to replicate \u2013 or perhaps even improve \u2013 the results."], "link": "http://flxlexblog.wordpress.com/2012/08/06/combining-short-and-long-reads-choosing-between-pacbiotoca-and-the-new-allpaths_lg/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "ftp://ftp.broadinstitute.org/pub/papers/assembly/Ribeiro2012": 1, "http://flxlexblog.wordpress.com/": 3, "http://genome.cshlp.org/": 1, "http://www.nature.com/": 1, "http://www.broadinstitute.org": 1, "http://www.broadinstitute.org/": 3}, "blogtitle": "In between lines of code"}, {"content": ["Today, a paper entitled \u2018Hybrid error correction and de novo assembly of single-molecule sequencing reads\u2019 came out in Nature Biotechnology by Sergey Koren, Michael Schatz and others. In it, the authors describe a method to error-correct PacBio reads and use them in de novo genome assembly. I was gracefully given an advance copy by Mike Schatz which I used to prepare the following post. \n The PacBio RS instrument from Pacific Biosciences \u00a0gives extremely long reads (several 1000 bases), but with high single-pass error rates (85% accuracy \u2013 15% error). Alternatively, one can use the short-insert mode, where each fragment is sequenced mutliple times (Circular Consensus Sequencing \u2013 CCS), resulting in high quality, but much shorter (up to 1 kb \u2013 1000 bases) reads. \n  Source: http://atom.smasher.org/ (http://bit.ly/ORrljX) \n Even though in principle, longer reads are ideal for de novo genome assmebly, using the high-error PacBio reads natively is hard: for alignment, the between-read error rate doubles to 30%. So, the long PacBio reads would be most advantageous if the error can be overcome. This is what the authors of the Koren et al. paper try to achieve. In the following, I\u2019ll summarise their main findings. \n First, the authors tested where along the reads the error occurs,\u00a0and, as claimed by the company, there was no bias detected: the average error rate was tightly distributed around the mean along the entire read length. Also, coverage of PacBio reads over the genome they were derived from (in this case, yeast) was very even. \n Unless one uses a de Bruijn Graph approach (most approriate for short read datasets), assembly starts by finding overlaps between reads in the so-called Overlap Layout Consensus approach, or OLC assembly. So, the authors next looked at the theoretical detection of overlaps versus error rates. From the paper: \n \u201cFor both 454 and Illumina, over 80% of the overlaps are detected by 3% error. By contrast, on PacBio, only 10% are detected at 15% error\u201d \n In other words, finding overlaps between short, high quality reads and the long PacBio reads is much easier than between PacBio reads themselves. \nLong reads are expected to help most in OLC, and that is what they focussed on. \n The authors have developed a method to use the high-quality short reads, be it Illumina, 454 or CCS PacBio reads, to correct the errors in the long, low quality PacBio reads. They built a pipeline for achieving this on top of the Celera assembler. Celera was developed for Sanger type of reads, and is the only assembly program from that era that has been adjusted to be able to tackle the newest technologies as well (first 454, then Illumina, and now PacBio). \nThe pipeline, called PacBioToCA, is available with the latest Celera release, see this link . \n How does this approach work? \n- first, create long, single-pass PacBio reads and short, high quality Illumina or 454 reads, or PacBio circular consensus reads \n- map the short reads to the long reads, start by finding perfect 14 nucleotide overlaps \n- for PacBio reads containing repeats, choose the mapping which maximizes the identity between the short read and the PacBio read \n- build consensus (using AMOS) for the long read based on the aligned short reads, breaking the long reads when there is no coverage with short reads \n Special care is needed when dealing with repeats, and the pipeline tackles this by trying to divide the short reads among the different copies in such a way as to maximise the chance that the correct repeat copy is reconstructed in the end. In an email, Mike Schatz told me he expects repeats that are up to 99% identical to be corrected to the right sequence. This is, however, dependent on the eveneness of both the read coverage and the errors in the PacBio reads. \n Error correction yields slightly shorter reads, due to some regions not having (enough) short-reads coverage. Also, some reads are too short to be included for correction. In total, about 50%-60% of the raw PacBio reads end up in the corrected set. But still, after correction, one has a long-read dataset, with many reads of several Kbp and with >99% accuracy! \n The pipeline was tested on lambda, E. coli , yeast and parrot (from assemblathon2 ) datasets, with 50X of Illumina reads for the correction. 50X was determined the sweet spot for short-read coverage.\u00a0From the paper: \n \u201cThe accuracy of the long reads improved from \u2248 85% to over 99.9%, and chimeric and improperly trimmed reads measured < 2.5% and < 1% respectively\u201d \n Next, these corrected reads were used in hybrid de novo assembly using Celera. For this, the maximum input read size in Celera had to be increased to deal with long- error-corrected reads. Except for parrot, reference genomes could be used to check the resulting assemblies. \n The effect of adding corrected PacBio reads were very promising. Up to tripling of contig sizes (as measured by contig N50\u2032s) were found, without introducing more errors (The N50 size is the size such that half tot total length is in contigs minimally of this length). For bacteria, contig N50s of several hundred Kbps were obtained; one assembly even had a contig N50 of more than 0.5 Mbp. \nThese gains were mainly due to resolving long repeats. As repeats are the biggest problem for reconstructing genomes from reads, this is a significant finding. The authors simulations showed that with increases in read length, single contig (!) assembly for E. coli type bacterial genomes are possible using this approach. \n For the large heterozygous parrot genome assembly, the best assembly was obtained using 454 + PacBio reads corrected using Illumina data, with a contig N50 of almost 100kbp. This large contig N50 is usually very hard to obtain!\u00a0The parrot Illumina only (using ALLPATHS_LG ) and 454 only (using Celera) assemblies with optimal read datasets had contig N50\u2032s of 47Kbp and 75 Kbp, respectively. Mapping known transcripts to the contigs and scaffolds showed good quality gene reconstructions in the assemblies with error-corrected PacBio reads. Interestingly, ALLPATHS_LG managed to assemble and scaffold more exons then Celera. Notable was also that the >70% GC promotor region in the ERG1 genes was only assembled without a gap using PacBio reads. \n So,\u00a0assemblies done using lower coverage PacBio corrected reads can outperform higher coverage short read only assemblies, even without the use of mate pairs. Hybrid assemblies reached maximum N50\u2032s at around 10X cov of corrected PacBio reads. \n Finally, the authors show how error-corrected PacBio reads also aid in transcriptome assembly, and can help in splice variant discovery. \n Conclusions \nThe high error-rate of the single-pass PacBio reads results in a good deal of skepsis in the community on how useful this technology is. With this paper, a first step is made to show that in combination with other technologies (or CCS reads), long PacBio can be very useful for de novo genome assembly, also of large complex genomes. Significantly longer contigs can be obtained, and single-contig bacterial genome assemblies may become possible in the near future with modest increases in PacBio raw read lengths. \nFor de novo assembly, long PB reads can be an attractive alternative to mate pairs. \n There are a few drawbacks of using this approach. The error-correction is computationally intensive, requiring a good dose of CPUs. The correction of the parrot data took a week. If one were ever to generate a suitable long-read PacBio dataset for the human genome, the authors estimated error-correcting these to take 10 days on 250 cpus. So, adding PacBio reads and error-correcting them effectively doubles assembly times. Also, not all labs may have access to the required compute resources. Another problem is the low throughput (50-60% of the reads survive the error correction). Here, methods to overcome the low short-read coverage (which breaks up some long reads) are needed. \n My take \nI am very excited by this paper. It shows how the main problem people have with PacBio, the high error-rate, can be overcome. Using the PacBio, combined with cheap, short reads, can result in reads up to several Kbp of very high quality. This is a dream for everyone doing de novo genome assembly. I also have hopes that these long reads, with appropriate tools, can help assembling heterozygous genomes, and aid in in haplotype separation. As raw PacBio read lengths overlap the lengths of whole transcripts, de novo transcriptome assembly using this approach may give much more full-length transcripts assembled then is currently possible. \n I think we have now seen the first of hopefully more approaches to correct PacBio reads. I would think other smart bioinformaticians are now inspired to search for less computationally-intensive methods (and faster) for correction. \n Many people talk about how nanopore-based extremely long read technologies will make PacBio obsolete in the near future. I keep saying, seeing is believing (who outside the companies has real data in their hands?). And, if you want long reads today , PacBio is your only option. \n Pacific Biosciences needs papers like this to show that their technology actually is going to deliver. Now it is up to the research community to use these long high-quality reads for de novo assembly of complex genomes. This is something we in our group have recently started with. \n Links \nDatasets, recipies etc can be found here ."], "link": "http://flxlexblog.wordpress.com/2012/07/01/paper-error-correcting-pacbio-reads-using-high-quality-short-reads/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.umd.edu/": 1, "http://sourceforge.net/": 2, "http://pacificbiosciences.com/": 1, "http://www.nature.com/": 1, "http://assemblathon.org/": 1, "http://flxlexblog.wordpress.com/": 1, "http://www.broadinstitute.org/": 1}, "blogtitle": "In between lines of code"}, {"content": ["\u201cLoman et al reflects the past, not the present\u201d says Life Technologies/ Ion Torrent in a slide set accompanying a response , published yesterday,\u00a0to the recent paper by Nick Loman et al,\u00a0\u201cPerformance comparison of benchtop high-throughput sequencing platforms\u201d ( Loman et al, 2012 ). See also my coverage of this paper in my previous blog post . \n  Image credit: technorati.com http://bit.ly/uSYZIb \n It is a critique I have read and heard more often: the data used for the analyses in the Loman et al paper is already old, as the technologies have now improved. This is of course true, particularly so for Ion Torrent. However true, it is not a fair critique. Researchers, and Nick Loman and yours truly are not an exception, are bound by the \u2018publish or perish\u2019 mantra. We are dependent on publishing peer-reviewed articles for obtaining grants, establishing our reputation, and for getting our next job. Peer review takes time: \u201cRight now the time lag between finishing a paper, and the relevant worldwide research community seeing it, is between 6 months and 2 years.\u201d ( source ). Nick\u2019s paper was \u2018Received 19 December 2011\u2033, \u201cAccepted 30 March 2012\u2033 and finally \u201cPublished online 22 April 2012\u2033. This is actually quite fast, taking into consideration the authors developed numerous new tools for the analyses (see the github repository accompanying the paper). \n Sure, we can use a blog to circumvent the time delay, and publish a finding\u00a0immediately, something Nick is\u00a0actively\u00a0doing \u00a0(as am I through this blog). But, sometimes we need to go the peer-review route, for the reasons explained above. \n It is therefore unavoidable that articles, like the one from Nick Loman, contain \u2018old\u2019 data. Heck, there are still lot\u2019s of papers coming out based on 454 GS FLX and Illumina GA II(x) data. In addition, there must be many groups analysing IonTorrent/454/Illumina data they obtained using the same \u2018generation\u2019 of kits as were used for the Loman et al paper. These people will\u00a0absolutely\u00a0want to know about the different error types and accuracy levels. At the very least, the data presented in the paper give an overview of the relative performance\u00a0 at the time of studying , which might reflect on today\u2019s performance. \n I appreciate Ion Torrent people jumping on the occasion, requesting a sample from the strain used for the paper, sequencing with their latest chemistry, and redoing some of the analyses, all in less than three weeks. The results look promising, although they need to be quality checked by the community (bloggers like us, I guess ). But don\u2019t blame the messenger for taking the established route. Let\u2019s rather congratulate Nick Loman et al for a job well done and a well deserved publication in Nature Biotechnology! \n Agree? Disagree? Feel free to drop me a comment below!"], "link": "http://flxlexblog.wordpress.com/2012/05/09/loman-et-al-reflects-the-past-not-the-present-a-rebuttal/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://techcrunch.com/": 1, "http://flxlexblog.wordpress.com/": 3, "http://www.iontorrent.com/": 1, "http://www.nature.com/": 1, "http://pathogenomics.ac.uk/blog": 1}, "blogtitle": "In between lines of code"}, {"content": ["Nick Loman was kind enough to give me an advanced copy of his paper in Nature Biotechnology entitled \u201cPerformance comparison of benchtop high-throughput sequencing platforms\u201d ( Loman et al, 2012 ). I thought to present a quick summary of the paper here and add some comments of my own. \n The paper sets out to \u201ccompare the performance of three sequencing platforms [Roche GS Junior, Ion Torrent PGM and Illumina MiSeq] by\u00a0analysing\u00a0data with commonly used assembly and analysis pipelines.\u201d To do this, they chose a strain from the\u00a0outbreak of food-borne illness caused by Shiga-toxin-producing E. coli O104:H4, which caused a lot of trouble in Germany about a year ago. The study is unique in that it is focuses on the use of these instruments for de novo sequencing, not resequencing. \n First, they used the \u2018big brother\u2019 of the GS Junior, the GS FLX, to generate a reference genome (combining long reads obtained using the GS FLX+, and mate pairs using Titanium chemistry). Then, the same strains were sequenced on the benchtop instruments, and these reads were compared to the reference assembly. The reads were both compared directly, and after assembly with a few commonly used programs. \n This is the first study for which the same strain was sequenced on three different instrument, which makes for an interesting comparison. So, what did they find? Ignoring run costs, throughput and all that for the moment, I\u2019ll focus on the quality of the reads and assemblies first. The main findings regarding reads were: \n \n the reads from all platforms covered the genome very evenly \n read quality scores, recalculated based on alignment to the reference, showed the MiSeq to be superior \n Ion PGM data showed underestimated raw read quality scores, the GS Junior and MiSeq slightly overestimated them \n Ion Torrent showed \u00a0about four times more indel errors (when calculated per 100 bases) compared to the 454, these errors were\u00a0practically\u00a0absent from MiSeq data \n Ion showed significantly more homopolymer errors than 454 \n \n Many people (including me) skip the supplementary sections of paper, but in this case I suggest to\u00a0pay particular attention to supplementary figure 3 for a comparison of indel errors relative to homopolymer length. Here, Ion Torrent doesn\u2019t look so good for longer ones. \n Having reads of with errors is one thing, but in a way more interesting is if there exists software that can overcome, or compensate for, the errors. The article describes thorough comparisons of assemblies, using Velvet, MIRA, CLC Assembly Cell, and newbler, with all the data that were generated. Here the main findings were: \n \n the least fragmented assemblies were generated by combining two GS Junior runs (assembly with MIRA and newbler), or by using the paired end information in the MiSeq reads (assembly with CLC or Velvet) \n assemblies with Ion Torrent data lagged behind and were much more fragmented \n other combinations (single 454 runs, not using paired end information) resulted in fragmented assemblies as well \n GS Junior based assemblies covered more of the reference genome then the others \n the newbler program (GS Junior and Ion PGM data) showed the\u00a0least\u00a0amount of misassemblies \n although MIRA contigs covered more of the reference, they showed more misassemblies \n Ion PGM based assemblies showed many more gaps due to homopolymer errors, even for short ones (2-3 mers), then GS Junior based assemblies \n newbler showed less homopolymer-associated gaps for 454 data then MIRA \n with Ion PGM data, newbler handles correcting read errors in homopolymer tracts >3 bases better than MIRA, for short homopolymer tracts, the situation is reverse \n homopolymer-associated gaps were basically absent in the MiSeq based assemblies \n gaps not associated with homopolymers were present in about equal number regardless of dataset or assembler \n \n One potential problem with these analysis is that the reference genome was generated using 454 technology and newbler, which could potentially give the GS Junior, especially its newbler assemblies, and advantage. When I asked Nick Loman about this, he pointed to the supplementary tables 1-3, which show results of the mapping of reads against two other reference genomes, one generated with PacBio+Illumina, and the other with Sanger reads. The indel error rates for these mappings are very similar, indicating that the choice of reference should not mater here. I tend to aree, but it would still be interesting to check the homopolymer associated gaps of the assemblies against these other references. Finally, unfortunately, the runtimes of the different assemblies were not included in the article, this would have been useful information for those of us interested in the\u00a0 fastest results. \n Perhaps more important than the correctness of an assembly is the biological value it has. To that end, the assemblies generated were tested for the presence of 31 pathogenetically important protein coding genes, and 7 genes used for MLST typing: \n \n the MiSeq-based assemblies were more complete (at least 92% of the genes found full-length) than those based on the other technologies (less than 87% found) \n there were clear differences between the different assembly programs in how well they reconstructed gene space; for example, Velvet was much worse to assemble a certain type of gene with Illumina data relative to CLC and MIRA \n \n Conclusion \n So, which instrument is best? As usual, there can not be a single winner, as what is defined is \u2018best\u2019 is dependent on the requirements one has. The authors also don\u2019t pick a winner, which I fully understand. For my own, and the readers of this blog\u2019s sake, I though to summarise the specs of the instruments, and the findings of the paper, in a table. I placed an \u2018X\u2019 for each instrument that performed \u2018best\u2019 for the particular feature: \n  \n So: \n \n if you want fast and cheap reads from a cheap instrument, buy an Ion PGM (but you\u2019ll get \u2018dirty\u2019 data) \n if you lot\u2019s of high quality, short read data without too much hassle in the lab, choose a MiSeq (but be prepared to wait for the data) \n if you want long reads for high quality assemblies, use a GS Junior (but prepared to pay a lot) \n \n Final remarks \n I applaud Nick and coauthors for making all raw data, assemblies, analysis scripts and results available online: through the Sequence Read Archive, and a github repository . \n Also, check out the interviews with Nick Loman\u00a0by \u2018Nature News and Views\u2019\u00a0 here , and by fellow blogger James Hadfield here ."], "link": "http://flxlexblog.wordpress.com/2012/04/22/fast-genome-sequencing-of-pathogenic-bacteria-which-benchtop-instrument-to-choose/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.nature.com/": 2, "https://github.com/": 1, "http://flxlexblog.wordpress.com/": 2, "http://core-genomics.blogspot.com/": 1}, "blogtitle": "In between lines of code"}, {"content": ["(The impatient reader might want to skip to the conclusion at the end of this post\u2026) \n Last wednesday, Ion Torrent released a tech note and associated run data with shotgun (single-end) and Mate Pair runs for Escherichia coli K12, substrain MG1655. Both a 3.5 kb and 8.9 kbp insert size, as well as a shotgun library, were sequenced on a 316 chip each. In the tech note, they describe assemblies using different combinations of the data, and show how adding the mate pairs yields assemblies with fewer scaffolds and gaps. The Ion mate-pair protocol is very similar to the one used by 454 Life Sciences for their (unfortunately called) Paired-end libraries: long fragments are circularized using a linker sequence, and sequencing is peformed across this linker, allowing for easy identification of the pair halves. \n This is the first real \u2018long-distance\u2019 Mate Pair data from Ion Torrent, which is exciting and made me have a close look at it. I was especially interested in how the newbler program, developed by 454 Life Science for their 454 reads, would perform on these data. \n Mapping \n First, some impressive numbers: the mate pair runs yielded 2.5 \u2013 3 million reads (average lengths 192 and 200 bp). When you generate these kinds of libraries for bacterial genomes using 454, one would ordinarily sequence 1 lane of a plate divided into 8 lanes, and get less then 1 tenth of the throughput. \n I first mapped the mate pair runs to the reference genome sequence using gsMapper from the newbler software suite. I immediately came across the first problem: the linker sequence used by Ion is different from the one used by 454. This means that newbler does not recognize the reads as mate pairs. I am really hoping a future version of newbler will allow for custom mate pair linker sequences, but given that Ion Torrent is a big competitor for Roche/454, I doubt this will ever happen\u2026 \n So, I resorted to what Ion also used: taking the sff_extract program to split the mate pair reads, and outputting them as fasta and qual files. Examining the results showed that 37% of the reads from the 3.5kb run were mate pairs (had the linker according to sff_extract), and 25% for the 8.9 kbp run. The remainder of the reads were shotgun reads. As for the 454 mate pair protocol, there is always going to be a fraction of non-linker-containing reads. However, the fractions for the Ion runs are considerably higher than what we usually get for 454 mate pair (\u2018paired end\u2019) runs. However, the higher throughput with Ion Torrent more than compensates for the low fraction of mates. \n After extracting the pair halves, I adjusted the files so that newbler would accept them as mate pairs, and performed mapping using newbler (gsMapping/runMapping program). Then I plotted the distances between the mapped mate halves for both libraries: \n \n The figure shows a nice distribution with sharp peaks around 3.3kb and 9kb, respectively, confirming the mapping shown in Ion Torrents tech note. \n Assembly \n Next, I set out to perform assemblies. Ion Torrent provided files with reduced amounts of reads so that there would be about 20X and 40X coverage of the 5.4 Mbp genome. I decided to use these files, rather than make my own scaled down versions. I do note, however, that the 20X and 40X files of the mate pair library runs mostly contain mate pair reads, and very few singletons (based on the sff_extract results). Whether Ion determined which reads were mate pair reads by mapping to the reference genome, or by using sff_extract, and only taking reads that were split along the linker, I cannot tell (in a real de novo setting, the second option is the only possible, of course). \n The table below shows metrics for five assemblies. The first two are described in the Ion Torrent tech note as being done using MIRA for contig building and SSPACE for scaffolding: \n- the first column is for the shotgun (single-end, fragment reads) only contigs \n- the second column for the scaffold. \nColumn 3 to 5 describe the three assemblies I produced for this post using newbler: \n- using the shotgun reads only (40X coverage) \n- using shotgun (20X) plus the 3.5 kb and 8.9 kb mate pair reads (20X each) \n- using shotgun (20X) plus the 3.5 kb and 8.9 kb mate pair reads (20X each) but with the addition of the \u2018scaffold\u2019 option; newbler here fills gaps caused by repeats with a consensus sequence of the repeat copies \n  \n Newbler manages to generate a single scaffold for the this genome! (the second scaffold in the \u201920X SG+20X MP\u2019 assmbly is a contig larger than 2 kb, which newbler automatically lists under the scaffolds). However, the contigs produced by MIRA are longer than those produced by newbler. Also, MIRA assembled the genome into fewer contigs compared to newbler, and with less gap bases. Looking closer at the MIRA/SSPACE scaffolds shows that the genome is in fact assembled in four large pieces of 2.3 Mbp, 1.3 Mbp, 700 kbp and 200 kbp, with another 10 kbp scaffold (rRNA?) and 17 very small scaffolds. \n I tried different assembly strategies, but I could not improve on the N50 and longest size reported here. By the way, the N50 contig size in brackets is for the scaffolded contigs in the \u2018-scaffold\u2019 assembly, where gaps have been filled using consensus repeat sequences, thereby merging contigs into single, bigger ones. \n Reducing the coverage to 20X shotgun + 10X mate pair (each library), or even 20X shotgun and only the 8.9 kbp mate pair reads also resulted in a single scaffold for the genome (\u2018not shown\u2019). \n Now, it is nice to have \u2018better\u2019 assemblies in terms of contig or scaffold lengths, but how good are these, really? Luckily, we can compare the assemblies to the reference genome. For this, I used\u00a0 Mauve Assembly Metrics \u00a0(see also Nick Loman\u2019s post about this program here ). (Due to a bug in the Mauve, at least that is what I think caused the crash, I could not get the \u2018-scaffold\u2019 newbler assembly analysed). The program gives a lot of results, but I will only share the most important ones here. First, the alignment of the scaffolds against the reference (scaffolds reordered relative to the reference genome for the Ion Torrent assembly). Newbler scaffolds: \n  \n MIRA/SSPACE (done by Ion Torrent) scaffolds: \n  \n Whereas the alignment is perfect for the single newbler scaffold, the MIRA/SSPACE scaffolds show numerous misassemblies (vertical red bars depict scaffold boundaries). \n Some numbers from Mauve: \n  \n SNPs seems to be the sum of uncalled bases \u00a0(\u2018N\u2019s) and miscalled bases. Newbler seems to be able to assemble with fewer miscalled bases, but with many more uncalled bases (although Mauve reports far fewer than the assembly does). I don\u2019t really understand the difference between gaps in the reference and in the assembly. MIRA misses fewer bases (gaps), but it\u2019s contigs (not it\u2019s scaffolds) have more extra bases (insertions). Also, the set of newbler contigs contains some few (probably very small) contigs that could not be mapped to the reference genome. Finally,\u00a0the number of intact and broken CDSs (coding sequences of genes)\u00a0numbers don\u2019t surprise me, as the contigs of MIRA are so much longer, they are able to capture much more of the gene space. MIRA contigs already have 98% of the CDSs complete. \n Conclusions \n Ion Torrent seems to have done a good job in enabling mate pair sequencing on their platform, with nice tight size distributions, and impressive throughput relative to 454. These mate pair reads, together with the shotgun (single end) reads, are very useful for de novo assembly. The de novo assembly approach Ion Torrent chose, using sff_extract, MIRA and SSPACE, seems to be giving quite long contigs, with almost all genes complete. However, newbler outperfoms SSPACE in scaffolding. Newbler is able to assemble the reads into a single scaffold, even with shotgun reads only supplemented with the 8.9 kb mate pairs. However, newbler\u2019s algorithm is not able to produce as long contigs as MIRA does. So, a viable strategy for de novo assembly, using Ion Torrent shotgun (single end) plus mate pair reads, would be to generate contigs with MIRA, contigs and scaffolds with newbler, and elongate the newbler contigs with the MIRA contigs to reduce the number of gaps in the newbler scaffold(s). What is missing from this analysis, though, is a comparison of a similar assembly using 454 reads, something I hope to be able to do in the near future\u2026 \n With adding the possibility of long insert mate pair library construction and sequencing, Ion Torrent\u00a0has moved into another niche of Roche/454 Life Sciences. With the fast run time and high throughput of the PGM relative to the 454, Ion Torrent is on its way to become a viable option for de novo small genome sequencing. Apparently, making longer insert libraries (> 10kb) is cumbersome using the Illumina protocols, but not so much with the 454 version, and hopefully also feasible with the Ion Torrent protocol. In addition, the use of a linker allows for easy separation of the mate halves relative to the linker-less Illumina mate-pair reads. It remains to be seen, though, if research groups are able to generate these mate pairs independent of the Ion Torrent labs. \n Code Used \n(As always, let me know in the comments if things are unclear of missing, or didn\u2019t work out for you.) \n Data and programs \nI downloaded some of the sequencing run files mentioned in the\u00a0 tech note , and unzipped them. For the \u2018whole run\u2019 files, I needed a newer version of unzip than the default installed on our system. So, I downloaded that from\u00a0 http://www.info-zip.org/ . I also downloaded the MIRA assemblies, the linker sequence file and the reference sequence, but for the annotated version (for Mauve), I went to Genbank instead. \nI download sff_extract 0.2.13 and, since it needs this program to find the linkers,\u00a0 ssaha 2.5.5 . I also installed Mauve 2.3.1 \u00a0and Mauve assembly metrics scripts . \n Preparing the reads \n For getting the mate pairs split along the linker, I used (for runs\u00a0FRA-257 and\u00a0C28-140) \n sff_extract_0_2_13 -c -l LMP_Linkers.fasta somefile.sff \n \n The \u2018-c\u2019 option ensures the clipping points present in the sff file are used to give trimmed sequences. This takes quite some time and uses a lot of space on /tmp (I could only run one sff_extract at a time). This gives three files: a fasta file, a corresponding qual file, and an xml file which I didn\u2019t need (but MIRA, for example, needs it). \n The sequence headers of the resulting fasta files look like this: \n >PT1DY:2303:781.r \n>PT1DY:2303:781.f \n \n(for the forward and reverse half, respectively). \n In order for newbler to accept these reads as paired (mate pairs), they need to look like this: \n >PT1DY:2303:781.r template=PT1DY:2303:781 dir=r library=FRA-257_3.5kb \n>PT1DY:2303:781.f template=PT1DY:2303:781 dir=f library=FRA-257_3.5kb \n \nI used the following command to change the headers: \n cat FRA-257_3.5kb.fasta |awk '{if (!/>/){print $0} \nelse{split($1,e,\".\"); print $1\" template=\"substr(e[1],2)\" dir=\"e[2]\" library=FRA-257_3.5kb\" \n}}' >FRA-257_40X_for_newbler.fasta \n Same for the corresponding \u2018qual\u2019 file.\u00a0 Important : in order for newbler to incorporate the quality vaules, the qual file needs to have the same name, with \u2018fasta\u2019 replaced by \u2018qual\u2019, so in this case \u2018FRA-257_40X_for_newbler.qual\u2019 \n I adjusted the \u2018library=\u2019 accordingly for the 8.9 kbp mate pair reads. \n Read statistics \n In order to get the statistics for the percentage of mate pair reads, I used the sff_extract output. this program will annotate reads as being \u2018f\u2019 and \u2018r\u2019, mate par halves, \u2018fn\u2019, no-mate pair read (regular shotgun read), and \u2018part#\u2019, when only a partial linker was found.\u00a0As an example, for the full run for the\u00a03.5 kbp library,\u00a0FRA-257, I used these commands. \n To summarize the different sff_extract \u2018categories\u2019 (with output from the command): \n cat FRA-257.fasta |awk '/>/ {split($1,a,\".\"); x[a[2]]++}END{for(i in x){print i\"\\t\"x[i]}}' \nfn 1557426 \nf 880617 \nr 880617 \npart1 23495 \npart2 22337 \npart3 5 \n \nTo get the total number of unique reads: \n cat FRA-257.fasta |awk '/>/ {split($1,a,\".\"); x[a[1]]++}END{for(i in x){print i\"\\t\"x[i]}}' |wc -l \n2461538 \n \nSo,\u00a01557426 \u2018fn\u2019 reads out of\u00a02461538 = 63%, leaving 37% of reads being mate pairs. \n Mapping \n Using newbler 2.6, and with the provided reference file (MG1655.fasta), I ran: \n runMapping -o map_all_matepair MG1655.fasta\u00a0FRA-257_for_newbler.fasta\u00a0C28-140_for_newbler.fasta \n \n(newbler will recoginze the \u2018.qual\u2019 file if has the same name, and is located in the same folder as the \u2018.fasta\u2019 file). \n For plotting the insert distances, I used the following R script: \n # extract from the 454PairStatus.txt file: \n# - the first 5 characters from the read name \n# - the pair-halves distances \n d = read.table(pipe(\"cat 454PairStatus.txt | awk '$3>0{print substr($1,1,5),$3}'\"), header=T) \n # 3.5 kb library \nh3=hist(d[d$Templ==\"PT1DY\",2],breaks=0.25*max(d$Distance),plot=F) \npeak3=h3$mids[which(h3$counts==max(h3$counts))] \n # 8.9 kb \nh9=hist(d[d$Templ==\"6S604\",2],breaks=0.25*max(d$Distance),plot=F) \npeak9=h9$mids[which(h9$counts==max(h9$counts))] \n pdf(\"AllPairDist.pdf\") \nplot(h3$mids,h3$counts,type=\"l\",xlim=c(0,12000),main=\"Mapped paired-end distances\",xlab=\"distance (bp)\", ylab=\"Count\") \nlines(h9$mids,h9$counts, col=\"blue\") \nabline(v=peak3,lty=6,col=gray(0.4)) \ntext(peak3,100,labels=round(peak3)) \nabline(v=peak9,lty=6,col=gray(0.4)) \ntext(peak9,100,labels=round(peak9)) \nlegend(x=\"topright\", legend=c(\"3.5 kbp, FRA-257\",\"8.9 kbp, C28-140\"),text.col=c(\"black\",\"blue\"),bg=gray(1)) \ndev.off() \n \nNote how, instead of reading in the entire\u00a0454PairStatus.txt file, I use the \u2018pipe\u2019 trick of R to get the output of a shell command instead\u2026 \n Assemblies \n Using again newbler 2.6, I ran, for instance: \n runAssembly -o 20xSG+20xMP \u00a0-p FRA-257_20X_for_newbler.fasta\u00a0-p C28-140_20X_for_newbler.fasta\u00a0C11-127_20X.sff \n The \u2018-p\u2019 flag forces newbler to trat the files as mate pairs (paired end, in newbler technology). I used the native sff file for the shotgun run. \n Assembly statistics were obtained using\u00a0my newblermetrics script, MIRA assembly metrics were from the tech note. \n Mauve \n I first collected the genbank reference file, and relevant assemblies (either contigs, or scaffolds) in a single folder, but rather then copying the files, I made symlinks to them. Then, I used a shell script to run this programn, as per the website: \n export PATH=$PATH:/path/to/mauve_snapshot_2011-07-18 \nexport CLASSPATH=\"/path/to/mauve_snapshot_2011-07-18/ext/*\" \n # run the scoring \njava -cp /path/to/mauve_snapshot_2011-07-18/Mauve.jar org.gel.mauve.assembly.ScoreAssembly -reference NC_000913.gbk -assembly Ion.fa -reorder Ion -outputDir Ion \njava -cp /path/to/mauve_snapshot_2011-07-18/Mauve.jar org.gel.mauve.assembly.ScoreAssembly -reference NC_000913.gbk -assembly newbler.fa -reorder newbler -outputDir newbler \n # Generate plots of metrics \n# ./ indicates the parent directory containing the assembly scoring directories -- the current working directory in our example \nmauveAssemblyMetrics.pl ./ \n I opened the \u2018alignmentx\u2019 file in mauve from the \u2018alignmentx\u2019 folder with the highest number. The table was based on the \u2018summaries.txt\u2019 files produced."], "link": "http://flxlexblog.wordpress.com/2012/03/02/ion-torrent-mate-pairs-and-a-single-scaffold-for-e-coli-k12-substr-mg1655/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://asap.wisc.edu/": 1, "http://sourceforge.net/": 1, "http://flxlexblog.wordpress.com/": 5, "http://lifetech-it.jivesoftware.com/": 3, "ftp://ftp.sanger.ac.uk/pub4/resources/software/ssaha2/ssaha2_v2.5.5_x86_64.tgz": 1, "http://code.google.com/": 2, "http://pathogenomics.ac.uk/blog": 1, "http://www.info-zip.org/": 1, "http://bioinf.upv.es/": 1}, "blogtitle": "In between lines of code"}, {"content": ["Each sequencing company has a workhorse genome they sequence a lot. PacBio sequences the lambda virus, Illumina uses PhiX. Both Ion Torrent and 454 use E. coli DNA, but while Ion Torrent takes E. coli K12 substr. DH10B, 454 chose E. coli K12 substr. MG1655. \n I am interested in finding out to what extent Ion reads can replace the much more expensive 454 reads for de novo genome assembly (my field of speciality). Currently, the Ion read length is too short for the technology to be competitve, but this might change later this year, when (if \u2026) the promised 400 bp reads become a reality. \n In my comparisons of Ion data with 454 reads, I have always been hampered by the the fact that the strains the platforms use as test samples were not completely identical.\u00a0Luckily for me, today Ion Torrent\u00a0 released \u00a0(behind the Ion Community login) a dataset on E coli MG1655, a run with ID BEL-335. Imagine my joy! (Saves me from having our centre generate one on our \u2013 yet to be unpacked- PGM). The data is from a 314 chip, has 468 thousand reads and 54 Mbp raw data. That represents around 11x coverage of the MG166 genome. This is a bit too low for what I ideally would like to have (around 30 x), but alright. I set out to try this data in a de novo assembly using newbler, together with an equivalent data set of 454 reads. \n First, I checked the read length distribution (line marked \u2018BEL-335\u2032). \n  \n Note the small peak at 193 bases, such peaks are somehow usually present for Ion Torrent data. \n I next created a comparable 454 dataset. I used the same GS FLX sff file as for previous posts, resetting the number of cycles to 48 \u00a0(shortening the GS FLX reads)\u00a0to get the same length distribution, which is shown in the plot above as well. Note how the peak of the Ion reads is slightly narrower (a tighter distribution around the mode length). Finally, I randomly picked the same amount of bases as in the Ion dataset, to get the same coverage. \n Assemblies were done with the latest release of newbler (2.6) and default settings. As no paired end (mate pairs) were available, all that was generated were contigs. This is the first time I was able to do a direct comparsion of these data from the exact same genomic source. \n The table below summarizes the metrics (\u2018large\u2019 contigs are those of at least 500 bp): \n  \n As the table shows, newbler performed much better with the 454 data than with the Ion data, even though the read dataset was as much comparable (length and coverage) as possible. The Ion data resulted in more reads partially assembled, more singletons, and more, shorter contigs. Interestingly, the total length (\u2018all contigs\u2019) is very similar between the two assemblies. Both were 94 kbp short of the \u2018real\u2019 genome size of\u00a04,639,675 bases, probably caused by collapsed repeats. \n This exercise is of course not completely fair: I am using the assembly program optimized for 454 reads with reads from another technology. However, the results show that when it comes to de novo genome assembly, one cannot simply replace 454 reads with Ion Torrent reads and expect the same results. Assembly software needs to be tuned to the technology platform specific error model, even when the sequencing technologies are very similar (454 and Ion both measure homopolymer length, and both suffer from homopolymer length errors). \n The next step for this comparison would be assemblies using other programs, especially those that say they are tailored towards Ion data, or both Ion and 454 (and other types). Candidates are Mira (free), the (commercial) \u2018Floton assembler\u2019 from Softgenetics, CLCBio (also commercial), and I probably missed a few more. \n Code used \n The sff file is called BEL-355 and was downloaded via http://lifetech-it.hosted.jivesoftware.com/docs/DOC-2492 (Ion Community login required). \n Read length distribution (using sffinfo from the Roche/454 software suite): \n sffinfo -s BEL-335.sff|fasta_length | awk '{x[$1]++}END{for (i in x){print i\"\\t\"x[i]}}'| sort -n >read_length_hist.tsv \n \n fasta_length is a simple perl script which spits out the length of each fasta file (available upon request) \n 454 data: I used the files\u00a0EBO6PME01.sff and\u00a0EBO6PME02.sff, which were part of a demo GS FLX data set that came with software DVD for newbler 1.1.03. To reset the trimpoints to 48 cycles (yielding a read length distribution comparable to the Ion reads), I used \n sfffile -c 48 -o EBO6PME01_48_cycles.sff EBO6PME01.sff \nsfffile -c 48 -o EBO6PME02_48_cycles.sff EBO6PME02.sff \nsfffile -o EBO6PME_48_cycles.sff EBO6PME01_48_cycles.sff EBO6PME02_48_cycles.sff \n The last command combines the reads into one file. The read length distribution was obtained as for the Ion reads (see above). \n To get the final sff file for assembly, with the same amount of bases as the Ion dataset: \n sfffile -pick 54292729 -o EBO6PME_48_cycles_54Mbp.sff EBO6PME_48_cycles.sff \n Assemblies were run with: \n runAssembly -o outputdir reads.sff \n \nMetrics were obtained with my newblermetrics script."], "link": "http://flxlexblog.wordpress.com/2012/02/01/ion-torrent-data-on-e-coli-k12-mg1655-a-fairer-comparison-with-454/", "bloglinks": {}, "links": {"http://sourceforge.net/": 1, "http://flxlexblog.wordpress.com/": 2, "http://lifetech-it.jivesoftware.com/": 2, "http://feeds.wordpress.com/": 1}, "blogtitle": "In between lines of code"}, {"content": ["Supposedly, if you need long reads of high quality and throughput, you should be using GS FLX+, yielding 750-800 peak read length, from Roche/454, right? Illumina reads are getting longer, but 250 is for now the limit (notwithstanding the 300 bp run done by the broad ). IonTorrent promises 400 bases, but we will have to see what the quality is going to be. And PacBio, well we all know they are long, but with relatively low throughut and quality peaking at 85% \u2013 86% accuracy (useful nonetheless). \n Commercially, GS FX+ had been around for more than half a year. So far, the community is reporting some success, see this thread at SeqAnswers. But, there are problems all around when you talk to people. Our own centre ( the Norwegian Sequencing Centre ) got the upgrade in August. I\u2019ll spare you the details on numerous control/test fragment runs, short read runs etc, but the bottom line is that we still haven\u2019t been able to sucessfully sequence a FLX+ library on our GS FLX+. Right now, we are having a service visit, and I intend to chain the guy to the instrument until we see some real good data from one of our own libraries\u2026 \n There is another thing that surprises me too: I am attending PAGXX, the Plant and Animal Genome conference in San Diego, where Roche is one of the main sponsors. However, when you look at the little text they have as an exhibitor, well, let me just quote a part of it for you ( source ): \n \u201cRoche Applied Science introduces the latest innovations in reagents and instruments for genomics research with the launch of our new Titanium reagents generating 400 to 500 bp sequencing read lengths using 454\u2019s ultra-fast pyrosequencing technology for whole genome sequencing projects.\u201d \n What? \u2018latest innovation\u2019, \u2018our new Titanium reagents\u2019, \u2019400 to 500bp\u2019? Nothing about GS FLX+, 700-800 bp, let alone 1000 bp reads? Surely, this is a marketing glitch, right, just somebody who forgot to update the standard text. Well, today I passed the booth Roche has here at PAGXX. Sure, there was the obvious GS FLX instrument. But, to my amazement, it was a GS FLX, NOT a GS FLX+! You can tell since the loading bay for reagents is bigger on the GS FLX+, and it says \u2018GS FLX+\u2019 on it as well. The exhibited instrument had none of that. \n Should we read anything into this? Is there no pride in the Roche system about their uniquely long reads? Surely there is, they have talks around the longer reads, for example here and here ,\u00a0and a workshop with GS FLX+ in the title: \u201cDe Novo Assembly of Complex Genomes: The GS FLX+ System Makes a Difference.\u201d \n The buzz at PAGXX is not very positive on GS FLX+ either. For example, I talked to a core facility head that complained a lot about how their machine didn\u2019t want to run GS FLX+ after the upgrade either. They repeatedly had to have them fixed, or even exchanged. And apparently, they are not the only ones. \n I have been, and still are, a big believer in the 454 technology, and have used it\u00a0successfully\u00a0for my research. Our core facility has great experience with the instruments and our users are generally very happy with the data. But, the current developments worry me. The niche 454 has always been the longest possible high-throughput (next-gen) reads. But, we, and others, have seen less shotgun projects last year, in favor of long amplicon sequencing, due to the much higher price of 454 sequencing relative to Illumina (and SOLiD). Also, IonTorrent is catching up on read length, with 400 bp coming this year (seeing is believing, though). The PGM, and especially the upcoming Proton instrument, has a troughput that is staggering when you\u2019re used to 454. But also the newly announced upgrade to the MiSeq later this year will get that platform up to 2\u00d7250 bases PE sequencing. This means that a 400-450 bp PCR product, currently only sequencable on the 454, can soon easily be done on the MiSeq, at lower cost and higher throughput. \n This means that, in principle, the upgrade to GS FLX+ could reinvigorate the interest in this platform, for example for de novo transcriptome and genome sequencing. But the outlook is bleak, at best. Unless GS FLX+ becomes stable all over the world, and reports on the results start becoming very positive, 454 is very fast losing it\u2019s niche to the competitors. To me, it doesn\u2019t look like Roche is taking this situation very seriously. This makes me wonder if they perhaps also have lost faith in 454? \n EDIT: \n Check out the comments below, and this discussion at SeqAnswers ."], "link": "http://flxlexblog.wordpress.com/2012/01/17/what-is-going-on-with-454-gs-flx-sequencing/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.invitrogen.com/": 1, "http://seqanswers.com/": 2, "http://www.marketwatch.com/": 1, "http://flxlexblog.wordpress.com/": 1, "http://core-genomics.blogspot.com/": 1, "http://pag12.mapyourshow.com/": 1, "http://pag.confex.com/": 3}, "blogtitle": "In between lines of code"}, {"content": ["Life technologies released reads from a new benchmark run for the Grand Challenge. For those with an account at the Ion community, here are the links to the announcement, and the data (sff, fastq and reports). It was a 316 chip that was used. \n Let me start by saying the data set is pretty impressive: \n-\u00a0 2.76 million reads \n- peak length at 251 bases, longest 428 \n- 664 Mbp raw sequence \n Compare this to where we were when we obtained the GS FLX in October 2007: around 400 000 reads of 250 bases, totaling approximately 100 Mbp\u2026 Then again, how good are these data. \n With the release follows a report which shows mapping statistics, and they are pretty impressive. I won\u2019t go into the details, as I expect an application note (with the obligatory comparison with the MiSeq data) to appear soon, and/or other bloggers throwing themselves onto this data. i thought to use the same kind of analysis as I used for my previous post on this data set. \n First: the read length distribution. I first extracted the same amount of reads from the sff file as I used in my previous post to keep the scales similar: \n  \n The distribution of the B15_410 data shows a remarkably sharp peak around 250, with hardly any shorter and longer reads. Also, the weird peak around 400 seen in the B14 data is gone. \n Then, I again looked at the homopolymers. First, the frequencies of homopolymers of all lengths, comparing again to 454 data, and one of the other Ion runs: \n  \n At first sight, the B15_410 run seems to be following the E coli and 454 data more parallel, but there is still the deviation at the larger homopolymers, as was seen with other Ion runs. \n I then again counted the starting positions for all homopolymers of each read. For comparison, I repeat the graphs for 454 and the B14 long reads from my last post: \n 454 data \n  \n B14 long data: \n  \n New, B15_410 data: \n  \n It looks to me like the problem has gotten worse: from around 230, first the frequency of 1-mers goes slightly up, then it goes down dramatically until it comes below the 2-mer frequency, which goes up steeper than in the B14 data set. \n So what, is this data set than the same as the previous, long read data set? Well, there is one point of significant improvement to be had. For assemblies, the reads perform much better than the B14_387 data. I again selected the same amount of reads for all assemblies, except for the B15_410 full length assembly, where I picked the same number of bases (same coverage): \n  \n The B15_410 assemblies show half the amount of contigs compared to the B14_387 reads, with much better contig N50s. The trimming back to 230 bases did not improve the assembly , though, so the homopolymer extension does not explain the poorer performance to the 454 assemblies. Hey, do you notice a slightly longer \u2018longest\u2019 contig for the b15_410 full length assembly compared to the 454 one? \n Note that the assemblies based on the GS FLX reads still significantly outperform the Ion Torrent-based assemblies when it comes to contig numbers and N50. How much of this is due to the fact the DH10B genome (Ion Torrent reads) is much harder to assemble than the MG1655 strain (454 reads) remains to be seen. Only a more fair comparison, by generating MG1655 Ion Torrent reads of the same length, can answer that question. Also, a more thorough comparison, with mapping contigs back the reference genomes, will give much better information regarding the assembly qualities."], "link": "http://flxlexblog.wordpress.com/2011/09/30/iontorrent-many-long-reads-still-longer-homopolymers/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://flxlexblog.wordpress.com/": 7, "http://lifetech-it.jivesoftware.com/": 2}, "blogtitle": "In between lines of code"}, {"content": ["A quick summary for the impatient: \n An analysis of the homopolymer distribution of the recently released \u2018longer\u2019 Ion Torrent reads indicates a possible significant over-calling of homopolymer lengths towards the ends of the reads. Trimming the ends off, however, only marginally improved de novo assembly of the reads using newbler. \n Life recently released \u2018long\u2019 IonTorrent reads (B14_387, resequencing of E coli strain DH10B, available through the Ion Community here ). There is an accompanying application note that brags about the read\u2019s accuracy, especially over reads from the MiSeq platform. These accuracy measurements are logically based on alignment to a reference genome. \n But what about de novo assembly? Thing is, the dataset presented, with a peak length of 241 (see below) and 350 000 read, is quite similar to what a full plate of GS FLX gave you in 2007 (peak length 250 bases, 400 000 reads). And 454 reads were very useful at the time for de novo assembly (in fact, the only reads available for this purpose, obviously besides Sanger reads). \n Nick Loman had a stab at using these reads for assembly with newbler, the program developed by 454 Life Sciences, on his blog . He reported, compared to the shorter Ion reads, much worse results for the longer reads. On twitter , he wrote that the assembly programs MIRA and CLC Bio performed really bad with these reads. \n So I wondered what could be the problem with these reads, that presumably show good mapping accuracy, but perform not very well for assembly. For comparison, I generated a similar 454 GS FLX dataset, 350 000 reads with a peak length at 250 bases. I used sff files that came with the software installation discs of newbler version 1.1, these represent a resequencing run on E coli K12 strain MG1655. I randomly selected at the exact number of reads as are in the Ion Torrent B14_387 dataset. \n First, I looked into the sff file and noticed that the run had 520 flows, twice that was used earlier. Using my \u20192.5 bases per four flows\u2019 calculation I described before , this translates in a potential 325 bases. The flow order used was reported as TACG all the way. In a run I discussed earlier , I noticed a 32 base repeated flow order which presumably yields better results, but and this flow order was apparently not used for this run as well [thanks to Nils Homer for pointing out my oversight here, I've ordered new contact lenses...). \n I then took a closer look at the read length distribution of both datasets \n  \n The Ion reads peak slightly lower than the 454 reads (241 and 253, respectively). But, they also show a strange peak around 400 bases. On the Ion Community forums, this peaks was discussed, but I couldn't find a satisfactory explanation for it. \n For problems with the data generated by both 454 and Ion Torrent, the usual suspect is homopolymers, consecutive runs of the same bases. Due to the way the bases are read (basically not reading single bases, but the length of homopolymers). I calculated the frequencies of homopolymers of all lengths present in both the Ion long reads dataset, the B13_328 dataset (100 bases run), the 454 dataset, and the E coli K12 MG1655 genome (both forward and reverse strands). (The results for DH10B were identical to MG1655 and are therefore not shown), see the graph. \n  \n In the figure, the counts of the homopolymers of lengths 1-11 are plotted, with the Y-axis on a log scale. A homopolymer length of 1 represent all single bases (monomers), length 2 represents all occurrences of AA, CC, GG and TT, length 3 represents all AAA, CCC, GGG and TTTs, etc. \n The line representing the E coli genome (the lowest one due to the 2x coverage, while the run data have much higher coverage) showed a steady (exponential) decline in the counts, accelerating beyond 8-mers. The 454 reads nicely follow this line and showed the same pattern. The Ion reads, on the other hand, deviated significantly. The showed a slowing down in the decline beyond 8-mers. So, it looks like the Ion reads have significantly more longer homopolymers than expected based on the composition of the genome. This could mean that there is a profound over-calling of homopolymers in these reads. \n (As an aside, the Ion B14_387 long reads had no homopolymers beyond 11-mers, the B13_328 short run had a few above that, up to 23-mers, the 454 data showed very few above 10-mers and maxed out at a whopping 31-mers (artifacts, perhaps?). The E coli genome did not have longer homopolymers than 10-mers.) \n I next tested whether these longer homopolymers were located uniformly along the reads, or where perhaps clustered more towards the ends. To this end, I counted the starting positions for all homopolymers of each read, for the 454 and B14 long read datasets. \n  \n The 454 reads, above, showed a uniform distribution of homopolymers of the different lengths along the read up to about 320 bases. The pattern for the last bases is a result of the very few reads that are of this length, so that N is very low for these positions. \n  \n The long Ion reads, on the other hand, show a significant reduction of the fraction of 1-mers beyond base 230, and an increase of the 2-mer and longer homopolymers. The very long reads (the peak around 400 nt) show the opposite, a reduction in the number of homopolymers in favor of monomers. \n In an analysis of the test fragments spiked into the IonTorrent runs, lek2k showed on his biolektures blog (posts here and here ) both undercalls and overcalls for two-mers in the test fragments for longer reads deep into the reads [thanks to lek2k for correcting me on this, see his interesting comment below]. My results indicates that overcalls are the bigger of the two problems. \n These analysis indicate (but in no way prove) that homopolymer overcalls could contribute significantly to a lower accuracy at the end of the longer IonTorrent reads. Is this problem then explaining the low assembly performance of these reads? \n In order to test this, I created new sff files for both B14_387 and the 454 dataset, with the reads trimmed beyond 230 bases. Then, I performed assemblies for the original files, and the retrimmed files using newbler 2.6 with default settings. \n  \n (\u2018Large\u2019 contigs are those of at least 500 bp). The table shows not that much difference between the full-length 454 reads, and those trimmed at 230 bases, with only a few more contigs and a 20% lower N50. for the latter assembly.\u00a0 The full-length Ion reads assembly is similar to the one Nick Loman produced, three times more contigs and an N50 that is a fifth of the full-length 454 assembly. Note that many reads are only partially assembled (meaning that a part of them could be used for alignment against the other reads, the rest was too different), whereas close to 99% of the 454 reads are fully assembled. Finally, the assembly using the trimmed Ion reads is only marginally better. \n In conclusion, these results indicate that I have found a problem, but that it most likely is not fully able to explain the poor performance of the long Ion reads for assembly. Also, the GS FLX reads (peak length at 250), around for more than four years now, perform significantly better than Ion reads of comparable length for de novo assembly. \n Finally, I\u2019d be interested to see an analysis of the homopolymer errors based on mapping the reads to the reference genome for both these datasets. Since this is something that would take me way too much time, I hope somebody else will do this analysis (I can help if needed, and you\u2019re welcome to write a guest post on my blog if you want)! \n Code used \nRandomly picking 350109 reads from the 454 sff files to get the same number as the IonTorrent B14_387 run \nsfffile -pickr 350109 -o EBO6PME_350k_reads.sff EBO6PME0*.sff \n Readlength distributions \n(see also this post ): \nsffinfo -s reads.sff|fasta_length | awk \u2018{x[$1]++}END{for (i in x){print i\u201d\\t\u201dx[i]}}\u2019| sort -n >read_length_hist.tsv \n Homopolymer counts \nI used the following script (thanks to \u2018Deep\u2019 for pointing out a bug in this script, see the Comments section. The results changed slightly after fixing the bug but this did not influence the conclusions. Below is the correct version): \n #!/usr/bin/perl\n\nuse strict;\nuse warnings;\n\nmy $seq;\nmy $seen;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# for tracking homopolymers\nmy $i;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# running length of homopolymer\nmy $max=0;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# longest homopolymer found\nmy @hompol;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# hompol = homopolymer counts, e.g. hompol[\"A\"][3] is the count of AAA\nmy %bases; $bases{\"A\"} = 1;$bases{\"C\"} = 2;$bases{\"G\"} = 3;$bases{\"T\"} = 4;\n\n# set the record separator to the '&gt;' symbol\n$/=\">\";\n<>;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# remove the empty first 'sequence'\n# loop over the sequences\nwhile (<>){\n$seen = \"\";\n$i=1;\n# remove the trailing '>' symbol\nchomp;\n# split the entry into individual lines based on the newline character\nmy @lines = split(/\\n/,$_);\n# the header is the first line (now without the '>' symbol)\nmy $header = shift @lines;\n# the sequence is the rest\nmy $seq = join \"\", @lines;\n\n# loop over the bases\nfor (split (//, uc($seq)))\u00a0 {\n# skip N's\nnext if /N/;\n# when the current base is the same as the previous one\nif ($seen eq $_){$i++}\n# otherwise, the homopolymer run has ended\nelse {\n$hompol[$bases{$seen}][$i]++ if $seen ne \"\";\n$max=$i if $i>$max;\n$i=1;\n}\n$seen=$_;\n}\n# do not forget last homopolymer\n$hompol[$bases{$seen}][$i]++ if $seen ne \"\";\n$max=$i if $i>$max;\n}\n$/=\"\\n\"; # reset the record separator\n\n# output\nprint \"\\tA\\tC\\tG\\tT\\n\";\n# homopolymer length loop\nfor $i (1..$max){\nprint $i;\n# bases loop\nfor my $j (1..4){\nprint \"\\t\";\nprint $hompol[$j][$i] || 0;\n}\nprint \"\\n\";\n}\n \n This produces a table with homopolymer length in rows, and bases in columns and the counts as values in the cells. I than (using excel) summed over the rows to get the totals, and plotted these in excel. \n Homopolymer starting positions \nFor this, I used a modified version of the above script\u00a0(thanks to \u2018terry\u2019 for pointing out a bug in this script, see the Comments section. Again, the results changed very slightly after fixing the bug, and again this did not influence the conclusions. Below is the correct version): \n #!/usr/bin/perl\nuse strict;\nuse warnings;\n\nmy $seq;\nmy $seen;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# for tracking homopolymers\nmy $i;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# running length of homopolymer\nmy $pos;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# running counter of base position\nmy $max_hpol=0;\u00a0\u00a0 \u00a0# maximum homopolyer length encountered\nmy $max_len=0;\u00a0\u00a0 \u00a0# maximum sequence length encountered\nmy @poscounts; \u00a0\u00a0 \u00a0# hash counting starting positions for all homopolymers; $poscounts[basepos][hompollength] = count\n\n$/=\">\"; # set the record separator to the '>' symbol\n<>;\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0# remove the empty first 'sequence'\n# loop over the sequences\nwhile (<>){\n$seen = \"\";\n$i=1;\n$pos = 0;\nchomp;\u00a0\u00a0 \u00a0# remove the trailing '>' symbol\nmy @lines = split(/\\n/,$_);\u00a0\u00a0 \u00a0# split the entry into individual lines based on the newline character\nmy $header = shift @lines;\u00a0\u00a0 \u00a0# the header is the first line (now without the '>' symbol)\n# the sequences is now the rest\nmy $seq = join \"\", @lines;\n$max_len = length ($seq) if length($seq) > $max_len;\n\n# Loop over the bases\nfor (split (//, uc($seq)))\u00a0 {\n$pos++;\n# skip N's\nnext if /N/;\n# when the current base is the same as the previous one\nif ($seen eq $_){$i++}\n# otherwise, the homopolymer run has ended\nelse{\n# $pos is now position of current base, one beyond the last homopolymer\n# $i is length of last homopolymer\n# $pos-$i =\u00a0 starting base of last homopolymer\n$poscounts[$pos-$i][$i]++ if $seen ne \"\";\n$max_hpol=$i if $i>$max_hpol;\n$i=1;\n}\n$seen=$_;\n}\n# do not forget last homopolymer\n$pos++;\n$poscounts[$pos-$i][$i]++ if $seen ne \"\";\n\n}\n$/=\"\\n\"; # reset the record separator\n\n# output\nprint join \"\\t\", (\"\", 1..$max_hpol);\nprint \"\\n\";\n# bases loop\nfor $i (1..$max_len){\nprint $i;\nfor my $j (1..$max_hpol){\n# homopolymer length loop\nprint \"\\t\";\nprint $poscounts[$i][$j] || 0;\n}\nprint \"\\n\";\n}\n \n The output of this script is a table with the base positions in the rows, and the homopolymer lengths as columns, the counts as values in the cells. \n Resetting the trimpoints to 230 bases \nFirst, I created a trimpoint file: \n sffinfo R_2011_07_19_20_05_38_user_B14-387-r121336-314_pool30-ms_B14-387_cafie_0.05.sff|awk '{ \nif ($0 ~ />/){id=$0} \nif ($0 ~/Clip Qual Right/){l=$4; if(l>230){l=230}} \nif ($0 ~/Clip Adap Left/){print id\" 5 \"l} \n}' |sed 's/>//' >B14_387_cafie_0.05_new_trimpoints_max_230.txt \n Then, I used the sfffile command to generate a new sff file, with new right trimpoints for those reads that are over 230 bases: \n sfffile -o\u00a0 B14_387_cafie_0.05_new_trimpoints_max_230.sff -tr B14_387_cafie_0.05_new_trimpoints_max_230.txt R_2011_07_19_20_05_38_user_B14-387-r121336-314_pool30-ms_B14-387_cafie_0.05.sff \n Assemblies \nAll assemblies were done with newbler 2.6 with default parameters, e.g. \n runAssembly -o trim_at_230 B14_387_cafie_0.05_new_trimpoints_max_230.sff \n For the summary of the assembly statistics, I used my newblermetrics script . > <"], "link": "http://flxlexblog.wordpress.com/2011/08/10/iontorrent-longer-reads-longer-homopolymers/", "bloglinks": {}, "links": {"http://biolektures.wordpress.com/": 2, "https://twitter.com/": 1, "http://feeds.wordpress.com/": 1, "http://sourceforge.net/": 1, "http://www.iontorrent.com/": 1, "http://flxlexblog.wordpress.com/": 8, "http://lifetech-it.jivesoftware.com/": 1, "http://pathogenomics.ac.uk/blog": 1}, "blogtitle": "In between lines of code"}, {"content": ["(Picture found on rockstar-pickup.com, of all places) \n I recieved an email today via the assemblathon mailing list. Erich Jarvis, from the Howard Hughes Medical Institute, wrote that he had asked somebody at Pacific Biosciences for some feedback on my previous post regarding the PacBio parrot reads. \n There are a few things in the response worth repeating here. \n First, it was pointed out that, with an per-read accuracy of 85%, phred quality values of 8-11 are to be expected. I had already realised this when I considered that a phred score of 10 means a 1:10 chance of a base being wrong, i.e. a 90% accuracy. For a phred score of 8, these numbers are 1 in 10 to the power of 0.8 error possibility, this is 1:6.3, or 16%, or 84% accuracy. \n Second, the errors are spread randomly over a read, meaning that a little extra coverage (5x and more) should get rid of them. \n Third, the PacBio person wrote: \n It is a bit unfortunate that existing 2nd generation tools do not work well with this error profile and the short-read/high-accuracy mindset takes a while to adjust to the characteristic of our reads. \n I could not agree more with that second part. In a way, I deliberately took a naive look at the PacBio reads, with the mindset of second-generation reads (I love the \u2018short-read/high accuracy\u2019 description\u2026). The PacBio data is in a different league, a totally different error model, but potentially very long reads. There also is a very nice option to obtain high-quality reads using circular consensus sequencing, as shown for the E coli data just released by PacBio . \n So, new tools are needed to work with these data, and I am eagerly looking forward to get the PacBio software installed (which will have to wait till after my holidays). \n Finally, putting my \u2018sequencing core facility hat\u2019 on, we already are getting questions from our users along the lines of \u201cwhen this machine finally arrives, can I then use it for this and this project?\u201d. Often, the expectations are unrealistically high. It will be my and my colleagues\u2019 task to guide these people into a direction that will get them the best results for their money. \n As with second-generation sequencing, getting the data will be relatively easy (and cheap), making sense out of it will be something different alltogether\u2026"], "link": "http://flxlexblog.wordpress.com/2011/07/13/overcoming-the-second-generation-sequencing-mindset/", "bloglinks": {}, "links": {"http://blog.pacificbiosciences.com/": 1, "http://flxlexblog.wordpress.com/": 2, "http://feeds.wordpress.com/": 1}, "blogtitle": "In between lines of code"}]