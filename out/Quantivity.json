[{"blogurl": "http://quantivity.wordpress.com\n", "blogroll": [], "title": "Quantivity"}, {"content": ["Tadas asked an interesting question in his recent post: Where did all the finance bloggers go? A variety of folks gave thoughtful replies: Josh Brown , Flex Salmon , David Merkel , Scott Bell , the Macro Men , and bunch of anonymous professional traders . Undoubtedly, there is truth in all their observations. \n Yet, perhaps there is a common root cause at work, not yet stated: implicit momentum bias . \n \n Let me break that down, as there is a bit of intentional double entendre. By \u201cmomentum\u201d, I intend a sustained trend, whether conceptual or technical. By \u201cimplicit bias\u201d, I intend a cognitive preference which is rarely stated and often partially or fully unconscious. \n You can see this bias in numerous ways, affecting both blog readers and authors: \n \n Retail trading : the vast majority of retail investors and traders are pursuing discretionary strategies which boil down to technical momentum, whether conceived explicitly ( e.g. ad hoc moving average strategies or more arcane technical analysis voodoo) or implicitly ( e.g. cocktail party stock tips or watching CNBC). Thus, the vast majority of retail exhibit this bias. \n Asset managers : many active asset managers are pursuing strategies which also essentially boil down to technical momentum, either due to stock picking or executing strategies aligned with historically strong long-term risk-adjusted returns of momentum. This is exemplified by a quick scan of the Top Papers from SSRN, for which 2 of 10 top are momentum papers (followed by lots more of varying sophistication). Thus, majority of asset managers exhibit this bias. \n Quant funds : many low-, and medium-frequency funds run strategies which are essentially momentum dressed in sophisticated clothing, often tracking one or more risk premia (whether due to anomalies or market structure) rather than raw prices. This is nicely exemplified by Ilmanen\u2019s well-reviewed Expected Returns , who now works at AQR. Thus, many hedge funds exhibit this bias. \n Mainstream financial media : ratings are driven by viewers\u2019 perception \u201csomething is happening\u201d, which almost always occurs in strongly trending markets: bubbles or crashes. In contrast, range-bound markets are often remarked to be \u201cboring\u201d or \u201cfrustrating\u201d, with corresponding tanking in ratings. Thus, mainstream media exhibits this bias. \n Blogs / Twitter : building readership depends on establishing a distinctive perspective or \u201cvoice\u201d, enabling a blog or stream to stand out amongst all the noise. This demands the corresponding content to be formulaic in some predictable sense. Blogs have an even higher bar than Twitter, as blogs are expected to hold up as being \u201cgood\u201d despite hindsight bias\u2014meaning their shelf life must extend until they are read at some indeterminate point in the future. Falkenblog is a good example, which has been a tireless advocate of the low-vol anomaly for many years. Thus, many blogs and twitter exhibit this bias. \n Alpha research : much of the search for alpha is tested via historical backtests that span many years, invariably including both trending and range-bound regimes. Invariably, majority of both alpha and beta strategies perform best in momentum-driven trending markets, of which the past 30 years have mostly been; those strategies that perform well in range markets often are regime-aware, and trade different across regimes. Careful review of nearly all popular tactical and dynamic asset allocation strategies exhibit this (along with many exhibiting a pervasive, unstated bias from the 30-year bond bull run). Thus, alpha research often exhibits this bias. \n \n Momentum originates from one of the most influential aspects of human psychology: confirmation bias. Simply put, humans \u201clike to be right\u201d: enjoying and finding comfort in writing and reading information that confirms their existing beliefs or hypotheses (scientifically attributable to stimulus-response dopamine effects). \n Thus, range-bound markets are frustrating for many folks because they form opinions conceived as directional bets. For them, their momentum bias generates significant pain in markets like this\u2014which makes them tune-out . This is also seen in hedge funds, which in aggregate have a comparatively poor record in range-bound markets. In contrast, many computers find enjoyment in markets which generally defy non-quantitative directional prediction. \n Coming full circle back to Tadas\u2019 question, perhaps this implicit momentum bias partially explains the durability of his blog\u2014and, in doing so, perhaps contributes to the question he raised. He astutely blends the benefits of momentum, while avoiding its detriments: \n \n Is a \u201cforecast free blog\u201d, thus ensuring he cannot be wrong in hindsight \n Follows a well-established presentation style, thus ensuring his distinctive voice \n Aggregates and synthesizes from materials authored by others, ensuring both a constant stream of material and diversity of opposing perspectives to satisfy both sides of any debate \n Varies topical coverage on a daily basis to remain relevant and responsive to the trends which dominate investor and trader mindshare \n \n If only we could all replicate alpha strategies as durably as his blog has effectively contributed to the financial blogosphere over the years."], "link": "http://quantivity.wordpress.com/2012/10/25/implicit-momentum-bias/", "bloglinks": {}, "links": {"http://www.thereformedbroker.com/": 1, "http://feeds.wordpress.com/": 1, "http://hq.ssrn.com/": 1, "http://alephblog.com/": 1, "http://www.rmdfx.com/": 1, "http://abnormalreturns.com/": 1, "http://iheartwallstreet.com/": 1, "http://falkenblog.blogspot.com/": 1, "http://www.amazon.com/": 1, "http://blogs.reuters.com/": 1, "http://macro-man.blogspot.com/": 1}, "blogtitle": "Quantivity"}, {"content": ["GOOG unexpectedly disclosed their Q3 earnings early last week, on October 18th. While earnings were marginally interesting, much more amusing was the corresponding hiccup in intraday trading. This event provides an opportunity to dig into TAQ data, view through a HFT lens, and build intuition from some elegant ideas due to Mendelbrot, Clark, and An\u00e9. \n \n Begin by taking a look at the time-series plot of intraday trade prices for GOOG from the 18th (from one exchange): \n  \n The tape clearly illustrates some unhappy traders, with what appears to be a gap down in the noon hour (followed by trading halt until 3pm). Intraday dynamics can be better understood with a time-series summary of the corresponding trades and quotes: \n  \n Although the price appears to gap when viewed on a low-frequency daily chart, drilling into the noon hour illustrates the contrary: \n  \n Rather than a single print gap, the price action evolved more slowly over 12:30 to 12:40. The spread quickly expanded after 12:30, presumably driven by the jump in volume subsequent to disclosure. A 5,000 share trade printed shortly after 12:32, as spreads were beginning to stabilize. This precipitated some fear and greed, as market makers blew the spreads out to $4. Spread dynamics after noon appear to be fairly typical response to uncertainty and toxicity, as the summary is qualitatively similar to other similar events such as the morning after Bear in 2008. \n By way of comparison, below is plot for TAQ summary on 3:25 \u2013 4pm: \n  \n Now that we are familiar with the TAQ action as measured in human chronology, let\u2019s view it through a different lens\u2014following the intuition from Mandelbrot (1963), Clark (1971), and most recently Easley et al. (2012). \n To do so, begin by considering trades for GOOG from another day which is more normal, say the following day Oct 19. Start with human chronology, and calculate the time series of first differences for 1 and 5 minute bars. Next estimate the Epanechnikov kernel density from first differences. Finally, fit the corresponding Gaussian for the same first differences. Or, in R: \n \n# first differences\nmin1Diff <- diff(to.minutes(trades$price)[,4], na.pad=F)\nmin10Diff <- diff(to.minutes5(trades$price)[,4], na.pad=F)\n\n# kernel densities\nmin1Density <- density(min1Diff, kernel=\"epanechnikov\")\nmin10Density <- density(min10Diff, kernel=\"epanechnikov\")\n\n# corresponding normals\nmin1Normal <- fitdistr(coredata(min1Diff), \"normal\")$estimate\nmin10Normal <- fitdistr(coredata(min10Diff), \"normal\")$estimate\n \n Which are plotted below for Oct 19, with solid red being 1-minute difference density and solid blue being 5-minute difference bars (ignore black for now); Gaussian fits for the same differences of each are dashed: \n  \n This reproduces the familiar stylized distribution of high-frequency returns, consistent with Figure 1 from Clark (1971) and Exhibit 2 from Easley et al. (2012). Although certainly leptokurtic, the distributions are not too dissimilar from normal. \n Now, ignore chronological time and instead consider the trades from the perspective of a clock based on volume of shares traded over the day . In other words, generate a new process defined as the sequence of prices coinciding with shares traded on equal-sized partitions of total volume over the day. This process generates a \u201cvolume clock\u201d for the trades, in doing so exemplifying a beautiful time-series transformation. \n A similar kernel density can be estimated from this trade volume clock, which is plotted above in black (with dashed corresponding normal fit), assuming partition size of 50. This exhibits distribution characteristics consistent with Exhibit 2 from Easley et al. (2012). \n With intuition on volume clock densities, we can finally take a look at Oct 18 through the lens of a computer via the following plot: \n  \n Needless to say, this plot for Oct 18 looks nothing like the previous plot for Oct 19. The normal fits for both 5-min bars (blue) and volume clock (black) are especially interesting, as they have little resemblance to their corresponding kernel estimates. Care is required for trading on days that look like this. \n For those interested to learn more, see Clark (1971) for theory, including subordinated stochastic processes (originally due to Bochner (1960)). An\u00e9 and Geman (2000) apply a bit more sophistication, including arguing in favor of cumulative transaction count rather than cumulative traded volume. Perhaps worth a follow-up post to see if this still makes sense, given current era of predominantly algorithmic program trading. \n The R code to generate the above plots is: \n \nplot(trades20121018$price, main=\"GOOG Trades (2012-10-18)\")\nplotTAQSummary(\"GOOG\", \"2012-10-18\", as.POSIXct(\"2012-10-18 09:30:00\"), as.POSIXct(\"2012-10-18 16:05:00\"))\nplotTAQSummary(\"GOOG\", \"2012-10-18 | 12:00 - 13:00\", as.POSIXct(\"2012-10-18 12:00:00\"), as.POSIXct(\"2012-10-18 13:00:00\"))\nplotTAQSummary(\"GOOG\", \"2012-10-18 | 15:00 - 16:00\", as.POSIXct(\"2012-10-18 15:25:00\"), as.POSIXct(\"2012-10-18 16:05:00\"))\nplotTimeChangeDensity(\"GOOG\", as.Date(\"2012-10-19\"), trades20121019)\nplotTimeChangeDensity(\"GOOG\", as.Date(\"2012-10-18\"), trades20121018)\n \n And, a bit of literature on subordinated processes, volume clocks, and price processes: \n \n Bochner, Harmonic Analysis and the Theory of Probability , University of California Press, Berkeley, 1960. \n Mandelbrot, \u201cThe Variation of Certain Speculative Prices\u201d, Journal of Business , Vol. 36 (1963), p. 394-419. \n Clark, \u201cA Subordinated Stochastic Process Model with Finite Variance for Speculative Prices\u201d. Center for Economic Research. University of Minnesota, 1971. \n An\u00e9 and Geman, \u201cOrder Flow, Transaction Clock, and Normality of Asset Returns\u201d, Journal of Finance , Vol. 55, No. 5 (2000), pp. 2259-2284. \n Murphy and Izzeldin, \u201cOrder Flow, Transaction Clock, and Normality of Asset Returns: An\u00e9 and Geman (2000) Revisited\u201d, 2006. \n Easley, de Prado, and O\u2019Hara, \u201cThe Volume Clock: Insights into the High Frequency Paradigm\u201d. Journal of Portfolio Management , May 2012."], "link": "http://quantivity.wordpress.com/2012/10/23/volume-clock-gaps-and-goog/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://quantivity.wordpress.com/": 6, "http://en.wikipedia.org/": 1}, "blogtitle": "Quantivity"}, {"content": ["Quantivity disliked undergrad macroecon , as it was largely a waste of time: fancy theory lacking compelling evidence, amplified by no consensus within the field \u00e0 la saltwater versus freshwater . \n Few folks could be blamed for such flippancy, as it was mostly harmless throughout the great moderation . In fact, traders took apparent pride in their ignorance of macro\u2014except the global macro guys, obviously. Then, along came a credit crisis. \n With that swan, Quantivity concluded it was high time to formulate a systematic macro perspective: a \u201ctop down\u201d complement to calibrate \u201cbottom up\u201d quant models. Quantivity brought great humility to this effort, due to both intrinsic complexity and comparatively weaker background in macro. \n This post kicks off a few thoughts derived from this effort; hopefully a welcome addition alongside micro analysis. Two caveats are worth noting. First, confirmation bias is particularly dangerous with macro, and thus emphasis is placed on broadly considering diverse viewpoints. Second, these thoughts are posted with a bit of trepidation, given intense desire to avoid politics and policymaking. \n \n Understanding macro is built from formulating intuition around two themes: \n \n Combining both backward- and forward-looking perspectives \n Contextualizing sovereign and institutional politics within those joint perspectives, in real-time \n \n Backward-looking perspective requires careful study of economic history\u2014admittedly, not a topic many folks enjoy reading at bedtime. Much more empirically challenging is the statistical rarity of credit crises; only twice in the past century: globally 1927 \u2013 1934 and in Japan 1989 \u2013 2005. \n The following are excellent long form introductions (ignore Koo\u2019s ridiculous title): \n \n Does Central Bank Independence Frustrate the Optimal Fiscal-Monetary Policy Mix in a Liquidity Trap? , by McCulley and Pozsar (2012) \n Holy Grail of Macroeconomics , by Koo (2011) \n Has Financial Development Made the World Riskier? , by Rajan (2005) \n \n The first two eloquently summarize history and key dynamics of the current macroeconomic climate, including credit crisis , balance sheet recession , and liquidity trap . Rajan presents a shockingly prescient perspective on the impact and dangers caused by financial development, equally applicable today as in 2005. \n Arguably the most important collective insight is the remarkably important role which economic orthodoxy plays among market participants (including regulators). McCulley and Pozsar (p. 3) nicely summarize as: \n Intellectual paralysis borne of inertia from dogma that, in the present circumstances, do not apply. \n Neatly tucked into this definition are three fundamental scaffolds: \n \n Dogma : understanding what is the prevailing conventional economic \u201cwisdom\u201d among market participants , in sufficient detail to trade on it; in other words, economic theory, market structure, regulation, secular demographic trends, sociopolitical context, etc . \n Present circumstances : accessing and understanding real-time economic data and analysis, again in sufficient detail; in other words, market data and corresponding econometric analysis \n Applicability : speculating when dogmatic wisdom is applicable to present circumstances, versus when it is not and thus resulting in intellectual inertia \n \n Although in a new guise, these three boil down to a familiar trading concept: contrarian versus trend following. When dogma is appropriate for present circumstances, then follow the trend; if not, then go contrarian. The challenge is having to make this call in real-time based on incomplete information, simultaneously understanding all three in sufficient confidence to risk capital. \n Towards this end, FRED is a nice source for macro data. Given data, next is building intuition and understanding of dogmatic applicability via analysis and synthesis while seeking to minimize confirmation bias. \n The blogosphere is as good source for this as any (certainly better than nearly all sell side research), although much of it suffers from varying degrees of partisanship and politicking. A few exemplar blogs, from the roll, include: \n \n Econbrowser , by James Hamilton \n Interfluidity , by Steve Waldman \n Macro Man"], "link": "http://quantivity.wordpress.com/2012/05/09/macro-matters-and-orthodoxy/", "bloglinks": {}, "links": {"http://www.interdependence.org/": 1, "http://www.econbrowser.com/": 1, "http://www.chicagobooth.edu/": 1, "http://www.kansascityfed.org/": 1, "http://weber.ucsd.edu/": 1, "http://www.interfluidity.com/": 1, "http://feeds.wordpress.com/": 1, "http://research.stlouisfed.org/": 1, "http://books.google.com/": 1, "http://macro-man.blogspot.com/": 1, "http://en.wikipedia.org/": 4}, "blogtitle": "Quantivity"}, {"content": ["Index Return Decomposition prompted several readers to inquire about forecasting the signs of returns , as implied by the decomposition variable. This is an interesting topic worth review, quick survey of intuition from the literature, and some R code for exploratory analysis. \n This topic is known as direction-of-change forecasting in the literature. Needless to say, successful prediction of the sign for future returns is quite interesting from a trading perspective. Traditionally, only univariate return series were considered; Anatolyev (2008) is an exception, modeling two or more interrelated markets via dependence ratios. This literature tends to be a bit obtuse, due to commonly unstated stylistic assumptions regarding conditional return dynamics. \n \n The traditional formulation for this topic considers the estimation of the probabilities of returns exceeding an upper or lower threshold , optionally conditioned on an information set from the previous time step: \n  \n  \n If , the probabilities reduce to forecasting positive or negative returns; for trading, a natural choice for is roundtrip transaction costs: \n  \n  \n Estimating these probabilities can be undertaken via several techniques. One approach is to use a logit model, based upon the logistic function: \n  \n Where are explanatory variables from the previous time step. Challenge of this model is proper selection of explanatory variables. \n An alternative approach is to consider the following functional decomposition for univariate return series: \n  \n Where is the conditional expected value, is the conditional variance, and is a martingale with zero mean, unity variance, and conditional distribution function . From which the direction of change probabilities can be expressed: \n  \n  \n With corresponding conditional expectations: \n  \n  \n These expectations simplify to the following when , assuming (otherwise, expectation is constant and thus uninteresting): \n  \n  \n These expectations can be evaluated explicitly via calculating the empirical distribution function (requiring assumption of a parametric distribution), where is the indicator function: \n  \n Alternatively, this decomposition suggests one potential formulation for the logit parameters from the above model, where is estimated by the logit and are historical observations: \n  \n Of course, the non-trivial work is generating forecast estimates for next-step average conditional return and conditional variance . \n An alternative way to model the logit parameters is to apply ARMA intuition with a binary autoregression (BARMA) due to Startz (2006) , including lags for both autoregressive parameters and past indicator values, due to Anatolyev (2008) : \n  \n A survey of estimation techniques may be considered in a subsequent post, depending on reader interest. \n \n One exploratory analysis technique relevant to sign forecasting is visualizing up/down runs, signed difference ( i.e. up-down), and corresponding averages for a return series. \n \nreturnRuns <- function(r, bound=0, doPlot=TRUE, startAvg=5, avgLen=-1)\n{\n # Generate up/down runs and average runs for a return series, optionally\n # plotting them.\n #\n # Args:\n # r: return series\n # bound: symmetric upper and lower bound, aka c\n # doPlot: flag indicating whether plots should be generated for runs\n # startAvg: Number of average runs which should be excluded for\n #    eliminating unstable average with few leading observations\n # avgLen: number of periods over which to generate average; of -1 for\n #   entire period\n #\n # Returns: none\n \n up <- cumsum(ifelse(r > bound, 1, 0))\n down <- cumsum(ifelse(r < -bound, 1, 0))\n \n if (doPlot)\n {\n plot(up, main='Signed Runs: Up & Down', ylim=range(up,down))\n lines(down, col='red')\n legend(\"topleft\",legend=c(\"Up\",\"Down\"), fill=colors, cex=0.5)\n \n plot(up-down, main=\"Signed Run Difference (up-down)\")\n }\n \n if (avgLen == -1)\n {\n avgUp <- xts(sapply(c(1:length(up)), function(i) {\n  up[i]/i\n }), order.by=index(up))\n avgDown <- xts(sapply(c(1:length(down)), function(i) {\n  down[i]/i\n }), order.by=index(up))\n } else\n {\n avgUp <- xts(sapply(c(avgLen:length(up)), function(i) {\n  start <- i - avgLen + 1\n  last(cumsum(ifelse(r[start:i] > bound, 1, 0))) / avgLen\n }), order.by=index(up[avgLen:length(up)]))\n avgDown <- xts(sapply(c(avgLen:length(down)), function(i) {\n  start <- i - avgLen + 1\n  last(cumsum(ifelse(r[start:i] < bound, 1, 0))) / avgLen\n }), order.by=index(up[avgLen:length(up)]))\n }\n \n if (doPlot)\n {\n n <- length(avgUp)\n plot(avgUp[startAvg:n], main=paste(\"Average Runs: Up & Down (\",avgLen,\" periods)\",sep=\"\"), type='l', ylim=range(avgUp,avgDown))\n lines(avgDown[startAvg:n], col='red')\n legend(\"topleft\",legend=c(\"Up\",\"Down\"), fill=colors, cex=0.5)\n }\n  \n return (list(up=up, down=down, avgUp=avgUp, avgDown=avgDown))\n}\n \n For example, the following plots illustrate CRM run dynamics from 2005 to present. First plot illustrates the running sums for both up and down returns, indicating negative returns are more prevalent: \n  \n Second plot illustrates the difference in signed sums, showing time-dynamics for the difference in up and down returns. Not surprising, this difference mirrors the CRM price curve closely: \n  \n Third plot illustrates the average probabilities for both up and down, running incrementally over the entire timeframe: \n  \n \n The following are representative papers from the direction-of-change literature, ignoring the early papers focused on evaluating market efficiency ( e.g. run tests): \n \n Forecasting Stock Indices: A Comparison of Classification and Level Estimation Models , by Leunga, Daoukb, and Chenc (2000) \n Financial, Asset Returns, Market Timing, and Volatility Dynamics , by Christoffersen and Diebold (2002) \n Direction-of-Change Forecasts Based on Conditional Variance, Skewness and Kurtosis Dynamics: International Evidence , by Christoffersen et al (2004) \n Are the Directions of Stock Price Changes Predictable? Statistical Theory and Evidence , by Hong and Chung (2003) \n Evaluating Direction-of-change Forecasting: Neurofuzzy Models vs. Neural Networks , by Bekiros and Georgoutsos (2005) \n Modeling Financial Return Dynamics via Decomposition , by Anatolyev and Gospodinov (2007) \n Direction-of-change Forecasts and Trading Strategy Profitability at Intra-Day Horizons , by Deliya (2007) \n Multi-Market Direction-of-Change Modeling Using Dependence Ratios , by Anatolyev (2008) \n Forecasting the Direction of the U.S. Stock Market with Dynamic Binary Probit Models , by Nyberg (2008) \n Direction-of-Change Financial Time Series Forecasting using Bayesian Learning for MLPs , by Skabar (2008) \n A Kernel-Based Technique for Direction-of-Change Financial Time Series Forecasting , by Skabar (2008)\n \n Optimal Probabilistic and Directional Predictions of Financial Returns , by Thomakos and Wang (2009) \n Directional Prediction of Returns under Asymmetric Loss: Direct and Indirect Approaches , by Anatolyev and Kryzhanovskaya (2009) \n Markets Change Every Day: Evidence from the Memory of Trade Direction , Skouras and Axioglou (2011) \n \n Finally, Kinlay briefly surveyed this topic in two posts on volatility sign prediction ."], "link": "http://quantivity.wordpress.com/2012/01/16/sign-direction-of-change-forecasting/", "bloglinks": {}, "links": {"http://ethesis.helsinki.fi/": 1, "http://papers.ssrn.com/": 4, "http://feeds.wordpress.com/": 1, "http://quantivity.wordpress.com/": 5, "http://www.iaeng.org/": 1, "http://www.cefir.ru/": 2, "http://www.edu.sg/": 1, "http://www.nes.ru/": 4, "http://www.neurogest.com/": 1, "http://jonathankinlay.com/": 1, "http://www.springerlink.com/": 1, "http://econlab.uom.gr/": 1}, "blogtitle": "Quantivity"}, {"content": ["Quantivity is fortunate to be acquainted with numerous folks who have earned consistent returns over multiple decades without significant drawdown. Although they have varying trading strategies, there is a common theme which unifies them: top-down systematic focus on the sociology of market participants . \n This focus is not behavioral finance, in search of anomalies driven by cognitive biases divergent from equilibrium (although majority do that too). Rather asking inferential sociological questions, such as: was the market \u201cefficient\u201d, in the Fama sense, during the post-war decades prior to 2000 because people expected it to be (blissfully ignoring a few hiccups ); in contrast to how it is commonly understood and formalized, with reverse causality: market is assumed to be efficient, thus people understand it as such. \n Similarly, have the past 15 years been \u201cinefficient\u201d, in the bubble and anomaly sense, because cultural faith among investors in such \u201cefficiency\u201d was lost; or, did they lose faith because the market became inefficient? Big difference. \n In other words: is finance governed by physics, biology, or Peltzman ? \n \n The traditional answer of market hypothesis , provided by financial economics via microeconomic principles of equilibrium and efficiency : causality flows from market to investor. This explanation comes in two variants, known by their colloquial analogical fields: \n \n Physics : market is governed by immutable mathematical principles and can be formalized into coherent predictive models, either in favor or contradiction of excess returns; exemplified by classic weak/strong EMH theory \n Biology : market is governed by evolutionary principles ala Darwin, as exemplified by Lo\u2019s 2004 AMH article: \u201cVery existence of active liquid financial markets implies that profit opportunities must be present. As they are exploited, they disappear. But new opportunities are also constantly being created as certain species die out, as others are born, and as institutions and business conditions change.\u201d (p. 24) \n \n Yet, both these explanations suffer from implicitly begging the question: conjure \u201ca market\u201d with desired attributes and then derive conclusions. The physics perspective assumes immutability, conceivability, and mathematical expressiveness for its hypothesized market. While the biology perspective endows the hypothesized market with even more sophisticated Darwinian traits, presumably driven by underlying physical principles so inscrutable as to defy mathematical formalization. \n An alternative explanation is to apply the self-fulfilling Peltzman effect to financial markets, and reverse causality: markets behave as they do because of investor sociology , rather than arising emergent from implicit cooperation of equilibrium-seeking rational microeconomic agents. \n In other words: when investors believe the market is rational (irrespective of whether that belief is well-founded), then they embody Dunning-Kruger by ex ante faithfully dumping money into their 401K each month; in doing so collectively, the investment management industry undertakes its rent seeking activity resulting in the market possessing ex post \u201cefficient\u201d characteristics. Conversely, when investors believe the market is irrational, they either: go to cash, pursue uninformed non-collective trading, or both. Both of which result in anomalous market behavior, uncontrollable by the industry, either due to decreased liquidity or absence of predictable momentum. \n If the market is indeed Peltzmanian, then the real question is how to best quantify and model primary and spillover effects resulting from investor sociology as they unfold ephemerally."], "link": "http://quantivity.wordpress.com/2012/01/03/physics-biology-peltzman-finance/", "bloglinks": {}, "links": {"http://web.mit.edu/": 1, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 7}, "blogtitle": "Quantivity"}, {"content": ["A variety of techniques exist for estimating parameters of the return decomposition model, previously introduced in Index Return Decomposition . This post considers estimation of an independent mixture model via maximum likelihood (MLE), a workhorse of frequentist statistics and always a nice place to begin. \n Recall is unobserved , and thus the model cannot be directly estimated via MLE. Thus, need to decide how to approach estimation for this latent variable. One way is to be naive, and simply assume is the deterministic difference in return between stock and index (technically, this generates a profile likelihood as formalized by Severini and Wong [1992] , which Murphy and Van Der Vaart [2000] verify is well-behaved consistent with exact likelihood): \n  \n This assumption permits focus on estimating , providing insight into the mixing behavior of the return being decomposed: if a stock return behaves like its index, then mixing is low with small (in the limit, when a stock behaves identical to its index, as no mixing is required); in contrast, the stock return behaves independent from its index on a regular basis, then mixing is high with a large . \n \n Autocorrelation of and is worth consideration, as that helps determine whether time indexing is required for . For returns with insignificant autocorrelation (common for signed equity returns), the time index is dropped and a single is estimated. Yet, conditional dependence often exists between and , consistent with previous posts in the Proxy / Cross Hedging series (illustrating r-z copula for CRM / QQQ example below): \n  \n Use of this identity for transforms the decomposition model into: \n  \n The model is further simplified into a familiar independent mixture model by dropping sign and , and estimating via MLE using density and return distribution functions : \n  \n MLE estimation requires assumption of parametric distributions for , of which common choices from the literature are normal, student-t, skew-t, or skew hyperbolic student-t ( Aas and Haff [2006] ). Next question is how to estimate the parameters: and family of parameters ( e.g. and if is assumed to be normal). As is observed, one way to proceed is via two-step estimation: \n \n Estimate parameters via MLE from \n Jointly estimate and parameters via MLE on the mixture, holding parameters constant \n \n For both, recall the likelihood , and log likelihood , are defined as: \n  \n  \n From which MLE of the mixture is maximization of the likelihood over , where log is chosen for numeric stability: \n  \n This optimization can be performed numerically in R via minimization using DEoptim of the negative log likelihood negLogLikeFun (negative is due to minimization in DEoptim versus maximization of ). DEoptim is chosen due to rapid convergence on non-smooth global optimizations. \n For example, continuing the example of CRM / QQQ introduced in the previous posts on Proxy / Cross Hedging generates the results: \n \n> symbols <- c(\"CRM\", \"QQQ\")\n> endDate <- Sys.Date()\n> startDate <- endDate - as.difftime(52*5, unit=\"weeks\")\n> quoteType <- \"Close\"\n> p <- do.call(cbind, lapply(symbols, get.hist.quote, start=startDate, end=endDate, quote=quoteType))\n> colnames(p) <- symbols\n> doReturnDecomp(p)\nnormal mix likelihood: -3485.55 phi1 params: 0.0003471366 0.01673634 params 0.2546208 -0.001208877 0.0113988\nskew-t mix likelihood: -3566.512 phi1 params: 0.003844969 0.01079941 -0.3252923 2.893228 params 0.2357737 -0.004188977 0.01099266 0.4157174 26.5643\nskew-hyp-t likelihood: -3083.700 phi1 params: 0.01051675 0.1485529 -3.945452 10.10836 params 0.8295289 -0.0003636940 0.03071332 -0.5 5\n \n These results correspond to the following density functions for the skew-t mixture: \n  \n One interesting observation of these densities is their location parameters are on opposing side of zero: has positive location, while has negative location. One interpretation of this is positive returns from CRM disproportionately originate from the idiosyncratic , while negative returns originate from the index . Economically, this is plausible: positive news is often idiosyncratic, while negative news is often market-wide. \n Several additional inferences can be drawn from these results: \n \n Model selection: likelihood suggests skew-t is the preferred model, indicating long tails and skewness (matching stylized facts) \n Mixing: indicating that over 75% of CRM returns are determined by the corresponding QQQ index; remaining 25% are determined by the unobserved return series \n Tails: CRM df = 2.89 which indicates significantly thicker tails than QQQ df = 26.56 (matching stylized facts for individual stocks versus indices) \n \n Subsequent posts may consider alternative estimation techniques for this model. \n \nR code for generating two-stage MLE estimation of return decomposition via mixing: \n \nlibrary(\"MASS\")\nlibrary(\"stats\")\nlibrary(\"DEoptim\")\nlibrary(\"sn\")\nlibrary(\"SkewHyperbolic\")\n\nnormalMixtureIndexDecomp <- function(r, i)\n{\n # Two-step MLE estimation of return decomposition model, assuming both\n # return distributions are normal.\n #\n # Args:\n # r: return series being decomposed\n # i: index series used for decomposition\n #\n # Return value: MLE parameter estimates\n \n z <- r - i\n id <- fitdistr(i, \"normal\")$estimate\n negLogLikeFun <- function(p) {\n a <- p[1]; mu1 <- p[2]; s1 <- p[3];\n ll <- (-sum(log(a * dnorm(z,mu1,s1) + (1 - a) * dnorm(i, id[1], id[2]))));\n return (ll); \n }\n mle <- DEoptim(negLogLikeFun, c(0, -0.5, 0), c(1, .5, .5), control=list(trace=FALSE))\n \n cat(\"normal mix likelihood:\", last(mle$member$bestvalit), \"phi1 params:\",id, \"params\", last(mle$member$bestmemit),\"\\n\")\n mle <- last(mle$member$bestmemit)\n \n x <- seq(-.25,.25,length.out=500)\n dnorm1 <- dnorm(x, id[1], id[2])\n dnorm2 <- dst(x, mle[2], mle[3])\n plot(x, dnorm1, type='l', ylim=c(0, max(dnorm1,dnorm2)), ylab=\"Density\", main=\"Normal Mixture\")\n lines(x, dnorm2, col='red')\n abline(v=id[1], lty=2)\n abline(v=mle[2], col='red', lty=2)\n legend(\"topleft\",legend=c(\"phi1\", \"phi2\"), fill=colors, cex=0.5)\n \n return (mle)\n}\n\nmixtureSkewTIndexDecomp <- function(r, i)\n{\n # Two-step MLE estimation of return decomposition model, assuming both\n # return distributions are skew-t.\n #\n # Args:\n # r: return series being decomposed\n # i: index series used for decomposition\n #\n # Return value: MLE parameter estimates\n\n z <- r - i\n idp <- st.mle(y=i)$dp\n negLogLikeFun <- function(p) {\n a <- p[1]; mu1 <- p[2]; s1 <- p[3]; s2 <- p[4]; df1 <- p[5]\n ll <- (-sum(log(a * dst(z,location=mu1,scale=s1,shape=s2,df=df1) + (1 - a) * dst(i, dp=idp))));\n return (ll); \n }\n mle <- DEoptim(negLogLikeFun, c(0, -0.5, 0, 0, 2), c(1, .5, .5, 5, 50), control=list(trace=FALSE))\n \n cat(\"skew-t mix likelihood:\", last(mle$member$bestvalit), \"phi1 params:\", idp, \"params\", last(mle$member$bestmemit),\"\\n\")\n mle <- last(mle$member$bestmemit)\n\n \n x <- seq(-.25,.25,length.out=500)\n dst1 <- dst(x, dp=idp)\n dst2 <- dst(x, dp=mle[2:5])\n plot(x, dst1, type='l', ylim=c(0, max(dst1,dst2)), ylab=\"Density\", main=\"Skew T Mixture\")\n lines(x, dst2, col='red')\n abline(v=idp[1], lty=2)\n abline(v=mle[2], col='red', lty=2)\n legend(\"topleft\",legend=c(\"phi1\", \"phi2\"), fill=colors, cex=0.5)\n \n return (mle)\n}\n\nmixtureSkewHypTIndexDecomp <- function(r, i)\n{\n # Two-step MLE estimation of return decomposition model, assuming both\n # return distributions are skew hyperbolic student-t.\n #\n # Args:\n # r: return series being decomposed\n # i: index series used for decomposition\n #\n # Return value: MLE parameter estimates\n\n z <- r - i\n iparam <- skewhypFit(i,plots=FALSE,printOut=FALSE)$param\n negLogLikeFun <- function(p) {\n a <- p[1];\n ll <- (-sum(log(a * dskewhyp(z,param=p[2:5]) + (1 - a) * dskewhyp(i, param=iparam))));\n return (ll); \n }\n mle <- DEoptim(negLogLikeFun, c(0, -5, 0, -0.5, 0), c(1, 5, .5, -0.5, 5), control=list(trace=FALSE))\n \n cat(\"skew-hyp-t likelihood:\", last(mle$member$bestvalit), \"phi1 params:\",iparam,\"params\", last(mle$member$bestmemit),\"\\n\")\n mle <- last(mle$member$bestmemit)\n\n \n x <- seq(-.25,.25,length.out=500)\n dskewhyp1 <- dskewhyp(x, param=iparam)\n dskewhyp2 <- dskewhyp(x, param=mle[2:5])\n plot(x, dskewhyp1, type='l', ylim=c(0, max(dskewhyp1,dskewhyp2)), ylab=\"Density\", main=\"Skew Hyperbolic Student-T\")\n lines(x, dskewhyp2, col='red')\n abline(v=iparam[1], lty=2)\n abline(v=mle[2], col='red', lty=2)\n legend(\"topleft\",legend=c(\"phi1\", \"phi2\"), fill=colors, cex=0.5)\n\n return (mle)\n}\n\ndoReturnDecomp <- function(p)\n{\n # Decompose return of two series, using several parametric distributions.\n #\n # Args:\n # p: p[,1] is return being decomposed; p[,2] is index returns\n #\n # Return value: none\n\n r <- ROC(p[,1], type=\"discrete\", na.pad=FALSE)\n i <- ROC(p[,2], type=\"discrete\", na.pad=FALSE)\n \n normalMixtureIndexDecomp(r, i)\n mixtureSkewTIndexDecomp(r,i)\n mixtureSkewHypTIndexDecomp(r,i)\n}"], "link": "http://quantivity.wordpress.com/2011/12/28/estimating-mixture-index-return-decomposition-via-maximum-likelihood/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://quantivity.wordpress.com/": 5, "http://people.mit.edu/": 1, "http://www.ku.dk/": 1, "http://en.wikipedia.org/": 1, "http://www.jstor.org/": 1}, "blogtitle": "Quantivity"}, {"content": ["Risk is deeply underappreciated. \n Moreover, it is misunderstood\u2014even by many who have smelled it up close personally via big trading loses on hedged positions. Aaron Brown\u2019s most recent text, Red-Blooded Risk , explains why. \n In doing so, it is simultaneously brilliant and flawed . For the former, Brown deserves credit; for the latter, the publisher presumably deserves most of the blame. \n \n First, the brilliance; summarized in one word, with two intended meanings: pragmatics . Oh yeah, the book includes \u6f2b\u753b-style comic strips, helpfully provided as idiot self-detectors. \n First, well-known meaning is the philosophical tradition linking practice and theory. Arguably unique for risk books, Brown builds deep intuition around the concept of risk and its manifestation from structural to marked positions to historical roots in tulips. Brown has clearly lived and breathed risk management for many years (self-proclaimed from its modern origin in the 1980s), and that wisdom shines through via first-person prose combining insight, intuition, arrogance, pride, greed, humility, regret, condescension, and insecurity. Perhaps the first book ever to make VaR sound geek-sexy\u2014 with nary a technical definition . \n Second, lesser-known meaning is the subfield of linguistics which investigates ways in which context contributes to meaning. Unlike technical risk texts, Brown spends much of the book deep diving into context and letting meaning emanate from therein. As he states, \u201cdifferent aspects are easier to understand from different vantages\u201d (p. 57). As a reader who never personally worked at a big bank, this is deeply informative for attuning mental models (akin to Harris for trading mechanics, Rebonato for derivatives, and Taleb for hedging). Explaining the lineage of quant hedge funds through the corresponding frequentist versus Bayesian disposition of their founders is fascinating, and indeed makes sense in retrospect. Slicing away the credibility mystique and exposing the raw underbelly of banks, down to explaining front, middle, and back office in depth. For readers familiar with disciplined metrics-driven tech companies, the apparent intellectual and technical sloppiness of big banks is simply jaw dropping. \n One excerpt simply must be quoted, beginning of Chapter 4, as it is perhaps the most accurate and beautiful summary of self-entitlement believed by geeks with -IQ time immemorial: \n The rocket scientists came together on Wall Street in the 1980s and began the process that eventually explained the modern concept of probability and reconstructed the global financial system. We were not individually ambitious. All we wanted was to make more money than any rational person could spend, without ever putting on a tie or being polite to anyone we didn\u2019t like. We didn\u2019t have any use for the money, except for maybe some books and cool computer equipment. We didn\u2019t want to throw (or go to) fancy parties or buy political power\u2014and we didn\u2019t spend it on cars, jewelry, or places to live, and least of all on clothes. We\u2019d probably give the money away, but until then, it would give us the power to say \u201cf- you\u201d to anyone, except that we were mostly pretty soft-spoken and civil in our expressions \n Now, the flawed parts. \n One reviewer caveat worth advance mention is 9 books out of 10 read by Quantivity have content dense with math, code, or both. On the positive side, reading of Brown\u2019s book indicates positive noteworthiness due to its statistical abnormality (given it has neither); on the negative side, any review is biased through such lens. \n First, lots of effort was expended by the publisher making this text appeal to a mass audience, clearly rushing to fill the void of perceived post-financial crisis publishing opportunity. From the ridiculous title (and cover) to the silly use of \u201csecret history\u201d meme to hilarious tongue-in-cheek back cover reviewer comments by Gatheral, Taleb, Wilmott, and Thorp. Parts of the book are prone to hyperbole, which read like they were edited in for sales effect. While these nuisances detract credibility, such can be ignored and arguably contributes the positive benefit of reducing its purchase price to mass market ( i.e. under $25). \n Second, the book lacks unifying organization. While Brown provides the following disclaimer for such, the editor equally deserves some blame (p. 57): \n If I had all the theory worked out, I could write a textbook organized in logical sequence. Instead, I\u2019m going to intersperse theoretical discussions with accounts of the development of the ideas. \n While this makes sense, it is admittedly a bit jarring to see that disclaimer juxtaposed alongside supposition of having \u201cexplained the modern concept of probability\u201d. Thus, the reader is left wondering whether perhaps either of the following may be true: \n \n Brown has a theory of risk, but was refused in editing due to overabundance of equations \n Brown has a new theory of probability, but could not muster a theory of risk \n \n Either are intriguing, although perhaps the former seems more likely as Brown includes the following tongue-in-cheek disclaimer regarding use of a tiny bit of high school-level math included in Chapter 5 (p. 73): \n Warning, this chapter contains a little math. It\u2019s nothing intimidating, mostly multiplication and some simple algebra, but I know a lot of people don\u2019t like it. If that describes you, I urge you to read the chapter anyway. It\u2019s one of the most important in the book. You can skip the math and get the ideas anyway. \n Having never met Brown and thus unfamiliar with his personality, cannot escape the sense that he is making gentle fun of readers which possess such bias. Either way, it\u2019s amusing. \n While modest disorganization is a textual flaw, astute readers may perhaps perceive it as subtle financial opportunity : if such theory was sufficiently well-defined to warrant standard textbook treatment, then there would undoubtedly be much less juice possible from doing it really well. \n In either case, remedy for this shortcoming is to read the book in as few distinct sittings as possible. Having read it in two sittings, the wisdom was able to percolate together and expand personal mental models nicely. Well worth the read."], "link": "http://quantivity.wordpress.com/2011/12/17/risk-pragmatics/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://quantivity.wordpress.com/": 1, "http://books.google.com/": 4}, "blogtitle": "Quantivity"}, {"content": ["Unmasking a phenomenon into its constituent parts , via functional decomposition , is one of the great beauties of mathematics: \n  \n This technique finds surprisingly often use in quant models. \n Ongoing analysis and trading based on proxy hedging, exemplified by series beginning with Proxy / Cross Hedging , suggests potential for an equity decomposition model based on the relationship between returns of a stock and its corresponding index : \n  \n To explain this model, let\u2019s build it up from intuition. \n \n To begin, consider a trading observation: interday returns of individual stocks have a subtle relationship with their corresponding index. On some days, return for a given stock follows its index; other days, returns of stock and index diverge strongly. This distinction in behavior is commonly attributed to stock-specific \u201cnews\u201d, interpreted broadly\u2014whether known publicly or only privately. \n This intuition can be formalized into two-state regime: \n \n Uninformed regime : stock return follows an index , scaled by a proportional factor \n Informed regime : stock return follows an idiosyncratic path , conditionally independent of its index \n \n Relationship between regimes can be modeled in two ways via . A switching model arises when regimes are binary: . An ensemble model arises when regimes are smooth: . For the latter, can be understood as proportional decomposition weighting of the respective return series, and thus can provide smooth mixing between the regimes. Finally, sign of returns are explicitly decomposed as , acknowledging greater regularity of absolute-valued return series. \n Worth noting is the following are latent variables: idiosyncratic path from the informed regime, proportional factor , and regime parameter . Obviously, challenge of this model lies in their estimation. One potential trick is to exploit triangular relationships, as described below. \n One stylized fact not explicitly accommodated by this model is well-known asymmetry of uninformed regimes , arising from analysis of market breadth: stocks uniformly go down together (think big down days), but much less often uniformly go up together (majority of rallies). Unclear whether this fact naturally arises via or needs to be explicitly modeled. \n Readers familiar with machine learning (ML) may recognize how to reformulate this as an additive model : \n  \n Where . \n This model can be interpreted in numerous ML ways, depending on the desired objective. For example, and can be interpreted as basis functions. Alternatively, boosting can be applied by interpreting them as weak classifiers. Graphical models can be applied by introducing conditional dependence between , , and . Hierarchical models and decision trees naturally arise when and are further functionally decomposed. \n Given this model, an interesting question is how to use it predicatively \u2014whether directional or not. For example, combining models for two stocks which share a common index to introduce the notion of equity triangle arbitrage on the joint ."], "link": "http://quantivity.wordpress.com/2011/12/14/index-return-decomposition/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://quantivity.wordpress.com/": 1}, "blogtitle": "Quantivity"}, {"content": ["Quantivity is pleasantly surprised to discover an increasing number of folks are deriving value from the Curated Quant Research Feed on @Quantivity . Indeed, the combo of daily curated feed with single-source retrospective search has become indispensable for personal research. Towards understanding why, Kedrosky provides nice explanation in his Curation is the New Search is the New Curation post earlier this year: \n Head back to curation and watch new algos emerge on top of that next-gen curation again. Think of Twitter as a new stab at curation. Curated sites will re-seed a new generation of algorithmic search sites. In short, curation is the new search. \n Indeed, intent of curation here is to maintain high signal-to-noise ratio for a mix of preprint and classics in a highly-specialized literature ( i.e. combo of retail and prop ) for which strong motivation exists elsewhere to obfuscate; and search over the stream provides ability to both rewind time and to integrate conceptual connectivity spanning time. \n \n One addition being contemplated is keyword search over all literature cited in feed, providing deep content search over the feed. Although, unclear yet what is the best technical avenue to implement this (please comment, if you have suggestions). \n So, with this positive start, curation input set is being modestly expanded to coincide with increased personal research activity and availability of several new quant sources\u2014 while maintaining the same focus and high signal-to-noise goal . Specifically, curation is expanding to include the following SSRN working papers: ARPM Series and JEL Codes G11 (Portfolio Choice), G12 (Asset Pricing), G13 (Contingent Pricing; Futures Pricing), G14 (Information and Market Efficiency), C21 (Cross-Sectional Models), C22 (Time-Series Models), C51 (Model Construction and Estimation), and C53 (Forecasting and Other Model Applications). Selection of JEL codes is data-driven: feed links were ranked by JEL classification and most cited classifications were chosen. \n Authors are encouraged to ensure correct use of JEL codes, to ensure your articles are picked up. \n Curious what readers think? Are there other high-value sources worth adding to curation input set? What else could make this more useful?"], "link": "http://quantivity.wordpress.com/2011/11/04/update-curated-quant-research-feed/", "bloglinks": {}, "links": {"http://papers.ssrn.com/": 1, "http://feeds.wordpress.com/": 1, "http://quantivity.wordpress.com/": 1, "http://paul.kedrosky.com/": 1, "http://www.aeaweb.org/": 1, "http://twitter.com/": 1}, "blogtitle": "Quantivity"}, {"content": ["Algebraic geometry and topology traditionally focused on fairly pure math considerations. With the rise of high-dimensional machine learning, these fields are increasing being pulled into interesting computational applications such as manifold learning . Algebraic statistics and information geometry offer potential to help bridge these fields with modern statistics, especially time-series and random matrices. \n Early evidence suggests potential for significant intellectual cross-fertilization with finance, both mathematical and computational. Geometrically, richer modeling and analysis of latent geometric structure than available from classic linear algebraic decomposition ( e.g. PCA, one of the main workhorses of modern finance); for example, cumulant component analysis. Topologically, more effective qualitative analysis of data sampled from manifolds or singular algebraic varieties; for example, persistent homology (see CompTop ). \n \n As evidence by Twitter followers, numerous Quantivity readers are familiar with these fields. Thus, perhaps the best way to explore is to seek insight from readers. \n Readers : please use comments or twitter to suggest applied literature from these fields ; ideally, although not required, that of potential relevance to finance modeling. All types of literature are requested, from intro texts to survey articles to preprint working papers on specific applications. \n These suggestions will be synthesized into one or more subsequent posts, along with appropriate additions to People of Quant Research ."], "link": "http://quantivity.wordpress.com/2011/10/31/algebraic-geometry-and-topology-inquiry/", "bloglinks": {}, "links": {"http://comptop.stanford.edu/": 1, "http://feeds.wordpress.com/": 1, "http://quantivity.wordpress.com/": 2}, "blogtitle": "Quantivity"}]