[{"blogurl": "http://blogperso.univ-rennes1.fr/arthur.charpentier\n", "blogroll": [], "title": "Arthur Charpentier"}, {"content": ["As\nmentioned during the past few weeks, the blog has been transfered (please\nupdate links and bookmarks). You will be redirected (shortly) to \n \n http://freakonometrics.blog.free.fr"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/10/01/Blog-transfert", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Pour la cinqui\u00e8me s\u00e9ance de cours, nous allons continuer sur la mod\u00e9lisation des co\u00fbts, \u00e0 partir des transparents en\nligne ici .\nLa premi\u00e8re partie portera sur la comparaison du mod\u00e8le Gamma et du\nmod\u00e8le lognormal, et la seconde sur l\u2019\u00e9cr\u00eatement des gros sinistres. Il convient de faire attention \u00e0 la transformation logarithmique. Un mod\u00e8le de la forme \n para\u00eetra toujours \" meilleur \" qu\u2019un mod\u00e8le (pour peu que les soient bien plus grands que 1). Regardons un jeu de donn\u00e9es classique, liant vitesse du v\u00e9hicule et distance de freinage. > summary(lm(dist~speed,data=cars)) Call: lm(formula = dist ~ speed, data = cars) Coefficients:    Estimate Std. Error t value Pr(>|t|)  (Intercept) -17.5791  6.7584 -2.601 0.0123 *  speed   3.9324  0.4155 9.464 1.49e-12 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 Residual standard error: 15.38 on 48 degrees of freedom Multiple R-squared: 0.6511,  Adjusted R-squared: 0.6438 F-statistic: 89.57 on 1 and 48 DF, p-value: 1.490e-12  \n > summary(lm(log(dist)~speed,data=cars)) Call: lm(formula = log(dist) ~ speed, data = cars) Coefficients:    Estimate Std. Error t value Pr(>|t|)  (Intercept) 1.67612 0.19614 8.546 3.34e-11 *** speed  0.12077 0.01206 10.015 2.41e-13 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 Residual standard error: 0.4463 on 48 degrees of freedom Multiple R-squared: 0.6763,  Adjusted R-squared: 0.6696 F-statistic: 100.3 on 1 and 48 DF, p-value: 2.413e-13  \nLe R2 est plus faible, l\u2019\u00e9cart type des r\u00e9sidus aussi, la log-vraisemblance si on l\u2019avait, etc. Il\nne faut jamais oublier qu\u2019on doit ensuite prendre l\u2019exponentiel sur\nmod\u00e8le en log. C\u2019est d\u2019ailleurs ce qui est fait par exemple dans la\ntransformation de Box Cox, o\u00f9 l\u2019on compare les sommes des carr\u00e9s des\nr\u00e9sidus sur les , i.e. et o\u00f9 . \n \n > s=summary(lm(log(dist)~speed,data=cars))$sigma > mean((cars$dist-predict(lm(dist~speed,data=cars)))^2) [1] 227.0704 > mean((cars$dist-exp(predict(lm(log(dist)~speed,data=cars))+.5*s^2))^2) [1] 296.2027 Tout \u00e7a pour conclure que le mod\u00e8le lin\u00e9aire n\u2019est peut \u00eatre pas si mauvais que \u00e7a...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/29/Statistique-de-l-assurance-STT6705V%2C-partie-5", "bloglinks": {}, "links": {"http://perso.univ-rennes1.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Following my previous post on optimization and mixtures ( here ), Nicolas told me that my idea was probably not the most clever one ( there ). \nSo, we get back to our simple mixture model, \n \nIn order to describe how EM algorithm works, assume first that both and are perfectly known, and the mixture parameter is the only one we care about. \n The simple model, with only one parameter that is unknown \n \nHere, the likelihood is \n \nso that we write the log likelihood as \n \nwhich might not be simple to maximize. Recall that the mixture model can interpreted through a latent variate (that cannot be observed), taking value when is drawn from , and 0 if it is drawn from . More generally (especially in the case we want to extend our model to 3, 4, ... mixtures), and . \nWith that notation, the likelihood becomes \n \nand the log likelihood \n \nthe term on the right is useless since we only care about p, here. From here, consider the following iterative procedure, \nAssume that the mixture probability is known, denoted . Then I can predict the value of (i.e. and ) for all observations, \n \nSo I can inject those values into my log likelihood, i.e. in \n \nhaving maximum (no need to run numerical tools here) \n \nthat will be denoted . And I can iterate from here. \nFormally, the first step is where we calculate an expected ( E ) value, where is the best predictor of given my observations (as well as my belief in ). Then comes a maximization ( M ) step, where using , I can estimate probability . \n A more general framework, all parameters are now unkown \n \nSo far, it was simple, since we assumed that and were perfectly known. Which is not reallistic. An there is not much to change to get a complete algorithm, to estimate . Recall that we had which was the expected value of Z_{1,i}, i.e. it is a probability that observation i has been drawn from . \nIf , instead of being in the segment was in , then we could have considered mean and standard deviations of observations such that =0, and similarly on the subset of observations such that =1. \nBut we can\u2019t. So what can be done is to consider as the weight we should give to observation i when estimating parameters of , and similarly, 1- would be weights given to observation i when estimating parameters of . \nSo we set, as before \n \nand then \n \n \nand for the variance, well, it is a weighted mean again, \n \n \n \nand this is it. \n Let us run the code on the same data as before \n \nHere, the code is rather simple: let us start generating a sample \n > X1 = rnorm(n,0,1) \n> X20 = rnorm(n,0,1) \n> Z = sample(c(1,2,2),size=n,replace=TRUE) \n> X2=4+X20 \n> X = c(X1[Z==1],X2[Z==2]) \n then, given a vector of initial values (that I called and then before), \n > s = c(0.5, mean(X)-1, var(X), mean(X)+1, var(X)) \nI define my function as, \n > em = function(X0,s) { \n+ Ep = s[1]*dnorm(X0, s[2], sqrt(s[4]))/(s[1]*dnorm(X0, s[2], sqrt(s[4])) + \n+ (1-s[1])*dnorm(X0, s[3], sqrt(s[5]))) \n+ s[1] = mean(Ep) \n+ s[2] = sum(Ep*X0) / sum(Ep) \n+ s[3] = sum((1-Ep)*X0) / sum(1-Ep) \n+ s[4] = sum(Ep*(X0-s[2])^2) / sum(Ep) \n+ s[5] = sum((1-Ep)*(X0-s[3])^2) / sum(1-Ep) \n+ return(s) \n+ } \nThen I get , or .\nSo this is it\u2009! We just need to iterate (here I stop after 200\niterations) since we can see that, actually, our algorithm converges\nquite fast, \n > for(i in 2:200){ \n+ s=em(X,s) \n+ }  \nLet us run the same procedure as before, i.e. I generate samples of\nsize 200, where difference between means can be small (0) or large (4), \n \nOk, Nicolas, you were right, we\u2019re doing much better\u2009! Maybe we should\nalso go for a Gibbs sampling procedure\u2009?... next time, maybe...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/25/EM-and-mixture-estimation%2C-with-R-%28part-2%29", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Recently, one of my students asked me about optimization routines in R.\nHe told me he that R performed well on the estimation of a time series\nmodel with different regimes, while he had trouble with a (simple)\nGARCH process, and he was wondering if R was good in optimization\nroutines. Actually, I always thought that mixtures (and regimes) was\nsomething difficult to estimate, so I was a bit surprised... \n \nIndeed, it reminded me some trouble I experienced once, while I was talking about maximum likelihooh estimation ,\nfor non standard distribution, i.e. when optimization had to be done\non the log likelihood function. And even when generating nice samples, giving appropriate\ninitial values (actually the true value used in random generation), each time I tried to optimize my log likelihood, it\nfailed. So I decided to play a little bit with standard optimization\nfunctions, to see which one performed better when trying to estimate\nmixture parameter (from a mixture based sample). Here, I generate a\nmixture of two gaussian distributions, and I would like to see how\ndifferent the mean should be to have a high probability to estimate\nproperly the parameters of the mixture.\n \nThe density is here proportional to The true model is , and being a parameter that will change, from 0 to 4. The log likelihood (actually, I add a minus since most of the optimization functions actually minimize functions) is > logvraineg <- function(param, obs) { + p <- param[1]  +  m1 <- param[2]  +  sd1 <- param[3]  +  m2 <- param[4]  +  sd2 <- param[5]  +  -sum(log(p * dnorm(x = obs, mean = m1, sd = sd1) + (1 - p) *  +  dnorm(x = obs, mean = m2, sd = sd2)))  +  } The code to generate my samples is the following, >X1 = rnorm(n,0,1) > X20 = rnorm(n,0,1) > Z = sample(c(1,2,2),size=n,replace=TRUE) > X2=m+X20 > X = c(X1[Z==1],X2[Z==2]) Then I use two functions to optimize my log likelihood, with identical intial values, > O1=nlm(f = logvraineg, p = c(.5, mean(X)-sd(X)/5, sd(X), mean(X)+sd(X)/5, sd(X)), obs = X) > logvrainegX <- function(param) {logvraineg(param,X)} > O2=optim( par = c(.5, mean(X)-sd(X)/5, sd(X), mean(X)+sd(X)/5, sd(X)), + fn = logvrainegX) Actually, since I might have identification problems, I take either or , depending whether or is the smallest parameter. \nOn the graph above, the x-axis is the difference between means of the mixture (as on the animated grap above). Then, the red point is the median of estimated parameter I have (here ), and I have included something that can be interpreted as a confidence interval , i.e. where I have been in 90% of my scenarios: the black vertical segments. Obviously, when the sample is not enough heterogeneous (i.e. and \nrather different), I cannot estimate properly my parameters, I might\neven have a probability that exceed 1 (I did not add any constraint).\nThe blue plain horizontal line is the true value of the parameter, while the blue \ndotted horizontal line is the initial value of the parameter in the\noptimization algorithm (I started assuming that the mixture probability\nwas around 0.2). The graph below is based on the second optimization\nroutine (with identical starting values, and of course on the same\ngenerated samples),  \n(just\nto be honest, in many cases, it did not converge, so the loop stopped,\nand I had to run it again... so finally, my study is based on a bit\nless than 500 samples (times 15 since I considered several values for\nthe mean of my second underlying distribution), with 200 generated\nobservations from a mixture). The graph below compares the two (empty circles are the first algorithm, while plain circles the second one), \nOn\naverage, it is not so bad.... but the probability to be far away from\nthe tru value is not small at all... except when the difference between\nthe two means exceeds 3... If I change starting values for the\noptimization algorithm (previously, I assumed that the mixture\nprobability was 1/5, here I start from 1/2), we have the following graph \nwhich\nlook like the previous one, except for small differences between the\ntwo underlying distributions (just as if initial values had not impact\non the optimization, but it might come from the fact that the surface\nis nice, and we are not trapped in regions of local minimum). Thus, I am far from being an expert in optimization routines in R (see here \nfor further information), but so far, it looks like R is not doing so\nbad... and the two algorithm perform similarly (maybe the first one\nbeing a bit closer to the true parameter)."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/23/Optimization-and-mixture-estimation%2C-with-R", "bloglinks": {}, "links": {"http://cran.r-project.org/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Avant toute autre chose, pour revoir le dernier cours, il suffit\nd\u2019aller ici \net l\u00e0 .\n \nBon, sinon, comme promis, les bases de donn\u00e9es pour les\nprojets sont en\nligne ici. Le principe est simple. Il y a 28 bases de\ndonn\u00e9es,\ntoutes semblables (mais bien s\u00fbr diff\u00e9rentes), par\nles\nnum\u00e9ros ci-dessous. Comme toujours, premier\narriv\u00e9,\npremier servi, donc les bases vont \u00eatre attribu\u00e9es\nau fur\net \u00e0 mesure \n \u00e9tape\n1: choisir une base \n \nVous choisissez un num\u00e9ro et cliquez\ndessus , afin de me\npr\u00e9venir (par courriel, \u00e0 l\u2019aide du tableau\nci-dessous) de votre choix. Les projets se font par\ndeux, merci de mettre dans le corps du message les noms des deux\npersonnes qui travailleront sur le projet. \n \n \n \n \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \u00e9tape\n2: r\u00e9cup\u00e9rer les donn\u00e9es \n \nune fois que j\u2019aurais confirm\u00e9 que personne n\u2019a choisi la\nm\u00eame base, vous pouvez les r\u00e9cup\u00e9rer\nsous R \u00e0 l\u2019aide du code suivant, \n >\nk=1 \n>\nnom=paste(\"http://perso.univ-rennes1.fr/arthur.charpentier/6705V/UdM-baseC-\", \n+  k,\".txt\",sep=\"\") \n> baseC=read.table(nom,header=TRUE) \npour la base de contrats (et pour le groupe qui aurait choisi la base\n1) et pour la base des sinistres, \n >\nnom=paste(\"http://perso.univ-rennes1.fr/arthur.charpentier/6705V/UdM-baseS-\", \n+  k,\".txt\",sep=\"\") \n> baseS=read.table(nom,header=TRUE) \n Ensuite, c\u2019est parti, il s\u2019agit de me\nproposer diff\u00e9rents\nmod\u00e8les de tarification, et de calculer les primes pures\navec les diff\u00e9rents mod\u00e8les, pour une personne\nparmi les listes des personnes ayant les caract\u00e9ristiques\nsuivantes, \n >\nclient=data.frame( \n+ exposition=rep(1,9), \n+ zone=c(\"A\",\"A\",\"A\",\"C\",\"D\",\"E\",\"F\",\"F\",\"F\"), \n+ puissance=c(6,7,11,6,7,11,6,7,11), \n+ agevehicule=c(0,1,5,10,5,1,0,6,10), \n+ ageconducteur=c(25,18,55,55,55,40,21,20,18), \n+ bonus=c(80,100,50,60,55,50,100,125,100), \n+ marque=c(1,2,12,12,12,1,1,1,2), \n+ carburant=c(\"D\",\"E\",\"E\",\"D\",\"D\",\"E\",\"E\",\"D\",\"D\"), \n+ densite=rep(3000,9), \n+ client=rep(baseC$region[1],9)) \n (si\ncertains modalit\u00e9s ne sont pas pr\u00e9sentes dans\nla base, il faut choisir quelqu\u2019un d\u2019autre.... sur les neuf, il doit\nbien en avoir un(e) qui pourrait \u00eatre pr\u00e9sent(e)\ndans votre base de donn\u00e9es). En cas de probl\u00e8me,\nvous avez mon adresse \u00e9lectronique.... Et je reviendrais\nult\u00e9rieurement sur la forme de ce que j\u2019attends."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/23/Statistique-de-l-assurance-STT6705V%2C-partie-4b", "bloglinks": {}, "links": {"http://www.umontreal.ca/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["La derni\u00e8re fois (il y a quelques mois, ici )\non avait \u00e9voqu\u00e9 le lien entre cote et probabilit\u00e9s\nrisques neutres (induites). J\u2019avais \u00e9voqu\u00e9 les cotes\n\u00e0 un instant donn\u00e9 donn\u00e9, et montr\u00e9 comment\non pouvait construire la probabilit\u00e9 risque neutre\nassoci\u00e9e. L\u2019id\u00e9e est ici de suivre l\u2019\u00e9volution des\nprobabilit\u00e9s pendant la coupe du monde, au fur et \u00e0\nmesure que les matchs r\u00e9v\u00e8lent de l\u2019information sur le vrai niveau des \u00e9quipes... \nA l\u2019aide des donn\u00e9es \" World Cup Group A Betting \" (\" Win Market \")\nsur http://www.oddschecker.com/ (Vincent, alias @Vicnent , avait fait des sauvegarde\nr\u00e9guli\u00e8res des pages html), on peut suivre les cotes\ntoutes les 30 minutes, entre le 10 juin et le 22 juin, au sein du\ngroupe de l\u2019\u00e9quipe de France. \n \nPour le premier site de pari (bet365), on a les probabilit\u00e9s\nsuivantes, avec les probabilit\u00e9s associ\u00e9es \u00e0\nl\u2019\u00e9quipe de France (en bleu ), \u00e0 l\u2019Afrique du Sud (en jaune ), l\u2019Uruguay (en rouge ) et le Mexique (en vert ),\n \nOn note que les probabilit\u00e9s sont stables en dehors des matchs,\nautrement dit seul le comportement sur le terrain semble\nint\u00e9resser les parieurs (les matchs ont lieu pendant les date\no\u00f9 l\u2019on observe des traits bleus clairs verticaux). \nPour le troisi\u00e8me, on observe quelque chose de tr\u00e8s proche (les valeurs\nmanquants signifient que le tableau en ligne \u00e9tait vide, ou que j\u2019ai\nrat\u00e9 ma lecture du fichier html sous R), avec des variations du m\u00eame\nordre \u00e0 la fin des matchs, \n \nPour le sixi\u00e8me on a \npour le neuvi\u00e8me,  \net pour le douzi\u00e8me (je n\u2019ai pas affich\u00e9 tous les graphs) \nOn\nnotera qu\u2019\u00e0 la fin, apr\u00e8s les seconds matchs plus personnes ne semblait\ncroire \u00e0 l\u2019\u00e9quipe de France (en France en tous les cas), sa probabilit\u00e9\nde gagner pour les parieurs est rest\u00e9 \u00e9lev\u00e9e, et ce, chez tous les\nsites de paris.... il aurait probablement \u00e9t\u00e9 int\u00e9ressant de parier \u00e0\nla fin sur le Mexique (m\u00eame si je sais que r\u00e9trospectivement, il est\ntoujours facile de dire ce qu\u2019il fallait alors faire). Attendre le\ndernier moment pour faire des paris n\u2019est pas forc\u00e9ment stupide... La prochaine \u00e9tape c\u2019est de regarder non pas les pronostiques\npar poule, mais globalement, sur le futur vainqueur. Nous verrons en\nparticulier comment les matchs des autres influences les\nprobabilit\u00e9s d\u2019une \u00e9quipe...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/23/Soccer%2C-probabilit%C3%A9-%28et-assurance%29-partie-3", "bloglinks": {}, "links": {"http://twitter.com/": 1, "http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Je\nm\u2019\u00e9tais promis que j\u2019\u00e9voquerais une bizarrerie rencontr\u00e9e avec SAS lors\nd\u2019une formation.... \u00c9crire ce billet permettra \u00e0 ceux qui auraient des\n\u00e9l\u00e9ments d\u2019explication de poster un commentaire. Pour cela, comparons une r\u00e9gression logistique faite avec deux outils diff\u00e9rents, sous SAS, avec la proc\u00e9dure logistique \n \nLe code pour faire une r\u00e9gression logistique ressemble \u00e0 \u00e7a PROC LOGISTIC DATA=base_logistq; FORMAT age_soc f2_ageso.; CLASS sexe_soc age_soc fract_paiemt; MODEL SPOCAM = sexe_soc age_soc fract_paiemt / selection=stepwise; RUN; QUIT; ce qui donne la sortie suivante (je passe l\u2019introduction pour insister sur les coefficients)         The LOGISTIC Procedure      Analyse des estimations de la vraisemblance maximum              Erreur   Khi 2 Param\u00e8tre     DF Estimation   std  de Wald Pr > Khi 2 Intercept      1  1.7833  0.0676  696.9022  <.0001 sexe_soc  Femme   1  -0.2429  0.0619  15.4237  <.0001 age_soc  1_AGESOC_-60  1  0.4578  0.0667  47.1020  <.0001 fract_paiemt Annuel   1  0.6021  0.0997  36.4862  <.0001 fract_paiemt Mensuel   1  -0.5410  0.0842  41.2342  <.0001  avec la proc\u00e9dure genmod (car la r\u00e9gression logistique est un glm) \n \nOn peut faire exactement la m\u00eame chose (th\u00e9oriquement) en ajustement un mod\u00e8le GLM, PROC GENMOD DATA=base_logistq; FORMAT age_soc f2_ageso.; CLASS sexe_soc age_soc fract_paiemt; MODEL SPOCAM = sexe_soc age_soc fract_paiemt / dist = binomial; RUN; et la sortie ressemble \u00e0 \u00e7a          The GENMOD Procedure      Analyse des r\u00e9sultats estim\u00e9s de param\u00e8tres             Erreur  Wald 95Limites Param\u00e8tre      DF Estimation standard  de confiance %  Khi 2 Intercept      1  1.5073  0.1501  1.2131  1.8014 100.85 sexe_soc  Femme   1  -0.4859  0.1237 -0.7284 -0.2434  15.42 sexe_soc  Homme   0  0.0000  0.0000  0.0000  0.0000  . age_soc  1_AGESOC_-60 1  0.9156  0.1334  0.6542  1.1771  47.10 age_soc  Z_AGESOC_+60 0  0.0000  0.0000  0.0000  0.0000  . fract_paiemt Annuel   1  0.6634  0.1770  0.3165  1.0104  14.05 fract_paiemt Mensuel   1  -0.4798  0.1510 -0.7759 -0.1838  10.09 fract_paiemt Semestriel  0  0.0000  0.0000  0.0000  0.0000  . Scale       0  1.0000  0.0000  1.0000  1.0000  comparaison des deux sorties \n \nSi on regarde l\u2019impact du sexe par exemple, dans la premi\u00e8re sortie on peut lire  sexe_soc  Femme   1  -0.2429  0.0619  15.4237  <.0001 alors que dans la seconde sortie, on a sexe_soc  Femme   1  -0.4859  0.1237 -0.7284 -0.2434  15.42 sexe_soc  Homme   0  0.0000  0.0000  0.0000  0.0000   On\ndira ce qu\u2019on veut, mais moi je trouve cette diff\u00e9rence troublante....\nDans la seconde sortie, le coefficient vaut le double de l\u2019autre.... Alors\nSAS semble s\u2019y retrouver car si on lui demande d\u2019afficher le score\npr\u00e9dit pour un individu au hasard (le premier de la base par exemple),\nles pr\u00e9dictions sont tr\u00e8s proches,        fract_         proba1_  proba1_ Obs sexe_soc age_soc paiemt  SPOCAM proba1_logit   1  Homme   71 Annuel   0  0.10242637 0.10241302  Si\nquelqu\u2019un sait interpr\u00e9ter ce qui est fait avec cette proc\u00e9dure\nlogistique (car R donne la m\u00eame chose que la sortie GLM), je suis\npreneur....."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/23/Qui-peut-m-aider-%C3%A0-comprendre-les-sorties-de-SAS", "bloglinks": {}, "links": {}, "blogtitle": "Arthur Charpentier"}, {"content": ["On continue le cours avec la fin sur la mod\u00e9lisation de la fr\u00e9quence, o\u00f9 on parlera de surdispersion (\u00e9voqu\u00e9e ici par exemple), de quasi-lois ( ici ou l\u00e0 ), de r\u00e9gression binomiale n\u00e9gative ( ici ), ou de mod\u00e8les \u00e0 inflation de z\u00e9ros ( l\u00e0 \npar exemple). Et si \u00e7a ne suffit pas, on commencera \u00e0 parler de la\nmod\u00e9lisation des co\u00fbts individuels. Les lois de bases seront les lois lognormales, et gamma, \n \n Les transparents seront bient\u00f4t en\nligne ici . \n \n Dans le prochain billet, je mettrais les liens vers les bases de\ndonn\u00e9es pour le projet d\u2019\u00e9conom\u00e9trie. Sinon pour les rappels de la\nsemaine derni\u00e8re, je renvoie Sinon pour les rappels de la\nsemaine derni\u00e8re, je renvoie ici \net l \u00e0 ."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/22/Statistique-de-l-assurance-STT6705V%2C-partie-4", "bloglinks": {}, "links": {"http://perso.univ-rennes1.fr/": 1, "http://www.umontreal.ca/": 3, "http://freakonometrics.free.fr/": 5}, "blogtitle": "Arthur Charpentier"}, {"content": ["Bon, je vais faire un rapide billet pour r\u00e9pondre \u00e0 un commentaire qui avait\n\u00e9t\u00e9 post\u00e9 ici ,\nqui remettait en cause mes\nconclusions (ce qui ne me g\u00eane pas, loin de l\u00e0, c\u2019est\ncomme \u00e7a qu\u2019on va faire avancer les choses), et surtout demandait des\ninformations compl\u00e9mentaires. J\u2019avais un peu forc\u00e9 le trait en\naffirmant que \" une personne de 70 ans vivra \u00e0 peine plus longtemps - en moyenne -\nqu\u2019une personne de 70 ans en 1950 \"\n(c\u2019est peut \u00eatre le cas pour les centenaires, mais je n\u2019ai pas de\ndonn\u00e9es), mais le but de mon pr\u00e9c\u00e9dant billet \u00e9tait de montrer que les\ngains sont beaucoup plus faibles que pour les plus jeunes. J\u2019ai\nd\u00e9cid\u00e9 de refaire l\u2019\u00e9tude, de mani\u00e8re plus\nclaire peut \u00eatre, et plus compl\u00e8te. Et en utilisant des donn\u00e9es de l\u2019INED, qui est une source a priori fiable de donn\u00e9es.\n \n Les donn\u00e9es (d\u00e9taill\u00e9es) \n \nComme je le disais, tout d\u2019abord, les donn\u00e9es: je prends ici les\ndonn\u00e9es de l\u2019INED (et pas de http://mortalityorg). On peut trouver en\nligne (au format xls) des donn\u00e9es de population par \u00e2ge et par sexe, ici \n(populations par \u00e2ge, de 0 \u00e0 100 ans, au premier janvier,\nde 1899 \u00e0 1998, selon le territoire couvert par la statistique\ndes d\u00e9c\u00e8s, ensemble des deux sexes, avec depuis 1946, les\ndonn\u00e9es INSEE et avant 1946, des reconstitutions\nintercensitaires (Vallin, 1973)). Et l\u00e0 ,\non a les donn\u00e9es du nombre de d\u00e9c\u00e8s par \u00e2ge\net par sexe (d\u00e9c\u00e8s par \u00e2ge et par\ng\u00e9n\u00e9ration, de 1899 \u00e0 1997, ensemble des deux\nsexes, avec pour les moins de 100 ans, depuis 1907, d\u00e9c\u00e8s\nhors pertes militaires (SGF, puis INSEE), pour les ann\u00e9es\nde guerre : d\u00e9c\u00e8s (INSEE + estimations des pertes\nmilitaires (Vallin, 1973)) et entre 1899 et 1906 : d\u00e9c\u00e8s\npar \u00e2ge de l\u2019INSEE, r\u00e9partition entre les triangles de\nLexis (Vallin, 1973). Pour les centenaires, jusqu\u2019en 1967 : SGF, INSEE\n+ r\u00e9partitions estim\u00e9es par les auteurs, et depuis 1968 :\nINSEE (extrait d\u2019enregistrements individuels fournis par l\u2019INSEE dans\nle cadre d\u2019un avenant \u00e0 la convention INED-INSEE)). Voil\u00e0\npour les donn\u00e9es, ou presque.... \nPar paresse, et pour pouvoir partager les donn\u00e9es plus facilement, j\u2019ai mis en ligne des fichiers csv sur ma page. \nLes donn\u00e9es (brutes) contiennent des colonnes sans\nint\u00e9r\u00eat, que je me suis permis de virer. Sinon dans la\nbase des populations, certaines lignes ont \u00e9t\u00e9 supprim\u00e9es car en 1914 il\ny avait deux populations, sur territoire d\u2019avant 1914 (sans\nl\u2019Alsace-Lorraine: Moselle, Bas-Rhin,\nHaut-Rhin), et le territoire sans op\u00e9rations militaires\n(manquent, outre l\u2019Alsace-Lorraine: Aisne, Ardennes, Marne,\nMeurthe-et-Moselle, Meuse, Nord, Oise, Pas-de-Calais, Somme et Vosges).\nC\u2019est cette derni\u00e8re qui est retenues dans la base des\nd\u00e9c\u00e8s c\u2019est donc cette derni\u00e8re que l\u2019on utilisera\npour calculer l\u2019exposition. \n > tabB=read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/tabB.csv\", \n+  sep=\";\",header=FALSE) \n> ANNEE=tabB[,1] \n> BASEB=tabB[,seq(2,246,by=2)] \n> BASEB=BASEB[,1:100] \n> tabC=read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/tabC.csv\", \n+  sep=\";\",header=FALSE) \n> BASEC=tabC[,2:101] \n> BASEC=BASEC[-c(16,23,43,48,51,53),] \n> BASEC=BASEC[1:nrow(BASEB),] \n> AGE=0:99 \n Les taux de d\u00e9c\u00e8s (population enti\u00e8re) \n \nCommen\u00e7ons par calculer, et tracer les taux de\nd\u00e9c\u00e8s, qui sont les ratios entre les nombres de\nd\u00e9c\u00e8s et la population totale. \nMon premier point (pour r\u00e9pondre au commentaire) est que si l\u2019on\nregarde un peu la surface, les gains en terme de taux de\nmortalit\u00e9 instantan\u00e9s sont plus importants \u00e0 la\nnaissance (courbe rouge ) qu\u2019\u00e0 des \u00e2ges beaucoup plus \u00e9lev\u00e9s, comme 90 ans (en bleu ) voire \u00e0 65 ans (en vert ), \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Un peu de prospective \n \nComme on le voit, sur nos donn\u00e9es, on est limit\u00e9 car on\nn\u2019a qu\u2019un si\u00e8cle de donn\u00e9es, et c\u2019est donc d\u00e9licat\nde suivre les esp\u00e9rances de vie. \nLe plus simple est alors de reprendre ce que j\u2019avais fait (en ligne ici ), utilisons les fonctions de Rob Hyndman, en ligne sur son blog ( l\u00e0 ), mais sur nos donn\u00e9es (la derni\u00e8re fois j\u2019avais utilis\u00e9 les siennes). \n > library(demography) \n Le chargement a n\u00e9cessit\u00e9 le package : forecast \n Le chargement a n\u00e9cessit\u00e9 le package : fracdiff \n This is forecast 2.05 \n Le chargement a n\u00e9cessit\u00e9 le package : rainbow \n Le chargement a n\u00e9cessit\u00e9 le package : hdrcde \n Le chargement a n\u00e9cessit\u00e9 le package : locfit \n Le chargement a n\u00e9cessit\u00e9 le package : akima \n Le chargement a n\u00e9cessit\u00e9 le package : lattice \n locfit 1.5-6  2010-01-20 \n Le chargement a n\u00e9cessit\u00e9 le package : ash \n Le chargement a n\u00e9cessit\u00e9 le package : ks \n Le chargement a n\u00e9cessit\u00e9 le package : KernSmooth \n KernSmooth 2.23 loaded \n Copyright M. P. Wand 1997-2009 \n Le chargement a n\u00e9cessit\u00e9 le package : mvtnorm \n hdrcde 2.13 loaded \n Attachement du package : \u2019hdrcde\u2019 \n Le chargement a n\u00e9cessit\u00e9 le package : MASS \n Le chargement a n\u00e9cessit\u00e9 le package : pcaPP \n Le chargement a n\u00e9cessit\u00e9 le package : cluster \n Le chargement a n\u00e9cessit\u00e9 le package : ftsa \n This is demography 1.03 \n Il y a eu 13 avis (utilisez warnings() pour les visionner) \nCommen\u00e7ons par mettre les donn\u00e9es au bon format. Pour des raisons num\u00e9riques, je suis oblig\u00e9 \nde me d\u00e9barrasser des \u00e2ges \u00e9lev\u00e9s pour lesquels j\u2019ai pu avoir une\npopulation nulle, ou pour lesquels j\u2019ai des valeurs manquantes. J\u2019ai\ndonc exclu les \u00e2ges sup\u00e9rieurs \u00e0 90 ans, \n > donnees = demogdata(data=t(as.matrix(BASEB[,1:90]))/t(as.matrix(BASEC[,1:90])), +      pop= t(as.matrix(BASEC[,1:90])), +      ages=AGE[1:90], +      years=ANNEE, +      type=\"mortality\", label=\"France\", name=\"total\", lambda=0) > donnees Mortality data for France  Series: total  Years: 1899 - 1997  Ages: 0 - 89 Ensuite, on peut reprendre le code que j\u2019avais fait. Notons qu\u2019on\nutiliser ici une mod\u00e9lisation \u00e0 la Lee et Carter, ce qui\na \u00e9t\u00e9 utilis\u00e9e par l\u2019INED pour extrapoler\ncertaines donn\u00e9es dans la table. \n > france.LC2 <- lca(donnees,adjust=\"dt\",series=\"total\") > france.fcast <- forecast(france.LC2) > L2 <- lifetable(france.fcast) > ex2=L2$ex > L1=lifetable(donnees,series=\"total\") > ex1=L1$ex Pour l\u2019esp\u00e9rance de vie \u00e0 la naissance ,\net son \u00e9volution dans le temps, si on regarde les gains moyens ann\u00e9e\napr\u00e8s ann\u00e9e, on voit que l\u2019on gagne, en moyenne, 1/4 d\u2019esp\u00e9rance de\nvie, soit 3 mois par an (comme dans mon pr\u00e9c\u00e9dant billet), > age=0 > ex=c(ex1[age+1,],ex2[age+1,]) > plot(1899:2047,ex,col=\"blue\") > I=(1950:2000)-1898 > y=ex[I] > x=1950:2000 > lm(y~x) Call: lm(formula = y ~ x) Coefficients: (Intercept)   x  -421.0384  0.2549 > abline(lm(y~x),col=\"red\") > points(x,y,pch=19,col=\"red\")  \nOn\nnotera qu\u2019en utilisant les autres m\u00e9thodes d\u2019estimation, j\u2019obtiens la\nm\u00eame pente. En revanche, si je regarde l\u2019esp\u00e9rance de vie r\u00e9siduelle \u00e0\n65 ans, elle n\u2019est plus que de 0,16 ann\u00e9es, soit moins de deux mois\nchaque ann\u00e9es, > age=65 > ex=c(ex1[age+1,],ex2[age+1,]) > plot(1899:2047,ex,col=\"blue\") > I=(1950:2000)-1898 > y=ex[I] > x=1950:2000 > lm(y~x) Call: lm(formula = y ~ x) Coefficients: (Intercept)   x  -295.6812  0.1612 > abline(lm(y~x),col=\"red\") > points(x,y,pch=19,col=\"red\")  \nDit\nautrement, entre un b\u00e9b\u00e9 qui nait en 1975 et un b\u00e9b\u00e9 qui nait en 1995,\nil y a un gain d\u2019environ 6 ans d\u2019esp\u00e9rance de vie, passant de 81,8 ans\n\u00e0 87,7, alors que pour une personne qui atteignait les 65 ans en 1975,\nil avait 22 ans \u00e0 vivre encore, en moyenne, contre seulement 26,5\npour une personne de 65 ans en 1995. Donc je peux maintenir ce que\nj\u2019affirmais la derni\u00e8re fois, sur d\u2019autres donn\u00e9es.... En fait, si on\nveut \u00eatre plus pr\u00e9cis, on peut pr\u00e9cis\u00e9ment regarder l\u2019\u00e9volution de la\npente, correspondant au gain moyen d\u2019esp\u00e9rance de vie chaque ann\u00e9e, \nBref,\nde consid\u00e9rables progr\u00e8s ont \u00e9t\u00e9 effectu\u00e9s en terme de gains\nd\u2019esp\u00e9rance de vie, mais beaucoup plus sur les jeunes que sur les\npersonnes plus \u00e2g\u00e9es..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/14/Esp%C3%A9rance-de-vie-%C3%A0-la-naissance%2C-ou-%C3%A0-65-ans", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 1, "http://robjhyndman.com/": 1, "http://freakonometrics.free.fr/": 1, "http://www.ined.fr/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Suite \u00e0 une question lors du cours de mercredi dernier (et comme Brice\nm\u2019a pos\u00e9 exactement la m\u00eame par mail tout \u00e0 l\u2019heure), je vais faire un\nrapide billet sur les liens entre \" mod\u00e8le(s) GLM sur variable(s) qualitative(s) \" (exclusivement) et \" moyenne par classe \". Ce\nque je disais en classe, c\u2019est qu\u2019en tarification, faire des GLM sur\ndes donn\u00e9es exclusivement qualitatives, c\u2019\u00e9tait \u00e9quivalent \u00e0 calculer\nla moyenne par modalit\u00e9s (par classe tarifaire finalement). J\u2019en avais\nd\u00e9j\u00e0 parl\u00e9 dans un billet sur la m\u00e9thode des marges (car c\u2019est de \u00e7a qu\u2019on parle en fait), ici .\nMais ici, je voulais illustrer ce point sur un exemple (ou plut\u00f4t\nplusieurs exemples), afin d\u2019expliquer ce que j\u2019entendais pas \" c\u2019est la m\u00eame chose \"... \n en pr\u00e9sence d\u2019une unique variable qualitative \n \nBon, commen\u00e7ons par simuler un jeu de donn\u00e9es pour faire de l\u2019\u00e9conom\u00e9trie... > set.seed(1) > X = as.factor(sample(c(\"A\",\"B\",\"C\",\"D\"),size=1000, +  replace=TRUE,prob=c(.4,.3,.2,.1))) > F=data.frame(X=c(\"A\",\"B\",\"C\",\"D\")) > M = rep(NA,1000) > M[X==\"A\"]=1 > M[X==\"B\"]=1.5 > M[X==\"C\"]=2 > M[X==\"D\"]=2.5 > Z = rexp(1000,rate=M) > Y = rpois(1000,Z) > base=data.frame(X,Y) > table(base)  Y X  0 1 2 3 4 5 6 7 8 11  A 188 102 62 28 12 3 6 2 1 1  B 169 69 22 20 8 2 0 1 0 0  C 140 39 8 6 1 0 0 0 0 0  D 85 17 5 3 0 0 0 0 0 0 Le bon mod\u00e8le est un mod\u00e8le de Poisson avec un lien logarithmique.... Regardons d\u00e9j\u00e0 ce que sont les moyennes par modalit\u00e9s, > mean(Y[X==\"A\"]) [1] 1.091358 > mean(Y[X==\"B\"]) [1] 0.7628866 > mean(Y[X==\"C\"]) [1] 0.3969072 > mean(Y[X==\"D\"]) [1] 0.3272727 Pour comparer avec des mod\u00e8les GLM, commen\u00e7ons par le bon mod\u00e8le, > predict(reg,newdata=F,type=\"response\")   1   2   3   4 1.0913580 0.7628866 0.3969072 0.3272727 ce\nqui est exactement ce que nous avions, en moyenne, par classes. Que se\npasse-t-il si on se trompe de mod\u00e8le\u2009? par exemple avec une autre\nfonction lien, > reg=glm(Y~X,data=base,family=poisson(link=\"identity\")) > predict(reg,newdata=F,type=\"response\")   1   2   3   4 1.0913580 0.7628866 0.3969072 0.3272727  et si on change aussi la loi, l\u00e0 aussi on retombe sur la moyenne par modalit\u00e9... > reg=glm(Y~X,data=base,family=gaussian(link=\"identity\"))  > predict(reg,newdata=F,type=\"response\")   1   2   3   4 1.0913580 0.7628866 0.3969072 0.3272727 On peut aussi tenter une r\u00e9gression binomiale n\u00e9gative par exemple, > library(MASS) > reg=glm.nb(Y~X,data=base) > predict(reg,newdata=F,type=\"response\")   1   2   3   4 1.0913580 0.7628866 0.3969072 0.3272727  Moralit\u00e9, on retrouve la m\u00eame chose avec les deux approches: moyenne par modalit\u00e9, ou GLM. en pr\u00e9sence d\u2019une unique variable qualitative et d\u2019une exposition diff\u00e9rente \n \nEn\npratique, si on cherche \u00e0 mod\u00e9liser la fr\u00e9quence individuelle de\nsinistres, il convient de prendre en compte l\u2019exposition (c\u2019est \u00e0 dire\nle fait que le nombre de sinistres observ\u00e9s dans la base n\u2019a pas \u00e9t\u00e9\nobtenu sur une ann\u00e9e compl\u00e8te, mais parfois quelques mois).\nClassiquement, le nombre de sinistres sera suppos\u00e9 proportionnel \u00e0\ncette exposition. Consid\u00e9rons la base suivante, > set.seed(1) > X = as.factor(sample(c(\"A\",\"B\",\"C\",\"D\"),size=1000, +  replace=TRUE,prob=c(.4,.3,.2,.1))) > E = runif(1000) > F=data.frame(X=c(\"A\",\"B\",\"C\",\"D\")) > M = rep(NA,1000) > M[X==\"A\"]=1 > M[X==\"B\"]=1.5 > M[X==\"C\"]=2 > M[X==\"D\"]=2.5 > Z = rexp(1000,rate=M/E) > Y = rpois(1000,Z) > base=data.frame(X,Y) > table(base)  Y X  0 1 2 3 4 5 6 7  A 287 76 24 7 6 2 2 1  B 222 44 17 8 0 0 0 0  C 156 29 7 2 0 0 0 0  D 90 15 2 2 1 0 0 0 Les moyennes empiriques par modalit\u00e9 doivent \u00eatre pond\u00e9r\u00e9es par ces expositions. Le plus simple est de \" normaliser \",\nen modifiant les donn\u00e9es pour les passer sur une base annuelle (en\ndivisant par l\u2019exposition), puis en attribuant des poids proportionnels\n\u00e0 cette exposition, > weighted.mean(Y[X==\"A\"]/E[X==\"A\"],E[X==\"A\"]) [1] 1.001059 > weighted.mean(Y[X==\"B\"]/E[X==\"B\"],E[X==\"B\"]) [1] 0.6973345 > weighted.mean(Y[X==\"C\"]/E[X==\"C\"],E[X==\"C\"]) [1] 0.5110974 > weighted.mean(Y[X==\"D\"]/E[X==\"D\"],E[X==\"D\"]) [1] 0.5758276 Comme\nl\u2019exposition est suppos\u00e9e intervenir de mani\u00e8re proportionnelle (i.e.\nmultiplicative), il est logique de supposer un lien logarithmique, et\nsi l\u2019on consid\u00e8re une loi de Poisson, on obtient (en faisant une\npr\u00e9dicition pour une exposition d\u2019un an) > base=data.frame(X,Y,E) > F=data.frame(X=c(\"A\",\"B\",\"C\",\"D\"),E=1) > reg=glm(Y~X+offset(log(E)),data=base,family=poisson(link=\"log\")) > predict(reg,newdata=F,type=\"response\")   1   2   3   4 1.0010593 0.6973345 0.5110974 0.5758276  Bref,\nencore une loi les r\u00e9sultats sont identiques. Mais, si on fait une\nr\u00e9gression binomiale n\u00e9gative, on obtient des r\u00e9sultats l\u00e9g\u00e8rement\ndiff\u00e9rents (0,17%, 0,03%, 1,02% et 3,40% respectivement), mais il ne\nsont pas \u00e9gaux, > reg=glm.nb(Y~X+offset(log(E)),data=base) > predict(reg,newdata=F,type=\"response\")   1   2   3   4 0.9993223 0.6975693 0.5163053 0.5562640  Autrement dit, si on doit prendre en compte une exposition, les deux m\u00e9thodes ne co\u00efncident plus. en pr\u00e9sence de plusieurs variables qualitatives \n \nPour conclure, regardons ce qui se passe avec deux variables qualitatives, > set.seed(1) > X1 = as.factor(sample(c(\"A\",\"B\",\"C\",\"D\"),size=1000, +  replace=TRUE,prob=c(.4,.3,.2,.1))) > X2 = as.factor(sample(c(\"H\",\"F\"),size=1000, +  replace=TRUE,prob=c(.6,.4))) > M = rep(NA,1000) > M[(X1==\"A\")&(X2==\"H\")==TRUE]=1 > M[(X1==\"B\")&(X2==\"H\")==TRUE]=1.5 > M[(X1==\"C\")&(X2==\"H\")==TRUE]=2 > M[(X1==\"D\")&(X2==\"H\")==TRUE]=2.5 > M[(X1==\"A\")&(X2==\"F\")==TRUE]=1.1 > M[(X1==\"B\")&(X2==\"F\")==TRUE]=1.4 > M[(X1==\"C\")&(X2==\"F\")==TRUE]=2.1 > M[(X1==\"D\")&(X2==\"F\")==TRUE]=2.7 > Z = rexp(1000,rate=M) > Y = rpois(1000,Z) > base=data.frame(Y,X1,X2) Bon, on a compris l\u2019id\u00e9e, je vais me contenter de faire les calculs pour deux types d\u2019individus, > mean(Y[(X1==\"A\")&(X2==\"H\")==TRUE]) [1] 0.9227642 > mean(Y[(X1==\"C\")&(X2==\"F\")==TRUE]) [1] 0.4473684 Si on fait une r\u00e9gression sur les deux facteurs, on obtient > F=data.frame(X1=rep(c(\"A\",\"B\",\"C\",\"D\"),2),X2=rep(c(\"H\",\"F\"),each=4)) > reg=glm(Y~X1+X2,data=base,family=poisson(link=\"log\")) > predict(reg,newdata=F,type=\"response\")   1   2   3   4   5   6   7   8 0.9081317 0.6647936 0.4915430 0.3998411 0.9723245 0.7117857 0.5262885 0.4281045  Les r\u00e9sultats sont diff\u00e9rents des moyennes empiriques, tout simplement car on suppose ici les facteurs multiplicatifs, > summary(reg) Call: glm(formula = Y ~ X1 + X2, family = poisson(link = \"log\"), data = base) Coefficients:    Estimate Std. Error z value Pr(>|z|)  (Intercept) -0.02807 0.06824 -0.411 0.680851  X1B   -0.31191 0.08759 -3.561 0.000369 *** X1C   -0.61384 0.11336 -5.415 6.12e-08 *** X1D   -0.82032 0.15778 -5.199 2.00e-07 *** X2H   -0.06830 0.07587 -0.900 0.368009  --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 (Dispersion parameter for poisson family taken to be 1)  Null deviance: 1581.6 on 999 degrees of freedom Residual deviance: 1525.5 on 995 degrees of freedom AIC: 2489.6 Number of Fisher Scoring iterations: 6 > exp(coefficients(reg)[5])  X2H 0.93398  Autrement dit, pour un homme, je pr\u00e9dis toujours \u00eatre 6,4% inf\u00e9rieur \u00e0\nune femme. Or il n\u2019y a pas de raison que les deux facteurs\ninterviennent ind\u00e9pendamment... il faut donc plut\u00f4t regarder le produit\ncart\u00e9sien des modalit\u00e9s, > predict(reg,newdata=F,type=\"response\")   1   2   3   4   5   6   7   8 0.9227642 0.6416185 0.5423729 0.3243243 0.9496855 0.7457627 0.4473684 0.5833333  On\na alors 8 modalit\u00e9s possibles (le croisement des modalit\u00e9s de chacune\ndes variables), et cette fois, on retombe sur les moyennes par classes. Moralit\u00e9,\nles GLM sur des facteurs peuvent co\u00efncider avec les moyennes empiriques\npar classes, mais \u00e0 condition de le faire en suivant quelques r\u00e8gles\n(en particulier croiser toutes les modalit\u00e9s mais aussi faire attention \u00e0 l\u2019exposition)."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/11/Mod%C3%A8les-GLM-et-variables-qualitatives", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Dans un pr\u00e9c\u00e9dant billet ( ici )\nje volais un dessin de Martin Vidberg qui parlait de google et de\nstatistiques... Google trend est un outil magique pour noter des\ntendances et faire des statistiques simples et marrantes. Par exemple,\npuisque le ramadan touche \u00e0 sa fin, on peut regarder ce qui\nmotivent les alg\u00e9riens pendant le ramadan. Manifestement, le sexe n\u2019est pas une priorit\u00e9, que ce soit en anglais, \n  \nou en fran\u00e7ais,  \nDe mani\u00e8re g\u00e9n\u00e9rale, le mot cl\u00e9 le plus\ntap\u00e9 sous google a tendance \u00e0 se faire tout petit pendant\nla p\u00e9riode du ramadan, alias porno ,  \nEn fait, on cherche un peu moins les femmes sur le net,  \nPar contre, visiblement, on a faim, et on veut bien manger: cuisine explose pendant le ramadan,  \nOn peut aussi regarder les autres pays \u00e0 forte population musulmane, et observer des tendances similaires, comme le Maroc,  \nles \u00c9mirats Arabes Unis,  \nou encore le Pakistan,  \n(dans ce dernier cas, on observe un fort retour du mot cl\u00e9 sex une\nfois le ramadan pass\u00e9... une esp\u00e8ce de manque virtuel). Bref, avec\nGoogle, on peut vraiment observer des choses int\u00e9ressantes... non\u2009?"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/10/Ramadan%2C-entre-le-sexe-et-la-faim...", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1, "http://www.google.com/": 8}, "blogtitle": "Arthur Charpentier"}, {"content": ["Hier soir (ou ce matin, je suis perdu avec ce d\u00e9calage horaire) Christelle me demandait de parler un peu de pr\u00e9vision avec R ( ici ).\nAu lieu de renvoyer vers l\u2019aide en ligne, penons un exemple pratique\n(et simple, voire si possible int\u00e9ressant): la fr\u00e9quentation d\u2019un blog\n(en l\u2019occurrence http://blogperso.univ-rennes1.fr/arthur.charpentier/ ). Consid\u00e9rons le nombre de pages vues , par jour, selon Google Analytics. > base=read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/million.csv\", + sep=\";\",header=TRUE )> X=base$nombre > D0=as.Date(\"08/11/2008\",\"%d/%m/%Y\") > D=D0+1:length(X) > plot(D,X) > plot(acf(X,lag=90)) La s\u00e9rie a l\u2019allure suivante (oui, le compteur n\u2019a \u00e9t\u00e9 install\u00e9 qu\u2019il y a deux ans), \n \nLes autocorr\u00e9lations montre une forte saisonnalit\u00e9 hebdomadaire, avec moins plus (cf ici ) de consultations en semaine que le week-end, \nLa question que l\u2019on cherche \u00e0 r\u00e9soudre est \" ai-je une chance d\u2019atteindre le million de pages vues d\u2019ici la fin de l\u2019ann\u00e9e\u2009? \". \nOn peut traduire cette question de deux mani\u00e8res quelle est la probabilit\u00e9 que le 1er janvier, j\u2019ai atteint le million de pages vues, \n quelle est la probabilit\u00e9 que la date o\u00f9 le million de pages vues sera atteint soit avant le 1er janvier \n \nUne fois formalis\u00e9e la question, reste \u00e0 faire un peu d\u2019\u00e9conom\u00e9trie.  mod\u00e9lisation \u00e9conom\u00e9trique \n \nEn\nfaisant simple et rapide, afin de prendre en compte la corr\u00e9lation\nforte avec la semaine pr\u00e9c\u00e9dente, et le fait que l\u2019on s\u2019int\u00e9resse \u00e0 la\nsomme cumul\u00e9e, on peut consid\u00e9rer un mod\u00e8le ARIMA avec un retard d\u2019ordre 7 pour les composantes moyennes mobiles et autor\u00e9gressives, \n avec une s\u00e9rie int\u00e9gr\u00e9e \u00e0 l\u2019ordre 1, \n \nL\u2019ajustement se fait de la mani\u00e8re suivante, > X=cumsum(base$nombre) > model <- arima(X,c(7 , # partie AR +            1, # partie I +            7)) # partie MA et la pr\u00e9vision se fait via > forecast <- predict(model,200) Ensuite, ce n\u2019est qu\u2019une repr\u00e9sentation graphique, > u=max(D)+1:200 > polygon( c(u,rev(u)), c(forecast$pred - 1.96*forecast$se, + rev(forecast$pred + 1.96*forecast$se)), col = \"yellow\", border=NA) > lines(u,forecast$pred,col=\"blue\",lwd=2) > lines(u,forecast$pred- 1.96*forecast$se,col=\"blue\",lty=2) > lines(u,forecast$pred+ 1.96*forecast$se,col=\"blue\",lty=2) > abline(v=as.Date(\"01/01/2011\",\"%d/%m/%Y\"),col=\"red\") > abline(h=1000000,col=\"red\")  \nAfin de r\u00e9pondre \u00e0 la question pos\u00e9e, on peut \u00e9tudier les diff\u00e9rentes probabilit\u00e9s envisag\u00e9es, probabilit\u00e9 que le 1er janvier, j\u2019ai atteint le million de pages vues \n \nDans\nun premier temps, on utilise la normalit\u00e9 des pr\u00e9dictions (en supposant\nune normalit\u00e9 du bruit) pour obtenir la loi de la pr\u00e9diction \u00e0 une date\nquelconque, > k=which(u==as.Date(\"01/01/2011\",\"%d/%m/%Y\")) > x=seq(800000,1100000,by=100) > y=dnorm(x,mean=forecast$pred[k],sd=forecast$se[k]) > plot(x,y,type=\"l\",lwd=2) > x0=x[x>=1000000] > polygon( c(x0,rev(x0)), c(y[x>=1000000],rep(0,length(x0))), col = \"yellow\",border=NA) > lines(x,y,type=\"l\",lwd=2) > abline(v=1000000)  \n Le cas de la probabilit\u00e9 d\u2019atteindre plus d\u2019un million de visiteur le 1er janvier est alors > k=which(u==as.Date(\"01/01/2011\",\"%d/%m/%Y\")) > 1-pnorm(1000000,mean=forecast$pred[k],sd=forecast$se[k]) [1] 0.2604821 \n probl\u00e8me dual, \n \nDans\nun second temps, on peut envisager une autre approche, consistant \u00e0 se\ndemander quelle pourrait \u00eatre la loi de la date du jour o\u00f9 le million\nde pages vues sera atteint, > P=rep(NA,300) > for(k in 1:300){ + P[k]=1-pnorm(1000000,mean=forecast$pred[k],sd=forecast$se[k])} > plot(max(D)+1:300,P,type=\"l\",lwd=2) > x=max(D)+1:300 > x0=x[x<=as.Date(\"01/01/2011\",\"%d/%m/%Y\")] > polygon( c(x0,rev(x0)), c(P[x<=as.Date(\"01/01/2011\",\"%d/%m/%Y\")],rep(0,length(x0))), col = \"yellow\", border=NA) > lines(max(D)+1:300,P,type=\"l\",lwd=2) > abline(v=as.Date(\"01/01/2011\",\"%d/%m/%Y\"))  \nLa probabilit\u00e9 que cette date soit ant\u00e9rieure au 1er janvier est alors > P[u==as.Date(\"01/01/2011\",\"%d/%m/%Y\")] [1] 0.2604821 (ce\nqui, fort heureusement, correspond \u00e0 la probabilit\u00e9 calcul\u00e9e par le\nprobl\u00e8me primal). Bref, j\u2019ai 1 chance sur 4 d\u2019atteindre le million de\npages vues avant la nouvelle ann\u00e9e.... sensibilit\u00e9 \u00e0 un changement de mod\u00e8le \n \nC\u2019est\nbien joli tout \u00e7a, mais ces calculs sont largement soumis \u00e0 la\ncontrainte du choix de mod\u00e8le que j\u2019ai fait arbitrairement au d\u00e9but...\non peut se demander si en changeant de mod\u00e8le, les r\u00e9sultats changent\nsensiblement, ou pas. Au lieu de tenter un autre mod\u00e8le ARIMA (voire\nSARIMA), j\u2019ai pr\u00e9f\u00e9r\u00e9 changer la s\u00e9rie de r\u00e9f\u00e9rence... et me focaliser\nsur 2010 uniquement. D\u2019un c\u00f4t\u00e9 j\u2019enl\u00e8ve les premi\u00e8res semaines o\u00f9 le\nniveau de fr\u00e9quentation \u00e9tait tr\u00e8s faible, de l\u2019autre, je donne un\npoids tr\u00e8s important aux vacances d\u2019\u00e9t\u00e9, i.e. la p\u00e9riode juin-ao\u00fbt,\npendant laquelle les internautes semblent moins sensibles \u00e0 la\nmod\u00e9lisation \u00e9conom\u00e9trique. Si l\u2019on mod\u00e9lise la fr\u00e9quentation pour 2010, seulement, on obtient \navec les distributions suivantes, tout d\u2019abord pour la densit\u00e9 du nombre de visiteur atteint au 1er janvier 2011, \net pour la fonction de r\u00e9partition de la date o\u00f9 sera atteint le million de pages vues, soit une probabilit\u00e9 de l\u2019ordre de 35%, dans les deux cas, > P[u==as.Date(\"01/01/2011\",\"%d/%m/%Y\")] [1] 0.3531648 > k=which(u==as.Date(\"01/01/2011\",\"%d/%m/%Y\")) > 1-pnorm(1000000,mean=forecast2$pred[k],sd=forecast2$se[k]) [1] 0.3531648 On\nobtient des probabilit\u00e9s relativement proches avec les deux mod\u00e8les, et\nj\u2019aurais envie de croire que l\u2019objectif est envisageable.... \" challenge accepted \"\ncomme dirait mon ami Barney Stinson (oui, c\u2019est mon ami). Reste juste \u00e0\ntrouver des sujets qui attireront du monde en cette fin d\u2019ann\u00e9e..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/09/Le-million-%21-le-milllion-%21", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 2, "http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Apr\u00e8s un long week end - f\u00eate du travail oblige - alors que les\n\u00e9tudiants Rennais reprenaient le chemin de la fac, on va poursuivre le\ncours de statistique de l\u2019assurance. Ce premier cours permettra de pr\u00e9senter un peu la th\u00e9orie des mod\u00e8les GLM, les mod\u00e8les lin\u00e9aires g\u00e9n\u00e9ralis\u00e9s , \n \nle but sera de survoler la th\u00e9orie, en insistant sur leur utilisation en assurance. Les transparents seront mis en ligne ici mercredi, un peu avant le cours. Pour\nles \u00e9tudiants Rennais, j\u2019avais envoy\u00e9 un lien (ou plut\u00f4t fait envoy\u00e9,\ndonc consult\u00e9 vos mails) pour voir les 3 heures d\u2019introduction qui\navait \u00e9t\u00e9 film\u00e9es la semaine derni\u00e8re (cf ici ). Pour des raisons de droits d\u2019auteur, je crois que je ne suis pas autoris\u00e9 \u00e0 donner le lien (mais je vais me renseigner). \n Je vous encourage \u00e0 aller passer 3 heures devant votre ordinateur avant le cours de mercredi 1 .... \n c\u00f4t\u00e9\nprogrammation, j\u2019ai mis un peu de code R en ligne afin d\u2019importer les\nbases que l\u2019on utilisera au cours des premi\u00e8res session, et les mettre\nun peu en fome..... \n > sinistreUdM <- read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/sinistreUdM.txt\", + header=TRUE,sep=\";\") > sinistres=sinistreUdM[sinistreUdM$garantie==\"1RC\",] > nrow(sinistres) > contratUdM <- read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/contratUdM.txt\", + header=TRUE,sep=\";\") A partir de ces deux bases, on peut r\u00e9cup\u00e9rer le nombre de sinistres, par police, \n > T=table(sinistres$nocontrat) > T1=as.numeric(names(T)) > T2=as.numeric(T) > nombre1 = data.frame(nocontrat=T1,nbre=T2) > I = contratUdM$nocontrat%in%T1 > T1=contratUdM$nocontrat[I==FALSE] > nombre2 = data.frame(nocontrat=T1,nbre=0) > nombre=rbind(nombre1,nombre2) .... et voil\u00e0, on peut commencer, \n > base = merge(contratUdM,nombre) > head(base) nocontrat exposition zone puissance agevehicule ageconducteur bonus 1  27  0.87 C   7   0   56 50 2  115  0.72 D   5   0   45 50 3  121  0.05 C   6   0   37 55 4  142  0.90 C  10   10   42 50 5  155  0.12 C   7   0   59 50 6  186  0.83 C   5   0   75 50  marque carburant densite region nbre 1  12   D  93  13 0 2  12   E  54  13 0 3  12   D  11  13 0 4  12   D  93  13 0 5  12   E  73  13 0 6  12   E  42  13 0  1 de toutes fa\u00e7ons, la rentr\u00e9e des s\u00e9ries t\u00e9l\u00e9s n\u2019a pas commenc\u00e9.... il faudra attendre 2 semaines avant que How I Met Your Mother ne reprenne, ainsi que House , Dexter ou encore The Big Bang Theory . Bref, pas d\u2019excuse (pour l\u2019instant), il n\u2019y a rien d\u2019autre \u00e0 la t\u00e9l\u00e9...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/09/06/Statistique-de-l-assurance-STT6705V%2C-partie-2", "bloglinks": {}, "links": {"http://perso.univ-rennes1.fr/": 1, "http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Le premier cours commencera mercredi, pavillon Roger Gaudry , salle S116 .\nLe premier cours ne sera pas transmis en visio-conf\u00e9rence (la rentr\u00e9e\ndes masters \u00e0 Rennes n\u2019ayant lieu que la semaine prochaine). Ce cours\nproposera une introduction au cours qui aura lieu tous les mercredis\nmatins pendant la session d\u2019automne. Les transparents du premier cours\nsont en ligne ici . Pour les prochains cours, je mettrais en avance les lignes le code que j\u2019utiliserais. \n \nSinon\npour le bar\u00e8me, il y aura deux mini-projets (25-25) de provisionnement\net de mortalit\u00e9, et un plus gros (50) de tarification. Le cours se\nbasera sur R, mais pour les amateurs de SAS (ou autre), je suis\nouvert...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/31/Statistique-de-l-assurance-STT6705V%2C-partie-1", "bloglinks": {}, "links": {"http://perso.univ-rennes1.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Plus le temps passe, plus j\u2019ai l\u2019impression d\u2019\u00eatre un dinosaure....\nLorsque j\u2019\u00e9tais jeune, j\u2019\u00e9coutais de la musique sur des cassettes 1 . Je me souviens m\u00eame avoir \u00e9t\u00e9 enchant\u00e9 par l\u2019arriv\u00e9e du premier walkman r\u00e9versible :\non n\u2019\u00e9tait alors plus oblig\u00e9 de tourner la cassette au bout de 20\nminutes. Quand soudain, \u00e0 la fin des ann\u00e9es 80 est arriv\u00e9 le compact disc , et les lecteurs avec leur fonction Random , ou Shuffle . \n \nJ\u2019aurais\ndu me douter que je tournerais mal parce que je me souviens avoir jou\u00e9\navec la platine de mes parents, en essayant de voir si la premi\u00e8re chanson pouvait passer en premier avec une lecture al\u00e9atoire, mais aussi la seconde pouvait\npasser juste apr\u00e8s. Bon, avec le recul, je me rends compte que j\u2019ai du\nbeaucoup user la platine parce qu\u2019avec un disque de 10 titres, il faut\n90 (ou 100) essais (en moyenne) pour qu\u2019une lecture al\u00e9atoire commence\neffectivement par 1 et 2. C\u2019est d\u2019ailleurs \u00e0 ce moment que j\u2019ai aussi\nd\u00e9couvert les tirages avec et sans remises: avec certaines platines, on ne pouvait pas obtenir deux fois le m\u00eame titre, alors que d\u2019autres oui. Puis\nsont arriv\u00e9s les lecteurs mp3, o\u00f9 on n\u2019avait plus 10 titres tir\u00e9s au\nhasard, mais 200 voire 1000. Et l\u2019autre jour en courant, je me suis\npos\u00e9 les m\u00eames questions que lorsque j\u2019\u00e9tais tout petit (oui, je suis\nun dinosaure, mais j\u2019\u00e9tais jeune \u00e0 cette \u00e9poque): si le lecteur propose\nune fonction random avec remise, combien de chansons dois-je attendre, en moyenne, avant de r\u00e9entendre une chanson que j\u2019avais d\u00e9j\u00e0 entendu\u2009? De la probabilit\u00e9 d\u2019entendre deux fois une chanson\u2009? \n \nPour faire un peu formel, supposons que mon lecteur contienne chansons. La probabilit\u00e9 de n\u2019avoir aucune r\u00e9p\u00e9tition sur lectures (on suppose ) est  qui correspond aussi au nombre d\u2019arrangements de \u00e9l\u00e9ments parmi , divis\u00e9 par , soit Si le ratio est faible (ou n est grand), on peut utiliser l\u2019approximation et donc la probabilit\u00e9 s\u2019approche par soit tout simplement - toujours en cherchant une approximation - une probabilit\u00e9 de n\u2019avoir aucune r\u00e9p\u00e9tition de Si on se donne une probabilit\u00e9 de n\u2019avoir aucune r\u00e9p\u00e9tition de , alors Avec =200 chansons sur mon lecteur mp3, en jouant 17 titres, j\u2019ai environ 1\nchance sur 2 d\u2019avoir au moins un titre jou\u00e9 deux fois. Et 17 chansons,\n\u00e7a correspond \u00e0 ce que j\u2019\u00e9coute en faisant mon jogging.... Bref, le\ntemps de faire mon jogging, j\u2019ai 1 chance sur 2 d\u2019entendre un titre\ndeux fois. Mais que se passerait-il si je tenais le raisonnement inverse\u2009? Probl\u00e8me dual et jogging interminable.... \n \nAutrement\ndit, combien de temps vais-je courir si je d\u00e9cide de m\u2019arr\u00eater lorsque\nj\u2019entends (pour la premi\u00e8re fois) un titre pour la seconde fois\u2009? En\nfait, Donald Knuth en parle un peu dans son livre the art of computer programming :\non peut montrer que l\u2019esp\u00e9rance du nombre de titres qu\u2019il faudra jouer\npour avoir une premi\u00e8re r\u00e9p\u00e9tition parmi n titres est de la forme qui admet un d\u00e9veloppement asymptotique soit,\ntoujours avec 200 titres dans mon lecteur, une vingtaine de titres \u00e0\njouer, en moyenne. Ce qui rallongera un peu mon jogging, finalement... \n > n=200 > sqrt(2*n*log(2)) [1] 16.65109 > 1+sqrt(pi*n/2)-1/3+1/12*sqrt(pi/2*n)-4/135/n [1] 19.8681 Formellement, le premier correspond \u00e0 une m\u00e9diane, alors que le second \u00e0 une esp\u00e9rance. Ce qui explique la diff\u00e9rence. Notons que si mon jogging classique dure la dur\u00e9e de 17 titres, la probabilit\u00e9 d\u2019avoir un jogging deux fois plus long est quand m\u00eame d\u2019une chance sur 10, \n > k=31 > exp(-k*(k-1)/(2*n)) [1] 0.09778344 > sqrt(2*n*log(1/.01)) [1] 42.91932 autrement\ndit, j\u2019ai une chance sur 100 (ce qui devrait donc arriver une fois dans\nl\u2019ann\u00e9e, en moyenne) de courir pendant 43 chansons... qui vont me\npara\u00eetre tr\u00e8s tr\u00e8s longues.... 1 \nje passais mon temps l\u2019oreille sur la radio (car j\u2019ai connu les\npremi\u00e8res radios libres, comme on disait alors), pr\u00eat \u00e0 enregistrer d\u00e8s\nqu\u2019une super chanson passait.. je me souviens avoir guetter des jours\ndurant le passage de you spin me around ( ici ) que mes \u00e9l\u00e8ves (qui n\u2019ont pas connu cette \u00e9poque) ou les cin\u00e9philes connaissent probablement seulement sous la version right round ( l\u00e0 ) qui a abondamment sampl\u00e9 le th\u00e8me. Oui, je suis un dinosaure.."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/22/Passer-des-cassettes-aux-mp3", "bloglinks": {}, "links": {"http://www.youtube.com/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Pour compl\u00e9ter le pr\u00e9c\u00e9dant billet ( ici ) on peut se demander en quoi la Bretagne est diff\u00e9rente des autres r\u00e9gions fran\u00e7aises... \n \n Nous avions vu ici le niveau de pr\u00e9cipitation moyen, jour apr\u00e8s jours pendant les mois d\u2019\u00e9t\u00e9, en Bretagne. A Rennes. En revanche, \u00e0 Paris on obtient la moyenne suivante, \n \nque l\u2019on peut comparer \u00e0 Marseille , \nou encore \u00e0 Strasbourg , \nSur la figure ci-dessous, on voit que la probabilit\u00e9 d\u2019avoir de la pluie \u00e0 Paris (au sens au moins 0.1 mm d\u2019eau dans la journ\u00e9e, en trait gras bleu , au moins 2 mm d\u2019eau dans la journ\u00e9e, en trait bleu ) est sup\u00e9rieure \u00e0 la probabilit\u00e9 d\u2019avoir de la pluie \u00e0 Rennes (respectivement en bleu clair gras, et en bleu clair fin) \nOn est certes tr\u00e8s au dessus de Marseille , \nmais tr\u00e8s en dessous de Strasbourg ,  Mais au del\u00e0 des lois marginales, ces villes sont diff\u00e9rentes de la Bretagne si l\u2019on regarde les matrices de transition .  Transition d\u2019un jour sur l\u2019autre \n \nPour Rennes , si\non regarde jour apr\u00e8s jour, on obtient \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 1955 \n 612 \n 2567 \n \n \n \n pluie \n \n 606 \n 723 \n 1329 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 76,15 % \n 23,85 % \n \n \n \n pluie \n \n 45,60 % \n 54,40 % \n \n \n Pour Paris , la probabilit\u00e9 de transition jour apr\u00e8s jour a la forme suivante \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 2689 \n 959 \n 3648 \n \n \n \n pluie \n \n 946 \n 1466 \n 2412 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 73,71 % \n 26,29 % \n \n \n \n pluie \n \n 39,22 % \n 60,78 % \n \n \n Pour Marseille , la probabilit\u00e9 de transition jour apr\u00e8s jour a la forme suivante \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 2527 \n 375 \n 2902 \n \n \n \n pluie \n \n 362 \n 216 \n 578 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 87,08 % \n 12,92 % \n \n \n \n pluie \n \n 62,63 % \n 37,37 % \n \n \n Pour Strasbourg , la probabilit\u00e9 de transition jour apr\u00e8s jour a la forme suivante \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 31 \n 128 \n 159 \n \n \n \n pluie \n \n 132 \n 1464 \n 1596 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 19,50 % \n 80,50 % \n \n \n \n pluie \n \n 8,27 % \n 91,73 % \n \n \n Transition d\u2019une semaine sur l\u2019autre \n \nSi en revanche on regarde les matrices de transition semaine par semaine, on a des r\u00e9sultats assez diff\u00e9rents. Une bonne semaine signifie aucun jour avec plus de 2 dm de pluie. Pour Rennes , si\non regarde semaine apr\u00e8s semaine \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 379 \n 25 \n 404 \n \n \n \n pluie \n \n 26 \n 7 \n 33 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 93,81 % \n 6,19 % \n \n \n \n pluie \n \n 78,79 % \n 21,21 % \n \n \n Pour Paris , la probabilit\u00e9 de transition semaine apr\u00e8s semaine a la forme suivante \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 576 \n 46 \n 622 \n \n \n \n pluie \n \n 53 \n 4 \n 57 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 92,60 % \n 7,40 % \n \n \n \n pluie \n \n 92,98 % \n 7,02 % \n \n \n Pour Marseille , la probabilit\u00e9 de transition semaine apr\u00e8s semaine a la forme suivante \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 274 \n 59 \n 333 \n \n \n \n pluie \n \n 47 \n 9 \n 56 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 82,28 % \n 17,72 % \n \n \n \n pluie \n \n 83,93 % \n 16,07 % \n \n \n Pour Strasbourg , la probabilit\u00e9 de transition semaine apr\u00e8s semaine a la forme suivante \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n jour \n \n \n beau temps \n \n 1494 \n 614 \n 2018 \n \n \n \n pluie \n \n 613 \n 939 \n 1552 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n jour \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n jour \n \n \n beau temps \n \n 70,87 % \n 29,13 % \n \n \n \n pluie \n \n 39,50 % \n 60,50 % \n \n \n Les tests du chi deux, d\u2019ind\u00e9pendance d\u2019une semaine sur l\u2019autre donnent \u00e0 Rennes, une statistique du chi-deux de 8,054, soit une p-value de 0,45% \n \u00e0 Paris, une statistique du chi-deux de 0,025, soit une p-value de 87,26% \n \u00e0 Marseille, une statistique du chi-deux de 0,012, soit une p-value de 91,24% \n \u00e0 Strasbourg, une statistique du chi-deux de 0,7649, soit une p-value de 38,18% \n \nautrement dit l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance est accept\u00e9e partout, sauf \u00e0 Rennes.... Moralit\u00e9\u2009? \n \nDe\nmani\u00e8re assez paradoxale, on pr\u00e9tend que la Bretagne a un temps\nchangeant, et pour reprendre le titre du pr\u00e9c\u00e9dant billet,\neffectivement, en Bretagne, il peut faire beau plusieurs fois p ar jour.\nMais sur le long terme, d\u2019une semaine sur l\u2019autre, le temps est au\ncontraire tr\u00e8s corr\u00e9l\u00e9, contrairement aux autres r\u00e9gions. A Paris,\nMarseille ou Strasbourg, qu\u2019il ait fait beau, ou qu\u2019il ait plu la\nsemaine pr\u00e9c\u00e9dente, cela n\u2019apporte aucune information sur la\nprobabilit\u00e9 d\u2019avoir de la pluie la semaine o\u00f9 l\u2019on vient en\nvacances.... Mais pas en Bretagne: manifestement, il existe donc des\n\u00e9t\u00e9s pourris, o\u00f9 il pourra pleuvoir toutes les semaines, et des \u00e9t\u00e9s\nsuperbes o\u00f9 il ne pleut jamais...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/21/Des-%C3%A9t%C3%A9s-pluvieux-en-Bretagne-une-r%C3%A9alit%C3%A9-statistique...", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["bon, et \u00e0 l\u2019occasion il peut pleuvoir un peu.... Il peut donc \u00eatre\nint\u00e9ressant pour planifier un peu ses vacances de calculer la\nprobabilit\u00e9 d\u2019avoir de la pluie. \n \nLes calculs ont \u00e9t\u00e9 fait sur les donn\u00e9es de pluviom\u00e9trie \u00e0 Rennes, en ligne sur le site eca&d, ici (donn\u00e9es de qualit\u00e9 gratuites ). Probabilit\u00e9 d\u2019avoir de la pluie, pendant les vacances d\u2019\u00e9t\u00e9 \n \nConsid\u00e9rons ici la s\u00e9rie du niveau de pr\u00e9cipitation par jour, en par 0,1 mm,  \net la probabilit\u00e9 d\u2019avoir de la pluie dans la journ\u00e9e (r\u00e9gression logistique), \nBref,\nil peut pleuvoir \u00e0 Rennes l\u2019\u00e9t\u00e9. Mais \u00e7a n\u2019aide pas vraiment pour\nplanifier ses vacances. Car si \u00e7a se trouve, il y a des \u00e9t\u00e9s sans pluie,\net des \u00e9t\u00e9s o\u00f9 il pleut sans cesse. Dynamique et matrice de transition (par jour et par semaine) \n \nAutrement\ndit, au lieu de regarder les lois marginales (comme la probabilit\u00e9\nd\u2019avoir de la pluie dans la journ\u00e9e), on peut s\u2019int\u00e9resser \u00e0 la\ndynamique de la s\u00e9rie, mod\u00e9lis\u00e9e sous la forme d\u2019une cha\u00eene de Markov.\nSi on regarde jour apr\u00e8s jour, avec les 30 mois de juillet et ao\u00fbt\nentre 1980 et 2009,    jour    beau temps  pluie   jour  beau temps 871 298 1169  pluie 292 335 627 ce qui donne les probabilit\u00e9s de transition suivantes,    jour    beau temps  pluie  jour  beau temps 75,51 % 25,49 %  pluie 46,57 % 53, 43 % autrement, s\u2019il fait beau aujourd\u2019hui, on a 3 chances sur 4 d\u2019avoir du beau temps demain. Si on regarde semaine apr\u00e8s semaine, o\u00f9 l\u2019int\u00e9r\u00eat sont les semaines sans pluie ,   semaine    beau temps  pluie   semaine  beau temps 13 26 39  pluie 23 140 163 avec l\u00e0 aussi les probabilit\u00e9s de transition suivantes,    semaine    beau temps  pluie  semaine  beau temps 33,33 % 66,67 %  pluie 14,11 % 85,89 % Si on regarde semaine apr\u00e8s semaine, o\u00f9 l\u2019int\u00e9r\u00eat sont les semaines avec six jours sans pluie (on s\u2019autorise une journ\u00e9e de pluie),   semaine    beau temps  pluie   semaine  beau temps 36 36 72  pluie 30 100 130 ce qui donne les probabilit\u00e9s de transition suivantes,    semaine    beau temps  pluie  semaine  beau temps 50,00 % 50,00 %  pluie 23,08 % 76,92 % \nBon, maintenant, si on pense qu\u2019avoir 2 mm de pluie dans la journ\u00e9e,\nc\u2019est juste un peu d\u2019humidit\u00e9 dans l\u2019air, les matrices de transitions\nsont sensiblement diff\u00e9rentes, dans le cas o\u00f9 on s\u2019autorise une journ\u00e9e\ndans la semaine avec un peu d\u2019humidit\u00e9 pour parler de beau temps ,\n \n \n \n \n \n \n semaine \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n semaine \n \n \n beau temps \n \n 106 \n 36 \n 142 \n \n \n \n pluie \n \n 31 \n 29 \n 60 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n semaine \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n semaine \n \n \n beau temps \n \n 75,65 % \n 25,35 % \n \n \n \n pluie \n \n 51,67 % \n 48,33 % \n \n \n Autrement dit, on retrouve une matrice proche de celle que nous\navions sur les donn\u00e9es journali\u00e8re: s\u2019il a fait beau la semaine avant\nde venir en Bretagne, on a 3 chances sur 4 d\u2019avoir du beau temps. En\nrevanche, s\u2019il a fait mauvais, on a une chance sur deux d\u2019avoir beau\ntemps la semaine suivante. Si notre d\u00e9finition de pluie \nest encore plus laxiste (il faut qu\u2019il y ait eu un d\u00e9luge une journ\u00e9e\ndans la semaine, \u00e0 savoir plus de 2 cm d\u2019eau dans la journ\u00e9e), alors\ncette fois, on obtient,\n \n \n \n \n \n \n semaine \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n \n semaine \n \n \n beau temps \n \n 171 \n 12 \n 183 \n \n \n \n pluie \n \n 14 \n 5 \n 130 \n \n \n \nce qui donne les probabilit\u00e9s de transition suivantes, \n \n \n \n \n \n \n semaine \n \n \n \n \n \n \n beau temps \n \n \n pluie \n \n \n \n \n semaine \n \n \n beau temps \n \n 93,44 % \n 6,56 % \n \n \n \n pluie \n \n 73,68 % \n 26,32 % \n \n \n Autrement dit, s\u2019il y a eu au moins une tr\u00e8s mauvaise journ\u00e9e\nla semaine pass\u00e9e, on a 1 chance sur 4 d\u2019en avoir \u00e9galement une la\nsemaine suivante. En revanche, s\u2019il a fait beau tout le temps, on a\npresque 95% de chances d\u2019avoir du beau temps la semaine suivante. Moralit\u00e9, quelle que soit la d\u00e9finition retenue pour d\u00e9finir le beau temps , le temps \u00e0 Rennes n\u2019est pas ind\u00e9pendant d\u2019une semaine sur l\u2019autre: il y a manifestement des \u00e9t\u00e9s pluvieux, et d\u2019autre non."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/23/En-Bretagne%2C-il-fait-beau-plusieurs-fois-par-jour", "bloglinks": {}, "links": {"http://eca.knmi.nl/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": [], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/08/Allez%2C-on-va-fermer-%28temporairement...%29", "bloglinks": {}, "links": {}, "blogtitle": "Arthur Charpentier"}, {"content": ["Apr\u00e8s plusieurs billets sur les accidents de la route ( ici et l\u00e0 ), parlons\nun peu de vitesse des v\u00e9hicules. J\u2019ai pu r\u00e9cup\u00e9rer\ndes donn\u00e9es via l\u2019inrets, d\u2019observations pr\u00e8s de Saclay\nen r\u00e9gion parisienne, sur une 2x2 limit\u00e9e \u00e0 110\nkm/h. \n \nJ\u2019ai ainsi le passage de plusieurs millions de v\u00e9hicules, sur quelques jours, incluant tous \nles v\u00e9hicules. Mais si j\u2019ai sign\u00e9 des papiers quant \u00e0 la\nconfidentialit\u00e9 des donn\u00e9es, je dois pouvoir mettre des dessins\nconstruits \u00e0 partir de ces donn\u00e9es... \n A quelle vitesse roulent les conducteurs parisiens\u2009? \n Comme\npour les accidents, distinguons semaine et week end. En semaine, on\nretrouve les pics du matin et du soir, qui poussent les gens \u00e0\nralentir. La courbe en noir est la vitesse moyenne des v\u00e9hicules qui passent sur ce tron\u00e7on, et la courbe en bleu , la vitesse des 5% des v\u00e9hicules les plus rapides, \nCes pics sont l\u00e9g\u00e8rement att\u00e9nu\u00e9s le week end, \navec ici un trafic tr\u00e8s dense tout l\u2019apr\u00e8s midi, entre 15 et 20 heures.\n Quelle proportion de v\u00e9hicules d\u00e9passe la limite autoris\u00e9e\u2009? \n \nComme pour les accidents, distinguons semaine et week end. En semaine, on retrouve les pics du matin et du soir, avec en gras la proportion des v\u00e9hicules qui d\u00e9passe 110 km/h, et en trait fin , la proportion qui d\u00e9passe 115 km/h (soit 5% de plus), \navec des choses finalement assez proches le week end,  \nAutrement\ndit, aux heures de pointe, peu de monde d\u00e9passe la vitesse autoris\u00e9e,\nmais vers midi, ou surtout la nuit, environ un v\u00e9hicule sur quatre\nd\u00e9passe la vitesse autoris\u00e9e (mais seulement un sur six la d\u00e9passe de\nplus de 5%).\n Vitesse et respect des distances de s\u00e9curit\u00e9 \n La\nbase est tr\u00e8s compl\u00e8te, puisque j\u2019ai tous les v\u00e9hicules qui sont pass\u00e9s\npr\u00e8s de Saclay, avec l\u2019heure d\u2019observation, la vitesse, ce qui permet\nd\u2019obtenir la distance entre deux v\u00e9hicules qui se suivent (en supposant\nque le premier v\u00e9hicule maintienne sa vitesse constante). Rappelons qu\u2019il existe un lien th\u00e9orique entre la vitesse d\u2019un v\u00e9hicule et la distance de freinage (expliqu\u00e9 ici ou l\u00e0 ). Pour faire simple, la distance de freinage est la somme d\u2019une distance parcourue avant de r\u00e9agir , et d\u2019une distance de freinage proprement dite , Si on suppose que la personne met une seconde \u00e0 r\u00e9agir, la distance de r\u00e9action est simplement la vitesse (exprim\u00e9e en m/h), i.e. Quant au freinage \u00e0 prorement parler, rappelons que les \u00e9quations qui charact\u00e9risent le mouvement d\u2019un objet sont de la forme  Aussi, en mettant la deuxi\u00e8me au carr\u00e9, et en combinant avec la troisi\u00e8me, on a avec (on cherche \u00e0 arr\u00eater le v\u00e9hicule) et , aussi, avec , on obtient quelque chose qui peut s\u2019\u00e9crire \u00e0\ncondition d\u2019avoir des vitesses en km/h. Aussi en agglom\u00e9ration, il faut\n23 m pour s\u2019arr\u00eater. Et sur notre route \u00e0 Saclay, pour une personne qui\nroule \u00e0 110 km/h, il faut compter 110 m (ce qui correspond au message\nv\u00e9hicul\u00e9 par la s\u00e9curit\u00e9 routi\u00e8re). Sur nos donn\u00e9es, on peut repr\u00e9senter en rouge les individus qui ne respectent pas la distance de s\u00e9curit\u00e9. \nLa courbe bleue est la distance moyenne des v\u00e9hicule en fonction de la distance qui les s\u00e9pare du v\u00e9hicule qui les pr\u00e9c\u00e8de, Aussi,\nen faisant une r\u00e9gression logistique (liss\u00e9e), on obtient que 60% des\nconducteurs qui roulent entre 50 et 110 km/h ne respectent pas les\ndistances de s\u00e9curit\u00e9."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/07/Vitesse-et-comportement-dangereux-au-volant-%28partie-3%29", "bloglinks": {}, "links": {"http://www.ilephysique.net/": 1, "http://www.msr.lu/": 1, "http://freakonometrics.free.fr/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Je poursuis un peu mon \u00e9tude des accidents de la route (ou pour \u00eatre\nplus pr\u00e9cis, des accidents corporels, ayant caus\u00e9 des bless\u00e9s, et ayant\nfait l\u2019objet d\u2019un rapport de police). \n \nJ\u2019avais parl\u00e9 ici \nde la tendance de long terme, sur 6 ou 7 ans, montrant qu\u2019il y avait\nune tendance \u00e0 la baisse aussi bien du nombre de tu\u00e9s que du nombre de\nbless\u00e9s grave. On avait observ\u00e9 que les cycles annuels avaient tendance\n\u00e0 diminuer \u00e9galement. Mais qu\u2019en est-il des cycles \u00e0 court terme\u2009? En particulier, on peut se demander s\u2019il n\u2019existe pas des cycles dans la\njourn\u00e9e. En semaine, l\u2019hiver (entre novembre et d\u00e9but\nf\u00e9vrier), on observe les tendances suivantes, \n \n(les\npoints sont les moyennes brutes, par tranche d\u2019une demi heure, et la\ncourbe est un lissage de ces points). On distinguera les ann\u00e9es\n2002-2004 (en rouge ) et 2004-2007 (en bleu ) Si l\u2019on regarde les semaines l\u2019\u00e9t\u00e9 (entre mai et d\u00e9but ao\u00fbt), on a \n \nNotons que si l\u2019on compte non plus les accidents, mais les bless\u00e9s\ngraves ou les tu\u00e9s (il y a g\u00e9n\u00e9ralement plusieurs victimes dans ces\naccidents), on observe les cycles suivants, \npour l\u2019hiver, alors que l\u2019\u00e9t\u00e9, \nAutrement\ndit, les tendances sont les m\u00eames: les accidents du matin ne sont pas\nmoins meutriers que ceux du soir... Les week-ends, les tendances sont\nassez diff\u00e9rentes entre l\u2019hiver et l\u2019\u00e9t\u00e9, avec respectivement \n \net \n \nSi l\u2019on regarde les bless\u00e9s graves et les tu\u00e9s, on a les cycles suivants, \net"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/06/Quand-surviennent-les-accidents-de-la-route-%28partie-2%29", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Tout le monde semble se r\u00e9jouir de la baisse du nombre de morts sur la route\ndepuis quelques ann\u00e9es (longtemps attribu\u00e9 \u00e0 la\npr\u00e9sence de radars automatiques, mais qu\u2019on pourrait aussi associer \u00e0\nl\u2019instauration du permis \u00e0 point). Et c\u2019est tant mieux. Mais sans\nvouloir jouer les\ncyniques, et en tous les cas c\u2019est une statistique qui int\u00e9resse\nprobablement davantage les assureurs, il peut sembler int\u00e9ressant de\ns\u2019int\u00e9resser aussi aux accidents\ncorporels graves .... \n \nLe principal soucis m\u00e9thodologique est qu\u2019au 1er janvier 2005, la d\u00e9finition d\u2019accidents graves ou encore de d\u00e9c\u00e8s a chang\u00e9. Avant 2005, un tu\u00e9 ,\n\u00e9tait une victime d\u00e9c\u00e9d\u00e9e sur le coup ou dans les six jours qui suivent\nl\u2019accident. A partir de 2005, les 6 jours passent \u00e0 30 jours. De m\u00eame,\nun bless\u00e9 grave \u00e9tait\nun bless\u00e9 dont l\u2019\u00e9tat n\u00e9cessitait plus de six jours d\u2019hospitalisation.\nApr\u00e8s 2005, cette classe disparait, et seuls restent les bless\u00e9s qui\nont pass\u00e9 plus de 24 heures \u00e0 l\u2019h\u00f4pital.... Bref, on est un peu bloqu\u00e9\npour vraiment analyser les tendances sur une longue p\u00e9riode...\n Nombre de morts sur la route \n \nLe graphique suivant montre le nombre quotidien de morts sur les\nroutes, passant d\u2019une vingtaine en 2002 \u00e0 une douzaine (en 2008). \n \nOn notera qu\u2019il n\u2019y a pas eu de r\u00e9elle rupture en 2005, malgr\u00e9 la nouvelle d\u00e9finition de d\u00e9c\u00e8s . \n Nombre de bless\u00e9s graves \n \nEn revanche, les statistiques sur les bless\u00e9s (graves) ont connu une r\u00e9elle rupture en 2005. \n \nOn peut essayer de corriger afin de rendre les s\u00e9ries comparables (au moins en terme d\u2019ordre de grandeur), \n Au del\u00e0 de la tendance (et de la rupture de la forte baisse observ\u00e9e en\n2002-2004), on notera que le cycle annuel est plus faible. De la m\u00eame\nmani\u00e8re que les cycles dans les naissances semblent avoir diminu\u00e9\n(comme je l\u2019\u00e9voquais ici ), on notera que le cycle dans les accidents graves (mais aussi dans une moindre mesure les d\u00e9c\u00e8s) a fortement diminu\u00e9..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/05/Nombre-de-morts-sur-la-route-en-baisse...-et-%28partie-1%29", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Il existe une mythologie aujourd\u2019hui bien ancr\u00e9e, consistant \u00e0 penser qu\u2019il faut \u00eatre\npropri\u00e9taire de son logement. C\u2019est d\u2019ailleurs le message que l\u2019\u00c9tat\n(c\u2019est \u00e0 dire nous tous) v\u00e9hicule sur diff\u00e9rentes affiches, comme celle\n\u00e0 droite. Tout le monde a d\u00e9j\u00e0 entendu l\u2019adage pr\u00e9tendant que \" payer un loyer, c\u2019est jeter de l\u2019argent par les fen\u00eatres \"....mais personnellement, j\u2019ai l\u2019impression que rembourser un cr\u00e9dit, c\u2019est aussi \u2019\" jeter de l\u2019argent par les fen\u00eatres \".\nDans le premier cas, c\u2019est le propri\u00e9taire qui est derri\u00e8re la fen\u00eatre,\nalors que dans le second cas c\u2019est une banque (mais aussi un notaire,\net l\u2019\u00e9tat qui r\u00e9cup\u00e8re beaucoup d\u2019imp\u00f4t). Visiblement, l\u2019\u00c9tat semble\navoir pris le parti qu\u2019enrichir les propri\u00e9taires, c\u2019est mal, alors\nqu\u2019enrichir son banquier c\u2019est bien... Peut \u00eatre est-ce mieux dans l\u2019 int\u00e9r\u00eat collectif \n(car c\u2019est le but ultime de l\u2019\u00c9tat me semble-t-il) d\u2019\u00eatre tous\npropri\u00e9taires\u2009? Mais qu\u2019en est-il de l\u2019int\u00e9r\u00eat individuel\u2009? Ai-je\nvraiment int\u00e9r\u00eat \u00e0 pr\u00e9f\u00e9rer \u00eatre propri\u00e9taire plut\u00f4t que locataire\u2009? On\npeut essayer de faire un petit mod\u00e8le simple, voire simpliste...\nConsid\u00e9rons quelqu\u2019un poss\u00e9dant un apport personnel de 100 000 \u20ac (je\nmets des sommes rondes pour simplifier le mod\u00e8le), et souhaitant loger\ndans une maison qui vaut 300 000 \u20ac. Il a deux possibilit\u00e9s, \n devenir propri\u00e9taire du bien \n \u00eatre locataire du bien (on suppose que le choix est possible pour le m\u00eame bien) \n \n Dans\nle premier cas, on va supposer que qu\u2019il est possible d\u2019avoir un cr\u00e9dit\nsur 20 ans pour un taux de 5,5%. On va supposer qu\u2019il y a 10% de frais d\u2019acquisition, entre l\u2019agent immobilier et le notaire. On oubliera les\ncharges annuelles en tant que propri\u00e9taire, et les \u00e9ventuels cr\u00e9dit\nd\u2019imp\u00f4t sur les int\u00e9r\u00eats. Dans le second cas, il paye un loyer de 1\n000 \u20ac par mois, \u00e9ventuellement revaloris\u00e9 (+2% par an par exemple),\nmais il a la possibilit\u00e9 d\u2019\u00e9pargner (en plus de capitaliser ce qui\nconstituait l\u2019apport initial s\u2019il achetait). En fait, dans les deux\ncas, il \u00e9pargne. On peut partir du fait qu\u2019il a 2 000 \u20ac allouer au\nlogement (ce montant peut \u00e9galement \u00eatre revaloris\u00e9). Dans le second\ncas, il peut \u00e9pargner 1 000 \u20ac par mois, et dans le premier, la\ndiff\u00e9rence entre les mensualit\u00e9s qu\u2019il doit \u00e0 la banque et les 2 000 \u20ac.\nOn suppose que les taux de placement sont \u00e0 4,5%. Au bout de 15 ans, la maison vaut 500 000 \u20ac. Notons que la revalorisation \u00e0 2% est plus faible que la croissance de\nla valeur du bien (passer de 300 \u00e0 500 en 20 ans correspond \u00e0 une\nvalorisation annuelle de 2,5%). Le propri\u00e9taire a-t-il vraiment fait une bonne affaire\u2009? > capital=100000 > revenu =2000 > loyer =1000 > i1  =.055 > i2  =.045 > i3  =.02 > T  =20 > maison1=300000 > maison2=500000 > mensualite=(maison1*1.1-capital)/sum((1+i1)^(-(1:(T*12))/12)) > mensualite [1] 1564.789 > (1+i3)^T [1] 1.485947 > locataire = capital*(1+i2)^T+sum((1+i3)^((1:(T*12))/12)*(revenu-loyer)*(1+i2)^((1:(T*12))/12)) > proprietaire = maison2+sum((revenu-mensualite)*(1+i3)^((1:(T*12))/12)*(1+i1)^((1:(T*12))/12)) > locataire [1] 728274.5 > proprietaire [1] 738245.5 > (proprietaire-locataire)/proprietaire [1] 0.01350635 Autrement\ndit pour \u00eatre propri\u00e9taire, les mensualit\u00e9s sont de 1 564 \u20ac par mois\n(constantes), alors que le locataire aurait eu un loyer de 1 000 \u20ac initialement, revaloris\u00e9 de 2% par an, soit 1 485 \u20ac par mois sur la\nfin. Bref, au final, le locataire, qui a mis 1 000 \u20ac par mois\n(revaloris\u00e9s l\u00e0 aussi \u00e0 2%) se retrouve \u00e0 la t\u00eate de 728 milliers \u20ac\nalors que le propri\u00e9taire a vu son bien se valoriser (et atteindre 500\nmilliers \u20ac), mais il a \u00e9galement pu \u00e9pargner un peu. Bref, il se\nretrouve \u00e0 la t\u00eate d\u2019un patrimoine de 738 milliers \u20ac. Ce qui est\ncomparable.... Moralit\u00e9, on jette autant d\u2019argent par les fen\u00eatres dans\nles deux cas, mais du point de vue de la personne qui loge dans\nl\u2019appartement, \u00e7a se vaut... Mais au del\u00e0 des montants, on peut surtout \u00e9tudier l\u2019impact des diff\u00e9rents param\u00e8tres.... Le graphique ci-dessous montre l\u2019impact du capital initial: s\u2019il est trop faible, \u00eatre locataire est beaucoup plus int\u00e9ressant, \n(les\nordonn\u00e9es positives signifie que la richesse en tant que propri\u00e9taire\nexc\u00e8de celle que l\u2019on aurait en tant que locataire). Autrement dit,\navec un faible capital initial, il sera co\u00fbteux de devenir propri\u00e9taire.\nIci, il faut disposer d\u2019au moins 1/3 du prix de la maison en apport.\nMais grosso modo, plus on est riche, plus on peut avoir int\u00e9r\u00eat \u00e0\ndevenir propri\u00e9taire. On peut aussi regarder l\u2019impact du taux d\u2019emprunt , \n(o\u00f9 l\u2019on retrouve que si le taux d\u2019emprunt est trop \u00e9lev\u00e9, je n\u2019ai aucun int\u00e9r\u00eat \u00e0 emprunter) ou encore du revenu que l\u2019on souhaite allouer au logement, \nPour comparer l\u2019impact de la dur\u00e9e ,\non va supposer que la valorisation du loyer et du salaire reste \u00e0 2%,\nmais que la valorisation du bien immobilier est de 2,5% par an. \nCertes,\nle locataire \u00e9pargne moins sur une dur\u00e9e courte, mais le locataire a de\ntelles mensualit\u00e9s \u00e0 payer qu\u2019il vaut mieux \u00e9pargner. Bon, bien s\u00fbr la\ndifficult\u00e9 est de supposer qu\u2019on peut faire varier ces param\u00e8tres ind\u00e9pendamment les uns des autres, mais j\u2019ai l\u2019impression que cela\npermet de mieux comprendre qui peut \u00eatre int\u00e9ress\u00e9 pour devenir propri\u00e9taire.... en l\u2019occurrence ceux qui ont le temps, et ceux qui ont de l\u2019argent.... Bref,\nje laisse ceux qui le souhaite am\u00e9liorer le mod\u00e8le car il est ici tr\u00e8s\n(trop\u2009?) simpliste... m\u00eame s\u2019il laisse \u00e0 penser que le choix d\u2019acheter\nou de louer n\u2019est pas aussi simple qu\u2019il y para\u00eet, loin de l\u00e0....."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/04/Propri%C3%A9taire-ou-locataire", "bloglinks": {}, "links": {}, "blogtitle": "Arthur Charpentier"}, {"content": ["Depuis pas mal de temps, je re\u00e7ois des bases envoy\u00e9es par diff\u00e9rentes\npersonnes, pour faire des stats, et si la plupart ont la bonne id\u00e9e de me\nles envoyer directement en format csv, malheureusement, je re\u00e7ois parfois\ndes bases sas dont je ne sais trop que faire (car je n\u2019ai pas sas). La library(foreign) de R propose d\u2019importer des bases SAS qui sont au format xport ( ici ). Mais g\u00e9n\u00e9ralement, les donn\u00e9es SAS sont au format sas7bdat. Pendant l\u2019ann\u00e9e, j\u2019avais l\u2019habitude d\u2019aller squatter la \nsalle informatique qui dispose de SAS, pour ouvrir SAS et exporter les\ndonn\u00e9es en csv. Mais avec la fermeture estivale de la facult\u00e9, j\u2019ai \u00e9t\u00e9\nun peu bloqu\u00e9. J\u2019ai alors d\u00e9couvert la version online \nde SAS sur l\u2019ent de l\u2019universit\u00e9. Malheureusement, il faut que les donn\u00e9es\nsoient dans un r\u00e9pertoire sur le r\u00e9seau, ce qui n\u00e9cessite au pr\u00e9alable\nd\u2019envoyer des donn\u00e9es sur le r\u00e9seau au lieu de pointer sur un\nr\u00e9pertoire local (ce qui peut \u00eatre loin, voir impossible si la base est\ntrop grosse). Fort heureusement, l\u2019autre jour, j\u2019ai d\u00e9couvert, \n  On peut en effet t\u00e9l\u00e9charger gratuitement sur le site de www.sas.com un lecteur de base, qui permet ensuite d\u2019exporter la base en csv... et ensuite de l\u2019importer facilement sous R... Trop facile\u2009!"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/08/03/Importer-une-base-SAS-sous-R", "bloglinks": {}, "links": {"http://www.google.fr/": 1, "http://www.sas.com/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Les premiers rapports sur Xynthia, ses cons\u00e9quences mais aussi ses causes commencent \u00e0 \u00eatre diffus\u00e9s, comme ici \npar exemple. Pour rappels, dans la nuit du 27 au 28 f\u00e9vrier, la temp\u00eate\nXynthia a balay\u00e9 plusieurs pays europ\u00e9ens, causant de tr\u00e8s nombreux\nd\u00e9g\u00e2ts en France, principalement en Charente Maritime et en Vend\u00e9e. A\nl\u2019ile de R\u00e9, des rafales de 160 km/h ont \u00e9t\u00e9 mesur\u00e9es. Mais les effets\nde la temp\u00eate ont \u00e9t\u00e9 d\u00e9cupl\u00e9s par la pr\u00e9sence de polders, l\u2019oc\u00e9an\nreprenant alors en quelques heures des territoires que l\u2019homme croyait\ns\u2019\u00eatre appropri\u00e9 depuis quelques si\u00e8cles. Comble de malchance, cette\ntemp\u00eate est survenu lors des grandes mar\u00e9es, ce qui a de nouveau\naccentu\u00e9 les effets : afin de compenser la baisse de pression\natmosph\u00e9rique, le niveau des oc\u00e9ans monte davantage, cr\u00e9ant ainsi une\nsurcote, et causant des inondations d\u00e8s que le niveau des digues est\natteint. \n \nCes\ninondations ont caus\u00e9 environ 420 000 sinistres, pour un co\u00fbt de 700\nmillions \u20ac (soit un co\u00fbt moyen par sinistre de l\u2018ordre de 1700 \u20ac). 22\n000 sinistres ont \u00e9t\u00e9 des dommages mat\u00e9riels de v\u00e9hicules (pour un co\u00fbt\ntotal de l\u2019ordre de 35 millions \u20ac), 69 000 sinistres ont \u00e9t\u00e9 des\ndommages de professionnels (pour 255 millions \u20ac) et pr\u00e8s de 330 000\nsinistres ont \u00e9t\u00e9 des dommages \u00e0 des particuliers, pour mon montant\ntotal de 425 millions d\u2019euros. Le risque inondation est couvert par le\nr\u00e9gime dit cat nat, instaur\u00e9 par la loi du 13 juillet 1982, qui est\nobligatoire dans tout contrat de dommages aux biens, et qui couvre tous\nles dommages caus\u00e9es par une \u00ab intensit\u00e9 anormale d\u2019un agent naturel \u00bb,\nexcept\u00e9 les temp\u00eates. Si une commune touch\u00e9e est class\u00e9e comme \u00e9t\u00e9\nvictime d\u2019une catastrophe naturelle, les habitants peuvent alors faire\njouer la garantie. Toutefois, si la commune n\u2019a pas fait de Plan de\nPr\u00e9vention des Risques,. Le montant des primes d\u2019assurance affect\u00e9e par\nces catastrophes est fix\u00e9 par l\u2019Etat, ou plut\u00f4t la part de la prime\nhabitation (pour les particulier) qui sera affect\u00e9e \u00e0 la couverture du\nrisque, en l\u2018occurrence 12%: peu importe le risque r\u00e9el, l\u2019Etat impose\nun m\u00e9canisme de solidarit\u00e9 entre les assur\u00e9s. Les assureurs touchent\nainsi environ 1,3 milliards \u20ac au titre de l\u2019ensemble des risques dit\ncat nat (ce qui inclus la s\u00e9cheresse ou les mouvements de terrain, en\nplus des inondations). Les assureurs peuvent \u00e9galement souscrire un\ncontrat de r\u00e9assurance aupr\u00e8s de la Caisse Centrale de R\u00e9assurance\n(CCR, poss\u00e9dant une garantie illimit\u00e9e de l\u2018Etat) en quote-part,\nconduisant \u00e0 un partage pour moiti\u00e9 entre l\u2019assureur et le r\u00e9assurance.\nLa CCR offre de plus une clause de limitation des pertes, dans le cas\no\u00f9 un sinistre couterait plus du double des primes encaiss\u00e9es. Compte\ntenu du r\u00f4le central jou\u00e9 par la CCR, c\u2019est cette derni\u00e8re qui fournit\ng\u00e9n\u00e9ralement les statistiques sur les \u00e9v\u00e8nements naturels. Les\ntemp\u00eates ont fait l\u2019objet d\u2019une extension obligatoire, en juin 1990,\ndans tous les contrats dommage, avec une couverture qui incluse\ng\u00e9n\u00e9ralement la gr\u00eale et le poids de la neige. La temp\u00eate en elle m\u00eame\na caus\u00e9 environ 35 000 sinistres, pour un co\u00fbt de 700 millions \u20ac (soit\nun co\u00fbt moyen par sinistre de l\u2018ordre de 20 000 \u20ac). Environ 10 500 de\nces sinistres sont li\u00e9s \u00e0 des couvertures de v\u00e9hicules, avec un co\u00fbt\nmoyen de l\u2019ordre de 5 000 \u20ac. 5 500 ont caus\u00e9 des dommages \u00e0 des\nprofessionnels, pour un co\u00fbt total de l\u2019ordre de 250 millions \u20ac, alors\nque 400 millions ont \u00e9t\u00e9 affect\u00e9 \u00e0 19 000 sinistres de particuliers.\nCette garantie est certes elle aussi obligatoire, mais l\u2019Etat ne\npropose plus d\u2019intervenir, et ce sont les r\u00e9assureurs priv\u00e9s qui\npermettent aux assureurs de se couvrir contre les tr\u00e8s grosses temp\u00eates\n(on aura en m\u00e9moire celles de d\u00e9cembre 1999, dont le montant r\u00e9\u00e9valu\u00e9\natteint les 10 milliards \u20ac). Notons que pour couvrir le risque\ntemp\u00eates, les assureurs collectent environ 1 milliard \u20ac par an. Dans le\ncas de la garantie temp\u00eate, la principale source de donn\u00e9es est la\nF\u00e9d\u00e9ration Fran\u00e7aise des Soci\u00e9t\u00e9s d\u2019Assurance, qui collecte les donn\u00e9es\ndes diff\u00e9rentes mutuelles et compagnies d\u2019assurance. Cet \u00e9v\u00e8nement a\nmarqu\u00e9 les esprits non seulement \u00e0 cause du nombre \u00e9lev\u00e9 de victimes,\nmais aussi car il rappelle deux \u00e9v\u00e8nements tr\u00e8s similaires. Le premier\nest survenu dans la nuit du 31 janvier au 1er f\u00e9vrier 1953, au Pays\nBas, o\u00f9 une temp\u00eate a travers\u00e9 le pays \u00e0 quelques jours des grandes\nmar\u00e9es ( ici ).\nLes digues subirent plus de 400 br\u00e8ches, et plus de 500 km de digues\nfurent \u00e0 reconstruire. Mais surtout, 1 836 personnes trouv\u00e8rent la\nmort, ce qui, ramen\u00e9 \u00e0 la population des Pays-Bas de l\u2019\u00e9poque (de\nl\u2019ordre de 10 millions d\u2019habitants), correspondrait \u2013 aujourd\u2019hui \u2013 \u00e0\nune catastrophe causant la mort de plus de 10 000 personnes en France\n(ou plus de 60 000 aux Etats Unis). Le second est l\u2019ouragan Katrina qui\na d\u00e9truit la Nouvelle Orl\u00e9ans le 28 ao\u00fbt 2005, o\u00f9 le raz de mar\u00e9e qui\nest survenu au m\u00eame moment a caus\u00e9 davantage de d\u00e9g\u00e2ts que les rafales\nde vent (pourtant tr\u00e8s fort). Ce risque avait \u00e9t\u00e9 \u00e9voqu\u00e9 par des\ning\u00e9nieurs quelques mois plus t\u00f4t. Le parall\u00e8le avec Katrina est\nd\u2019autant plus saisissant que, comme \u00e0 la Nouvelle Orl\u00e9ans, on retrouve\ndes rapports d\u2019experts qui \u00e9voquent la catastrophe quelques mois plus\nt\u00f4t. En particulier, l\u2019id\u00e9e d\u2019avoir un \u00ab Plan\nde Pr\u00e9vention des Risques de submersion marine dans l\u2019estuaire du Lay\nsur les communes de La Faute-sur-Mer et de l\u2019Aiguillon-sur-Mer, o\u00f9 la\nconjonction de deux ph\u00e9nom\u00e8nes, de crue dans l\u2019estuaire du Lay et de\nsubmersion marine, pourrait avoir un impact tr\u00e8s important sur les\nzones densifi\u00e9es \u00e0 l\u2019arri\u00e8re d\u2019un r\u00e9seau de digues vieillissant . \u00bb ( ici ). A La Faute-sur-Mer, \u00ab la rupture des digues sur ce secteur engendrerait des d\u00e9g\u00e2ts majeurs aux biens et aux personnes \u00bb, ajoute le rapport en citant l\u2019exemple des temp\u00eates Lothar et Martin qui \u00ab ont d\u00e9montr\u00e9 que les zones c\u00f4ti\u00e8res pouvaient \u00eatre submerg\u00e9es par la mer, notamment en zone estuarienne \u00bb. \n (le graphique a \u00e9t\u00e9 vol\u00e9 sur le site de 20min, ici ). En effet, depuis les temp\u00eates de d\u00e9cembre 99, les temp\u00eates majeures en Europe sont jug\u00e9es \u00ab rares \u00bb, mais plus \u00ab exceptionnelles \u00bb.\nOn a m\u00eame vu des cyclones tropicaux se diriger vers l\u2019Europe (au lieu\nde se diriger vers les Am\u00e9riques) comme la temp\u00eate Grace en octobre\n2009 ( ici ).\nAutrement dit le risque existe. Et comme toujours quand on fait face \u00e0\nun risque, deux solutions se posent \u00e0 nous : accepter de prendre le\nrisque, quitte \u00e9ventuellement \u00e0 transf\u00e9rer la cons\u00e9quence financi\u00e8re \u00e0\nun tiers (un assureur le plus souvent), ou tout mettre en \u0153uvre pour le\nr\u00e9duire (on parlera alors de pr\u00e9vention). Mais encore faut-il toutefois\navoir conscience de l\u2019existence de ces risques, en l\u2019occurrence de\nrisques \u00e0 long terme. Et ce n\u2019est pas essentiel si l\u2019on m\u00e8ne des\npolitiques \u00e0 court terme."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/30/Temp%C3%AAte-et-inondation%3A-1%2C5-milliards-d-euros-pour-Xynthia", "bloglinks": {}, "links": {"http://cache.20minutes.fr/": 1, "http://freakonometrics.free.fr/": 2, "http://www.senat.fr/": 1, "http://www.lafautesurmer.net/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Il y a quelques semaines, le service de centralisation des statistiques de l\u2019assurance accidents LAA ,\npar l\u2019interm\u00e9diaire de Stefan Scholz Odermatt, a\npubli\u00e9 une \u00e9tude sur la survenance d\u2019accident automobile\nles jours de match de foot. \n Selon l\u2019\u00e9tude (mentionn\u00e9e ici ou l\u00e0 ) le nombre d\u2019accidents les jours o\u00f9 il y a match est sup\u00e9rieur aux jours sans match,  Ayant\nr\u00e9ussi \u00e0 r\u00e9cup\u00e9rer une base presque exhaustive d\u2019accidents corporels en\nFrance, je peux faire la m\u00eame \u00e9tude, entre janvier 2002 et d\u00e9cembre\n2007. Les conclusions sont moins flagrantes que sur le cas suisse, en\nparticulier les jours de semaine, o\u00f9 manifestement, personne ne\ns\u2019int\u00e9resse aux matchs (o\u00f9 en tous les cas pas assez pour \u00eatre moins\nvigilent au volant). En revanche, le week end, l\u2019effet devient plus prononc\u00e9 avec davantage d\u2019accidents dans la journ\u00e9e o\u00f9 il y a un match de l\u2019\u00e9quipe de France. Moralit\u00e9,\nl\u2019\u00e9quipe de France de foot devrait jouer seulement en semaine, \u00e0 chaque\nmatch, cela \u00e9viterait une vingtaine d\u2019accidents de la route \u00e0 chaque\nfois (et je ne parle que d\u2019accidents corporels, ayant caus\u00e9 des blessures et ayant donn\u00e9 lieu \u00e0 un constat de police)\u2009! Comme le disait un copain \u00e0 qui je racontais \u00e7a, \" l\u2019\u00e9quipe de France devrait jouer, \u00e7a serait un d\u00e9but \".... mais c\u2019est un autre sujet."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/24/L-%C3%A9quipe-de-France-de-foot-devrait-jouer-seulement-en-semaine", "bloglinks": {}, "links": {"http://www.tdg.ch/": 1, "http://www.suva.ch/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["En fin de semaine j\u2019\u00e9tais \u00e0 Lyon \u00e0 l\u2019\u00e9cole d\u2019\u00e9t\u00e9 organis\u00e9e par\nSt\u00e9phane, pour parler bootstrap et estimation de l\u2019incertitude associ\u00e9e\n\u00e0 l\u2019estimation du montant des r\u00e9serves (pour faire simple, les slides\nsont en ligne ici ). Je ferais un billet plus d\u00e9taill\u00e9 (avec les codes\npour r\u00e9pondre \u00e0 la demande que m\u2019ont faites plusieurs personnes). A la fin de l\u2019expos\u00e9, on discutait des estimateurs sans biais\navec quelqu\u2019un qui se plaignait du biais \u00e0 distance fini de certains\nestimateurs, et qui me demandait \" vous ne pouvez pas trouver un estimateur sans biais du msepc\u2009? \n\". Si \u00e0 la rigueur la recherche des propri\u00e9t\u00e9s d\u2019estimateur peut \u00eatre\nun passe temps occasionnel, je n\u2019ai pas eu la pr\u00e9sence d\u2019esprit de lui\nfaire remarquer qu\u2019il existe des param\u00e8tres dont il est impossible de construire un estimateur sans biais... \u00e9tonnant, non\u2009? \n \nPrenons la loi de Poisson de param\u00e8tre . Alors Supposons que l\u2019on cherche \u00e0 estimer . S\u2019il existe une statistique qui soit un estimateur sans biais de , alors Or est une fonction de , donc Cela signifie tout simplement que \u00c7a c\u2019est bien g\u00eanant parce que la fonction n\u2019admet pas de d\u00e9veloppement en s\u00e9rie enti\u00e8re (je renvoie \u00e0 des cours de pr\u00e9pa, ou l\u00e0 par exemple). Moralit\u00e9, il existe des grandeurs dont on ne peut pas construire un estimateur sans biais.... Qui l\u2019eut cru\u2009?"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/23/Estimateur%28s%29-sans-biais", "bloglinks": {}, "links": {"http://perso.univ-rennes1.fr/": 1, "http://hypography.com/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Le m\u00e9canisme bonus-malus a de tr\u00e8s nombreuses vertus, dont celui de\nrenforcer la solidarit\u00e9. Mais il semble qu\u2019il incite aussi \u00e0 ne pas\nd\u00e9clarer certains petits sinistres \u00e0 son assureur. C\u2019est ce que nous\nallons montrer ici. La distribution des personnes ayant un bonus de\n50% (ce qui est le niveau le plus bas que l\u2019on puisse atteindre, en\nth\u00e9orie) a la forme suivante, en fonction de l\u2019\u00e2ge du conducteur, \n > library(splines)\u2009; library(pscl)  > reg <- glm((bonus==50)~bs(ageconducteur),data=sinistres,family=binomial)  > base <- data.frame(ageconducteur=seq(18,80))  > y=predict(reg,newdata=base)  > plot(seq(18,80),y) \n \nComme je le notais d\u00e9j\u00e0 ici, une personne qui a un niveau\nde bonus bas peut \u00eatre incit\u00e9e \u00e0 ne pas\nd\u00e9clarer un petit sinistre \u00e0 son assureur (et de proposer\n un arrangement \u00e0 l\u2019amiable avec la contrepartie). \nCette sur-repr\u00e9sentation des 0 dans la base pour les tr\u00e8s\nbas niveaux de bonus peut \u00eatre prise en compte \u00e0 l\u2019aide\ndes mod\u00e8les dits zero-inflated . \nClassiquement, on supposait que dans le cas d\u2019un mod\u00e8le de Poisson. On va supposer ici que\nl\u2019assur\u00e9 peut d\u00e9cider de ne pas d\u00e9clarer certains\nsinistres. Autrement dit et pour  On peut consid\u00e9rer un mod\u00e8le logistique pour mod\u00e9liser\ncette probabilit\u00e9 de non-d\u00e9claration,  \nalors que pour le mod\u00e8le de Poisson \nEn fait, si l\u2019on suppose que l\u2019impact d\u2019une variable n\u2019est pas\nlin\u00e9aire, on peut introduire des splines pour estimer la\ntransformation optimale , \n > library(splines)\u2009; library(pscl)  \n> reg=zeroinfl(nombre~bs(ageconducteur,df=4) | bs(ageconducteur), data = nombre,  \n+ dist = \"poisson\",link=\"logit\",offset=log(exposition))  \n> age=seq(18,80)  \n> DT=data.frame(ageconducteur=age,exposition=1)  \n> Y=predict(reg,newdata=DT,type=\"zero\")  \n> plot(age,Y) Ce qui permet de ne faire une pr\u00e9diction que \nsur la composante d\u2019inflation zero. Sur une base de donn\u00e9es sur\nlaquelle je devrais revenir \u00e0 la rentr\u00e9e, on obtient la tendance\nsuivante, Malheureusement,\nl\u2019interpr\u00e9tation est plus d\u00e9licate, car avec une r\u00e9gression binomiale\nn\u00e9gative, qui autorise plus de variance, on obtient des ordres de\ngrandeur tr\u00e8s diff\u00e9rents \n> reg=zeroinfl(nombre~bs(ageconducteur,df=4) | bs(ageconducteur), data = nombre,  \n+ dist = \"negbin\",link=\"logit\",offset=log(exposition))  \n(on\noubliera le comportement de bord pour les \u00e2ges \u00e9lev\u00e9s, peu de personnes\n\u00e2g\u00e9es appartenant \u00e0 ce portefeuille, les r\u00e9sultats sont peu robustes \u00e0\ndroite). On retrouve toutefois une fonction croissante (au moins\nentre 20 et 60 ans), ce qui peut \u00eatre reli\u00e9 avec la distribution du\nbonus en fonction de l\u2019\u00e2ge: plus on est \u00e2g\u00e9, plus on a de chance\nd\u2019avoir un tr\u00e8s bon bonus, et plus on a de chances de ne pas d\u00e9clarer\nun sinistre \u00e0 son assureur... D\u2019ailleurs, si on fait la r\u00e9gression directement sur le niveau de bonus, et plus sur l\u2019\u00e2ge, on a l\u2019impact suivant \n> reg=zeroinfl(nombre~bs(bonus,df=4) | bs(bonus), data = nombre,  \n+ dist = \"poisson\",link=\"logit\",offset=log(exposition))  \nMoralit\u00e9,\nc\u2019est bien le niveau \u00e9lev\u00e9 de bonus qui incite les assur\u00e9s \u00e0 ne pas\nd\u00e9clarer de sinistres \u00e0 leurs assureurs (et pas forc\u00e9ment un effet d\u2019\u00e2ge que l\u2019on\npourrait associer \u00e0 de l\u2019Alzheimer)."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/23/Bonus-malus-et-non-d%C3%A9laration-de-sinistres", "bloglinks": {}, "links": {}, "blogtitle": "Arthur Charpentier"}, {"content": ["L\u2019autre jour, l\u2019INED avait fait beaucoup de bruit en publiant une \u00e9tude\nexpliquant que le pic des naissance classiquement observ\u00e9 en mai\nsemblait avoir disparu ( ici ou l\u00e0 ).\nOn va essayer de v\u00e9rifier ce point, \u00e0 partir de toutes les naissances\nobserv\u00e9es en France entre 1968 et 2005. Ca sera le billet sexe de l\u2019\u00e9t\u00e9... \n Le nombre quotidien de naissances en France \n \nLa s\u00e9rie brute ressemble \u00e0 \u00e7a, \nOn\npourrait \u00e0 la rigueur extraire les cycles directement sur cette s\u00e9rie,\nmais \u00e7a ne serait pas forc\u00e9ment convainquant (m\u00eame si visuellement on\nretrouve ce qui \u00e9tait \u00e9voqu\u00e9), \nmais la premi\u00e8re chose que l\u2019on observe est que la diff\u00e9rence entre le nombre de naissances le week end (en rouge ) et en semaine (en bleu ) s\u2019est fortement creus\u00e9e dans les ann\u00e9es 80-90. \nOn peut regarder le ratio du nombre de naissance le week end, par rapport aux jours de la semaine pr\u00e9c\u00e9dant le week end, \net on peut alors \" corriger \" ou \" lisser \" la s\u00e9rie, de mani\u00e8re \u00e0 maintenir la moyenne hebdomadaire, mais en transf\u00e9rant quelques naissances de la semaine au week end. \nSur cette s\u00e9rie liss\u00e9e, on peut extraire une tendance de long terme. \nC\u2019est sur le \" bruit \"\nautour de cette tendance que l\u2019on peut essayer d\u2019\u00e9tudier un \u00e9ventuel\ncycle annuel. Notons que l\u2019on retrouve les tendances sur la s\u00e9rie brute \n S\u00e9rie corrig\u00e9e de la tendance de long terme \n \nLa s\u00e9rie suivante permet de voir l\u2019\u00e9volution du \"cycle\" (s\u00e9rie brute \u00e0 laquelle on retranche la tendance observ\u00e9e ci-dessus) \u00e0 la fin des ann\u00e9es 60 (en rouge ), des ann\u00e9es 70 (en orange ), des ann\u00e9es 80 (en jaune ), voire des ann\u00e9es 90 (en presque blanc ), \nSur les ann\u00e9es 68-78, on a la tendance suivante \nalors que pour les ann\u00e9es 95-2005, cela donne \nAu final, les deux tendances sont les suivantes, avec en rouge les ann\u00e9es 70 et en bleu les ann\u00e9es 2000. \nEffectivement,\nle pic classiquement observ\u00e9 au mois de mai semble avoir disparu, et la\ns\u00e9rie aujourd\u2019hui semble beaucoup plus lisse. Et comme l\u2019INED, on note\nqu\u2019il y a effectivement un mini-pic au mois de septembre. Des naissances au comportement sexuel \n \nTout\ncomme l\u2019INED, on peut relier \u00e7a \u00e0 l\u2019activit\u00e9 sexuelle des fran\u00e7ais, que\nl\u2019on peut visualiser sur le graphique ci-dessous (en d\u00e9calant tout\nsimplement de 9 mois), \nBref,\nl\u2019activit\u00e9 sexuelle semblait s\u2019intensifier pendant les vacances\nscolaires, en particulier d\u00e9but ao\u00fbt (mais aussi plus g\u00e9n\u00e9ralement\npendant les vacances scolaires, que ce soit P\u00e2ques, les vacances d\u2019\u00e9t\u00e9,\nou les vacances de No\u00ebl d\u2019ailleurs). Ou peut \u00eatre pourrait-on conclure\n\u00e0 la rigueur que l\u2019\u00e9t\u00e9, on sort moins couvert.... car on observe les\ncomportements sexuels associ\u00e9s \u00e0 une naissance 9 mois plus tard (ce qui\nne doit pas correspondre \u00e0 l\u2019unique activit\u00e9 sexuelle). Les fran\u00e7ais\nsemblent avoir intensifi\u00e9 leur activit\u00e9 sexuelle pendant une p\u00e9riode\nplus fra\u00eeche (novembre-mars), au d\u00e9triment de la p\u00e9riode estivale. Un\neffet du r\u00e9chauffement climatique (il fait trop chaud pour batifoler\nl\u2019\u00e9t\u00e9)\u2009? Un effet des 35 heures et des RTT (et des vacances qui sont\nplus r\u00e9parties dans l\u2019ann\u00e9e qu\u2019avant)\u2009? L\u2019\u00e9t\u00e9 s\u2019ra chaud\u2009? \n \nEn\nfait, au lieu de penser en terme de saison, on peut penser en terme de\ntemp\u00e9rature (j\u2019ai pris la temp\u00e9rature quotidienne minimale \u00e0 Paris,\nc\u2019est \u00e0 dire la nuit... ).  \nc\u2019est \u00e0 dire en zoomant \nOn\npeut aussi chercher \u00e0 \u00e9tudier davantage ce qui se passait il y a 30 ans\net maintenant (comme dans la partie pr\u00e9c\u00e9dente). Il y a 30 ans (courbe rouge ) on observait un (tr\u00e8s) fort pic estival. Ce pic a disparu aujourd\u2019hui (courbe bleue ), avec un comportement comparable, qu\u2019il fasse 0 ou 20 degr\u00e9s dehors.  \n Quid de l\u2019impact des bourses\u2009? \n \nMais la recherche de la \nvariable qui pourrait expliquer cette saisonnalit\u00e9 ou ces pics est sans\nfin. Beaucoup de monde \u00e9voque la bonne sant\u00e9 \u00e9conomique pour expliquer\nles booms de natalit\u00e9.... \u00e7a marche peut \u00eatre \u00e0 long terme, mais \u00e0\ncourt terme\u2009? En fait, si on croisse l\u2019activit\u00e9 sexuelle avec le niveau de la bourse, on peut presque retrouver ce genre de comportement, \nune\nbourse en tr\u00e8s forte hausse semble provoquer un (l\u00e9ger) pic de natalit\u00e9\n9 mois plus tard, alors qu\u2019un effondrement de la bourse se traduit\nplut\u00f4t par une chute (l\u00e9g\u00e8re certes) 9 mois plus tard..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/20/Davantage-de-r%C3%A9gularit%C3%A9-dans-les-naissances", "bloglinks": {}, "links": {"http://www.lepoint.fr/": 1, "http://www.lemonde.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Suite \u00e0 une discussion au t\u00e9l\u00e9phone\nl\u2019autre jour avec Elsa, j\u2019ai voulu rajouter un petit billet - un peu\ntechnique - sur la multicolin\u00e9arit\u00e9. Ou comment interpr\u00e9ter le signe (et la valeur) d\u2019un coefficient dans une r\u00e9gression multiple. \n Interpr\u00e9ter une r\u00e9gression avec des\nvariables explicatives corr\u00e9l\u00e9es \n \nAssez souvent en\n\u00e9conom\u00e9trie, les variables explicatives peuvent\n\u00eatre corr\u00e9l\u00e9es. Ce qui peut introduire des erreurs\nd\u2019interpr\u00e9tation. Consid\u00e9rons le petit mod\u00e8le\nsuivant, o\u00f9 l\u2019on dispose de deux variables explicatives. \n> library(mnormt) \n> set.seed(1) \n> Z=rmnorm(200,c(0,0),matrix(c(1,-.8,-.8,1),2,2)) \n> Y=1+2*Z[,1]+.5*Z[,2]+rnorm(200) \n Si l\u2019on se contente\nd\u2019analyses univari\u00e9es, on notera que semble d\u00e9croitre avec . \n> summary(lm(Y~Z[,2])) \n Call: \n lm(formula = Y ~ Z[, 2]) \n Coefficients: \n    Estimate Std. Error t\nvalue Pr(>|t|)  \n (Intercept) 1.0229  0.1132 \n9.038 <2e-16 *** \n Z[, 2]  -1.0717  0.1171 \n-9.155 <2e-16 *** \n --- \n Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019\n0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n Residual standard error: 1.6 on 198\ndegrees of freedom \n Multiple R-squared: 0.2974, \nAdjusted R-squared: 0.2938 \n F-statistic: 83.81 on 1 and 198\nDF, p-value: < 2.2e-16  \nPourtant, si l\u2019on contr\u00f4le \u00e0 l\u2019aide la variable ,\non s\u2019aper\u00e7oit que l\u2019impact de sur va plut\u00f4t dans\nl\u2019autre sens, avec un signe positif, et significativement positif, \n> summary(lm(Y~Z[,1]+Z[,2])) \n Call: \n lm(formula = Y ~ Z[, 1] + Z[, 2]) \n Coefficients: \n    Estimate Std. Error t\nvalue Pr(>|t|)  \n (Intercept) 0.95647 0.07597 \n12.591 < 2e-16 *** \n Z[, 1]  2.04287 0.13082 \n15.615 < 2e-16 *** \n Z[, 2]  0.45621 0.12541 \n3.638 0.000351 *** \n --- \n Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019\n0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 \n Residual standard error: 1.073 on\n197 degrees of freedom \n Multiple R-squared: 0.686,  \nAdjusted R-squared: 0.6828 \n F-statistic: 215.2 on 2 and 197\nDF, p-value: < 2.2e-16  \nGraphiquement, on a le modele suivant \nautrement dit, croit fortement avec , mais \u00e0 fix\u00e9, on note que est croissant avec . L\u2019intuition derri\u00e8re est qu\u2019il faut regarder l\u2019impact de sur pour des individus qui se ressemblent - ou identique - c\u2019est \u00e0 dire\navec des valeurs de proches. Si l\u2019on consid\u00e8re le\nsous-\u00e9chantillon des petites \nvaleurs de , on obtient \n> I=Z[,1]<(-1) \n> plot(Z[,2],Y,xlab=\"\",ylab=\"\") \n> abline(lm(Y[I]~Z[I,2]),col=\"red\",lwd=2) \n \nalors que pour les grandes valeurs\nde , \n \nBref, l\u2019impact n\u00e9gatif semble ici positif. En consid\u00e9rant\nun \u00e9chantillon plus grand (en simulant 2000 valeurs au lieu de 200), on obtient des choses ressemblant\n\u00e0 \u00e7a, \n \n Interpr\u00e9tation sur le probl\u00e8me des\ntailles des classes \n \nSi l\u2019on revient au sujet de\nl\u2019impact de la taille de la classe (dans les \u00e9coles) sur les r\u00e9sultats scolaires,\non pourrait \u00eatre dans la m\u00eame configuration. Sans variable\nde contr\u00f4le, la taille de la classe impact de mani\u00e8re\npositive les r\u00e9sultats scolaires: plus la classe est nombreuse,\nmeilleures sont les r\u00e9sultats. Or comme le notaient Angrist et\nLavy ( ici ), la taille de la classe est tr\u00e8s\ncorr\u00e9l\u00e9e \u00e0 des variables socio-\u00e9conomiques.\nSi l\u2019on arrive \u00e0 trouver de bonnes variables de contr\u00f4le,\non peut parfaitement obtenir un impact n\u00e9gatif, comme dans\nl\u2019exemple simul\u00e9 ci-dessus..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/16/%284%29", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Suite \u00e0 une formation que je faisais en fin de semaine \u00e0 Brest (les slides sont ici et l\u00e0 ), je voulais revenir sur les histoires de tails of copulas ,\npour reprendre le titre de l\u2019article ( ici ) de Gary Venter (et qui correspond\n\u00e0 des choses que j\u2019avais pu pr\u00e9senter il y a quelques\nann\u00e9es \u00e0 Berlin, les slides \u00e9tant en ligne ici ).\n \n Quantifier la d\u00e9pendance de queue \n \nL\u2019id\u00e9e est de noter qu\u2019il est noter qu\u2019il existe deux\nmani\u00e8res de quantifier la d\u00e9pendance de queue. La\npremi\u00e8re est li\u00e9e \u00e0 l\u2019approche de Joe (1990, ici , ou 1997 pour le livre), qui\na introduit un ( strong ) tail dependence index . Par exemple pour la queue inf\u00e9rieure, soit  La seconde est li\u00e9e \u00e0 une id\u00e9e que l\u2019on retrouve dans les travaux de\nJanet Heffernan, Stuart Coles ou Jonathan Tawn. L\u2019intuition est la\nsuivante (on peut la retrouver en ligne ici ). Si et ont la m\u00eame loi et que l\u2019on suppose les variables ind\u00e9pendantes, alors \nEn revanche, si les variables sont comonotones (c\u2019est \u00e0 dire \u00e9gales comme on suppose les lois identiques), \nAussi, on peut supposer qu\u2019il existe un indice tel que \nLe soucis est que le cas d\u2019ind\u00e9pendance correspond \u00e0 =2, alors que le cas de d\u00e9pendance forte correspond au cas =1. Il est alors usuel de faire une transformation affine pour se\nramener sur [0,1], et que la force de la d\u00e9pendance soit\ncroissante avec , e.g. Posons alors qui pourra \u00eatre interpr\u00e9t\u00e9 comme un ( weak ) tail dependence index . \nBref, ces deux mesures donnent de l\u2019information sur le comportement dans les queues de distribution. \n Les fonctions de concentration dans les queues \n \nL\u2019id\u00e9e est de noter qu\u2019il est possible d\u2019\u00e9tudier ces fonctions afin de\nmieux comprendre le comportement dans les queues. En s\u2019inspirant de\nGary Venter, on peut d\u00e9finir  \npour \u00e9tudier le comportement dans la queue inf\u00e9rieure, et  pour la queue sup\u00e9rieure,o\u00f9 est la copule de survie associ\u00e9e \u00e0 , au sens o\u00f9  \net Cet outil permettra de mod\u00e9liser la d\u00e9pendance forte . On peut \u00e9galement poser, afin d\u2019\u00e9tudier la d\u00e9pendance faible, ou  \n Application statistique \n \nL\u2019id\u00e9e est de noter qu\u2019il est facile d\u2019estimer ces fonctions. Ces\noutils peuvent \u00eatre utiles pour mieux comprendre le comportement dans\nles queues. \nPar exemple pour une copule Gaussienne de corr\u00e9lation 0,5, on a la\nforme th\u00e9orique suivante pour les fonctions de concentration (au sens\nfort) \nStatistiquement, il est possible d\u2019estimer ces quantit\u00e9s en\ncomptant simplement le nombre d\u2019observations dans le coin\ninf\u00e9rieur gauche, ou le coin sup\u00e9rieur droit. \nSi on dispose d\u2019un \u00e9chantillon, on peut alors regarder ce que donnent les versions \net  Pour un\n\u00e9chantillon de taille n=500, on obtient les intervalles de\nconfiance \u00e0 90% de la forme suivante, \nLe code R ressemble \u00e0 \u00e7a \n > library(evd); data(lossalae) > cor(lossalae,method=\"spearman\")   Loss  ALAE Loss 1.000000 0.451872 ALAE 0.451872 1.000000 avec le code suivant pour la version empirique, \n > library(evd); data(lossalae)  > z=seq(0,.5,by=.001)  > U=rank(v[,1])/(nrow(v)+1) > V=rank(v[,2])/(nrow(v)+1) > Lemp=rep(NA,length(z)) > Remp=rep(NA,length(z)) > for(i in 1:length(z)){ + Lemp[i]=sum((U<=z[i])&(V<=z[i]))/sum(U<=z[i]) + Remp[i]=sum((U>=1-z[i])&(V>=1-z[i]))/sum(U<=z[i]) + } et pour la version th\u00e9orique, > Lg=(pcopula(copclayton,cbind(z,z)))/(z) > Rg=((1-2*(1-z)+pcopula(copclayton,cbind(1-z,1-z))))/(z)  > plot(c(1-z,z),c(Lg,Rg)) \nDe plus, on a des fonctions similaires pour la d\u00e9pendance au sens faible, avec le code suivant pour la version th\u00e9orique, \n > Lg=log(pcopula(cop,cbind(z,z)))/log(z) > Rg=log((1-2*(1-z)+pcopula(cop,cbind(1-z,1-z))))/log(z) > Lg=1/Lg*2-1 > Rg=1/Rg*2-1 et celui l\u00e0 pour la version empirique \n > z=seq(0,.5,by=.001) > v <- lossalae > U=rank(v[,1])/(nrow(v)+1) > V=rank(v[,2])/(nrow(v)+1) > Lemp=rep(NA,length(z)) > Remp=rep(NA,length(z)) > for(i in 1:length(z)){ + Lemp[i]=log(mean((U<=z[i])&(V<=z[i])))/log(mean(U<=z[i])) + Remp[i]=log(mean((U>=1-z[i])&(V>=1-z[i])))/log(mean(U<=z[i])) + } > Lemp=1/Lemp*2-1 > Remp=1/Remp*2-1 Bref,\non peut utiliser ces fonctions sur des vrais \u00e9chantillons. Consid\u00e9rons\nl\u2019exemple classique loss-alae (o\u00f9 l\u2019on couple les frais dans des\nsinistres assur\u00e9s, et les frais pay\u00e9s par l\u2019assureur). On souhaite\najuster une copule, sans trop savoir laquelle. On peut commencer par\n\u00e9tudier la d\u00e9pendance forte, et comparer avec une copule Gaussienne. La\ncopule Gaussienne de r\u00e9f\u00e9rence poss\u00e8de ici le m\u00eame rho de Spearman que\nl\u2019\u00e9chantillon dont on dispose, \n > library(evd); data(lossalae) > cor(lossalae,method=\"spearman\")   Loss  ALAE Loss 1.000000 0.451872 ALAE 0.451872 1.000000 > library(copula) > paramgauss=.47 > paramclayton=.9 > paramgumbel=1.45 > copgauss=normalCopula(paramgauss) > copclayton=claytonCopula(paramclayton, dim = 2) > copgumbel=gumbelCopula(paramgumbel, dim = 2) On obtient ici \nLa\ncourbe verte est l\u2019intervalle de confiance (ponctuel) \u00e0 95% pour une\ncopule Gaussienne et un \u00e9chantillon de m\u00eame taille. On voit qu\u2019on\nmod\u00e9lise mal la structure de d\u00e9pendance. Avec une copule duale de\nClayton, on obtient \n et enfin pour une copule de Gumbel, \n \nBref,\nla copule de Gumbel semble r\u00e9ellement bien adapt\u00e9e... Si on creuse en\n\u00e9tudiant la d\u00e9pendance au sens faible, on peut valider l\u00e0 aussi ce\nmod\u00e8le. En effet, si la r\u00e9f\u00e9rence est la copule Gaussienne, \nou pour une copule de Clayton, \nalors qu\u2019une copule de Gumbel donnerait"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/08/Tails-of-copulas%2C-une-lecture-graphique", "bloglinks": {}, "links": {"http://perso.univ-rennes1.fr/": 3, "http://econstor.eu/": 1, "http://hhttp//www.actuaries.org/ASTIN/Colloquia/Washington/Venter.pdf": 1, "http://ideas.repec.org/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Autant l\u2019admettre rapidement, transf\u00e9rer un blog n\u2019est pas aussi\nsimple que j\u2019aurais pu le penser... Certes, l\u2019essentiel des billets\nsont d\u00e9sormais lisibles sur freakonometrics.free.fr , mais les cat\u00e9gories\nn\u2019ont pas \u00e9t\u00e9 transf\u00e9r\u00e9es, ainsi que les\ncommentaires. Et comme certains commentaires qui m\u2019ont\n\u00e9t\u00e9 fait sont plus int\u00e9ressants que les billets\neux-m\u00eames, je trouve que \u00e7a manque. Mais surtout le\nr\u00e9f\u00e9rencement a du mal \u00e0 suivre... \n \nBref, autant faire simple, et je vais continuer \u00e0 nourrir les\ndeux blogs en parall\u00e8les (quite probablement \u00e0 ne pas\nmettre forc\u00e9ment tous les billets en ligne ici)."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/07/01/Transfert-du-blog%2C-suite", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Un court billet aujourd\u2019hui pour pr\u00e9senter deux ou trois petits\ngraphiques (je n\u2019ai malheureusement pas encore trouv\u00e9 comment\naller plus loin sur l\u2019analogie...). \n La\nrecherche et l\u2019 impact factor des revues \n \nDans le syst\u00e8me d\u2019\u00e9valuation des chercheurs (je pourrais\nm\u00eame dire des enseignants chercheurs) on est \u00e9valu\u00e9\ntout simplement en fonction des revues dans lesquelles on arrive\n\u00e0 placer nos papiers. Les revues sont en effet not\u00e9es,\nclass\u00e9es, le classement le plus connu \u00e9tant l\u2019 impact factor \n(j\u2019en avais parl\u00e9 l\u2019an dernier lors du versement de primes \u00e0 des\nchercheurs \u00e0 Lyon, ici ).\nQuelques\nrevues ont un facteur d\u2019impact tr\u00e8s\n\u00e9lev\u00e9, et ensuite se trouve des centaines voire des\nmilliers de revues sp\u00e9cialis\u00e9es. La distribution de ce\nfacteur d\u2019impact a l\u2019allure suivante, \n \nComme l\u2019ont not\u00e9 plusieurs auteurs ( ici \nou l\u00e0 ),\nla loi de dite de Zipf pourrait bien reproduire cette distribution\n(j\u2019en avais parl\u00e9 il y a plus d\u2019un an dans un billet\nsur la loi de George Kingsley Zipf ( ici ),\nque j\u2019avais fait un peu apr\u00e8s avoir jou\u00e9 avec la loi de Benford ( ici \net l\u00e0 )). \n La\npresse quotidienne, hebdomadaire ou mensuelle, et les nombres de tirages \n \nLe monde de la recherche essaye r\u00e9guli\u00e8rement de donner\nl\u2019illusion d\u2019une brillante autor\u00e9gulation, avec une\nreconnaissance par ses pairs, des relecteurs anonymes des articles\nsoumis dans les revues, etc. Mais le monde de la recherche n\u2019est pas si\n\u00e9loign\u00e9 que \u00e7a des querelles d\u2019\u00e9piciers sur le tirages de la presse....\nEn fait, si on regarde ce qu\u2019on appelle la diffusion de\nla presse payante technique et professionnelle ( ici ) et la presse\npayante grand public ( l\u00e0 ), on retrouve \u00e0\ntr\u00e8s peu de choses pr\u00eat la m\u00eame loi... \n Bref, plus publier dans une revue \u00e0 gros tirage veut dire qu\u2019on est\nun meilleur chercheur... ou quelque chose du genre. Le code est ici pour\nceux qui doutent, \n >\npresse=read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/presses.csv\",sep=\";\",header=TRUE) #\npresse=read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/Science_Journal_Ranking_Version_2003.csv\",sep=\";\",header=TRUE) >\nX=as.numeric(as.character(presse$Diffusion)) >\nX=X[is.na(X)==FALSE] > Xs=sort(X) >\nplot(length(X):1,Xs,col=\"blue\",xlab=\"Ranking\",ylab=\"Tirage (diffusion\nFrance pay\u00e9\u00e9)\") >\nplot(log(length(X):1),Xs,col=\"blue\",xlab=\"Ranking (log)\",ylab=\"Tirage\n(diffusion France pay\u00e9\u00e9)\") \n Liens\navec la th\u00e9orie des valeurs extr\u00eames \n \nEn fait, la loi de Zipf est tout simplement une loi de Pareto dans le\ncas discret (oui, les chercheurs ont un don pour donner des noms\ndiff\u00e9rents \u00e0 des objets identiques, \u00e7a permet de faire plusieurs\npapiers sans que personne ne se doute de rien... pour ceux qui en\ndoute, il suffit de croiser sous Google finance avec Zipf, Pareto,\nPower law, on va retrouver plein de monde qui raconte exactement la\nm\u00eame chose). Bref, la loi de Pareto donne des choses tr\u00e8s proches, \ny compris la loi exponentielle qui est un cas limite de la loi de Pareto \n On peut d\u2019ailleurs aller un peu plus loin, en estimant le param\u00e8tre\nde la puissance, avec l\u2019estimateur de Hill (discut\u00e9 l\u00e0 ), \nce\nqui confirme que les lois sont proches (des lois puissance), m\u00eame si\ndans le cas de la presse fran\u00e7aise, on est sur un indice proche de 1.5\n(\u00e0 droite), alors que la presse acad\u00e9mique est de l\u2019ordre de 2 (\u00e0\ngauche), avec toutefois une explosion du c\u00f4t\u00e9 des extr\u00eames (les 30 meilleures\n revues ayant visiblement un impact beaucoup trop important)."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/18/Econometrica-et-Ouest-France%3A-m%C3%AAme-combat", "bloglinks": {}, "links": {"http://www.ojd.com/": 2, "http://freakonometrics.free.fr/": 5, "http://papers.ssrn.com/": 1, "http://www.sciencedirect.com/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["(oui, je commence \u00e0 num\u00e9roter comme les Donjons,\nou comme lorsque je commence des sections ou des sous-sections quand je\nfais cours au tableau). J\u2019avais fait l\u2019autre jour un billet sur le\nparadoxe de Saint-P\u00e9tersbourg, expliquant que, bien que\nl\u2019esp\u00e9rance de ce jeu soit infinie, \u00e7a reste un\njeu qui s\u2019ach\u00e8ve tr\u00e8s rapidement, car en moyenne,\non n\u2019a le droit de ne faire que deux lancers... \nMais avant d\u2019aller plus loin, un petit retour en arri\u00e8re\ns\u2019impose. En fait, j\u2019ai toujours pr\u00e9sent\u00e9 en\ncours la th\u00e9orie de l\u2019esp\u00e9rance de\nl\u2019utilit\u00e9 comme une r\u00e9ponse \u00e0 ce\nparadoxe. Je me rend compte, apr\u00e8s avoir cherch\u00e9\nun peu, que c\u2019est un peu plus compliqu\u00e9 \u00e7a.\nInstallons nous confortablement, je vais raconter la petite histoire.... \n \n \nEn 1738, Daniel Bernoulli a publi\u00e9 un ouvrage\nintitul\u00e9 Specimen\ntheoriae novae de mensura sortis \n (que nous pourrons traduire th\u00e9orie sur la mesure\ndu risque , dont la traduction est longuement\ncomment\u00e9e dans un document de Pierre Charles Pradier, ici ,\nqui note que sortis est\nli\u00e9 au sort ,\net donc au hasard ,\nau risque ).\nC\u2019est dans cet ouvrage qu\u2019il introduit le concept d\u2019 esp\u00e9rance\nmorale . \nToute cette th\u00e9orie a \u00e9t\u00e9 reprise dans\nla th\u00e9orie\nanalytique des probabilit\u00e9s publi\u00e9e\npar Laplace en 1812, \n \n \n La\nconstruction de la fortune morale \n \nPour comprendre le cheminement de Daniel Bernoulli, supposons qu\u2019une\npersonne passe d\u2019une somme de monnaie \u00e0\nune somme .\nIl\npense que la valeur relative de cet incr\u00e9ment doit\n\u00eatre proportionnel \u00e0 l\u2019incr\u00e9ment ,\nmais inversement proportionnel \u00e0 la richesse .\nAutrement\ndit, la variation de la valeur \nv\u00e9rifie \nou encore  \nPour reprendre la terminologie de Laplace, \nest la fortune physique\n et\ny la fortune morale .\nDans la langue de Daniel Bernoulli, \nest appel\u00e9 emolunmentum ,\net summa\nbonorum .\nIntroduisons alors un peu d\u2019al\u00e9a. La personne commence avec\nune dotation physique .\nIl peut alors gagner \navec une\nprobabilit\u00e9 .\nDaniel Bernoulli propose alors de poser \nDaniel appelle emolunmentum\nmedium . En conservant une expression de la forme \nil convient de d\u00e9finir la fortune physique comme \nPour reprendre la terminologie de Laplace, \nest \" l\u2019accroissement\nde la fortune\nphysique qui procurerait \u00e0 l\u2019individu le m\u00eame\navantage moral qui r\u00e9sulte pour lui, de son expectative \",\nou encore, selon Daniel Bernoulli \nest le \" lucurm legitime\nexpectandum seu\nsors quoesita \". \nIl consid\u00e8re alors deux \u00e9tats possibles, \nde probabilit\u00e9s respectives .\nLa\nfortune physique est alors \nOn supposera que le jeu est juste, autrement dit ,\nou\navec les notations de Laplace, } \nDaniel Bernoulli (puis plus tard Laplace) montr\u00e8rent que\nm\u00eame si le jeu \u00e9tait juste, la fortune physique\n\u00e9tait toujours plus petite que .\nEn fait, la\nd\u00e9monstration est simple, car comme on a une mesure de\nprobabilit\u00e9, ,\net donc \nAutrement dit, on doit simplement montrer que \nL\u2019astuce pour montrer ce dernier r\u00e9sultat consiste\n\u00e0 noter que le terme de droite peut s\u2019\u00e9crire \nce que revient \u00e0 comparer une moyenne\ng\u00e9om\u00e9trique et une moyenne\narithm\u00e9tique (ce que j\u2019avais \u00e9voqu\u00e9\n ici\n avec des dessins). \nA partir de cette th\u00e9orie, Daniel Bernoulli propose ensuite\nune application en assurance. Un armateur poss\u00e8de une\nfortune physique ,\net il esp\u00e8re toucher x si le bateau\narrive \u00e0 bon port (ce qui surviendra avec\nprobabilit\u00e9 ).\nS\u2019il ne s\u2019assure pas, sa fortune physique\nsera  \nEn invoquant encore une fois cette comparaison entre moyenne\ng\u00e9om\u00e9trique et moyenne arithm\u00e9tique,\non en d\u00e9duit que \nDaniel Bernoulli en d\u00e9duit que l\u2019armateur a\nint\u00e9r\u00eat \u00e0 s\u2019assurer d\u00e8s lors\nque la prime d\u2019assurance ne d\u00e9passe pas \n(qui est la\nprime pure en terminologie actuarielle). \nEn notant e la prime d\u2019assurance, Bernoulli montre qu\u2019il faut chercher\nce que nous appellerions un \u00e9quivalent certain, solution de  \nIl essaye alors de r\u00e9soudre ce probl\u00e8me comme une\n\u00e9quation en \n(la prime d\u2019asssurance e \u00e9tant\nsuppos\u00e9e donn\u00e9e). Mais je m\u2019\u00e9gare un\npeu.... \n Le\nparadoxe de Saint Petersbourg \n \nUne fois pr\u00e9sent\u00e9 son probl\u00e8me\nd\u2019assurance, Daniel Bernoulli reprend un probl\u00e8me qu\u2019il\navait soumis \u00e0 Pierre R\u00e9mond de Montmort en\nseptembre 1713 ( ici ).\n \n \nDans ce jeu, un joueur lance un pi\u00e8ce en l\u2019air. Si\n\"face\" appara\u00eet au premier lancer, il touche 1 shilling.\n S\u2019il tombe sur \"pile\", il peut relancer la pi\u00e8ce.\nSi \"face\" appara\u00eet au second lancer, il touche 2 shilling.\n S\u2019il tombe sur \"pile\", il peut relancer la pi\u00e8ce.\nSi \"face\" appara\u00eet au troisi\u00e8me lancer, il touche\n4 shilling. S\u2019il tombe sur \"pile\", il peut relancer la\npi\u00e8ce, etc. A chaque fois qu\u2019il fait \"face\", il double ses\ngains. \nL\u2019esp\u00e9rance de gain est alors \nqui est une somme infinie. \nPour r\u00e9soudre le probl\u00e8me, Daniel Bernoulli\ncalcule la fortune physique i.e. \nqui est finie d\u00e8s lors que \nest finie. Par exemple avec ,\non obtient 2. Pour ceux qui veulent s\u2019en convaincre\n(num\u00e9riquement) \n >\nx0=0 \n> prod((x0+2^(0:100))^(1/2^(1:101)))-x0 \n [1] 2 \nPour une richesse initiale de 1000, Bernoulli montre que la fortune\nphysique est alors de 6, \n >\nx0=1000 \n> prod((x0+2^(0:100))^(1/2^(1:101)))-x0 \n [1] 5.972253 \nBref, Daniel Bernoulli avait ainsi montr\u00e9 que ses calculs de\nfortune physique permettait d\u2019expliquer pourquoi les personnes jouant\n\u00e0 son jeu \u00e9taient pr\u00e8s \u00e0\njouer une somme assez faible (en tous les cas compar\u00e9\n\u00e0 la valeur esp\u00e9r\u00e9e de gain). \n Montmort,\nBernoulli(s)\net Cramer \n \nEn fait, si Montmort avait \u00e9voqu\u00e9 ce paradoxe \nd\u00e8s 1713, on peut retrouver \u00e9galement ce probl\u00e8me soulev\u00e9 en 1728 par\nGabriel Cramer, toujours dans des lettres \u00e9chang\u00e9es avec Bernoulli,\nmais cette fois avec Nicolas, et pas Daniel, ici , \n \n \nA la lecture de\ntout cela, on se rend compte que le paradoxe de Saint-P\u00e9tersbourg a \u00e9t\u00e9\npropos\u00e9 \u00e0 Bernoulli par Montmort et Cramer, que Cramer introduit\n\u00e9galement la notion d\u2019 esp\u00e9rance morale , bien avant Bernoulli.... L\u2019histoire est\nsouvent bien injuste d\u00e8s qu\u2019on se penche sur les h\u00e9ritages..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/11/Le-paradoxe-de-Saint-P%C3%A9tersbroug%2C-partie-2", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1, "http://www.xu.edu/": 1, "http://picha.univ-paris1.fr/": 1, "http://gallica.bnf.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Le blog devrait progressivement migrer depuis \n http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/ vers cette nouvelle adresse, \n http://freakonometrics.blog.free.fr/index.php/ \nEncore\nquelques r\u00e9glage \u00e0 faire, et comprendre comment faire proprement du\ndotclear avec free... En attendant, je vais probablement mettre les\ndeux blogs \u00e0 jour de mani\u00e8re sumultan\u00e9e. Encore quelques semaines avant\nl\u2019inauguration.... et un basculement d\u00e9finitif probablement pour la rentr\u00e9e..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/11/Transfert-du-blog....", "bloglinks": {}, "links": {"http://freakonometrics.free.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Il y a plusieurs mois, j\u2019avais fait un billet sur le paradoxe de Simpson ( ici ).\nPour reprendre un exemple simple, supposons que l\u2019on a deux h\u00f4pitaux,\naffichant les statistiques suivantes, pour les personnes \" saines \", \n \n h\u00f4pital \n total \n survivants d\u00e9c\u00e8s taux de survie \n \n \n hopital A \n 600 \n 590 10 98%  x \n \n \n hopital B \n 900 \n 870 30 97%   et pour les\npersonnes \" malades \"\n(on suppose vraiment que le crit\u00e8re est identique pour les deux h\u00f4pitaux) \n \n h\u00f4pital \n total \n survivants d\u00e9c\u00e8s taux de survie \n \n \n hopital A \n 400 \n 210 190 53%  x \n \n \n hopital B \n 100 \n 30 70 30%   Bref, peu importe son \u00e9tat de sant\u00e9, on a toujours int\u00e9r\u00eat \u00e0 choisir l\u2019h\u00f4pital\nA. D\u2019o\u00f9 le paradoxe. En revanche, si on combine, on obtient \" globalement \",  \n \n h\u00f4pital \n total \n survivants d\u00e9c\u00e8s taux de survie \n \n \n hopital A \n 1000 \n 800 200 80% \n \n \n hopital B \n 1000 \n 900 100 90%  x  on note que l\u2019on a int\u00e9r\u00eat \u00e0 choisir l\u2019h\u00f4pital B. \u00c9tonnant non\u2009? (l\u2019exemple vient de Marco Scarsini, et je n\u2019ai jamais trouv\u00e9 plus \u00e9l\u00e9gant). J\u2019\u00e9voque souvent ce paradoxe en \u00e9conom\u00e9trie, et c\u2019est ce que j\u2019avais fait dans mon pr\u00e9c\u00e9dant billet ( ici ).\nEn fait, Roger Nelsen propose un petit exemple simple permettant de\ncomprendre comment on peut obtenir un tel paradoxe. L\u2019id\u00e9e est de noter\nque et et pourtant  Cela n\u2019a a priori rien d\u2019exceptionnel... On peut parfaitement avoir et tout en ayant, C\u2019est ce que l\u2019on peut visualiser ci-dessous, \nMerci Roger...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/10/Paradoxe-de-Simpson-expliqu%C3%A9-par-un-dessin", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Plusieurs personnes m\u2019ont laiss\u00e9 des messages et des\ncommentaires suites aux derniers dessins, disant en substance \" c\u2019est\nbien joli, mais est-ce que c\u2019est vraiment une preuve\u2009? \".\nEtant incomp\u00e9tent en \u00e9pist\u00e9mologie, je\nn\u2019aurais pas la pr\u00e9tention d\u2019apporter des\nlumi\u00e8res... En revanche, Olivier m\u2019a forward\u00e9 ( ici ) le\ndocument en ligne l\u00e0 \n(on l\u2019on retrouve page 4 un dessin plus beau que celui que j\u2019avais pu\nfaire pour montrer que la loi g\u00e9om\u00e9trique est une\n vraie loi\nde proba), et surtout, il m\u2019a fait d\u00e9couvrir ici \nla notion de \" preuve\nsans mots \", o\u00f9 l\u2019on retrouve une\ntr\u00e8s jolie preuve anim\u00e9e (je me rends compte que\nl\u2019animation aide vraiment \u00e0 la compr\u00e9hension,\nc\u2019est \u00e7a que j\u2019aurais du faire depuis le d\u00e9but)\ndu th\u00e9or\u00e8me de Pythagore (que je continue de\nmentionner en cours, comme l\u00e0 ,\nlorsque je dois parler de formule de d\u00e9composition de la\nvariance). Bref, sur la page wiki est mentionn\u00e9 des ouvrages\nde Roger Nelsen, que je d\u00e9vore depuis ce matin. \n \n \n \n \n \n \n \n \n \n \n \n (ainsi que quelques livres avec Claudi Alsina). Pour la petite\nhistoire, c\u2019est justement Roger qui m\u2019avait expliqu\u00e9 comme\nobtenir ces propri\u00e9t\u00e9s de la loi\ng\u00e9om\u00e9trique, \u00e0 l\u2019\u00e9poque\no\u00f9 je faisais un short\ncourse \u00e0 Samos, en , sur advanced statistical methods in\ninsurance . J\u2019avais \u00e9voqu\u00e9\nl\u2019importance de la loi g\u00e9om\u00e9trique dans le\nconcept de p\u00e9riode de retour, et Roger m\u2019avait expliquer\ncomment retrouver l\u2019histoire de l\u2019esp\u00e9rance de la loi\ng\u00e9om\u00e9trique sans \u00e9quations.... \nJ\u2019ai retrouv\u00e9 plusieurs dessins que j\u2019avais fait dans mes cahiers de pr\u00e9pas (que je\ncommence \u00e0 ressortir ces derniers temps, depuis que je dois\nrevenir sur quelques concepts fondamentaux de convexit\u00e9 et\nde topologie... et qui m\u2019avait servi pour les dessins que j\u2019avais d\u00e9j\u00e0 pu faire, ici \nen particulier), et surtout j\u2019en ai d\u00e9couvert un paquet\nd\u2019autres, que je ne pourrais pas m\u2019emp\u00eacher\nd\u2019\u00e9voquer dans de futurs billets... D\u2019ailleurs, je me suis rendu compte\nqu\u2019on pouvait retrouver un bon nombre de ces exemples directement sur\njstor, l\u00e0 . Par exemple, il propose une interpr\u00e9tation\nde la comparaison de plusieurs moyennes (arithm\u00e9tique, g\u00e9om\u00e9trique,\netc), plus compl\u00e8te que ce que j\u2019avais pu faire, \nSur la preuve par le dessin, Jean-Paul Delahaye apporte les \u00e9claircissements suivant, ici , en citant David Hilbert qui disait : \u00ab les\nr\u00e8gles pour \u00e9crire une d\u00e9monstration doivent \u00eatre si claires que si\nquelqu\u2019un vous propose une d\u00e9monstration, alors une proc\u00e9dure m\u00e9canique\ndoit pouvoir assurer que la d\u00e9monstration est correcte, c\u2019est-\u00e0-dire\nqu\u2019elle ob\u00e9it aux r\u00e8gles . \u00bb. Il d\u00e9taille par ailleurs cet argument, \" ce\nque les Grecs \u2013 par exemple Euclide \u2013 consid\u00e9raient comme une\nd\u00e9monstration est aujourd\u2019hui encore, moyennant quelques adaptations\ndans le style et les notations, toujours consid\u00e9r\u00e9 comme une\nd\u00e9monstration. En revanche, nous avons beaucoup avanc\u00e9 sur les\nnotations, la structure logique des arguments, et la compr\u00e9hension de\nce qu\u2019il est possible d\u2019attendre de la m\u00e9thode axiomatique. Nos\nnotations et la logique math\u00e9matique ont tellement progress\u00e9 que,\ncontrairement aux Grecs, nous savons formaliser les d\u00e9monstrations que\nnous \u00e9crivons .\" Bref, pour ma part, je trouve cela tellement\n\u00e9l\u00e9gant (et surtout je sais que Roger est beaucoup plus comp\u00e9tent en\nmaths que moi) que je ne m\u2019emp\u00eacherais pas de mettre ici le compte\nrendu de mes lectures\u2009! Merci Roger, et Olivier...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/10/Dessins-et-preuves", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 3, "http://accromath.uqam.ca/": 2, "http://www.inist.fr/": 1, "http://fr.wikipedia.org/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["J\u2019avais racont\u00e9 l\u2019autre jour l\u2019aventure de mon fils qui souhaitait faire des triangles avec des bouts de bois ( ici ). Il a r\u00e9cidiv\u00e9 ce soir. Sauf qu\u2019il n\u2019avait qu\u2019un grand baton, et que j\u2019avais ferm\u00e9 le portillon qui l\u2019emp\u00eachait d\u2019aller dans la for\u00eat en chercher (la nuance entre romancer et devenir mythomane doit \u00eatre mince). Je lui ai sugg\u00e9r\u00e9 de le casser en trois portions, puis de constituer un triangle avec les trois morceaux restant. Manque de bol, l\u2019exp\u00e9rience rate (comme l\u2019autre fois). Et me souvenant de mes p\u00e9nibles calculs de la derni\u00e8re fois, je me suis rappel\u00e9 qu\u2019il avait une chance sur deux de pouvoir effectivement construire un triangle... sauf que la mani\u00e8re d\u2019obtenir les trois bouts de bois me semble diff\u00e9rente... donc peut \u00eatre que ce 1/2 n\u2019est plus valide\u2009? En fait, je crois qu\u2019il faut d\u00e9finir clairement comment on va couper les bouts de bois. \n m\u00e9thode 1: le coupage simultan\u00e9 \n \nJe suppose que je peux couper d\u2019un coup le bout de bois en deux. Formellement, je tire deux points sur l\u2019intevalle correspondant \u00e0 mon bout de bois, les tirages \u00e9tant uniformes, et ind\u00e9pendants. On arrive alors \u00e0 trois morceaux, , et , en sachant que   \nla longueur du bout de bois dont je diposais initialement. On supposera que =1 pour simplifier (ce qui ne changera rien au r\u00e9sultat final, on s\u2019en doute). L\u2019ensemble des points que je peux obtenir peuvent alors se visualiser comme des points du simplexe,  \nautrement dit, on peut les repr\u00e9senter dans le triangle \u00e9quilat\u00e9ral ci-dessous. Or on ne peut constituer un triangle que si la plus grande des longeurs n\u2019exc\u00e8de pas la somme des deux autres. Supposons que  soit la plus grande longueur (et qu\u2019elle soit visualis\u00e9e graphiquement par rapport \u00e0 la distance au c\u00f4t\u00e9 en bas), le point rouge ci-dessous (correpondant \u00e0 un triplet de longueurs possibles) ne convient pas. En fait, d\u00e8s lors que l\u2019on entre dans l\u2019aire ros\u00e9e, on ne peut plus constituer un triangle... Moralit\u00e9, \u00e0 moins de tomber dans le triangle jaune, on ne peut pas constituer de triangle.... Ce qui veut dire qu\u2019on n\u2019a plus qu\u2019une chance sur quatre\u2009!  m\u00e9thode 2: le coupage s\u00e9quentiel \n \nUne autre m\u00e9thodde consister \u00e0 couper d\u2019abord en deux (en tirant un point au hasard sur mon intervalle), puis de recouper en deux le plus grand morceau... Damned, mon fils a de bonne id\u00e9e, mais \u00e7a sent les calculs \u00e0 n\u2019en plus finir \u00e7a...Ou alors va falloir continuer \u00e0 essayer de faire des dessins, car mine de rien, c\u2019est plus simple que les calculs de la derni\u00e8re fois. Bon, la longueur de mon bout de bois est la hauteur de mon triangle. Je commence par le couper en deux, la distance au c\u00f4t\u00e9 en bas devant \u00eatre la plus petite des deux.  Une fois fix\u00e9 la taille de ce petit bout, il me reste \u00e0 couper le plus grand morceaux en deux. Pour cela, je vais me d\u00e9placer le long du segment rouge ci-dessous, La position sur ce petit segment me donnant ensuite les deux autres longueurs des morceaux. Or rappelons que pour que nos bouts de bois forments un triangle, il faut forc\u00e9ment appartenir au triangle jaune, Pour r\u00e9sum\u00e9, nous avions exclus la partie sup\u00e9rieure du triangle, et conditionnellement au fait que nous sommes dans le lozange inf\u00e9rieur, nous souhaitons \u00eatre dans le triangle jaune. Arg, cette fois-ci la probabilit\u00e9 passe \u00e0 une chance sur trois\u2009!  couper d\u2019un coup comme d\u00e9coupage s\u00e9quentiel al\u00e9atoire\u2009? \n \n Mais attends deux minutes mon bonhomme (oui, je parle \u00e0 mon fils). Si j\u2019avais choisi de d\u00e9couper le plus petit morceau, on sait qu\u2019il est impossible de constituer un triangle car par d\u00e9finition  Supposons que l\u2019on casse le baton en deux, et que l\u2019on tire ensuite au hasard (avec probabilit\u00e9 1/2 et 1/2) quelle partie sera alors coup\u00e9e en deux, on doit se ramener au cas pr\u00e9c\u00e9dant non\u2009? La probabilit\u00e9 de constituer un triangle doit alors \u00eatre \nDamned, mais \u00e7a ne fait pas un quart \u00e7a\u2009! Bon, je crois que je vais prendre une aspirine et aller demander \u00e0 mon fils d\u2019aller ranger ses b\u00e2tons\u2009!"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/07/Discussions-geometriques-du-soir-2", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Je voulais faire quelques billets rapides sur les tableaux crois\u00e9s,\nsuite \u00e0 des demandes cons\u00e9cutives \u00e0 mon billet sur le sexe des b\u00e9b\u00e9s ( ici ).\nPour commencer, je vais m\u2019int\u00e9resser au cas o\u00f9 les variables suppos\u00e9es\nind\u00e9pendantes (d\u2019autres billets seront l\u2019occasion de tester d\u2019autres\ntypes d\u2019hypoth\u00e8ses). Histoire de trouver des donn\u00e9es un peu originales,\non peut tra\u00eener sur internet. Etant tomb\u00e9 (presque) par hasard sur le\nfichier pdf ici , j\u2019ai voulu m\u2019int\u00e9resser \u00e0 l\u2019 origine des enseignants-chercheurs recrut\u00e9s lors de la campagne 2008 \n(comme le dit le titre du document en ligne sur le site du minist\u00e8re).\nPrenons deux variables cat\u00e9gorielles g\u00e9ographiques, par exemple \n , ville (au sens large) o\u00f9 l\u2019on a soutenu sa th\u00e8se \n , ville (au sens large) o\u00f9 l\u2019on a obtenu un poste de ma\u00eetre de conf\u00e9rence \n \nCompte\ntenu des r\u00e8gles de non-recrutement local, on devrait pouvoir supposer\nque ces variables sont ind\u00e9pendantes. En fait, je ne raisonne pas ici\npar universit\u00e9, mais par ville, ce qui correspond \u00e0 un recrutement\nlocal au sens large (passer de Paris 6 \u00e0 Paris 7 sera ici consid\u00e9r\u00e9\ncomme \" local \"). L\u2019\u00e9tude a\n\u00e9t\u00e9 faite \u00e0 partir des donn\u00e9es disponibles ici et l\u00e0 . Petite pr\u00e9caution\nd\u2019usage: ce sont les donn\u00e9es de la campagne 2008, avant que les comit\u00e9s\nde s\u00e9lection ne soient mis en place. Il conviendrait de refaire l\u2019\u00e9tude\nune fois les nouvelles donn\u00e9es publiques. On a deux bases, une en\ndroit-\u00e9conomie-gestion, et une en sciences. J\u2019insiste sur le fait que\nl\u2019on a des donn\u00e9es sur une ann\u00e9e seulement, donc avec peu\nd\u2019observations... Mais comme l\u2019indique le titre, je vais parler des\noutils visualisation des tableaux crois\u00e9s, pas faire un billet sur le\nrecrutement local. Voil\u00e0 pour les pr\u00e9cautions d\u2019usage... Pour\nvisualiser ici, j\u2019ai fait des regroupements, car avec 25 villes, \u00e7a ne\ntient pas dans mon billet (mais par la suite, l\u2019analyse sera plus\nfine). Les donn\u00e9es de base sont les suivantes, les lignes \u00e9tant\nl\u2019endroit o\u00f9 un doctorant a soutenu sa th\u00e8se (en \u00e9conomie ou droit), et\nen colonnes, l\u2019endroit o\u00f9 a \u00e9t\u00e9 recrut\u00e9 un ma\u00eetre de conf\u00e9rence,  Rennes Province (autre) R\u00e9gion parisienne total  Rennes 5 8 0 13 Province (autre) 3 124 23 150 R\u00e9gion parisienne 1 13 49 63  total 9 145 72 226 Classiquement, on peut retraduire ces chiffres (qui sont des comptages) sous la forme de probabilit\u00e9s,  Rennes Province (autre) R\u00e9gion parisienne total  Rennes 2,21% 3,54% 0,00% 5,75% Province (autre) 1,32% 54,86% 10,18% 66,37% R\u00e9gion parisienne 0.44% 5,75% 21,68% 27,88%  total 3,98% 64,15% 31,86% 100,00% Les probabilit\u00e9s qui apparaissent sur les totaux \nsont les probabilit\u00e9s marginales. Aussi, au bout de la premi\u00e8re ligne\non trouve que 5,75% des doctorants qui ont eu un poste en 2008 avaient\nsoutenu leur th\u00e8se \u00e0 Rennes (ou dans la r\u00e9gion, i.e. les universit\u00e9s de\nRennes 1, de Rennes 2 et de Brest). De mani\u00e8re similaire, en bas de la\npremi\u00e8re colonne, 3,98% des ma\u00eetres de conf\u00e9rences recrut\u00e9s en 2008 ont\n\u00e9t\u00e9 recrut\u00e9s \u00e0 Rennes. Deux autres tableaux classiques sont alors les profils ligne et profils colonnes, c\u2019est \u00e0 dire les probabilit\u00e9s conditionnelles,  Rennes Province (autre) R\u00e9gion parisienne total  Rennes 38,46% 61,54% 0,00% 100,00% Province (autre) 2,00% 82,67% 15,33% 100,00% R\u00e9gion parisienne 1,58% 20,64% 77,78% 100,00%  total 3,98% 64,15% 31,86% 100,00% par\nligne, autrement dit, pour un \u00e9tudiant qui a soutenu sa th\u00e8se \u00e0 Rennes\n(et qui a eu un poste de Ma\u00eetre de Conf\u00e9rences), il avait 38,46% de\nchance de rester \u00e0 Rennes (ou plut\u00f4t en Bretagne), et 61,54% de chance\nd\u2019aller dans une autre ville de province. C\u00f4t\u00e9 colonnes,  Rennes Province (autre) R\u00e9gion parisienne total  Rennes 55,56% 5,52% 0,00% 5,75% Province (autre) 33,33% 85,52% 31,94% 66,37% R\u00e9gion parisienne 11,11% 8,96% 68,06% 27,88%  total 100,00% 100,00% 100,00% 100,00% Ici,\non lit que pour un Ma\u00eetre de Conf\u00e9rences recrut\u00e9 sur la r\u00e9gion\nparisienne, il y avait 68,06% de chances pour qu\u2019il ait fait en r\u00e9gion\nparisienne. Bref, \u00e0 partir de ces tableaux, on peut essayer de\ntester l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance, et si cette derni\u00e8re n\u2019est pas\nv\u00e9rifi\u00e9e, on va essayer de comprendre pourquoi. Mais avant, rappelons que l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance se traduit par le fait que La\nvaleur de gauche est la probabilit\u00e9 jointe (que l\u2019on retrouve au coeur\ndu second tableau), alors que les deux autres sont les probabilit\u00e9s\nmarginales, qui apparaissent en bas, et \u00e0 droite (dans les totaux).\nBref, on pourrait construire le tableau des probabilit\u00e9s que l\u2019on\ndevrait avoir sous hypoth\u00e8se d\u2019ind\u00e9pendance ,  Rennes Province (autre) R\u00e9gion parisienne total  Rennes 0,22% 3,69% 1,83% 5,75% Province (autre) 2,64% 42,58% 21,14% 66,37% R\u00e9gion parisienne 1,11% 17,88% 8,88% 27,88%  total 3,98% 64,15% 31,86% 100,00% que l\u2019on peut retraduire en terme d\u2019effectif esp\u00e9r\u00e9, toujours sous l\u2019hypoth\u00e8se d\u2019ind\u00e9pendance. Pour\nceux qui connaissent les copules, on change juste la structure de\nd\u00e9pendance ici, pas les lois marginales (ou les effectifs marginaux)\nqui restent inchang\u00e9s.  Rennes Province (autre) R\u00e9gion parisienne total  Rennes 0,5 8,3 4,2 13 Province (autre) 6,0 96,2 47,8 150 R\u00e9gion parisienne 2,5 40,5 20 63  total 9 145 72 226 L\u2019outils de base pour comparer ces donn\u00e9es est la distance du chi-deux. La statistique du chi-deux est  ou encore Karl\nPearson avait \u00e9t\u00e9 un des premiers \u00e0 \u00e9tudier ces grandeurs, sous des\nhypoth\u00e8ses de normalit\u00e9 (des sommes de lois normales ind\u00e9pendantes au\ncarr\u00e9 se ramenant \u00e0 des lois du chi-deux). On notera qu\u2019une approche alternative est de supposer que les nombres (ou les effectifs) suivent une loi de Poisson . On peut alors se demander si Si on suppose avoir une loi de Poisson, on peut s\u2019int\u00e9resser aux r\u00e9sidus de Pearson , autrement\ndit, on retrouve des grandeurs tr\u00e8s proches de la contribution au\nchi-deux (en prenant le carr\u00e9). On aussi le signe qui nous indique s\u2019il\ny a trop peu de monde, ou au contraire par assez.... Si je travaille sur la base totale, soit environ 25 \" villes \", on obtient \n > cont1=read.table(\"http://perso.univ-rennes1.fr/arthur.charpentier/contingence-universite_1.csv\", + header=TRUE,sep=\";\") > cont2=cont1 > for(i in 1:ncol(cont2)){ + for(j in 1:nrow(cont2)){ + if(is.na(cont2[j,i]==TRUE)){cont2[j,i]=0} } } > X=as.matrix(cont2[1:nrow(cont2),2:ncol(cont2)]) > rownames(X)=as.character(cont2[,1]) > colnames(X)=names(cont2[2:ncol(cont2)]) > X=X[,-2] > X=X[-2,] > X=X[,-9] > X=X[-9,] > Z=X[-nrow(X),] > Z=Z[,-ncol(Z)] > Z=Z[-nrow(Z),] > chisq.test(Z)   Pearson\u2019s Chi-squared test data: Z X-squared = 1342.546, df = 576, p-value < 2.2e-16 La\nbase est assez sale, j\u2019ai fait un copier/coller rapide du fichier pdf\n(et je vire les Antilles et la R\u00e9union car apr\u00e8s je voudrais faire un\npeu de visualisation sur une carte). Autrement dit, l\u2019hypoth\u00e8se\nd\u2019ind\u00e9pendance ne semble pas valid\u00e9e par le test du chi-deux. Essayons\nmaintenant de comprendre un peu mieux les interactions dans notre\ncarr\u00e9... Si on regarde les probl\u00e8mes de recrutement local, il faut regarder ce qui se passe sur la diagonale de la matrice des r\u00e9sidus, > CT=chisq.test(Z) Message d\u2019avis : In chisq.test(Z) : l\u2019approximation du Chi-2 est peut-\u00eatre incorrecte > Dia=diag(CT$residuals) > names(Dia)=rownames(Z) > Dia AIX-MARSEILLE  BESANCON  BORDEAUX   CAEN  CLERMONT   6.43901593  3.26116568  8.37261202  10.53607373  5.12692873    CORSE   DIJON  GRENOBLE   LILLE   LIMOGES   8.56426340  4.10931024  9.18869705  7.59456681  8.56426340    LYON  MONTPELLIER   NANCY   NANTES   NICE   3.16119625  4.18366071  8.84765951  2.67087742  7.86973591   ORLEANS  POITIERS   REIMS   RENNES   ROUEN   1.79480842  5.81144228  -0.06651901  6.22963060  6.57435350  STRASBOURG  TOULOUSE   PARIS   CRETEIL  VERSAILLES   5.64850464  4.12289099  5.46092001  -0.22061860  4.01351350  Visuellement, sur une carte de France, on obtient la distribution ci-dessous, pour les postes en \u00e9conomie et droit, \net ci-dessous pour les postes en sciences et pharmacie, \nSi\non regarde le tableau des r\u00e9sidus on obtient les valeurs suivantes sur\nla ligne correspondant \u00e0 Rennes, i.e. les endroits o\u00f9 vont les\n\u00e9tudiants ayant soutenu leur th\u00e8se \u00e0 Rennes, \navec en bleu les valeurs positives, et en rouge \nles valeurs n\u00e9gatives. A droite sont repr\u00e9sent\u00e9es les lignes, i.e. les\nendroits d\u2019o\u00f9 viennent les Ma\u00eetres de Conf\u00e9rences recrut\u00e9s \u00e0 Rennes. Au\nlieu de regarder ville par ville, on peut aussi faire une petite\nanimation, soit sur les endroits o\u00f9 sont partis les doctorants, \nou les endroits d\u2019o\u00f9 viennent les Ma\u00eetres de Conf\u00e9rence recrut\u00e9s, \n On peut \u00e9galement s\u2019il existe des interactions, ou des \u00e9changes d\u2019une ville \u00e0 l\u2019autre. Dans ce cas, les r\u00e9sidus crois\u00e9es et doivent \u00eatre positifs. \n Bref, au del\u00e0 du \"recrutement local\" on voit \u00e9galement appara\u00eetre de fortes interactions r\u00e9gionales (Rennes-Nantes, Toulouse-Bordeaux, Orl\u00e9ans-Paris). \n Avant de conclure, il existe une autre technique permettant de mieux visualiser les interactions, c\u2019est l\u2019 analyse factorielle des correspondances \n(je renvois ici pour des slides). Si on regarde les 2 premiers axes\nprincipaux, en sciences (\u00e0 gauche) et en droit et \u00e9conomie (\u00e0 droite),\non obtient les projections suivantes des modalit\u00e9s, avec en bleu les villes o\u00f9 les \u00e9tudiants ont soutenu leur th\u00e8se, en rouge les villes o\u00f9 les postes de Ma\u00eetre de Conf\u00e9rences ont \u00e9t\u00e9 obtenus, \n \net sur les les 2 axes suivants \nBref,\ncomme souvent en statistique, afin de mieux comprendre un ajustement de\nmod\u00e8le (ou comme ici lors la r\u00e9alisation d\u2019un test d\u2019hypoth\u00e8se) la\nvisualisation des r\u00e9sidus est la technique de base (c\u2019\u00e9tait l\u2019id\u00e9e\nd\u2019ailleurs du commentaire que j\u2019avais fait l\u00e0 ). Voil\u00e0 pour la premi\u00e8re partie, sur les mod\u00e8les o\u00f9 l\u2019on suppose l\u2019ind\u00e9pendance. La prochaine fois, on verra d\u2019autres formes d\u2019hypoth\u00e8ses."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/07/Comprendre-les-tableaux-crois%C3%A9s%2C-1", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 3, "http://media.gouv.fr/": 1, "http://perso.univ-rennes1.fr/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Pour r\u00e9pondre \u00e0 une question sur mon pr\u00e9c\u00e9dant billet ( ici ),\nje vais revenir sur un paradoxe assez classique, le paradoxe de Saint\nPetersbourg. Mais avant de parler du paradoxe, et de ses implications\nen th\u00e9orie de la d\u00e9cision dans l\u2019incertain, je voulais pr\u00e9senter le\njeu, et en profiter pour jouer \u00e0 faire des dessins puisque j\u2019ai cru\nremarquer que j\u2019avais des amateurs d\u2019explications g\u00e9om\u00e9triques. Le jeu est simple, c\u2019est un jeu de pile ou face r\u00e9p\u00e9t\u00e9, le jeu s\u2019arr\u00eatant \u00e0 la sortie du premier \" face \" (et on le verra par la suite, l\u2019id\u00e9e est de doubler ses gains chaque fois que \" pile \" sort). Aussi, le temps d\u2019arr\u00eat du jeu est dont la loi est simplement \n   ... etc. On\nretrouve ainsi une loi g\u00e9om\u00e9trique de param\u00e8tre 1/2. L\u2019esp\u00e9rance du\nnombre de lancers que l\u2019on ferra avant que le jeu ne s\u2019arr\u00eate est fini\n(alors qu\u2019on le verra le gain esp\u00e9r\u00e9 est infini), et vaut l\u2019inverse de\ncette probabilit\u00e9, i.e. 2. Pour ceux qui ont oubli\u00e9 la formule (que\nl\u2019on obtient de mani\u00e8re assez calculatoire, en faisant une d\u00e9rivation\ndans une s\u00e9rie), on peut refaire le calcul,  Personnellement, je ne sais pas calculer cette somme (infinie), mais comme dans mon pr\u00e9c\u00e9dant billet ( ici ), il existe une somme que l\u2019on peut calculer simplement en faisant un petit dessin, \nJ\u2019ai 1 carr\u00e9 ( mauve ) de surface 1, 2 ( bleus ) de surface 1/2, puis 3 ( verts ) de surface 1/4, puis 4 ( jaunes ) de surface 1/8, puis 5 ( rouges ) de surface 1/16, 6 ( oranges ) de surface 1/32, etc. Autrement dit Or\nl\u2019esp\u00e9rance que l\u2019on cherche \u00e0 calculer, c\u2019est cette somme, \u00e0 un\nfacteur 2 pr\u00e8s.... Aussi, en moyenne, on peut esp\u00e9rer faire 2 lancers\n(la moiti\u00e9 de la surface)\u2009! Ce qui correspond pr\u00e9cis\u00e9ment \u00e0 l\u2019esp\u00e9rance\nd\u2019une loi g\u00e9om\u00e9trique de param\u00e8tre 1/2. D\u00e9cid\u00e9ment, on peut faire plein\nde choses avec des petits dessins.."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/04/Le-paradoxe-de-Saint-P%C3%A9terbourg%2C-partie-1", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Ayant parl\u00e9 plusieurs fois de Luc Chatel sur mon blog je n\u2019ai pas pu m\u2019enp\u00eacher de mettre l\u2019image qui est \u00e0 la une du Canard de cette semaine. Pour\nceux qui se souviennent de leurs cours de philo au lyc\u00e9e, c\u2019est le\nparadoxe de Z\u00e9non (ou presque). Pour cela il faut fermer les yeux et\nimaginer une fl\u00e8che en vol (en fait non, je viens de faire\nl\u2019exp\u00e9rience, si on ferme les yeux, on ne peut pas lire la suite, \u00e7a ne\nmarche qu\u2019en cours cette histoire). La fl\u00e8che est lanc\u00e9e \u00e0 10 m\u00e8tre de\nsa cible. Il faut un certain temps, non nul, \u00e0 cette pierre pour\nparcourir la moiti\u00e9 de la distance qui la s\u00e9pare de la cible. Ensuite,\nil lui reste encore 5 m\u00e8tres \u00e0 parcourir,\ndont elle accomplit d\u2019abord la moiti\u00e9, 2,5 m\u00e8tres, ce qui lui prend un\ncertain temps. Puis elle parcourt la moiti\u00e9, puis la moiti\u00e9, et cela -\n\u00e0 chaque fois - avec un temps non nul. Z\u00e9non (\u0396\u03ae\u03bd\u03c9\u03bd, parfois appel\u00e9 aussi Z\u00e9non d\u2019El\u00e9e) en conclut que la pierre\nne pourra frapper l\u2019arbre qu\u2019au bout d\u2019un temps infini, c\u2019est-\u00e0-dire\njamais (ou pour la version illustr\u00e9e, je peux renvoyer \u00e0 un extrait de Kid Paddle, ici ). En fait, Z\u00e9non introduit la notion d\u2019 infini divisibilit\u00e9 \n(on dira qu\u2019un \u00e9l\u00e9ment est ind\u00e9finiment divisible, c\u2019est d\u2019ailleurs\nsurprenant ce lien en fran\u00e7ais entre l\u2019infini et l\u2019ind\u00e9fini), qui\nphilosophiquement, est un complexe qui a mis du temps \u00e0 s\u2019imposer: une\nsomme infinie d\u2019\u00e9l\u00e9ments non nuls peut \u00eatre finie (je renvois ici par exemple). \n Formellement, l\u2019histoire de la fl\u00e8che revient \u00e0 se demander si on peut calculer quelquechose qui ressemble \u00e0  soit, de mani\u00e8re plus formelle,  on retrouve la somme d\u2019une s\u00e9rie g\u00e9om\u00e9trique (au premier terme pr\u00e8s), dont on sait que Un application int\u00e9ressante est que la loi g\u00e9om\u00e9trique , v\u00e9rifiant  est effectivement une loi de probabilit\u00e9, (les\ntermes sont positifs, et la somme vaut 1). Il existe une d\u00e9monstration\n\u00e9l\u00e9gante de ce derni\u00e8re r\u00e9sultat, qui tient en un dessin (\u00e0 condition\nd\u2019accepter ce principe un peu fractal d\u2019infini divisibilit\u00e9): \nJe\nreviendrais un jour sur les lois de probas infiniment divisibles, car\nelles sont tr\u00e8s tr\u00e8s int\u00e9ressantes de par certaines propri\u00e9t\u00e9s...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/04/Z%C3%A9non%2C-sommes-infinies-et-loi-g%C3%A9om%C3%A9trique", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 1, "http://www.ac-nice.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Comme je l\u2019avais mentionn\u00e9 \u00e0 propos d\u2019un vieux papier paru dans Le Canard ,\nc\u2019est souvent compliqu\u00e9 de communiquer sur les retraites. C\u2019est un\nsujet complexe et assez rapidement technique. Historiquement, les\nabaques sont les tables de calculs, \n \nMais il faut admettre que retrouve un peu n\u2019importe quoi dans ce terme d\u2019abaque.... Certaines sont \u00e9voqu\u00e9es ici ou l\u00e0 , voire l\u00e0 \npour quelque chose de tr\u00e8s joli. On notera qu\u2019il existe une id\u00e9e sous\njacente commune, \u00e0 savoir construire un outil graphique permettant de\nvisualiser comment 3 param\u00e8tres peuvent \u00eatre li\u00e9s (g\u00e9n\u00e9rallement, ils\nsont li\u00e9s par une fonction complexe). Abaques et retraites \n \nParmi les outils que l\u2019on retrouve dans les documents du COR, j\u2019ai \u00e9t\u00e9 surpris de voir ressortir l\u2019utilisation des abaques (par exemple ici ). On nous en fait m\u00eame la promotion en vid\u00e9o, \n\n\n\n Pour la petite histoire, j\u2019avais \u00e9voqu\u00e9 son utilisation dans un graphique ici sur les probl\u00e8mes d\u2019\u00e9chantillonnage. C\u2019est effectivement un outil graphique int\u00e9ressant, \u00e0 condition de savoir le d\u00e9coder... L\u2019id\u00e9e est simple, \u00e0 savoir lier les trois param\u00e8tres essentiels (ou suppos\u00e9s comme tels) du bilan d\u2019un syst\u00e8me de retraite, \u00e0 savoir le niveau des pr\u00e9l\u00e8vements, \n le niveau de l\u2019\u00e2ge effectif de d\u00e9part en retraite \n le niveau des pensions. \n \n La\ndifficult\u00e9 est de trouver une forme graphique permettant de relier ces\ntrois niveaux (la dimension 3 \u00e9tant souvent trop abstraite). Par exemple dans les sondages, pour construire un intervalle de confiance, les abaques sont utilis\u00e9es (comme ici ) pour relier le niveau des intervalles de confiance (5%-95% ou 10%-90%) \n le niveau de probabilit\u00e9 attendu dans le sondage \n la taille de l\u2019\u00e9chantillon interrog\u00e9 \n \nOn peut d\u2019ailleurs trouver d\u2019autres exemple en statistiques ici ou l\u00e0 . Mais pour revenir au probl\u00e8me des retraite, sur le graphique ci-dessous (appendix 1 ici ), en abscisse on met le taux de remplacement (c\u2019est \u00e0 dire le ratio de la pension moyenne par rapport au revenu moyen) et en ordonn\u00e9e\non met le niveau d\u2019un hausse du taux de cotisation (en points de cotisation). chaque ligne repr\u00e9sente un \u00e2ge moyen de d\u00e9part en retraite effectif.  \n On\nse fixe alors des sc\u00e9narios d\u2019\u00e9quilibre et on regarde la mani\u00e8re dont\nse d\u00e9placent les \u00e9quilibres. J\u2019avais pr\u00e9vu toute une analyse technique,\nmais je viens de me rendre compte qu\u2019Antoine a mis en ligne un billet\nremarquable sur le sujet sur http://www.ecopublix.eu/ ( ici ),\nque je ne pourrais jamais \u00e9galer en claret\u00e9. Donc je vais proposer une\nautre construction d\u2019abaque afin de montrer son int\u00e9r\u00eat p\u00e9dagogique.  Comprendre l\u2019actualisation \n \nSur le site http://images.math.cnrs.fr/ ( ici ),\nXavier Caruso explique son \u00e9merveillement devant la difficult\u00e9 de faire\ndes calculs d\u2019actualisation. En un sens, \u00e7a me rassure de voir que les\ncalculs actuariels ne paraissent pas triviaux \u00e0 tout le monde (et\nsurtout plein de contre-intuitions). Xavier essaye d\u2019expliquer la\ndiff\u00e9rence entre \n ouvrir un compte r\u00e9mun\u00e9r\u00e9 \u00e0 4,5% sur lequel on d\u00e9pose 500 euros chaque mois pendant 10 ans \n ouvrir un compte r\u00e9mun\u00e9r\u00e9\n\u00e0 4,5% sur lequel on d\u00e9pose 60000 euros initialement sans plus jamais\nl\u2019alimenter ensuite. \n \nSi\neffectivement la somme des 120 versements de 500 euros correspond aux\n60000 euros, la valeur int\u00e9grant les int\u00e9r\u00eats n\u2019est pas du tout la m\u00eame\nau bout de 10 ans, car dans le premier car, comme le disent les\nbanquiers, \"l es int\u00e9r\u00eats se capitalisent \".\nBon, si Xavier fait des maths (et donc des d\u00e9veloppements limit\u00e9s par\navoir des formules ferm\u00e9s), on va plut\u00f4t faire ici un peu de calculs\nnum\u00e9riques. Tout d\u2019abord, pour visualiser l\u2019\u00e9volution des montants\ndisponibles sur les comptes, rien de plus simple, \n > tps=1/12*(1:120) > plot(tps,60000*(1+0.045)^tps,ylim=c(0,100000)) > lines(tps,cumsum(500*(1+0.045)^(tps))) les montants au bout de 10 ans \u00e9tant \n > sum(500*(1+0.08525)^(1/12*(1:120))) [1] 93180.33 > sum(500*(1+0.045)^(1/12*(1:120))) [1] 75514.32 Ce\nqui confirme l\u2019intuition que nous avions. Comme le note Xavier, il\nfaudrait un taux d\u2019int\u00e9r\u00eat deux fois plus grand pour le placement\nmensuel, la preuve \n > sum(500*(1+0.09)^(1/12*(1:120))) [1] 95543 > sum(500*(1+0.08525)^(1/12*(1:120))) [1] 93180.33 Xavier obtient cette valeur en effectuant des d\u00e9veloppements limit\u00e9s. Il r\u00e9sume \u00e7a \u00e0 l\u2019aide du dessin \n On\npeut aller plus loin, en notant qu\u2019\u00e0 taux sur le placement mensuel\ndonn\u00e9, ainsi que la maturit\u00e9, le taux sur le placement bloqu\u00e9 qui\nrapporte la m\u00eame valeur \u00e0 \u00e9ch\u00e9ance est donn\u00e9 par \n > ((sum(500*(1+0.09)^(1/12*(1:120))))/(500*12*10))^(1/10)-1 [1] 0.04762238 (dans\nle cas d\u2019un placement rapportant 9%). En fait, si on regarde le montant\ndont on dispose sur les comptes \u00e0 chaque date, on obtient pr\u00e9cis\u00e9ment\nles valeurs suivantes, en fonction du temps, \non\nretrouve effectivement qu\u2019\u00e0 4,5%, les deux placements ne donnent pas le\nm\u00eame montant \u00e0 \u00e9ch\u00e9ance (ici 10 ans). Alors que si le placement\naliment\u00e9 tous les mois rapportait 8,525%, on aurait exactement la m\u00eame\nsomme au bout de 10 ans \nou si on compare avec un placement \u00e0 9% (le double, ce qui est obtenu par d\u00e9veloppement limit\u00e9), on obtient \nPour trouver la valeur exacte du multiplicateur en fonction du taux offert sur le compte bloqu\u00e9 et la maturit\u00e9, la fonction est tout simplement \n > taux=4.5/100 > T=10 > f=function(k){(1*12*T)*(1+taux)^T-sum(1*(1+k*taux)^(1/12*(1:(12*T)))) } > uniroot(f,interval=c(0,20))$root [1] 1.894332 L\u2019id\u00e9e d\u2019utiliser une abaque peut s\u2019av\u00e9rer int\u00e9ressante, car on essaye ici de relier le taux de rendement de l\u2019argent bloqu\u00e9 \n le taux propos\u00e9 pour le placement aliment\u00e9 tous les mois \n la dur\u00e9e envisag\u00e9e du placement \n \nJe\npeux mettre en abscisse le taux du placement bloqu\u00e9. En ordonn\u00e9, je met\nle multiplicateur utilis\u00e9 pour le placement aliment\u00e9 tous les mois. A\nmaturit\u00e9 donn\u00e9 (par exemple 10 ans en rouge), on peut regarder la\nvaleur du multiplicateur pour que les valeurs \u00e0 \u00e9ch\u00e9ance co\u00efncident,  \nOn\nretrouve directement que le taux \u00e9quivalent pour un placement aliment\u00e9\ntous les mois doit \u00eatre 1,89 fois le taux offert sur le placement\nbloqu\u00e9 si on se fixe une \u00e9ch\u00e9ance de 10 ans pour \u00e9galiser les\nplacements. Le code est simplement le suivant \n > tx=seq(0.005,.05,by=.001) > tps=seq(5,25,by=.25) > M=matrix(NA,length(tps),length(tx)) > for(i in 1:length(tps)){ + for(j in 1:length(tx)){ + T=tps[i] + taux=tx[j] + f=function(k){ + (1*12*T)*(1+taux)^T-sum(1*(1+k*taux)^(1/12*(1:(12*T)))) + } + M[i,j]=uniroot(f,interval=c(0,2))$root + }} > contour(tps,tx,M,lwd=2, + xlab=\"Dur\u00e9e du placement (en ann\u00e9es)\",ylab=\"Taux (du placement bloqu\u00e9)\") > abline(v=seq(5,25,by=1),lty=2,col=\"grey\") > abline(h=seq(0,.05,by=.005),lty=2,col=\"grey\") Bref,\nl\u2019actualisation est un sujet d\u00e9licat (m\u00eame ce n\u2019est que des calculs de\nsommes et de puissances). Et faire des petits dessins permet souvent de\nmieux comprendre..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/04/Les-abaques-comme-outils-de-p%C3%A9dagogie-statistique", "bloglinks": {}, "links": {"http://www.ecopublix.eu/": 1, "http://www.campingcar-bricoloisirs.net/": 1, "http://blogperso.univ-rennes1.fr/": 3, "http://www.cor-retraites.fr/": 2, "http://www.iufm.fr/": 1, "http://fr.wikipedia.org/": 1, "http://images.cnrs.fr/": 1, "http://archive.numdam.org/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Compte tenu du nombre de billets qui agitent la blogosph\u00e8re en ce\nmoment, je me suis senti oblig\u00e9 de reprendre la plume pour poursuivre\nle billet pr\u00e9c\u00e9dant ( ici ).\nInitialement, le but \u00e9tait de montrer aux \u00e9l\u00e8ves comment mener une\n\u00e9tude \u00e9conom\u00e9trique et de r\u00e9pondre \u00e0 une question simple (en\nl\u2019occurence \" la taille d\u2019un classe a-t-elle un impact positif ou n\u00e9gatif sur les r\u00e9sultats scolaires\u2009? \").\nMalheureusement, j\u2019ai \u00e9t\u00e9 rattrap\u00e9 par l\u2019actualit\u00e9. Le premier billet\nmontrait qu\u2019en faisant un mod\u00e8le de r\u00e9gression simple, l\u2019effet semblait\npositif: plus la taille est grande, meilleure est la moyenne. Dans le\nsecond billet, nous avions not\u00e9 que cet effet pouvait cacher\nquelquechose, comme des conditions socio\u00e9conomiques. Essayons donc\nd\u2019aller un peu plus loin.... \n R\u00e9gression multiple \n \nEn faisant une estimation par moindres carr\u00e9s sur les deux variables, on obtient \n > summary(lm(avgverb~tipuach+classize,data=base0)) \nCoefficients: \n   Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 80.26363 0.74472 107.777 <2e-16 *** \ntipuach  -0.34994 0.01077 -32.486 <2e-16 *** \nclassize -0.03146 0.02221 -1.416 0.157  \n--- \nSignif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1  \nL\u2019impact de l\u2019indice socio-\u00e9conomique est clairement\nn\u00e9gatif, et si l\u2019effet taille a un impact n\u00e9gatif, ce\ndernier n\u2019est pas significatif. Il va falloir essayer d\u2019aller un peu plus loin.... \n Approche nonlin\u00e9aire \n \nOn peut ragarder rapidement ce qu\u2019aurait donn\u00e9 une r\u00e9gression nonlin\u00e9aire \n > library(mgcv) > reg=gam( avgverb ~s(tipuach,classize),data=base0)  > reg=gam(avgmath~s(tipuach,classize),data=base0) soit visuellement, les pr\u00e9dictions suivantes pour l\u2019\u00e9preuve de lecture \net la surface ci-dessous pour l\u2019\u00e9preuve de maths, \nBref,\nl\u2019effet ne parait pas forc\u00e9ment simple, m\u00eame si l\u2019on intuite une\n\u00e9volution croissante. Mais depuis le d\u00e9but, nous omettons un point\nimportant sur la base de Joshua Angrist et Victor Lavy: dans ces \u00e9coles\nisra\u00ebliennes, la r\u00e8gle de Maimonides est supposer s\u2019appliquer... et\njusqu\u2019\u00e0 pr\u00e9sent je n\u2019en ai jamais parl\u00e9\u2009! \n Prise en compte de la r\u00e8gle de Maimonides \n \nCompte\ntenu de la r\u00e8gle de Maimonides, la taille des classes en fonction de la\ntaille de l\u2019\u00e9cole n\u2019est pas du tout lin\u00e9aire, comme le montre la figure\nci-dessous,  En fait, si on d\u00e9fini le nombre th\u00e9orique d\u2019\u00e9l\u00e8ves dans les classes, avec cette m\u00e9thode (ce que j\u2019avais d\u00e9taill\u00e9 dans la section 3 ici ), on obtient des r\u00e9sultats qui pourrait ressembler\u00e0 ce que nous avions observ\u00e9 sur le nombre r\u00e9el d\u2019\u00e9l\u00e8ves dans les classes, \n > reg=lm(avgverb~func1,data=base0) > summary(reg) Coefficients:    Estimate Std. Error t value Pr(>|t|)  (Intercept) 70.62076 0.88004 80.25 < 2e-16 *** func1  0.12159 0.02789 4.36 1.37e-05 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1  On retrouve un effet positif (significatif) qui se visualise sur le graphique ci-dessous, \net surtout, le mod\u00e8le devient particuli\u00e8rement int\u00e9ressant si on r\u00e9gresse sur cette effectif th\u00e9orique et l e \" percent disadantaged \" (PD, d\u00e9crit dans le pr\u00e9c\u00e9dant billet, l\u00e0 ) \n > reg=lm(avgverb~func1+tipuach,data=base0) > summary(reg) Coefficients:    Estimate Std. Error t value Pr(>|t|)  (Intercept) 82.91057 0.78669 105.392 < 2e-16 *** func1  -0.11160 0.02321 -4.809 1.63e-06 *** tipuach  -0.35943 0.01050 -34.229 < 2e-16 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1  o\u00f9\ncette fois l\u2019impact de la taille de la classe est clairement\nsignificatif, et n\u00e9gatif: \u00e0 charact\u00e9ristiques socio-\u00e9conomique\nidentiques, si la taille des classes suivait la r\u00e8gle de Maimonides, la\ntaille de la classe aurait un impact n\u00e9gatif sur les r\u00e9sultats\nscolaires.... Une solution est d\u2019utiliser une r\u00e9gression par\nvariables instrumentales o\u00f9 l\u2019instrument permet\npr\u00e9cis\u00e9ment de prendre en compte cette\ndiscontinuit\u00e9. L\u2019id\u00e9e est ici qu\u2019il existe une variable telle que On\nvoit alors appara\u00eetre naturellement l\u2019id\u00e9e d\u2019utiliser les variables\ninstrumentales pour corriger d\u2019un \u00e9ventuel biais lors de l\u2019estimation\ndes coefficients lors de la r\u00e9gression (comme cela est d\u00e9velopp\u00e9 dans\nle chapitre 25.7 du microeconometrics de\nColin Cameron et Pravin Trivedi). Et c\u2019est pr\u00e9cis\u00e9ment ce qui survient\ndans nos \u00e9coles compte tenu de la r\u00e8gle de Maimonides, On utilise la\nvariable que nous avions d\u00e9finie comme le nombre th\u00e9orique d\u2019\u00e9l\u00e8ves dans la classe\u2009! Damned,\nl\u2019\u00e9tude n\u2019est donc toujours pas finie, il va falloir creuser encore\ndavantage (surtout qu\u2019il reste un paquet de variables dans cette\nbase)... mais pour les impatients, la conclusion \u00e0 laquelle arrivent\nJoshua Angrist et Victor Lavy est r\u00e9sum\u00e9 dans le tableau ci-dessous,"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/03/Qu-est-ce-que-la-mod%C3%A9lisation-%C3%A9conom%C3%A9trique-%283%29", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 3}, "blogtitle": "Arthur Charpentier"}, {"content": ["Dans un billet qui a presque un an ( ici ), j\u2019\u00e9tais revenu sur l\u2019id\u00e9e re\u00e7ue comme quoi \" tous les ans, on gagne un trimestre d\u2019esp\u00e9rance de vie \"\n. J\u2019avais du alors expliquer que c\u2019\u00e9tait effectivement le cas, en tous\nles cas pour l\u2019esp\u00e9rance de vie \u00e0 la naissance. C\u2019est ce que raconte le\ngraphique ci-dessous, avec une projection par la m\u00e9thode de Lee &\nCarter, pour les femmes uniquement \n \n > library(demography) > france.LC1 <- lca(fr.mort,adjust=\"e0\",series=\"female\",years=c(1900,2040)) > france.fcast <- forecast(france.LC1) > L2 <- lifetable(france.fcast) > ex2=L2$ex > L1=lifetable(fr.mort,series=\"female\") > ex1=L1$ex > age=0 > exF=c(ex1[age+1,],ex2[age+1,]) > plot(1816:2056,ex,col=\"blue\") On peut alors faire une r\u00e9gression pour quantifier un peu mieux ce qui s\u2019est pass\u00e9 au cours des 50 derni\u00e8res ann\u00e9es. > I=(1950:2000)-1815 > y=exF[I] > x=1950:2000 > lm(y~x) Coefficients: (Intercept)   x  -438.7249  0.2611  Avec\nun pente de 0.26, effectivement, pour une ann\u00e9e de plus (entre 1980 et\n1981 par exemple), l\u2019esp\u00e9rance de vie \u00e0 la naissance gagne 0,26 ann\u00e9e,\nsoit un trimestre. Pour les hommes, on a > I=(1950:2000)-1815 > y=exH[I] > x=1950:2000 > abline(lm(y~x),col=\"red\") > points(x,y,pch=19,col=\"red\") > points(x,y,pch=19,col=\"red\") > lm(y~x) Coefficients: (Intercept)   x  -357.9901  0.2164  que l\u2019on pourrait trouver tr\u00e8s proche. \nJ\u2019ai \u00e9t\u00e9 un peu \nsurpris de voir cet argument du trimestre gagn\u00e9 avanc\u00e9 lors du d\u00e9bat\nsur les retraites. Pour commencer, j\u2019ai \u00e9t\u00e9 surpris de voir Xavier\nBertrand pr\u00e9tendre que \" Quand vous preniez votre retraite \u00e0 60 ans en 1982, vous aviez dix\nans d\u2019esp\u00e9rance de vie, aujourd\u2019hui vous avez vingt ans d\u2019esp\u00e9rance de\nvie. Cette formidable bonne nouvelle, il faut la financer \". V\u00e9rifions puisqu\u2019on a les codes, il suffit de refaire tourner le programme avec > age=60 Le graphique est alors  \npour les femmes, et pour les hommes, on a \nC\u00f4t\u00e9 chiffres, pour les femmes, > exF[\"1982\"]  1982 22.69534 et pour les hommes, > exH[\"1982\"]  1982 17.65394 Bref, j\u2019ai du mal \u00e0 trouver d\u2019o\u00f9 sortent ces dix ans. Quant \u00e0 aujourd\u2019hui, pour les femmes, > exF[\"2010\"]  2010 26.44607 et pour les hommes, > exH[\"2010\"]  2010 21.11194 autrement dit on a d\u00e9pass\u00e9 les 20 ans. Si on calcule la pente de la r\u00e9gression, on obtient pour les femmes, > lm(y~x) Coefficients: (Intercept)   x  -281.0926  0.1533  et pour les hommes, > lm(y~x) Coefficients: (Intercept)   x  -192.7518  0.1063  Autrement\ndit le gain qui est de l\u2019ordre du trimestre par an \u00e0 la naissance est\nplut\u00f4t de l\u2019ordre de 2 mois pour les femmes et de 5 semaines pour les\nhommes. En gros, le gain est divis\u00e9 par deux entre la naissance et 60\nans. Le graphique ci-dessous montre le gain annuel d\u2019esp\u00e9rance de vie\nen fonction de l\u2019\u00e2ge (je suis pass\u00e9 en jours en ordonn\u00e9es), avec les\nfemmes (en rouge ) et les hommes (en bleu ) \nBref,\nsi le gain a \u00e9t\u00e9 tr\u00e8s important \u00e0 la naissance (la surmortalit\u00e9\ninfantile ayant fortement chut\u00e9 depuis les ann\u00e9es 50), il n\u2019est pas\naussi important par la suite, et chute m\u00eame pass\u00e9 55 ans\u2009! Autrement\ndit un b\u00e9b\u00e9 vivra - en moyenne - plus longtemps qu\u2019un b\u00e9b\u00e9 n\u00e9 en 1950.\nMais une personne de 70 ans vivra \u00e0 peine plus longtemps - en moyenne -\nqu\u2019une personne de 70 ans en 1950. Bref, parler de ce gain d\u2019un\ntrimestre par an dans un d\u00e9bat sur les retraites (comme cela est fait ici par une d\u00e9put\u00e9e de Meurthe-et-Moselle) n\u2019a pas de sens."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/03/Esp%C3%A9rance-de-vie-et-retraite", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 1, "http://www.lefigaro.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Continuons le billet commenc\u00e9 ici , o\u00f9 j\u2019essayais not\u00e9 que les enseignes qui se sont engag\u00e9 \u00e0 faire des cadeaux en cas de victoire de l\u2019\u00e9quipe de France s\u2019\u00e9taient couvertes via des contrats d\u2019assurance. En fait, elles peuvent \u00e9galement se couvrir sur les march\u00e9s (en l\u2019occurrence les sites de paris en ligne). Il existe plusieurs fa\u00e7ons de faire des paris. La\npremi\u00e8re est de faire (ou d\u2019acheter) un pari sur un\n\u00e9v\u00e9nement (la France ira en finale de la coupe du monde, par exemple).\nCe pari est assorti d\u2019une cote que d\u00e9fini le bookmaker. A une\ncote de 20 contre 1, si je paris 1 euro, je gagne 20 euros en cas de\nqualification pour la finale. \nDans ce cas particulier, le bookmaker est un agent important dans le march\u00e9, car il fixe le prix. Le site oddschecker.com ( ici ) permet de voir les cotes offertes par diff\u00e9rents bookmakers. \n \nMais il existe aussi des march\u00e9s de paris, sans bookmakers. Il\ns\u2019agit r\u00e9ellement d\u2019un march\u00e9, o\u00f9 les parieurs\nprennent des positions les uns contre les autres, le bookmaker ne\nservant que d\u2019interm\u00e9diaire et d\u2019animateur sur ce march\u00e9.\nC\u2019est le cas sur betfair.com ( l\u00e0 ) qui est un site d\u2019\u00e9change de paris. \n \nPour commencer simplement, imaginons que nous sommes quelques jours\navant une finale opposant la Cor\u00e9e du Sud aux Etats-Unis. Comme\nil ne peut y avoir qu\u2019un vainqueur il existe des contrats sym\u00e9trique,\no\u00f9 l\u2019on peut gagner 20 euros si la Cor\u00e9e gagner (et rien\nsi elle perd) et un autre o\u00f9 l\u2019on peut gagner 20 euros si la\nCor\u00e9e perd. Bref, soit je suis acheter sur la Cor\u00e9e, soit\nje suis vendeur. Le prix de ces contrats indiquent la\nprobabilit\u00e9 qu\u2019a la Cor\u00e9e de gagner. On parle aussi de\nback or lay, deux \u00e9v\u00e8nements \u00e9tant possibles. \n Lecture d\u2019une cote \n \nRegardons maintenant le match d\u2019ouverture, opposant l\u2019Afrique du Sud au\nMexique. Les cotes sont les suivantes, \" Mexique 6/4, Afrique du Sud\n15/8 et Draw (match nul) 11/5 \" sur un des sites. Le premier nombre\nindique ce que l\u2019on gagne (net) en misant le second, si mon pari est\ngagnant. Autrement dit, si je paris 5 euros sur un match nul, et que\nmatch nul il y a, alors mon b\u00e9n\u00e9fice sera de 11 (que je\ntouche en plus de ma mise). La cote est alors de 1+11/5, soit 16/5\n(j\u2019avais fait ici un billet sur le lien entre cote et\nprobabilit\u00e9). Elle indique non plus le b\u00e9n\u00e9fice,\nmais le gain: en misant 5 euros, j\u2019empoche 16 euros. On parlera de\nparis 1X2. \n \nPour aller plus loin, on peut aussi parier sur le nombre de buts marqu\u00e9s. On parlera de paris over/under. \n \nChez le m\u00eame bookmaker, si je paris 1 euro que la France gagne la\ncoupe du monde, je fais un b\u00e9n\u00e9fice de 18 euros. \n Cote ou probabilit\u00e9\u2009? \n \nLorsque j\u2019avais essay\u00e9 d\u2019expliquer l\u2019origine de la loi\nlogistique ( ici ), j\u2019avais expliqu\u00e9 qu\u2019il existait un lien simple\nentre probabilit\u00e9 et cote, la cote \u00e9tant la\nprobabilit\u00e9 que l\u2019\u00e9v\u00e8nement se r\u00e9alise,\ndivis\u00e9 par la probabilit\u00e9 qu\u2019il ne se r\u00e9aliste\npas. Malheureusement, les choses sont un peu plus complexes dans les\nparis... Zoomons un peu sur le match d\u2019ouverture, \n \nLa cote est la valeur offerte la par le bookmarker, \u00e0 laquelle on ajoute 1. La \" probabilit\u00e9 \" est alors l\u2019inverse de cette cote. \n \n \n \n r\u00e9sultat \n Bookie Offers \n Cote \n \"probabilit\u00e9s\" \n \n \n Mexico \n 11/8 \n 2.375 \n 42.10% \n \n \n South Africa \n 13/8 \n 2.623 \n 38.09% \n \n \n Draw \n 11/5 \n 3.200 \n 31.25% \n \n \n \n \n total \n 111.45% \n \n \n A-t-on rat\u00e9 quelquechose\u2009? car visiblement la \" probabilit\u00e9 \" n\u2019est pas une vraie probabilit\u00e9...\nEn fait, supposer que l\u2019on puisse construire une mesure de\nprobabilit\u00e9 \u00e0 partir des cotes doit pouvoir \u00eatre\nreli\u00e9 au \" th\u00e9or\u00e8me fondamental d\u2019asset pricing \" (et de probabilit\u00e9s risques neutres mentionn\u00e9s tout \u00e0 l\u2019heure)\n Relecture des cotes \u00e0 l\u2019aide du th\u00e9or\u00e8me fondamental \n \n \nAutrement dit, on suppose qu\u2019il existe une mesure telle que \no\u00f9 est l\u2019ensemble des gains possibles, et un \u00e9quivalent certain. \nOn suppose qu\u2019il existe un ensemble d\u2019\u00e9v\u00e8nements \n(mutuellement exclusif comme on dit, formant une partition de ),\nun seul pouvant survenir (en l\u2019occurrence l\u2019\u00e9quipe qui re\u00e7oit gagne, ou\nl\u2019\u00e9quipe qui re\u00e7oit perd, ou il y a match nul). On consid\u00e8re un\nbookmaker qui accepte\nde payer si l\u2019\u00e9v\u00e8nement i survient \u00e0 un joueur\nqui aura pari\u00e9 1 euro. On pourra l\u00e9gitimement pense que . Notre souhait est de relier ces et , si un\nlien quelconque pouvait exister. Ce probl\u00e8me avait\n\u00e9t\u00e9 soulev\u00e9 par Ramsey ou de Finetti. \nEn notant le montant total plac\u00e9 sur\nl\u2019\u00e9v\u00e8nement . On pourra dire que l\u2019on est \u00e0\nl\u2019\u00e9quilibre si est constant. Autrement dit, le ratio des cotes doit \u00eatre l\u2019inverse des ratios de montants plac\u00e9s, \nOn peut ainsi s\u2019int\u00e9resser \u00e0  qui est la fraction de l\u2019argent qui sera effectivement\nr\u00e9tribu\u00e9e aux parieurs, ou sera la part gard\u00e9e\npar le bookmaker. On peut penser qu\u2019un bookmaker fixe , et alors En fait, k peut \u00eatre vu comme un taux d\u2019actualisation\n(l\u2019\u00e9quivalent d\u2019un en math\u00e9matique\nfinanci\u00e8re). On peut d\u2019ailleurs noter que si on note le produit \nalors la strat\u00e9gie consistant \u00e0 parier \nsur l\u2019\u00e9v\u00e8nement ,\npour tout les \u00e9v\u00e8nements, aura un co\u00fbt unitaire (j\u2019ai ici constitu\u00e9 mon\nportefeuille si on revient \u00e0 la th\u00e9orie de l\u2019arbitrage). Aussi En l\u2019occurrence, sur le match d\u2019ouverture de la coupe du monde, on obtient de l\u2019ordre de 90% (l\u2019inverse de la somme de la derni\u00e8re colonne). Autrement dit, le bookmaker prend de l\u2019ordre de 10%. A partir de l\u00e0, on peut en d\u00e9duire de ces prix d\u2019Arrow\nDebreu une \"probabilit\u00e9 implicite\" qui est la probabilit\u00e9\nrisque neutre, ou la croyance des investisseurs dans la survenance des\n\u00e9v\u00e8nements. Pour cela, on pourrait poser \nsauf que dans ce cas, compte tenu de la derni\u00e8re relation, . On d\u00e9finie alors la probabilit\u00e9 implicite des parieurs sous la forme On a r\u00e9ussit \u00e0 construire une mesure de probabilit\u00e9 ,\nqui correspond \u00e0 l\u2019intuition des parieurs... mais qui n\u2019a rien \u00e0 voir\navec la probabilit\u00e9 r\u00e9elle. Si l\u2019on compare les bookmakers, on note que\nles probabilit\u00e9s induites sont tr\u00e8s proches, \net si on compare la part prise par le bookmaker sur ce match, on est l\u00e0 aussi sur des ordres de grandeurs comparables, Justin Wolfers et Eric Zitzewitz, dans Prediction Markets \nparu dans le Journal of Economic Perspectives de 2004 ( ici )\nexpliquent d\u2019ailleurs tout cela tr\u00e8s clairement. Je peux aussi\nrenvoyer \u00e0 un document de travail datant de novembre dernier de\nEgon Franck, Erwin Verbeek, et Stephan N\u00fcesch (en ligne ici ). \nJ\u2019ai parl\u00e9 tout \u00e0 l\u2019heure de probabilit\u00e9 risque\nneutre, mais cela sous-entend que l\u2019on puisse supposer les\nmarch\u00e9s efficients, et qu\u2019il n\u2019y ait absence\nd\u2019opportunit\u00e9 d\u2019arbitrage. C\u2019est pr\u00e9cis\u00e9ment ce\nqu\u2019avait regard\u00e9 Steven Levitt et Richard Gill ici , dans un vieux papier, \u00e9voqu\u00e9 \u00e9galement l\u00e0 .\nPour r\u00e9sumer les conclusions, il n\u2019y a pas d\u2019opportunit\u00e9s\nd\u2019arbitrage sur ces march\u00e9s, et il existe des \"market makers\",\nc\u2019est \u00e0 dire des parieurs misant \u00e9norm\u00e9ment, ce\nqui accro\u00eet consid\u00e9rablement la liquidit\u00e9 de ces\nmarch\u00e9s. Mais cette conclusion sur l\u2019absence d\u2019opportunit\u00e9 d\u2019arbitrage\na \u00e9t\u00e9 largement remise en cause en pratique. Pour aller plus loin, on\npeut aussi regarder un article de Moris Eaton et David Freedman, Dutch Book against some \u2019Objective\u2019 Prior ,\nparu dansBernoulli en 2004, en ligne l\u00e0 . C\u2019est marrant car cette relecture\nBay\u00e9sienne des jeux de pari peut \u00eatre \u00e9tudier en\npratique, pour la coupe du monde de foot sur worldcup.bayesialab.com ( l\u00e0 ,\nmerci Avner pour le\nlien). Je vais continuer de suivre \u00e7a lorsque les matchs commen\u00e7erons,\nafin de voir les mises \u00e0 jour des paris au fur et \u00e0 mesure de\nl\u2019\u00e9volution des matchs... \u00e0 suivre donc."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/03/Foot%2C-probabilit%C3%A9%2C-et-assurance%2C-partie-2", "bloglinks": {}, "links": {"http://bpp.upenn.edu/": 1, "http://papers.ssrn.com/": 1, "http://blogperso.univ-rennes1.fr/": 2, "http://soccer.betfair.com/": 1, "http://people.ucsc.edu/": 1, "http://www.google.fr/": 1, "http://worldcup.bayesialab.com/": 1, "http://freakonomics.nytimes.com/": 1, "http://www.oddschecker.com/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Suite \u00e0 mon billet sur les copules empiriques, un ancien coll\u00e8gue m\u2019a\nfait remarqu\u00e9 que je parlais peu des probl\u00e8mes de corr\u00e9lation ou de\ncopules, voire de tail dependence ,\nalors que \u00e7a a \u00e9t\u00e9 mes sujets de pr\u00e9dilection pendant quelques ann\u00e9es.\nHistoire de le faire mentir, je vais faire un billet pour raconter une\naventure \u00e9trange que j\u2019ai v\u00e9cue pas plus tard qu\u2019avant hier sur un\nprobl\u00e8me de corr\u00e9lation. La corr\u00e9lation au sens de Pearson est\nutilis\u00e9e partout, par exemple sur les s\u00e9ries temporelles quand on\nregarde les autocorr\u00e9lations. Pour montrer un petit exemple et faire\nun peu r\u00e9fl\u00e9chir, consid\u00e9rons un petit exemple simple (qui devrait\nfaire r\u00e9fl\u00e9chir les banquiers qui ont \u00e0 faire face \u00e0 des probl\u00e8mes\nsimilaires). Consid\u00e9rons deux villes ou passe la Seine, l\u2019une \u00e9tant -\non s\u2019en doute - en amont de l\u2019autre. Il existe des cartes construites par des hydrologues de vitesse d\u2019avanc\u00e9e moyenne de l\u2019eau.  En\ntant que statisticien, je me disait qu\u2019il serait possible d\u2019\u00e9tudier les\ns\u00e9ries  et  o\u00f9  serait a priori proche de la valeur indiqu\u00e9e\npar les hydrologues. Dans un monde id\u00e9al,  pourrait aussi \u00eatre un\nprocessus stochastique, mais je d\u00e9bute dans les probl\u00e8mes de\nchangements de temps.   \n Naturellement, comme tout statisticien qui\na pratiqu\u00e9 (et m\u00eame enseign\u00e9) les s\u00e9ries temporelles, je voulais\nutiliser la corr\u00e9lation entre les s\u00e9ries, pour trouver la valeur de h\noptimale. A priori, r\u00e9soudre  devrait \u00eatre une bonne id\u00e9e. Compte\ntenu des donn\u00e9es dont je dispose (en l\u2019occurrence les points sont s\u00e9par\u00e9s par une journ\u00e9e sur la figure ci-dessus), en extrapolant un peu \u00e0 l\u2019occasion\n(de mani\u00e8re lin\u00e9aire), je peux regarder des d\u00e9calages horaires. \u00c9trangement, j\u2019obtiens la courbe suivante pour l\u2019\u00e9volution de la\ncorr\u00e9lation,  Autrement dit, la corr\u00e9lation (au sens de Pearson)\ndonne une intuition assez \u00e9trange, \u00e0 savoir qu\u2019il pourrait \u00eatre\nint\u00e9ressant de regarder la s\u00e9rie d\u00e9cal\u00e9e de l\u2019ordre d\u2019une semaine. En\nfait, si j\u2019avais regard\u00e9 la corr\u00e9lation au sens de Spearman, j\u2019obtiens\ndes r\u00e9sultats beaucoup plus en accord avec l\u2019intuition physique,  \u00c9tonnant, non\u2009? Moralit\u00e9, il faut toujours se m\u00e9fier de la corr\u00e9lation\u2009!"], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/02/Des-dangers-de-la-corr%C3%A9lation-%28de-Pearson%29", "bloglinks": {}, "links": {}, "blogtitle": "Arthur Charpentier"}, {"content": ["En d\u00e9but de semaine, Luc Chatel semblait noter \" les\n\u00e9tudes les plus r\u00e9centes indiquent que la diminution des\neffectifs dans les classes n\u2019a pas d\u2019effet av\u00e9r\u00e9 sur les\nr\u00e9sultats des \u00e9l\u00e8ves \". Tout a \u00e9t\u00e9 repris, et d\u00e9taill\u00e9 ici , avec d\u2019ailleurs une copie de la fiche donn\u00e9e aux inspecteur, l\u00e0 . \nJe suis un peu surpris car un peu de bon sens (et peut \u00eatre\nd\u2019exp\u00e9rience) me laisser penser le contraire. Je me demandais\nqui avait bien pu conclure \u00e0 une chose pareil. J\u2019ai\ntra\u00een\u00e9 un peu sur le net, je suis tomb\u00e9 sur le\nrapport de Thomas Piketty et Mathieu Valdenaire par exemple ( ici pour le rapport et l\u00e0 \npour les slides), qui avait fait de l\u2019\u00e9conom\u00e9trie sur le\npanel primaire de 1997, montrant par exmple que la r\u00e9duction\nd\u2019un \u00e9l\u00e8ve par classe en CE1 augmentait de 0,7 points les\nr\u00e9sultats en maths en d\u00e9but de CE1. Bref, sur le lien\neffectifs et r\u00e9sultats, j\u2019avais l\u2019impression que l\u2019effet\n\u00e9tait av\u00e9r\u00e9. Les papiers de Robert Gary Bobo ( ici ) allaient dans le\nm\u00eame sens. Mais qui donc avait pu pondre ces \" \u00e9tudes les plus r\u00e9centes \"\u2009? \n  \nApr\u00e8s avoir cherch\u00e9 un peu, j\u2019ai fini par trouver un \u00e9l\u00e9ment de r\u00e9ponse sur mon blog ( ici ):\ndans un billet sur la mod\u00e9lisation \u00e9conom\u00e9trique,\nj\u2019avais commenc\u00e9 \u00e0 reprendre le papier de Joshua Angrist et Victor Lavy, \" using Maimonides\u2019 rule to estimate the effect the effect of class size on scholastic achivemen t\" (publi\u00e9 que le QJE en 1999, ici ).\nMalheureusement mon billet \u00e9tait long, j\u2019avais fait une\nr\u00e9gression simple, et conclu \u00e0 un effet croissant de\nl\u2019effectif (ici en abscisse) sur les r\u00e9sultats scolaires: plus grande est la classe, meilleurs sont les r\u00e9sulats\u2009!  \nDamned, c\u2019est moi\nle fautif\u2009? Je devrais me rejouir que Luc lise mon blog (maintenant\nque je sais qu\u2019il lit mon blog, je peux l\u2019appeler par son\npr\u00e9nom), sauf que mon billet se terminait par une mise en garde.\nCe que l\u2019on mesure est probablement un effet cach\u00e9, la mesure\nd\u2019autre chose.... Afin de justifier ce point, je vais enfin me lancer\n(avec 10 mois de retard) dans la r\u00e9daction de la suite\u2009! En fait, comme le notent\nJoshua Angrist et Victor Lavy, une variable importante dans l\u2019analyse est le \" school level index of student\u2019s socioeconomic status \", appel\u00e9 dans le texte original \" percent disadantaged \" (PD).\nCet indice est suffisement s\u00e9rieux pour que le Minist\u00e8re de l\u2019Education\nl\u2019utilise. Il est fonction du niveau d\u2019\u00e9ducation du p\u00e8re, et de la\ntaille de la famille de l\u2019enfant scolaris\u00e9. On le voit tr\u00e8s clairement,\nplus l\u2019indice est \u00e9lev\u00e9, i.e. plus l\u2019\u00e9cole est situ\u00e9 dans un endroit\nd\u00e9favoris\u00e9, moins bons sont les r\u00e9sultats scolaires. \nBref,\npour ceux qui concluaient apr\u00e8s le pr\u00e9c\u00e9dant billet qu\u2019il fallait faire\ndes classes les plus grands possibles pour avoir de meilleurs\nr\u00e9sultats, ici on pourrait conclure qu\u2019il faut interdire aux enfants de\nfamilles nombreuses, ou celles dont le p\u00e8re n\u2019a pas fait de longues\n\u00e9tudes, d\u2019aller \u00e0 l\u2019\u00e9cole. C\u2019est un peu stupide. Mais on se doute que\ntout cela est s\u00fbrement tr\u00e8s li\u00e9. Moralit\u00e9, il faudra faire une \u00e9tude\nplus pouss\u00e9e pour voir si, \u00e0 contexte socio-\u00e9conomique identique, la\ntaille des classes et le r\u00e9sultat aux tests sont positivement ou\nn\u00e9gativement corr\u00e9l\u00e9s... \u00e0 suivre donc...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/02/Qu-est-ce-que-la-mod%C3%A9lisation-%C3%A9conom%C3%A9trique-%282%29", "bloglinks": {}, "links": {"http://www.ens.fr/": 3, "http://blogperso.univ-rennes1.fr/": 1, "http://papers.ssrn.com/": 1, "http://www.cafepedagogique.net/": 2, "http://vidberg.lemonde.fr/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["Tout le monde conna\u00eet l\u2019exp\u00e9rience o\u00f9 l\u2019on compare les gains\nfinanciers obtenus par un traders exp\u00e9riment\u00e9 et un singe. M\u00eame si\nbeaucoup de sites \u00e9voquent \" la c\u00e9l\u00e8bre exp\u00e9rience \", je n \u2019en ai pas trouv\u00e9 beaucoup - en France - qui donnent une r\u00e9f\u00e9r\u00e9nce claire. Cette exp\u00e9rience \u00e9tait n\u00e9e d\u2019une th\u00e9orie de Burton Malkiel dans son livre a random walk down wall street , qui affirmait \" a blindfolded monkey throwing darts at a newspaper\u2019s financial pages\ncould select a portfolio that would do just as well as one carefully\nselected by experts \". En 1988, le Wall Street Journal a pris Burton Malkiel au mot, et a tent\u00e9 l\u2019exp\u00e9rience.Mais\nils n\u2019utilis\u00e8rent pas des singes, et ce furent des journalistes qui\ntiraient au hasard les actions sur lesquelles ils investiraient (alors\nque parall\u00e8lement, les m\u00eames sommes \u00e9taient confi\u00e9es \u00e0 des vrais\ninverstisseurs). Apr\u00e8s 6 mois d\u2019exp\u00e9rience, ils firent le bilan. \" On\nOctober 7, 1998 the Journal presented the results of the 100th\ndartboard contest. So who won the most contests and by how much? The\npros won 61 of the 100 contests versus the darts. That\u2019s better than\nthe 50% that would be expected in an efficient market. On the other\nhand, the pros losing 39% of the time to a bunch of darts certainly\ncould be viewed as somewhat of an embarrassment for the pros.\nAdditionally, the performance of the pros versus the Dow Jones\nIndustrial Average was less impressive. The pros barely edged the DJIA\nby a margin of 51 to 49 contests. In other words, simply investing\npassively in the Dow, an investor would have beaten the picks of the\npros in roughly half the contests (that is, without even considering\ntransactions costs or taxes for taxable investors). The pro\u2019s picks\nlook more impressive when the actual returns of their stocks are\ncompared with the dartboard and DJIA returns. The pros average gain was\n10.8% versus 4.5% for the darts and 6.8% for the DJIA. \" Il\nsemble que l\u2019Expansion en France ait aussi voulu tenter l\u2019exp\u00e9rience. \n La conclusion de cette exp\u00e9rience est qu\u2019en moyenne, il est dur de battre le hasard\u2009! Une autre\nconclusion peut aussi \u00eatre qu\u2019en mettant 500 singes dans une salle de\nmarch\u00e9s, au bout d\u2019un certain temps on peut en trouver un qui sera\naussi riche que Georges Soros, comme le disait Jean-Philippe Bouchaud.\nOn peut parler de grandes d\u00e9viations \u00e9ventuellement... Cette histoire peut pr\u00eater \u00e0 sourire, effectivement, mais un think tank plut\u00f4t conservateur outre atlantique (le National Center for Public Policy Research )\nenvisage de faire la m\u00eame chose pour pr\u00e9dire les ouragans: au lieu\nd\u2019\u00e9couter les climatologues, ils envisagent de faire appel \u00e0 un singe\n(comme mentionn\u00e9 ici ou l\u00e0 ). \nM\u00eame si les pr\u00e9visions des chercheurs ne sont pas toujours tr\u00e8s fiables\n(en particulier sur les catastrophes comme les s\u00e9ismes ou les\nouragants), elles reposent sur des \u00e9l\u00e9ments rationnels, avec souvent\ndes intervalles de confiance.... Oui, je suis le premier \u00e0 r\u00e2ler quand M\u00e9t\u00e9o France pr\u00e9voit un beau week end et qu\u2019au final il\npleut, mais je suis agac\u00e9 parces d\u00e9magogues qui critiquent le travail des scientifiques (surtout qu\u2019en l\u2019occurence, je pense que le NCPPR confond \nm\u00e9t\u00e9o et climat, mais c\u2019est un autre d\u00e9bat). Il y a un an, la\ncommunaut\u00e9 scientifique essayait de faire comprendre qu\u2019elle n\u2019\u00e9tait\npas un parasite, voil\u00e0 que maintenant il faut que l\u2019on se batte pour\nfaire comprendre qu\u2019on fait des choses qu\u2019un chimpanz\u00e9 ne saurait pas\nfaire\u2009! On est d\u00e9cid\u00e9ment tomb\u00e9 bien bas..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/01/Arr%C3%AAtez-donc-vos-singeries-%21", "bloglinks": {}, "links": {"http://www.prnewswire.com/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Tarek Zari a soutenu sa th\u00e8se au d\u00e9but du mois, pr\u00e9sentant une \" contribution \u00e0 l\u2019\u00e9tude du processus empirique de copule \", et sa th\u00e8se est en ligne ici . Je mets aussi une copie de ses slides l\u00e0 .\nHistoriquement, il semble que Frits Ruymgaart a \u00e9t\u00e9 le premier a parler\nde processus empirique de copules, en 1973 (sa th\u00e8se est en ligne ici ). \n \nPaul Deheuvels avait \u00e9galement introduit la notion en copule empirique d\u00e8s 1979 sous le nom de \" fonction de d\u00e9pendance empirique \". A la m\u00eame \u00e9poque, Ludger R\u00fcschendorf proposait \u00e9galement une \u00e9tude asymptotique des processus empiriques de copules ( ici en 1976), ou encore G\u007f\u00e4enssler et Stute dans leur seminar on empirical processes et Winfried Stute dans les ann\u00e9es 80 ( l\u00e0 ). Une revue de la litt\u00e9rature sur les processus empiriques multivari\u00e9s a \u00e9t\u00e9 publi\u00e9 \u00e0 cette \u00e9poque, en ligne l\u00e0 . Depuis Jean-David Fermanian a publi\u00e9 un papier ici \nsur la convergence faible, et Paul Deheuvels ou Ludger R\u00fcschendorf ont\npubli\u00e9 \u00e9norm\u00e9ment de choses, en particulier sur la vitesse de\nconvergence..."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/01/Copules-et-processus-empiriques", "bloglinks": {}, "links": {"http://www.e-fern.org/": 1, "http://tel.archives-ouvertes.fr/": 1, "file://H:/MES%20DOCUMENTS/slides-these-tarek.pdf": 1, "http://arno.uvt.nl/": 1, "http://oai.cwi.nl/": 1, "http://projecteuclid.org/": 2}, "blogtitle": "Arthur Charpentier"}, {"content": ["Depuis\nqu\u2019on peut facilement commander des livres d\u2019occasion outre-atlantique,\nj\u2019avoue en abuser un peu... La derni\u00e8re commande que j\u2019ai faite est le \" origins of value \" de Richard Michod, Lynn Nadel et Michael Hechter. Cet ouvrage, qui est une compilation d\u2019articles d\u2019histoire de la finance, contient un chapitre passionnant sur Leonardo Pisano alias Fibonacci . Pour remettre dans le contexte, Fibonacci vivait il y a fort fort\nlongtemps, autour de 1200 (300 ans avant la renaissance et L\u00e9onard de\nVinci). Je peux renvoyer ici par exemple pour une analyse de quelques uns de ses ouvrages. Pour\nmoi, Fibonnaci, c\u2019\u00e9tait l\u2019histoire de la reproduction des lapins, c\u2019est\n\u00e0 dire, l\u2019\u00e9tude des suites r\u00e9currentes pour ceux qui se souviennent de\nleurs cours de lyc\u00e9e, avec des suites de la forme Le probl\u00e8me que l\u2019on cherche \u00e0 r\u00e9soudre - pos\u00e9 par Fibonacci - est \" partant\nd\u2019un couple, combien de couples de lapins obtiendrons-nous apr\u00e8s un\nnombre donn\u00e9 de mois sachant que chaque couple produit chaque mois un\nnouveau couple, lequel ne devient productif qu\u2019apr\u00e8s deux mois\u2009? \n\". Pour ceux qui se souviennent de la r\u00e9solution, l\u2019\u00e9quation caract\u00e9ristique de cette relation de r\u00e9currence est un polyn\u00f4me de\ndegr\u00e9 2,  dont les solutions sont et Le premier nombre est souvent connu comme \u00e9tant le nombre d\u2019or ( ici ).\nBref, Fibonacci a \u00e9t\u00e9 recycl\u00e9 abondamment par l\u2019utilisation \u00e9sot\u00e9rique\nde cette racine de polyn\u00f4me de degr\u00e9 2. Aussi, lorsque j\u2019ai vu \u00e9voqu\u00e9\nle nom de Fibonacci dans cet ouvrage de finance, j\u2019ai eu peu peur,\nd\u2019autant plus que certaines personnes faisant de l\u2019 analyse chartiste utilisent\nce nombre d\u2019or (comme l\u2019\u00e9voque rapidement certains site, comme\nhttp://www.forexfibonacci.com/ ou http://www.fibonaccitrader.com/).\nAussi, parler de Fibonacci en finance, c\u2019est un peu comme parler de\nNostradamus dans les mod\u00e8les de pr\u00e9visions. \nQue nenni mon ami\u2009! Fibonacci est l\u2019auteur d\u2019un Liber Abaci , correspondant \u00e0 un livre des calculs .\nEn fait, Leonardo s\u2019est \u00e9norm\u00e9ment inspir\u00e9 des math\u00e9matiques arabes,\nalors que la norme \u00e9tait plut\u00f4t l\u2019arithm\u00e9tique romaine. Pour ceux qui\nont essay\u00e9 un jour dans leur vie de lire des nombres latins (m\u00eame dans\nAst\u00e9rix), la logique est assez d\u00e9routante\u2009! Par exemple 888 s\u2019\u00e9crit\nDCCCLXXXVIII... Essayez avec cette \u00e9criture de poser une addition\u2009! Les\nnombres arabes - eux - utilisaient le syst\u00e8me d\u00e9cimale, ce qui a fait gagn\u00e9\n\u00e9norm\u00e9ment de temps de calcul (on peut aussi remonter \u00e0 l\u2019arithm\u00e9tique\nindienne, mais on sort largement du cadre de mon billet). Fibonacci\n\u00e9voque d\u2019ailleurs constamment l\u2019utilisation de la r\u00e8gle de trois (sans l\u2019appeler ainsi, j\u2019\u00e9tais revenu ici sur l\u2019origine de cette r\u00e8gle). Dans\nson livre d\u2019arithm\u00e9tique des nombres arabes, Fibonacci montre une\napplication int\u00e9ressante: le calculs d\u2019actualisation et des marges\ncommerciales. William Goetzmann reprend tout cela dans un article publi\u00e9 en 2003, \"\n Fibonacci and the Financial Revolution \" en ligne ici .\nIl faut aller pour cela dans le chapitre 12 du Liber Abaci, o\u00f9\nFibonacci parle de partage du profit (ce que nous appellerions\nl\u2019allocation de capital, et que l\u2019on retrouve dans les travaux sur les\njeux coop\u00e9ratifs, en particulier les travaux de Shapley), mais aussi d\u2019 actualisation \nde flux futurs, et surtout, de r\u00e9flexions sur la diff\u00e9rence entre un\ntaux annuel et un taux trimestriel. La version en anglaise est la\nsuivante \n ou encore \n \n Dans la version originale, cela donne \n \nQuand\non pense qu\u2019auparavant l\u2019arithm\u00e9tique se faisait en chiffre romain, et\nn\u2019autorisait pas de raisonnements aussi subtils, on peut l\u00e9gitimement\npenser que Fibonacci est r\u00e9volutionn\u00e9 le calcul actuariel (certes, en\nse contentant de traduire des textes arabes). Si on cherche \u00e0 remonter\nencore dans le temps, on peut \u00e9galement citer un math\u00e9maticien indien,\nBrahmagupta, \u092c\u094d\u0930\u0939\u094d\u092e\u0917\u0941\u092a\u094d\u0924, vers 650, qui semble \u00e9galement avoir beaucoup\ninspir\u00e9 Fibonacci."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/06/01/Fibonacci%3A-des-lapins-%C3%A0-l-actualisation", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 1, "http://tony.free.fr/": 1, "http://papers.ssrn.com/": 1, "http://fr.wikipedia.org/": 1}, "blogtitle": "Arthur Charpentier"}, {"content": ["suite aux pressions g\u00e9n\u00e9rales, je vais reprendre mes discussions\nd\u2019alcoolique.... ou plut\u00f4t reprendre des classiques de finance, en\nexpliquant que ce\nsont simplement des probl\u00e8mes que se posent les amateurs de boissons\nfortes (de l\u00e0 \u00e0 conseiller plut\u00f4t de recruter dans les bars qu\u2019\u00e0 la\nsortie des grandes \u00e9coles...). Bref, avant d\u2019avoir entam\u00e9 sa marche al\u00e9atoire dans la rue de la soif ( ici , correspondant aux probl\u00e8mes d\u2019options \u00e0 barri\u00e8re traduit en termes financier), puis d\u2019avoir un soucis avec ses cl\u00e9s ( l\u00e0 ), puis la mar\u00e9chauss\u00e9e\n ( ici ), notre h\u00e9ros (car on peut maintenant l\u2019appeler un h\u00e9ros \napr\u00e8s 4 billets qui lui sont consacr\u00e9s) avait du choisir son bar... Le\nprobl\u00e8me est loin d\u2019\u00eatre simple. Il y a 20 bars dans la rue (disons pour faire quelque chose de plus formel). Il arrive de la place sainte Anne, et l\u00e0, il souhaite choisir le bar le moins cher. Le soucis est qu\u2019il n\u2019a pas le droit de faire demi-tour 1 \net il ne conna\u00eet pas les prix pratiqu\u00e9s dans les diff\u00e9rents bars. Il\npart avec un a priori qui est que le prix d\u2019une pinte est compris entre\n3 et 6 euros, que le prix est uniform\u00e9ment r\u00e9parti entre ces deux prix,\net que les prix sont ind\u00e9pendants d\u2019un bar \u00e0 l\u2019autre. Pour les\nfinanciers, il a une option (de commander une bi\u00e8re), et peut l\u2019exercer\nquand il le souhaite. Une option am\u00e9ricaine en quelque sorte. Supposons\nqu\u2019on soit arriv\u00e9 au bar . On peut soit payer \n(qui est suppos\u00e9 al\u00e9atoire, uniform\u00e9ment distribu\u00e9 et ind\u00e9pendant des\nautres bars), soit esp\u00e9rer que l\u2019on puisse payer moins cher plus loin, Soit la valeur de cette option, alors \n i.e. soit  o\u00f9 d\u00e9signe la loi du prix de la bi\u00e8re (soit ici une loi uniforme) avec une condition terminale de la forme  car il a soif, et ne quitera pas la rue sans avoir bu un verre\u2009! Classiquement, par backward induction, on peut r\u00e9soude ce programme, \u00e0 partir de la loi de . Posons . Alors et soit simplement soit enfin Je\nlaisse les plus courageux simplifier les calculs. La \"fronti\u00e8re\nd\u2019exercice\" est alors obtenue par r\u00e9curence. Num\u00e9riquement, le code est\nalors simplement \n > n=20 > u=rep(NA,n) > b=6;a=3 > u[n]=(b+a)/2 > for(k in (n-1):1){ + u[k]=1/(b-a)*(u[k+1]*(b-u[k+1])+(u[k+1]^2-a^2)/2) + }  \nD\u00e8s\nqu\u2019on atteind la barri\u00e8re, on s\u2019assoit au bar. On note que plus on\navance dans la rue, moins on est exigent: au tout d\u00e9but, on ne s\u2019assoit\npas \u00e0 moins de 3 euros 30... mais plus on avance, plus on rel\u00e8ve le\nseuil d\u2019exigence. Le calcul sous forme int\u00e9grale donne ici \n > u=rep(NA,n) > b=6;a=3 > u[n]=(b+a)/2 > for(k in (n-1):1){ + g=function(x){pmin(x,rep(u[k+1],length(x)))/(b-a)} + u[k]=integrate(g,lower=a,upper=b)$value + } J\u2019avais\nd\u00e9j\u00e0 abord\u00e9 ce probl\u00e8me dans un pr\u00e9c\u00e9dant billet, sur les options\nam\u00e9ricaines, mais on peut maintenant aller un peu plus loin... que se\npasse-t-il si on suppose que les prix sont discret (par exemple par\ntranches de 50 centimes ou 1 euro)\u2009? L\u2019avantage avec ces m\u00e9thodes\nnum\u00e9riques est que l\u2019on peut tr\u00e8s facilement enlever des hypoth\u00e8ses,\npar exemple ici on aurait > h=2 > K=(b-a)*h+1 > PRIX=seq(a,b,by=1/h) > u2=rep(NA,n) > b=6;a=3 > u2[n]=(b+a)/2 > for(k in (n-1):1){ + g=function(x){pmin(x,rep(u[k+1],length(x)))} + u2[k]=sum(g(PRIX)*1/K)} pour des seuils \u00e0 1 euros (les seuls prix possibles \u00e9tant 3,4,5 ou 6 euros). \n Ou la fronti\u00e8re suivante si les prix varient par tranche de 50 centimes. \n \nCompte tenu de la discr\u00e9tisation, notons que la vraie fronti\u00e8re devient alors ici \nBref,\ncomme toujours, les probl\u00e8mes d\u2019alcooliques rejoignent les probl\u00e8mes\nd\u2019exercice optimal d\u2019options am\u00e9ricaines, probl\u00e8me classique en finance\nde march\u00e9... 1 pour rendre cette histoire cr\u00e9dible, \u00e0\nchaque bar rencontr\u00e9 il demande le prix d\u2019une pinte. S\u2019il estime que\nc\u2019est trop cher, il s\u2019exclame \" mais c\u2019est bien trop cher ici\u2009! \"\net s\u2019en va. Sinon il commande et s\u2019installe. Cette exclamation rend\nimprobable - \u00e0 ses yeux - l\u2019id\u00e9e de revenir finallement s\u2019intaller au\nbar...."], "link": "http://blogperso.univ-rennes1.fr/arthur.charpentier/index.php/post/2010/05/31/Histoire-%C3%A9thylique%2C-partie-4", "bloglinks": {}, "links": {"http://blogperso.univ-rennes1.fr/": 3}, "blogtitle": "Arthur Charpentier"}]