[{"blogurl": "http://fishyoperations.com\n", "blogroll": [], "title": "FishyOperations"}, {"content": ["ShiftOptim is a tool that can be used to determine which employee shifts to use in order to meet business requirements while minimizing effective employee hours (thus minimizing personnel costs). ShiftOptim, for the moment, can only optimize shift allocation on a daily basis. In the future, a multi-day & cyclic optimization function might be added. \n The optimization algorithm used in ShiftOptim is an Integer Programming model. An evolutionary algorithm is also available, at the moment just for testing purposes. \n I just released a pre-alpha version of ShiftOptim. Downloads and additional information can be found here . \n The post ShiftOptim: a daily shift optimization tool appeared first on FishyOperations ."], "link": "http://fishyoperations.com/linear-programming/shiftoptim-a-daily-shift-optimization-tool/?utm_source=rss&utm_medium=rss&utm_campaign=shiftoptim-a-daily-shift-optimization-tool", "bloglinks": {}, "links": {"http://fishyoperations.com": 1, "http://fishyoperations.com/": 2}, "blogtitle": "FishyOperations"}, {"content": ["In the context of a shift optimization problem I have recently been looking into available open-source optimization libraries. More specifically, mixed-integer programming libraries. There are a lot of good, actively maintained libraries out there for whole bunch of different (scripting) languages. For example this far from exhaustive list; \n Python \n \n Pulp-or \n Coopr \n LpSolve \n And others \n \n R \n \n LpSolveApi \n RSymphony \n \n However, my search for open-source applications which use these packages has turned up almost empyty. So, while I was writing a GUI for my shift optimization problem I was thinking, why not write a a flexible open-source shift optimization tool? \n \nAnd that\u2019s exactly what I\u2019ve been doing in the meanwhile. The tool will, for the time being, have the following functionality; \n \n Daily shift optimization: based on hourly (or any time unit) requirements, a solution will be proposed which minimizes employee hours while respecting the given requirements. \n Cyclic shift optimization: given the requirements for a given time period, the optimum cyclic shift schedule will be proposed. \n \n So far the tool makes use of the following Python modules: \n \n \n pulp-or : a LP modeler wrritten in Python (making use of the COIN-or solver) \n deap : Distributed Evolutionary Algorithms in Python \n matplotlib : a python 2D plotting library \n wxPython : a GUI toolkit for the Python programming language \n \n \n I\u2019ll keep you posted! \n A few (highly) preliminary screenshots: \n    \n \n . \n The post An open-source shift optimization tool appeared first on FishyOperations ."], "link": "http://fishyoperations.com/linear-programming/an-open-source-shift-optimization-tool/?utm_source=rss&utm_medium=rss&utm_campaign=an-open-source-shift-optimization-tool", "bloglinks": {}, "links": {"http://matplotlib.sourceforge.net/": 1, "http://fishyoperations.com/": 5, "http://fishyoperations.com": 1, "http://wiki.python.org/": 1, "http://code.google.com/": 2, "http://wxpython.org/": 1}, "blogtitle": "FishyOperations"}, {"content": ["Genetic algorithm is a search heuristic. GAs can generate a vast number of possible model solutions and use these to evolve towards an approximation of the best solution of the model. Hereby it mimics evolution in nature. \n GA generates a population, the individuals in this population (often called chromosomes) have a given state. Once the population is generated, the state of these individuals is evaluated and graded on their value. The best individuals are then taken and crossed-over \u2013 in order to hopefully generate 'better' offspring \u2013 to form the new population. In some cases the best individuals in the population are preserved in order to guarantee 'good individuals' in the new generation (this is called elitism ). \n The GA site by Marek Obitko has a great tutorial for people with no previous knowledge on the subject. \n To explain the example I will use my version of the Knapsack problem . \n You are going to spend a month in the wilderness. You\u2019re taking a backpack with you, however, the maximum weight it can carry is 20 kilograms. You have a number of survival items available, each with its own number of \u201csurvival points\u201d. You\u2019re objective is to maximize the number of survival points. \n \n The following table shows the items you can choose from. \n \n item  survivalpoints  weight  \n  pocketknife  10.00  1.00  \n  beans  20.00  5.00  \n  potatoes  15.00  10.00  \n  onions  2.00  1.00  \n  sleeping bag  30.00  7.00  \n  rope  10.00  5.00  \n  compass  30.00  1.00  \n  \n In R I have used the package genalg to set-up the model. Later on, ggplot2 will be used to visualize the evolution of the model. \n Let's define the dataset and weight constraint; \n library(genalg)\nlibrary(ggplot2)\n\ndataset <- data.frame(item = c(\"pocketknife\", \"beans\", \"potatoes\", \n \"onions\", \"sleeping bag\", \"rope\", \"compass\"), survivalpoints = c(10, 20, \n 15, 2, 30, 10, 30), weight = c(1, 5, 10, 1, 7, 5, 1))\nweightlimit <- 20\n \n Before creating the model we have to set-up an evaluation function. The evaluation function will evaluate the different individuals (chromosomes) of the population on the value of their gene configuration. \n An individual can for example have the following gene configuration: 1001100 . \n Each number in this binary string represents whether or not to take an item with you. A value of 1 refers to putting the specific item in the knapsack while a 0 refers to leave the item at home. Given the example gene configuration we would take the following items; \n chromosome = c(1, 0, 0, 1, 1, 0, 0)\ndataset[chromosome == 1, ]\n \n ##   item survivalpoints weight\n## 1 pocketknife    10  1\n## 4  onions    2  1\n## 5 sleeping bag    30  7\n \n We can check to what amount of surivival points this configuration sums up. \n cat(chromosome %*% dataset$survivalpoints)\n \n ## 42\n \n Above we gave a value to the gene configuration of a given chromosome. This is exactly what the evaluation function does. \n The genalg algorithm tries to optimize towards the minimum value. Therefore, the value is calculated as above and multiplied with -1. A configuration which leads to exceeding the weight constraint returns a value of 0 (a higher value can also be given). \n We define the evaluation function as follows. \n evalFunc <- function(x) {\n current_solution_survivalpoints <- x %*% dataset$survivalpoints\n current_solution_weight <- x %*% dataset$weight\n\n if (current_solution_weight > weightlimit) \n  return(0) else return(-current_solution_survivalpoints)\n}\n \n Next, we choose the number of iterations, design and run the model. \n iter = 100\nGAmodel <- rbga.bin(size = 7, popSize = 200, iters = iter, mutationChance = 0.01, \n elitism = T, evalFunc = evalFunc)\ncat(summary.rbga(GAmodel))\n \n ## GA Settings\n## Type     = binary chromosome\n## Population size  = 200\n## Number of Generations = 100\n## Elitism    = TRUE\n## Mutation Chance  = 0.01\n## \n## Search Domain\n## Var 1 = [,]\n## Var 0 = [,]\n## \n## GA Results\n## Best Solution : 1 1 0 1 1 1 1 \n \n The best solution is found to be 1101111 . This leads us to take the following items with us on our trip into the wild. \n solution = c(1, 1, 0, 1, 1, 1, 1)\ndataset[solution == 1, ]\n \n ##   item survivalpoints weight\n## 1 pocketknife    10  1\n## 2  beans    20  5\n## 4  onions    2  1\n## 5 sleeping bag    30  7\n## 6   rope    10  5\n## 7  compass    30  1\n \n This in turn gives us the total number of survival points. \n # solution vs available\ncat(paste(solution %*% dataset$survivalpoints, \"/\", sum(dataset$survivalpoints)))\n \n ## 102 / 117\n \n Let's visualize how the model evolves. \n animate_plot <- function(x) {\n for (i in seq(1, iter)) {\n  temp <- data.frame(Generation = c(seq(1, i), seq(1, i)), Variable = c(rep(\"mean\", \n   i), rep(\"best\", i)), Survivalpoints = c(-GAmodel$mean[1:i], -GAmodel$best[1:i]))\n\n  pl <- ggplot(temp, aes(x = Generation, y = Survivalpoints, group = Variable, \n   colour = Variable)) + geom_line() + scale_x_continuous(limits = c(0, \n   iter)) + scale_y_continuous(limits = c(0, 110)) + geom_hline(y = max(temp$Survivalpoints), \n   lty = 2) + annotate(\"text\", x = 1, y = max(temp$Survivalpoints) + \n   2, hjust = 0, size = 3, color = \"black\", label = paste(\"Best solution:\", \n   max(temp$Survivalpoints))) + scale_colour_brewer(palette = \"Set1\") + \n   opts(title = \"Evolution Knapsack optimization model\")\n\n  print(pl)\n }\n}\n\n# in order to save the animation\nlibrary(animation)\nsaveMovie(animate_plot(), interval = 0.1, outdir = getwd())\n \n \n The x-axis denotes the different generations. The blue line shows the mean solution of the entire population of that generation, while the red line shows the best solution of that generation. As you can see, it takes the model only a few generations to hit the best solution, after that it is just a matter of time until the mean of the population of subsequent generations evolves towards the best solution. \n For more information on genetic algorithms, check out: \n \n Wikipedia \n Introduction by Marek Obitko \n A Field Guide to Genetic Programming \n A Genetic Algorithm Tutorial \n \n The post Genetic algorithms: a simple R example appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/genetic-algorithms-a-simple-r-example/?utm_source=rss&utm_medium=rss&utm_campaign=genetic-algorithms-a-simple-r-example", "bloglinks": {}, "links": {"http://fishyoperations.com/": 1, "http://www.org.uk/": 1, "http://www.obitko.com/": 2, "http://fishyoperations.com": 1, "http://en.wikipedia.org/": 2, "http://samizdat.mines.edu/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "FishyOperations"}, {"content": ["The OECD collects (among a lot of other statistics) information on the number of hospitals and hospital beds per country. These two parameters combined and its evolution over the years could give an indication on whether or not the country\u2019s hospital landscape is evolving towards large medical centers, small scale hospital settings or whether there is no trend to detect. \n \nIn the graph below you can see the evolution of the average number of beds per hospital for a number of countries. The first datapoint for every country serves as a reference point and equals 100 percent. A point above this reference point denotes an increase in the average numbers of beds per hospital. In contrast, a point below the reference point means a decrease in the average number of bed per hospital. Of course, a decrease/increase could mean an country-wide decrease/increase in the number of beds. However, assuming that in these \u2018developed\u2019 countries there would be no drastic sudden changes in total number of hospital beds, a continuous increase might indicate that the country\u2019s hospital landscape is evolving towards large scale medical centers (through for example mergers & acquisitions). \n  \n Even though no solid conclusion can be drawn from the two parameters at hand, countries like Greece, Finland, Ireland, Luxembourg, The Netherlands, New Zealand and Turkey do seem to be evolving towards a healthcare landscape with large scale medical centers. Conversely, Estonia, Italy, Korea, Poland, The Slovak Republic and Slovenia seem to evolve towards smaller scale hospitals. \n The R script to generate the graph: \n \nlibrary(ggplot2)\nlibrary(reshape)\ndataset<-read.csv('hospitaldata.csv')\n\nhospitals<-subset(dataset, Variable=='Hospitals')\nhospitalbeds<-subset(dataset, Variable=='Total hospital beds')\n\ndataset_new<-merge(hospitals, hospitalbeds, by=c('Country', 'Year'))\ndataset_new$ratio<-dataset_new$Value.y/dataset_new$Value.x\n\nratioperc<-function(ratio, country){\n\tvalue<-ratio/subset(dataset_new, Year==min(dataset_new[dataset_new$Country==country,]$Year) & Country==country)$ratio\n\tif(length(value)==0) return(NA)\n\telse return(value)\n}\n\ndataset_transform<-ddply(dataset_new, .(Country, Year), transform, refvalue=ratioperc(ratio, Country))\n\nggplot(dataset_transform, aes(x=Year, y=refvalue, group=Country, colour=refvalue)) + geom_line() + geom_point() + facet_wrap(~Country, ncol=4) + opts(axis.text.x=theme_text(angle=-90, hjust=0), legend.position='none') + scale_colour_gradient(low='blue', high='orange') + ylab('')\n \n The .csv file used: hospitaldata.csv (source: oecd.org ) \n The post Evolution average number of beds per hospital appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/evolution-average-beds-per-hospital/?utm_source=rss&utm_medium=rss&utm_campaign=evolution-average-beds-per-hospital", "bloglinks": {}, "links": {"http://fishyoperations.com": 1, "http://fishyoperations.com/": 2, "http://imgur.com/": 1, "http://www.oecd.org": 2}, "blogtitle": "FishyOperations"}, {"content": ["Estimating required hospital bed capacity requires a thorough analysis. There are a lot of ways of approaching a capacity requirement problem, but I think we can agree that a simple spreadsheet analysis just won't cut it. \n The approach described in this post makes use of discrete-event simulation and, just to clarify, makes abstraction from a lot of variables which should be taken into consideration in a real-life analysis. \n To explain the approach, the following case will be used: \n \n An emergency department of a small regional hospital receives complains about its emergency admission capacity. After investigation of its admission data it becomes clear that their service level is not up to par with that of other hospitals. Therefore, plans for the redesign of the emergency department and an investment in its emergency bed capacity are presented. The plan proposes a new bed capacity of 12 beds (coming from a previous of 10 beds). The Chief of Medicine wants to know what the effect of this investment will be on their emergency admission service level. \n \n The emergency department has recorded data on the interarrival times of patients that are admitted (or should be admitted) to an emergency bed. The following graph shows the interarrival distribution (triangular: mode=5, min=.1, max=12): \n \n \n The graph below shows the collected data on the LOS of patients in the emergency bed (triangular: mode=24, min=7, max=72): \n \n SimPy will be used to build the simulation model. More information on SimPy can be found on their website , another example can be found in a previous post . \n The following script shows how the model has been build. \n \nfrom SimPy.Simulation import *\nimport random\nimport csv\n\nclass EmergencyDepartment(Simulation):\n\tdef __init__(self,capacity,days_to_run):\n\t\tSimulation.__init__(self)\n\t\tself.capacity=capacity\n\t\tself.days_to_run=days_to_run\n\t\t\n\t\t\t\t\n\tdef runModel(self):\n\t\tself.initialize()\n\t\t\n\t\t#create a monitor to observe waitingtimes & value-adding time\n\t\tself.PROCmonitor=Monitor()\n\t\t\n\t\t#create the 'bed resource'\n\t\tself.beds=Resource(name='Beds', capacity=self.capacity, sim=self)\n\t\t\n\t\t#create patient arrivaltime list\n\t\t\n\t\tpatientarrivals=[]\n\t\tlast=0\n\t\twhile last<24*self.days_to_run:\n\t\t\tnew=random.triangular(.1,12,5) \n\t\t\tpatientarrivals.append(new+last)\n\t\t\tlast=new+last\n\n\t\t#create the patients\n\t\tfor p in patientarrivals: \n\t\t\t\n\t\t\tpatient=Patient(name=\"Patient %s\"%p,sim=self)\n\t\t\tself.activate(patient,patient.run(),at=p)\n\t\t\n\t\t#simulate for the defined number of days\n\t\tself.simulate(until=self.days_to_run*24)\n\t\t\n\t\n\nclass Patient(Process):\n\tdef run(self):\n\t\tself.admitted=0\n\t\t#save a reference to the 'arrival time' of the patient\n\t\tstarttime=self.sim.now() \n\t\t\n\t\t#get a length-of-stay time from the triangular distribution\n\t\tlos_time=random.triangular(7,72,24) \n\t\t\n\t\t#request a bed\n\t\t#renege clause, a patient can wait for a maximum of one hour before being transferred to another hospital\n\t\tyield (request,self,self.sim.beds),(hold,self,1)\n\t\tif self.gotResource(self.sim.beds):\n\t\t\tself.admitted=1\n\t\t\t#hold the bed for a duration of 'los_time'\n\t\t\tyield hold,self,los_time\n\t\t\t#release the bed after discharge or transfer\n\t\t\tyield release,self,self.sim.beds\n\t\t\n\t\t\n\t\t#observe admittance status\n\t\tself.sim.PROCmonitor.observe(self.admitted, t=starttime)\n\t\n\tdef gotResource(self,resource):\n\t\tresult=self in resource.activeQ\n\t\tif not result:\n\t\t\tresource.waitQ.remove(self)\n\t\t\tif resource.monitored:\n\t\t\t\tresource.waitMon.observe(len(resource.waitQ), t=now())\n\t\treturn result\n\t\n\t\n\t\n#create a list to collect results in\nresults=[]\n\nfor beds in range(1,21):\n\t\n\tfor run in range(1,10):\n\t\n\t\ts=EmergencyDepartment(capacity=beds,days_to_run=365)\n\t\ts.runModel()\n\n\t\t#extract results from the process monitor\n\t\tfor t in s.PROCmonitor:\n\t\t\tresults.append([run,beds]+t)\n\n\n# write out results to a CSV file\t\nwriter=csv.writer(open('model_results.csv','wb') ,quoting=csv.QUOTE_NONNUMERIC)\nwriter.writerow(['Run','Beds','Time','Status'])\nfor r in results:\n\twriter.writerow(r)\n \n The model runs every scenario (bed capacity of 1 up to 20 beds) 10 times in order to achieve information on the effect of variance. This means that the model will run 20 scenarios * 10 re-runs = 200 total runs of the model. Next to that, it saves information at every timepoint at which a patient enters the model. The result is a pretty hefty datafile: 250000+ rows. Please note that in a real-life model the number of re-runs would most likely be higher. \n The results will be analyzed using R. The model outputs and saves its results in the following style: \n results <- read.csv(\"model_results.csv\")\nhead(results)\n \n ## Run Beds Time Status\n## 1 1 1 12.667  0\n## 2 1 1 16.555  0\n## 3 1 1 22.673  0\n## 4 1 1 25.184  0\n## 5 1 1 30.294  0\n## 6 1 1 5.532  1\n \n To analyze the results we have to aggregate the results by day/week. Therefore we have to determine the day and week from the Time column. The time is in hours so we can determine the day and week as follows; \n results$day <- floor(results$Time/24)\nresults$week <- floor(results$Time/24/7)\nhead(results)\n \n ## Run Beds Time Status day week\n## 1 1 1 12.667  0 0 0\n## 2 1 1 16.555  0 0 0\n## 3 1 1 22.673  0 0 0\n## 4 1 1 25.184  0 1 0\n## 5 1 1 30.294  0 1 0\n## 6 1 1 5.532  1 0 0\n \n Next, lets summarize the data per day for each bed capacity scenario and each re-run. \n library(plyr)\nsummary_run_day <- ddply(results, .(Run, Beds, day, week), summarize, \n servicelevel = mean(Status))\n \n Now to summarize the re-run data we extract the mean, minimum and maximum servicelevel per day for each bed capacity configuration. \n summary_day <- ddply(summary_run_day, .(Beds, day, week), summarize, \n mean = mean(servicelevel), min = min(servicelevel), max = max(servicelevel))\n \n On to the visualization! For visualization purposes we aggregate the data on a weekly basis and then plot the results using ggplot2. \n summary_week <- ddply(summary_day, .(Beds, week), summarise, mean = mean(mean), \n max = max(max), min = min(min))\n\nlibrary(ggplot2)\nggplot(summary_week, aes(x = week, y = mean, ymin = min, ymax = max, \n colour = mean)) + geom_ribbon(colour = NA, fill = \"red\", alpha = 0.2) + \n geom_line(size = 1) + scale_colour_gradient(low = \"red\", high = \"green\") + \n facet_wrap(~Beds) + opts(legend.position = \"bottom\") + ylab(\"Servicelevel\") + \n xlab(\"Week\")\n \n \n The above plot shows the servicelevel for each bed capacity configuration. The line shows the mean servicelevel and the ribbon indicates the effect of variation. The upper and lower boundary of the ribbon corresponds to the minimum and maximum servicelevel calculated on a daily basis. As we can clearly see from the graph, the current configuration of 10 beds just won't cut it when working towards a servicelevel of around 100%. The proposed bed capacity of 12 beds would be a major step in the right direction, but as you can see, extremely busy days can lead to a sub-100% servicelevel. To be completly save, a bed capacity of 13 beds could be considered. \n As this model only tests the effect of 1 variable, bed capacity, it can easily be interpreted visually. However, when multiple variables (e.g. nurse/doctor utilization, patient to nurse ratio, acute/sub-acute bed) are to be analyzed and the optimal configuration has the be found, it becomes difficult to analyse the result visually. In this situation, linear/binary/integer-programming can offer a solution in finding the optimum configuration within all the scenarios that have been generated. When using this methodology an optimization objective could for example be to minimize costs while adhering to the model constraints, e.g. patient/nurse ratio of 2, always at least two doctors on duty and a minimum servicelevel of 99.5%. An example of building an linear/binary/integer programming model in R can be found in a previous post . \n Please note, this simulation example is only meant for illustration purposes and makes abstraction of a whole lot of important influential factors, such as e.g. day/night variation, available personnel, process flow within the ED. \n The post Estimating required hospital bed capacity appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/estimating-required-hospital-bed-capacity/?utm_source=rss&utm_medium=rss&utm_campaign=estimating-required-hospital-bed-capacity", "bloglinks": {}, "links": {"http://simpy.sourceforge.net/": 1, "http://fishyoperations.com": 1, "http://fishyoperations.com/": 3}, "blogtitle": "FishyOperations"}, {"content": ["R has great support for Holt-Winter filtering and forecasting. I sometimes use this functionality, HoltWinter & predict.HoltWinter , to forecast demand figures based on historical data. Using the HoltWinter functions in R is pretty straightforward. \n Let's say our dataset looks as follows; \n demand <- ts(BJsales, start = c(2000, 1), frequency = 12)\nplot(demand)\n \n \n \n Now I pass the timeseries object to HoltWinter and plot the fitted data. \n hw <- HoltWinters(demand)\nplot(hw)\n \n \n Next, we calculate the forecast for 12 months with a confidence interval of .95 and plot the forecast together with the actual and fitted values. \n forecast <- predict(hw, n.ahead = 12, prediction.interval = T, level = 0.95)\nplot(hw, forecast)\n \n \n As you can see, this is pretty easy to accomplish. However, as I use ggplot2 to visualize a lot of my analyses, I would like to be able to do this in ggplot2 in order to maintain a certain uniformity in terms of visualization. \n Therefore, I wrote a little function which extracts some data from the HoltWinter and predict.HoltWinter objects and feeds this to ggplot2; \n #HWplot.R\n\nlibrary(ggplot2)\nlibrary(reshape)\n\n\nHWplot<-function(ts_object, n.ahead=4, CI=.95, error.ribbon='green', line.size=1){\n\t\n\thw_object<-HoltWinters(ts_object)\n\t\n\tforecast<-predict(hw_object, n.ahead=n.ahead, prediction.interval=T, level=CI)\n\t\n\t\n\tfor_values<-data.frame(time=round(time(forecast), 3), value_forecast=as.data.frame(forecast)$fit, dev=as.data.frame(forecast)$upr-as.data.frame(forecast)$fit)\n\t\n\tfitted_values<-data.frame(time=round(time(hw_object$fitted), 3), value_fitted=as.data.frame(hw_object$fitted)$xhat)\n\t\n\tactual_values<-data.frame(time=round(time(hw_object$x), 3), Actual=c(hw_object$x))\n\t\n\t\n\tgraphset<-merge(actual_values, fitted_values, by='time', all=TRUE)\n\tgraphset<-merge(graphset, for_values, all=TRUE, by='time')\n\tgraphset[is.na(graphset$dev), ]$dev<-0\n\t\n\tgraphset$Fitted<-c(rep(NA, NROW(graphset)-(NROW(for_values) + NROW(fitted_values))), fitted_values$value_fitted, for_values$value_forecast)\n\t\n\t\n\tgraphset.melt<-melt(graphset[, c('time', 'Actual', 'Fitted')], id='time')\n\t\n\tp<-ggplot(graphset.melt, aes(x=time, y=value)) + geom_ribbon(data=graphset, aes(x=time, y=Fitted, ymin=Fitted-dev, ymax=Fitted + dev), alpha=.2, fill=error.ribbon) + geom_line(aes(colour=variable), size=line.size) + geom_vline(x=max(actual_values$time), lty=2) + xlab('Time') + ylab('Value') + opts(legend.position='bottom') + scale_colour_hue('')\n\treturn(p)\n\n}\n \n The above script is saved in a file called HWplot.R . If I load this file from R \u2013 via source() \u2013 I can directly call the function HWplot . The HWplot can be called as follows: \n HWplot(ts_object, n.ahead=4, CI=.95, error.ribbon='green',line.size=1)\n \n HWplot takes the following arguments; \n \n ts_object : the timeseries data \n n.ahead : number of periods to forecast \n CI : confidence interval \n error.ribbon : colour of the error ribbon \n line.size : size of the lines \n \n source(\"HWplot.R\")\ndemand <- ts(BJsales, start = c(2000, 1), frequency = 12)\nHWplot(demand, n.ahead = 12)\n \n \n It's also very easy to adjust the graph after it is returned by the function; \n graph <- HWplot(demand, n.ahead = 12, error.ribbon = \"red\")\n\n# add a title\ngraph <- graph + opts(title = \"An example Holt-Winters (gg)plot\")\n\n# change the x scale a little\ngraph <- graph + scale_x_continuous(breaks = seq(1998, 2015))\n\n# change the y-axis title\ngraph <- graph + ylab(\"Demand ($)\")\n\n# change the colour of the lines\ngraph <- graph + scale_colour_brewer(\"Legend\", palette = \"Set1\")\n\n# the result:\ngraph\n \n \n The HWplot R code: HWplot.R \n The post Holt-Winters forecast using ggplot2 appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/holtwinters-forecast-using-ggplot/?utm_source=rss&utm_medium=rss&utm_campaign=holtwinters-forecast-using-ggplot", "bloglinks": {}, "links": {"http://had.co.nz/": 1, "http://fishyoperations.com": 1, "http://fishyoperations.com/": 3}, "blogtitle": "FishyOperations"}, {"content": ["First of all, a shout out to R-bloggers for adding my feed to their website! \n Linear programming is a valuable instrument when it comes to decision making. This post shows how R in conjunction with the lpSolveAPI package , can be used to build a linear programming model and to analyse its results. \n The lpSolveAPI package provides a complete implementation of the lp_solve API . \n The example case; \n A trading company is looking for a way to maximize profit per transportation of their goods. The company has a train available with 3 wagons. When stocking the wagons they can choose between 4 types of cargo, each with its own specifications. How much of each cargo type should be loaded on which wagon in order to maximize profit? \n \nThe following constraints have to be taken in consideration; \n \n Weight capacity per train wagon \n Volume capacity per train wagon \n Limited availability per cargo type \n \n Let\u2019s assume we have the following information at hand: \n \n \n Train wagon \n Weight capacity (tonne) \n Space capacity (m2) \n \n \n \n w1 \n 10 \n 5000 \n \n\n w2 \n 8 \n 4000 \n \n\n w3 \n 12 \n 8000 \n \n \n \n \n Cargo type \n Available (tonne) \n Volume (m2) \n Profit (per tonne) \n \n \n \n c1 \n 18 \n 400 \n 2000 \n \n\n c2 \n 10 \n 300 \n 2500 \n \n\n c3 \n 5 \n 200 \n 5000 \n \n\n c4 \n 20 \n 500 \n 3500 \n \n \n Let\u2019s load the necessary libraries and define the datasets to be used. \n \nlibrary(lpSolveAPI)\n\n#used for result visualization\nlibrary(ggplot2)\nlibrary(reshape)\nlibrary(gridExtra)\n\n#define the datasets\n \ntrain<-data.frame(wagon=c('w1','w2','w3'), weightcapacity=c(10,8,12), spacecapacity=c(5000,4000,8000))\n\ncargo<-data.frame(type=c('c1','c2','c3','c4'), available=c(18,10,5,20), volume=c(400,300,200,500),profit=c(2000,2500,5000,3500))\n \n Next, we start with building the actual lp model, our goal is to end up with the following model: \n \n \n#create an LP model with 10 constraints and 12 decision variables\n\nlpmodel<-make.lp(2*NROW(train)+NROW(cargo),12)\n\n#I used this to keep count within the loops, I admit that this could be done a lot neater\ncolumn<-0\nrow<-0\n\n#build the model column per column\nfor(wg in train$wagon){\n\trow<-row+1\n\tfor(type in seq(1,NROW(cargo$type))){\n\tcolumn<-column+1\n\t\n\t#this takes the arguments 'column','values' & 'indices' (as in where these values should be placed in the column)\n\tset.column(lpmodel,column,c(1, cargo[type,'volume'],1), indices=c(row,NROW(train)+row, NROW(train)*2+type))\n\t}}\n\n#set rhs weight constraints\nset.constr.value(lpmodel, rhs=train$weightcapacity, constraints=seq(1,NROW(train)))\n\n#set rhs volume constraints\nset.constr.value(lpmodel, rhs=train$spacecapacity, constraints=seq(NROW(train)+1,NROW(train)*2))\n\n\n#set rhs volume constraints\nset.constr.value(lpmodel, rhs=cargo$available, constraints=seq(NROW(train)*2+1,NROW(train)*2+NROW(cargo)))\n\n#set objective coefficients\nset.objfn(lpmodel, rep(cargo$profit,NROW(train)))\n\n#set objective direction\nlp.control(lpmodel,sense='max')\n\n#I in order to be able to visually check the model, I find it useful to write the model to a text file\nwrite.lp(lpmodel,'model.lp',type='lp')\n \n So, let\u2019s have a look at the \u2018model.lp\u2019 file. \n /* Objective function */\nmax: +2000 C1 +2500 C2 +5000 C3 +3500 C4 +2000 C5 +2500 C6 +5000 C7 +3500 C8 +2000 C9 +2500 C10 +5000 C11\n +3500 C12;\n\n/* Constraints */\n+C1 +C2 +C3 +C4 \n This seems to be the table which we wanted to create! \n Now to see if the model will solve: \n \n#solve the model, if this return 0 an optimal solution is found\nsolve(lpmodel)\n\n#this return the proposed solution\nget.objective(lpmodel)\n \n solve(lpmodel) returns a 0, this implies that an optimal solutions was found. The value of the solution is returned by get.objective(lpmodel) , this shows a maximum total profit of 107500$ . \n Using ggplot2 we generate the following plot; \n \n We can conclude from this exercise what the maximum profit is given the current constraints in transportation resources and available cargo types. Furthermore, in the bottom half of the graph we can see that the proposed configuration leads to a maximum utilization of weight on all train wagons. As we have the model available it is easy to run it for alternative configurations, e.g. an extra trainwagon, new cargo types or different profit levels. \n The example R script can be downloaded here . \n The post Linear programming in R: an lpSolveAPI example appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/linear-programming-in-r-an-lpsolveapi-example/?utm_source=rss&utm_medium=rss&utm_campaign=linear-programming-in-r-an-lpsolveapi-example", "bloglinks": {}, "links": {"http://fishyoperations.com/": 4, "http://fishyoperations.com": 1, "http://lpsolve.sourceforge.net/": 1, "http://lpsolve.r-project.org/": 1, "http://www.r-bloggers.com/": 1}, "blogtitle": "FishyOperations"}, {"content": ["Discrete-event simulation is a very useful tool when it comes to simulating alternative scenario\u2019s for current of future business operations. Let\u2019s take the following case; \n Patients of an outpatient diabetes clinic are complaining about long waiting times, this seems to have an adverse effect on patient satisfaction and patient retention. Today, one doctor runs the outpatient clinic. Hospital management has decided that in order to increase patient retention rates, a nurse will be hired to assist and take over some of the doctor\u2019s tasks. This should result in decreased waiting times and, hopefully, increased patient retention rates. \n \nThe reasoning of the hospital management seems sound, however, if this investment does not result in shorter waiting times it will have been an expensive experiment. And that\u2019s exactly where discrete-event simulation comes into play. It can simulate the current situation, the alternative scenario and measure the variables under scope (in this case waiting time). This will enable hospital management to make a more substantiated investment decision. \n The tables below show the information we have at hand. The time refers to the number of minutes the process takes to finish. Furthermore, patient appointments are scheduled with an interval of 20 minutes. \n \n \n Scenario \n Resource \n Mininum time \n Mode time \n Maximum time \n \n \n \n Present \n Doctor \n 10 \n 20 \n 30 \n \n\n New \n Doctor \n 5 \n 15 \n 20 \n \n\n New \n Nurse \n 5 \n 8 \n 10 \n \n \n For the simulation model we will make a slight abstraction of actual process variations and use triangular time distributions to reflect the process time: \n \n When starting to build the simulation model it is important to have a clear understanding on the variables which need to be measured. In this case the focus will be on; \n \n Waiting times \n Value adding time (time the patient is actually with the doctor / nurse) \n Total length-of-stay \n \n To build the simulation model, SimPy will be used. SimPy is a Python module that can be used to easily build discrete-event simulation models. Check out the SimPy website for a definition, documentation, install instructions and examples. \n The script below shows an implementation of our two scenarios. The Clinic class is the actual simulation model from where patients will be created and an observation monitor is instantiated. The Patient class represents the patient and it\u2019s process through the system. In this class two alternate scenario are distinguished; if alternate=False the process will represent the current process, if alternate=True the patient process will resemble the proposed new process. In this case we assume that a timeunit in \u201csimulated time\u201d refers to 1 minute in real life. \n \nfrom SimPy.Simulation import *\nimport random\nimport csv\n\nclass Clinic(Simulation):\n\tdef __init__(self,alternative=False):\n\t\tSimulation.__init__(self)\n\t\tself.alternative=alternative\n\n\tdef runModel(self):\n\t\tself.initialize()\n\n\t\t#create a monitor to observe LOS & value-adding time\n\t\tself.PROCmonitor=Monitor()\n\n\t\tself.nurse=Resource(name='Nurse',capacity=1,sim=self)\n\t\tself.doctor=Resource(name='Doctor',capacity=1,sim=self)\n\n\t\t#a new patients arrives every 15 minutes for 8 hours\n\t\tfor p in range(0,8*60,15):\n\n\t\t\tpatient=Patient(name=\"Patient %s\"%p,sim=self)\n\t\t\tself.activate(patient,patient.run(),at=p)\n\n\t\t#simulate for 5 hours\n\t\tself.simulate(until=8*60)\n\nclass Patient(Process):\n def run(self):\n\t\tstarttime=self.sim.now() #save a reference to the 'creation time' of the patient\n\n\t\t#if alternative=False, simulate current process\n\t\tif not self.sim.alternative:\n\t\t\t#get a time from the triangular distribution\n\t\t\tdoc_time=random.triangular(10,30,20)\n\n\t\t\t#request a doctor\n\t\t\tyield request,self,self.sim.doctor\n\n\t\t\t#start the doctor process with a duration of 'doc_time'\n\t\t\tyield hold,self,doc_time\n\n\t\t\t#release the doctor after the process finishes\n\t\t\tyield release,self,self.sim.doctor\n\n\t\t\t#observe LOS, and value-adding time (waitingtime can be inferred from this later on)\n\t\t\tself.sim.PROCmonitor.observe([self.sim.now()-starttime, doc_time],t=starttime)\n\n\t\t#if alternative=True, simulate proposed new process\n\t\telse:\n\t\t\tdoc_time=random.triangular(5,20,15)\n\t\t\tnurse_time=random.triangular(5,10,8)\n\t\t\tyield request,self,self.sim.nurse\n\t\t\tyield hold,self,nurse_time\n\t\t\tyield release,self,self.sim.nurse\n\n\t\t\tyield request,self,self.sim.doctor\n\t\t\tyield hold,self,doc_time\n\t\t\tyield release,self,self.sim.doctor\n\n\t\t\tself.sim.PROCmonitor.observe([self.sim.now()-starttime, doc_time+nurse_time],t=self.sim.now())\n\n#create a list to collect results in\nresults=[]\n\ns=Clinic(alternative=False)\ns.runModel()\nfor t in s.PROCmonitor:\n\tresults.append(['Current',t[0]]+t[1])\n\ns=Clinic(alternative=True)\ns.runModel()\nfor t in s.PROCmonitor:\n\tresults.append(['Proposed',t[0]]+t[1])\n\n#write out results to a CSV file\nwriter=csv.writer(open('model_results.csv','wb') ,quoting=csv.QUOTE_NONNUMERIC)\nwriter.writerow(['Scenario','Time','LOS','VAtime'])\nfor r in results:\n\twriter.writerow(r)\n \n In the last few lines of the sourcecode, the results are written to a CSV file. Below, a few lines of the CSV file are shown. \n \n \n Scenario \n Time \n LOS \n time \n \n \n \n Current \n 0.0 \n 18.0451324578 \n 18.0451324578 \n \n\n Current \n 15 \n 21.7876475147 \n 18.7425150569 \n \n\n Proposed \n 20.7649485452 \n 20.7649485452 \n 20.7649485452 \n \n\n Proposed \n 35.2039187114 \n 20.2039187114 \n 20.2039187114 \n \n \n Now, let\u2019s move to R. Please note, result analysis could just as well be done using SimPy\u2019s built in plotting features or by using other Python modules \u2013 e.g. matplotlib \u2013 but I just love ggplot2 . \n To visually analyse the results, the following script is used; \n \nlibrary(ggplot2)\nlibrary(reshape)\nlibrary(gridExtra)\n\nresults<-read.csv('model_results.csv')\n\n#infer waitingtimes from LOS & valueadding time\nresults$waittime<-results$LOS-results$VAtime\nresults.melt<-melt(results,id=c('Scenario','Time'))\n\np1<-ggplot(results,aes(x=Scenario,y=waittime,fill=Scenario))+ geom_boxplot()+scale_fill_brewer(palette='Set1')+ opts(title='Waiting time',legend.position='none')+ylab('Time (m)')\n\np2<-ggplot(results,aes(x=Scenario,y=LOS,fill=Scenario))+ geom_boxplot()+scale_fill_brewer(palette='Set1')+ opts(title='Length-of-stay',legend.position='none')+ylab('Time (m)')\n\np3<-ggplot(results,aes(x=Scenario,y=VAtime,fill=Scenario))+ geom_boxplot()+scale_fill_brewer(palette='Set1')+ opts(title='Value-adding time',legend.position='none')+ylab('Time (m)')\n\ngrid.arrange(p1,p2,p3,ncol=3)\n \n These are the results; \n We can see here that the value adding time (time the patient is in contact with a doctor/nurse) increases slightly. However, waiting times and length of stay decreases tremendously. We can also note that process varation decreases. \n  \n The following graph shows the evolution of waiting times during the day. \n \nggplot(results,aes(x=Time/60,y=waittime,group=Scenario,color=Scenario)) + geom_line(size=1)+scale_colour_brewer(palette='Set1')+ opts(title='Waiting times during simulation',legend.position='bottom')+ ylab('Waiting time (m)')+ xlab('Simulation time (hr)')\n \n  \n If the main goal is to decrease waiting times, the proposed scenario seems a more than viable alternative. Ofcourse, in this simple example we do not take into account increased personnel costs, potential architectural costs, etc. These \u201cextra\u201d variables should be factored in during the development of the model or during the result analysis. \n Please note, in this example the model is only run once per scenario. However, in order to correctly interpret the effect process variation has on the variables under scope, it is essential that the model is run multiple times and analyzed accordingly. \n The post Using discrete-event simulation to simulate hospital processes appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/using-simpy-to-simulate-business-processes/?utm_source=rss&utm_medium=rss&utm_campaign=using-simpy-to-simulate-business-processes", "bloglinks": {}, "links": {"http://simpy.sourceforge.net/": 1, "http://fishyoperations.com": 1, "http://fishyoperations.com/": 4, "http://matplotlib.sourceforge.net/": 1, "http://had.co.nz/": 1}, "blogtitle": "FishyOperations"}, {"content": ["This post shows how to create a timeline graph by using ggplot2 . \n Let\u2019s start by loading the ggplot2 library. \n \nlibrary(ggplot2)\n \n \nNext let\u2019s create a dataset which we will use to feed the graph. In the last column (y), I create random positive values for the first three rows (which will be shown above the timeline) and random negative values for the last three rows (to be shown below the timeline). \n \ntimeset<-data.frame(year=c(1986,1995,2011,1990,1998,2010),text=c('I was born','Had a nice icecream','Spotted a dodo','First swim','Crashed my bicycle','Bought a helmet'),y=c(runif(3,.5,1),runif(3,-1,-.5)))\n \n Now comes the tricky part, creating the graph. \n \nplot<-ggplot(timeset,aes(x=year,y=0))\n \n Let\u2019s add the lines which will run from the timeline arrow to the text label. \n \nplot<-plot+geom_segment(aes(y=0,yend=y,xend=year))\n \n The text is added as follows; \n \nplot<-geom_text(aes(y=ytext,label=text),size=2.5,vjust=-1)\n \n Now to fancy it up a bit with some points at the end of the line segments; \n \nplot<-plot+geom_point(aes(y=y))\n \n Putting some limits on the y-axis. \n \nscale_y_continuous(limits=c(-2,2))\n \n As the y-values are, in this case, non-informative, let\u2019s hide \u2018em. \n \nplot<-plot+scale_y_continuous(limits=c(-2,2))\n \n Let\u2019s draw the timeline arrow . I agree this, is fiddling, and these values can possibly be extracted from the x-axis. \n \nplot<-plot+geom_hline(y=0,size=1,color='purple') #draw a vertical line\nplot<-plot+geom_segment(x=2011.4,xend=2012.2,y=.2,yend=0,color='purple',size=1) + geom_segment(x=2011.4,xend=2012.2,y=-.2,yend=0,color='purple',size=1) #drawing the actual arrow\n \n Now to finish it up, let\u2019s set some options. \n \nplot<-plot+opts(axis.text.y =theme_blank(),title='My timeline')+ylab('')+xlab('')\n \n Let\u2019s have a look at the result! \n \nprint(plot)\n \n  \n You can download the script here \n The post Timeline graph with ggplot2 appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/timeline-graph-with-ggplot2/?utm_source=rss&utm_medium=rss&utm_campaign=timeline-graph-with-ggplot2", "bloglinks": {}, "links": {"http://had.co.nz/": 1, "http://fishyoperations.com": 1, "http://fishyoperations.com/": 3}, "blogtitle": "FishyOperations"}, {"content": ["Welcome to this demo of how R code and results can be combined into an HTML report. This entire blogpost was generated by using a combination of R, knitr and markdown. \n Beforehand, make sure you have the following libraries installed (latest version); \n \n knitr \n markdown \n ggplot2 (to run the example script) \n \n Syntax references for markdown can be found here . \n Let's make two separate files, a .R file and an .Rmd file . \n \n The .R file \n The .R file should be used to load necessary libraries, define and transform data and, possibly, to define the plots beforehand . After a useful comment from Yihui I recommend including the code to define the dataset and create the plot within the .Rmd file. This in order to create a self-contained document that can be reproduced without special instructions. As, and I quote from the comment below: 'a reproducible report should be self-contained and not rely on external objects'. \n My .R file looks as follows: \n library(knitr)\n\n#transform the .Rmd to a markdown (.md) file and convert it to html\nknit2html('r-knitr-markdown.Rmd',fragment.only=TRUE)\n \n When using the output in a blogpost it is important to set the 'fragment.only' option to TRUE, otherwise it will generate a full-fledged HTML page (HTML tags / CSS / etc.). \n In the .Rmd file, the following code is used to start a so called R 'chunk'. A 'chunk' is a place in the .Rmd file where R code will be run and evaluated. An R chunk is defined as follows; \n \n Let's try this and check out the result. \n # your R code goes here, e.g.\n1 + 1\n \n ## [1] 2\n \n Let's create a dataset and define a plot for testing purposes. \n # define the dataset\ndataset <- data.frame(x = seq(1, 100), y = runif(100))\n\n# let's load the ggplot2 library\nlibrary(ggplot2)\n\n# define the plot to be used in the final HTML report\ngraph <- ggplot(dataset, aes(x = x, y = y)) + geom_line(colour = \"red\")\n \n What if we print the plot thas was defined in the 'graph' variable? This would be the result; \n # show the graph\ngraph\n \n \n There are a bunch of options available to e.g. hide the printout of the R script (echo) or to hide warning messages. Also the size of the graph output can be adjusted. All these option are explained on Yihui's knitr website . \n I hope this gives an idea of how R can be used to generate reproducible data analysis reports, easy-to-update business intelligence reports and of course blog posts. \n The files used to produce this output can be found here; \n \n R script \n Rmd file \n \n The post R, knitr & markdown = HTML appeared first on FishyOperations ."], "link": "http://fishyoperations.com/r/r-knitr-markdown-html/?utm_source=rss&utm_medium=rss&utm_campaign=r-knitr-markdown-html", "bloglinks": {}, "links": {"http://daringfireball.net/": 1, "http://yihui.name/": 1, "http://fishyoperations.com": 1, "http://fishyoperations.com/": 5}, "blogtitle": "FishyOperations"}]