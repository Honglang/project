[{"blogurl": "http://mirror2image.wordpress.com\n", "blogroll": [], "title": "Mirror Image"}, {"content": ["If you are an unfortunate person who have to use Windows 7 a lot (like me) undeletable files happen to you from time to time. It could be result of disk failure, or interrupted boot, or just for no reason at all. \nThe problem is, sometimes they can\u2019t be deleted even by Administrator. To fix this Administrator have have to \u201ctake ownership\u201d of the file in question. \nIt goes like this: \n1. run from the command line \nchkdsk /f \nThis step is necessary because file permissions could be corrupted \n2. login as Administrator. \n3. from the command line \ntakeown /f full_directory_and_path_name \n4. from the command line \nicacls full_directory_and_path_name /GRANT ADMINISTRATORS:F \n5.Now Administrator can delete file. If even that is not helping you can try delete from Safe mode."], "link": "http://mirror2image.wordpress.com/2012/03/01/deleting-files-wich-couldnt-be-deleted-by-administrator-in-win-7/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7}, "blogtitle": "Mirror Image"}, {"content": ["This post is inspired by Extremal Principles in Classical, Statistical and Quantum Mechanics in Azimuth blog. \n Total Variation used a lot in image processing. Image denoising , optical flow, depth maps processing. The standard form of Total Variation f or norm is minimizing \u201cenergy\u201d of the form \n \n(I\u2019m talking about Total Variaton- for now, not ) over all functions \nIn case of image denoising it would be \n \nwhere is original image and is denoised image \nPart is called \u201cfidelity term\u201d and is \u201cregularizer\u201d \nRegularizer part is to provide smoothness of solution and fidelity term is to force smooth solution to resemble original image (that is in case of image denoising) \nNow if we return to classical Action , movement of the point is defined by the minimum of functional \n , over trajectories where is kinetic energy and is potential energy, or \n \n One-dimensional total variation for image denoising is the same as classical mechanics of the particle, with potential energy defined by iamge and smoothness of denoised image as kinetic energy! For optical flow potential energy is differences between tranformed first image and the second \n and kinetic energy is the smoothness of the optical flow. \nOf cause the strict equality hold only for one-dimentional image and , and potential energy is quite strange \u2013 it depend not on coordinate but on velocity, like some kind of friction. \nWhile it hold some practical meaning, most of practical task have two or more dimensional image and or regulariser. So in term of classical mechanics we have movement in multidimensional time with non-classical kinetic energy \n \nwhich has uncanny resemblance to Lagrangian of relativistic particle \n \n So total variation in image processing is equivalent to physics of non-classical movement with multidimensional time, in the field with potential energy defined by image. I have no idea what does it signify, but it sounds cool . Holographic principle ? May be crowd from Azimuth or n-category cafe will give some explanation eventually\u2026 \nAnd another, related question: regularizer in Total Variation. There is inherent connection between regularizers and Bayesian priors. What TV-L1 regularizer mean from Bayesian statistics point of view? \n PS I\u2019m posting mostly on my google plus now, so this blog is a small part of my posts."], "link": "http://mirror2image.wordpress.com/2012/01/14/total-variation-in-image-processing-and-classical-action/", "bloglinks": {}, "links": {"http://johncarlosbaez.wordpress.com/": 3, "http://feeds.wordpress.com/": 7, "https://plus.google.com/": 1, "http://en.wikipedia.org/": 5, "http://golem.utexas.edu/": 1}, "blogtitle": "Mirror Image"}, {"content": ["Something I did for Samsung (kernel of tracker). Biggest improvement in SARI 1.5 is the sensors fusion, which allow for a lot more robust tracking. \nHere is example of run-time localization and mapping with SARI 1.5: \n  \n This is the AR EdiBear game ( free in Samsung apps store )"], "link": "http://mirror2image.wordpress.com/2011/11/14/samsung-sari-1-5-augmented-reality-sdk-is-out-in-the-wild/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://www.samsungapps.com/": 1, "http://mirror2image.wordpress.com/": 1}, "blogtitle": "Mirror Image"}, {"content": ["Interior point method for nonlinear optimization often considered as complex, or highly nontrivial etc. The fact is, that for \u201csimple\u201d nonlinear optimization it\u2019s quite simple, manageable and can even be explained in #3tweets. For those not familiar with it there is a simple introduction to it in wikipedia , which in turn follows an excellent paper by Margaret H. Wright . \nNow about \u201cif all you have is a hammer, everything looks like a nail\u201d. Some of applications of interior point method could be quite unexpected. \n Everyone who worked with Levenberg-Marquardt minimization algorithm know how much pain is the choice of the small parameter . Levenberg-Marquardt can also be seen as modification of Gauss-newton with a trust region . The of Levenberg-Marquardt do correspond to the trust region radius, but that dependence is highly complex and is difficult to estimate. You want trust region of the radius r , but what should be avlue of ? There is no easy answer to that question; there are some complex methods, or there is a testing with subdivision, which is what the original Levenberg-Marquardt implement. \nInterior point can help here. \nIf we choose shape of trust region for Gauss-Newton as hypercube or simplex or like, we can formulate it as set of L1 norm inequality constrains. And that is the domain of interior point method! For hypercube the resulting equations looks especially nice \n \n W \u2013 hessian, I \u2013 identity, diag \u2013 diagonal \nThat is a banded arrowhead matrix , and for it Cholesky decomposition cost insignificantly more than decomposition of original W . The matrix is not positive definite \u2013 Cholesky without square root should be used. \nNow there is a temptation to use single constrain instead of set of constrain , but that will not work. should have to be linearized to be tractable, but it\u2019s a second order condition \u2013 it\u2019s linear part is zero, so linearization doesn\u2019t constrain anything. \nThe same method could be used whenever we have to put constrain on the value of Gauss-Newton update, and shape of constrain in not important (or polygonal) \nNow last touch \u2013 Interior point method has small parameter of it\u2019s own. It\u2019s called usually . In the \u201cnormal\u201d method there is a nice rule for it update \u2013 take it as (in the notation from wikipedia article \u2013 is a value of constraint, is a value of slack variable at the previous iteration) That rule usually explicitly stated in the articles about Interior Point Method(IPM) for Linear Programming, but omitted (as obvious probably) in the papers about IPM for nonlinear optimization \nIn our case (IPM for trust region) we don\u2019t need update at all \u2013 we move boundary of the region with each iteration, so each is an initial value. Have to remember, is not a size of trust region, but strength of it\u2019s enforcement."], "link": "http://mirror2image.wordpress.com/2011/09/04/interior-point-method-if-all-you-have-is-a-hammer/", "bloglinks": {}, "links": {"http://citeseerx.psu.edu/": 1, "http://feeds.wordpress.com/": 7, "http://en.wikipedia.org/": 7}, "blogtitle": "Mirror Image"}, {"content": ["I seldom post in the this blog now, mostly because I\u2019m positing on twitter and G+ a lot lately. I still haven\u2019t figured out which post should go where \u2013 blog, G+ or twitter, so it\u2019s kind of chaotic for now. \nWhat of interest is going on: There are two paper on the CVPR11 which claim that compressed sensing(sparse recovery) is not applicable to some of the most important computer vision tasks: \n Is face recognition really a Compressive Sensing problem? \n Are Sparse Representations Really Relevant for Image Classification? \nBoth paper claim that space of the natural images(or their important subsets) are not really sparse. \nThose claims however dont\u2019t square with claim of high effectiveness of compact signature of Random Ferns . \nCould both of those be true? In my opinion \u2013 yes. Difference of two approaches is that first two paper assumed explicit sparsity \u2013 that is they enforced sparsity on the feature vector. Compressed signature approach used implicit sparsity \u2013 feature vector underling the signature is assumed sparse but is not explicitly reconstructed. Why compressed signature is working while explicit approach didn\u2019t? That could be the case if image space is sparse in the different coordinate system \u2013 that is here one is dealing with the union of subspaces . Assumption not of the simple sparsity, but of the union of subspaces is called blind compressed sensing . \nNow if we look at the space of the natural images it\u2019s easy to see why it is low dimensional. Natural image is the image of some scene, an that scene has limited number of moving object. So dimension of space images of the scene is approximately the sum of degree of freedom of the objects(and camera) of the scene, plus effects of occlusions, illumination and noise. Now if the add strong enough random error to the scene, the image is stop to be the natural image(that is image of any scene). That mean manifold of the images of the scene is isolated \u2013 there is no natural images in it\u2019s neighborhood. That hint that up to some error the space of the natural images is at least is the union of isolated low-dimensional manifolds . The union of mainfolds is obviously is more complex structure than the union of subspace, but methods of blind compressed sensing could be applicable to it too. Of cause to think about union of manifolds could be necessary only if the space of images is not union of subspace, which is obviously preferable case"], "link": "http://mirror2image.wordpress.com/2011/08/06/effectivness-of-compressed-sensing-in-image-processing-and-other-staff/", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://mirror2image.wordpress.com/": 1, "https://plus.google.com/": 1, "http://cvlab.epfl.ch/": 1, "http://users.edu.au/": 1, "http://feeds.wordpress.com/": 7, "http://en.wikipedia.org/": 1, "http://twitter.com/": 1, "http://www.cvpapers.com/": 1}, "blogtitle": "Mirror Image"}, {"content": ["I once asked, what\u2019s 3d registration/reconstruction/pose estimation is about \u2013 optimization or statistics? The more I think about it, the more I convinced it\u2019s at least 80% statistics. Often specifically optimization tricks like Tikhonov regularization have statistical underpinning. Stability of optimization is robust statistics(Yes I know, I repeat it way too often). Cost function formulation is a formulation for error distribution and define convergence speed. \n Now unrelated(almost) AR stuff: \nI already mentioned on Twitter that version of markerless tracker for which I did a lot of work is part of Samsung AR SDK (SARI) for Android and Bada. It was was shown at AP2011 (Presentaion and also include nice Bada code). AR SDK presentation is here . \nSome videos form presentation \u2013 Edi Bear game demo with non-reference tracking at the end of the video and less trivial elements of SLAM tracking . Other application of SARI SDK \u2013 PBI (This one seems use earlier version)."], "link": "http://mirror2image.wordpress.com/2011/07/04/stuff-and-ar/", "bloglinks": {}, "links": {"http://www.slideshare.net/": 1, "http://s-a-m-m-i.blogspot.com/": 1, "http://feeds.wordpress.com/": 7, "http://en.wikipedia.org/": 2, "http://www.youtube.com/": 3}, "blogtitle": "Mirror Image"}, {"content": ["I\u2019m Achilles! \nI\u2019m a turtle \nI\u2019m Spartacus! \nI\u2019m a turtle \nI think therefore I am! \nI\u2019m a turtle \nI\u2019m ClearCase! \nI\u2019m a turtle \nI am the alpha and the omega! \nI\u2019m a turtle \n via xkcd"], "link": "http://mirror2image.wordpress.com/2011/05/07/xkcd-turtles/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://xkcd.com": 1}, "blogtitle": "Mirror Image"}, {"content": ["Cauchy estimator have some nice properties (Gonzales et al \u201cStatistically-Efficient Filtering in Impulsive Environments: Weighted Myriad Filter\u201d 2002): \nBy tuning in \n \nit can approximate either least squares (big ), or mode \u2013 maximum of histogram \u2013 of sample set (small ). For small estimator behave the same way as power law distribution estimator with small . \nAnother property is that for several measurements with different scales estimator of their sum will be simple \n \nwhich is convenient for estimation of random walks \n I heard convulsion in the sky, \nAnd flight of angel hosts on high, \nAnd monsters moving in the deep \n Those verses from The Prophet by A.Pushkin could be seen as metaphor of profound mathematical insight, encompassing bifurcations , higher dimensional algebra and murky depths of statistics. \nI now intend to dive deeper into of statistics \u2013 toward \u201cdata depth\u201d. Data depth is a generalization of median concept to multidimensional data. Remind you that median can be seen either as order parameter \u2013 value dividing the higher half of measurements from lower, or geometrically, as the minimum of norm. Second approach lead to geometric median , about which I already talked about . \n First approach to generalizations of median is to try to apply order statistics to multidimensional vectors.The idea is to make some kind of partial order for n-dimensional points \u2013 \u201cdepth\u201d of points, and to choose as the analog of median the point of maximum depth. \nBasically all data depth concepts define \u201cdepth\u201d as some characterization of how deep points are reside inside the point cloud. \nHistorically first and easiest to understand was convex hull approach \u2013 make convex hull of data set, assign points in the hull depth 1, remove it, get convex hull of points remained inside, assign new hull depth 2, remove etc.; repeat until there is no point inside last convex hull. \nLater Tukey introduce similar \u201chalfspace depth\u201d concept \u2013 for each point X find the minimum number of points which could be cut from the dataset by plane through the point X. That number count as depth(see the nice overview of those and other geometrical definition of depth at Greg Aloupis page ) \nIn 2002 Mizera introduced \u201cglobal depth\u201d, which is less geometric and more statistical. It start with assumption of some loss function (\u201ccriterial function\u201d in Mizera definition) of measurement set . This function could be(but not necessary) cumulative probability distribution. Now for two parameters and , is more fit with respect if for all  . is weakly optimal with respect to if there is nor better fit parameter with respect to . At last global depth of is the minimum possible size of such that is not weakly optimal with respect to \u2013 reminder of measurements. In other words global depth is minimum number of measurements which should be removed for stop being weakly optimal. Global depth is not easy to calculate or visualize, so Mizera introduce more simple concept \u2013 tangent depth . \nTangent depth defined as . What does it mean? Tangent depth is minimum number of \u201cbad\u201d points \u2013 such points that for specific direction loss function for themis growing. \nThose definitions of \u201cdata depth\u201d allow for another type of estimator, based not on likelihood, but on order statistics - maximum depth estimators . The advantage of those estimators is robustness( breakdown point ~25%-33%) and disadvantage \u2013 low precision ( high bias ). So I wouldn\u2019t use them for precise estimation, but for sanity check or initial approximation. In some cases they could be computationally more cheap than M-estimators. As useful side effect they also give some insight into structure of dataset(it seems originally maximum depth estimators was seen as data visualization tool). Depth could be good criterion for outliers rejection. \nDisclaimer: while I had very positive experience with Cauchy estimator, data depth is a new thing for me.I have yet to see how useful it could be for computer vision related problems."], "link": "http://mirror2image.wordpress.com/2011/05/02/robust-estimators-iii-into-the-deep/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://mirror2image.wordpress.com/": 1, "http://en.wikipedia.org/": 10, "http://www.google.com/": 1, "http://cgm.mcgill.ca/": 1}, "blogtitle": "Mirror Image"}, {"content": ["In this post I was complaining that I don\u2019t know what breakdown point for redescending M-estimators is. Now I found out that upper bound for breakdown point of redescending of M-estimators was given by Mueller in 1995, for linear regression (that is statisticians word for simple estimation of p-dimensional hyperplane): \n \n \u2013 number of measurements and is little tricky: it is a maximum number of measurement vectors X lying in the same p-dimensional hyperplane. If number of measurements N >> p that mean breakdown point is near 50% \u2013 You can have half measurement results completely out of the blue and estimator will still work. \nThat only work if the error present only in results of measurements, which is reasonable condition \u2013 in most cases we can move random error from x part to y part. \nNow which M-estimators attain this upper bound? \nThe condition is \u201cslow variation\u201d(Mizera and Mueller 1999) \n \nMentioned in previous post Cauchy estimator is satisfy that condition: \n and its derivative \nIn practice we always work with , not so Cauchy estimator is easy to calculate. \nRule of the thumb: if you don\u2019t know which robust estimator to use, use Cauchy: It\u2019s fast(which is important in real time apps), its easy to understand, it\u2019s differentiable, and it is as robust as possible (that is for redescending M-estimator)"], "link": "http://mirror2image.wordpress.com/2011/04/19/robust-estimators-ii/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://mirror2image.wordpress.com/": 1, "http://en.wikipedia.org/": 3}, "blogtitle": "Mirror Image"}, {"content": ["This is continuation of my attempt to understand internal mechanics of robust statistics . First I want to say that robust statistics \u201cjust works\u201d. It\u2019s not necessary to have deep understanding of it to use it and even to use it creatively. However without that deeper understanding I feel myself kind of blind. I can modify or invent robust estimators empirically, but I can not see clearly the reasons, why use this and not that modification. \nNow about robust estimators. They could be divided into two groups: maximum likelihood estimators (M-estimators), which in case of robust statistics usually, but not always are redescending estimators (notable not redescending estimator is norm), and all the rest of estimators. \nThis second \u201call the rest\u201d group include subset of L-estimators (think of median, which is also M-estimator with norm.Yea, it\u2019s kind of messy), S-estimators (use global scale estimation for all the measurements) R-estimators, which like L-estimator use order statistics but use it for weights. There may be some others too, but I don\u2019t know much about this second group. \nIt\u2019s easy to understand what M-estimators do: just find the value of parameter which give maximum probability of given set of measurements. \n \nor \n \nwhich give us traditional M-estimator form \n \nor \n , \nPractically we are usually work not with measurements per se, but with some distribution of cost function of the measurements , so it become \n \nit\u2019s the same as the previous equation just defined in such a way as to separate statistical part from cost function part. \nNow if we make a set of weights it become \n \nWe see that it could be considered as \u201cnonlinear least squares\u201d, which could be solved with iteratively reweighted least squares \nNow for second group of estimators we have probability of joint distribution \n \nAll the global factors \u2013 sort order, global scale etc. are incorporated into measurements dependence. \nIt seems the difference between this formulation of second group of estimators and M-estimator is that conditional independence assumption about measurements is dropped. \nAnother interesting thing is that if some of measurements are not dependent on others, this formulation can get us bayesian network \n Now lets return to M-estimators. M-estimator is defined by assumption about probability distribution of the measurements. \nSo M-estimator and probabilistic distribution through which it is defined are essentially the same. Least squares, for example, is produced by normal (gausssian) distribution. Just take sum of logarithms of gaussian and you get least squares estimator. \nIf we are talking about normal (pun intended), non-robust estimator , their defining feature is finite variance of distribution. \nWe have central limit theorem which saying that for any distribution mean value of samples will have approximately normal (or Gaussian) distribution. \nFrom this follow property of asymptotic normality \u2013 for estimator with finite variance its distribution around true value of parameter approximate normal distribution. \nWe are discussing robust estimators, which are stable to error and have \u201cthick-tailed\u201d distribution, so we can not assume finite variance of distribution. \nNevertheless to have \u201ctrue\u201d result we want some form of probabilistic convergence of measurements to true value. As it happens such class of distribution with infinite variance exists. It\u2019s called alpha-stable distributions . \nAlpha stable distribution are those distributions for which linear combination of random variables have the same distribution, up to scale factor. From this follow analog of central limit theorem for stable distribution . \nThe most well known alpha-stable distribution is Cauchy distribution , which correspond to widely used redescending estimator \n \nCauchy distribution can be generalized in several way, including recent GCD \u2013 generalized Cauchy distribution( Carrillo et al ), with density function \n \nand estimator \n \nCarrillo also introduce Cauchy distribution-based \u201cnorm\u201d (it\u2019s not a real norm obviously) which he called \u201cLorentzian norm\u201d \n \n is correspond classical Cauchy distribution \nHe successfully applied Lorentzian norm based basis pursuit to compressed sensing problem, which support idea that compressed sensing and robust statistics are dual each other."], "link": "http://mirror2image.wordpress.com/2011/04/15/robust-estimators-understand-or-die-err-be-bored-trying/", "bloglinks": {}, "links": {"http://www.udel.edu/": 1, "http://feeds.wordpress.com/": 7, "http://en.wikipedia.org/": 22}, "blogtitle": "Mirror Image"}]