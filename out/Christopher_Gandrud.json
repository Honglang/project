[{"blogurl": "http://christophergandrud.blogspot.com\n", "blogroll": [], "title": "Christopher Gandrud"}, {"content": ["I really like this article by Amanda Licht in Political Analysis. It gives a lot of information on how to interpret nonproportional hazards and includes some nice graphs. Her source code is really helpful for learning the nuts and bolts of how to simulate quantities of interests over time. However, it's in Stata code, which doesn't really fit into my R-based workflow at the moment. So I decided to port the code over. This post gives an example of what I did. What is a non-proportional hazard & why use them? Here is my motivation for being interested in non-proportional hazards: In a few papers I used Cox Proportional Hazard (PH) models to examine countries' policy adoption decisions. The problem with Cox PH models is that they assume the risk of adopting a policy at each point in time changes proportionally. However, in my papers I had good reasons to suspect that this risk actually changed non-proportionally over time. To overcome these model-assumption-busting problems I included time interactions, a pretty common way of dealing with the problem. When you include time interactions, the variables' coefficients (\\( \\beta_{cc} \\)) are now a combination of the estimates for the base coeffiecient (\\( \\beta_{1} \\)) plus the time interaction (\\( \\beta_{2}(t) \\)): \\[ \\beta_{cc} = \\beta_{1} + \\beta_{2}(t) \\] You'll probably notice that the combined coefficient is different at each point in time \\( t \\). See Amanda's original article for a more detailed discussion of these issues. The Problem The problem I had before was conveying the substantive effect of the combined coefficient and our uncertainty about it over time. Licht's article solves these problems with Stata. How do we do it in R? Non-proportional Relative Hazards in R To demonstrate how to achieve these goals I'll recreate part of a figure from Licht's paper using R. This figure uses data/methods from an earlier paper by Golub & Steunenberg (2007) . You can download the data here (it's called GolubEUPdata.tab ). My goal is a figure plotting simulated values of the relative non-proportional hazard for the variable qmv between times 80 and 2000.  So, this is the figure I created:  The full R script for doing this is HERE .  Step-by-step These are the main steps I used to create the figure:  First, run the Cox PH model with time interactions using the coxph command from the survival package. Assign the output of this model to a new object (in my example I call the output object M1). Extract the coefficeient and variance-covariance matrices from the output object like this:  # Create coefficient matrix Coef <- matrix(M1$coefficients) # Create variance-covariance matrix VC <- vcov(M1)  Using the rmultnorm command from the MSBVAR package, simulate coefficients.  Drawn <- rmultnorm(n = 1000, mu = Coef, vmat = VC)  Clean up the data. For this example we only need two of the columns of simulated coefficients (numbers 1 & 13). We also need to create a simulation ID variable which will be important later.  # Keep qmv and Lqmv Drawn <- data.frame(Drawn[, c(1, 13)]) # Create Merge Variable (Simulation Number) Drawn$ID <- 1:1000  Now that we have the simulated coefficients, we can start calculate the simulated combined coefficient for qmv . This involves first creating a vector with the times over which we want to calculate the coefficients. We then combine this with the simulated data as in the equation above.  # Create Combined time interactionsCoefficient TVSim <- outer(Drawn[,2], Range) TVSim <- data.frame(melt(TVSim)) TVSim <- rename(TVSim, c(X1 = \"ID\", X2 = \"Time\", value = \"TVR\")) # Merge in the non-time interacted coefficient and combine TVSim <- merge(Drawn, TVSim, by = \"ID\") TVSim$CCqmv <- TVSim$qmv + TVSim$TVR  Now we can exponentiate the coefficient to get the relative hazard  # Create Combined Relative Hazard TVSim$HRqmv <- exp(TVSim$CCqmv)  Finally, after ordering the data by time, we can plot our simulated relative hazards over time.  # Order Variables TVSim <- TVSim[order(TVSim$Time),] # Graph Simulated Combined Hazard Ratios ggplot(TVSim, aes(Time, HRqmv)) + geom_point(shape = 21, alpha = I(0.01), colour = \"#FA9FB5\", size = 5) + geom_smooth() + geom_hline(aes(yintercept = 1), linetype = \"dotted\") + scale_y_continuous(breaks = c(-1, 0, 1, 3, 6)) + scale_x_continuous(breaks = c(0, 129), labels = c(80, 2000)) + xlab(\"Time in Days\") + ylab(\"Simulated QMV Relative Hazard\\n\") + ggtitle(\"Simulated Relative Hazard for QMV from times 80-2000\\n   Based on Licht (2011) Fig. 2\\n\") + theme_bw(base_size = 15)  Licht points out that the relative hazard is substantively interesting for dummy variables (which have values 0 and 1), but we probably want other approaches for continuous variables. Only a few modifications to the code above are required to implement these.  Thanks to Jeff Chweiroth for pointing me to Amanda Licht's article and motivating me to put together this code."], "link": "http://christophergandrud.blogspot.com/feeds/1853036335929035563/comments/default", "bloglinks": {}, "links": {"http://papers.ssrn.com/": 2, "http://people.sc.edu/": 1, "http://hdl.handle.net/": 2, "http://eup.sagepub.com/": 1, "http://en.wikipedia.org/": 1, "http://personal.ac.uk/": 1, "http://pan.oxfordjournals.org/": 1, "https://gist.github.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["The other day Critical Juncture put up an API for the Federal Register . I thought it would be great if there was a package that could use this API to download data directly into R (much like the excellent WDI package ). This would make it easier to analyse things like:  the frequency of regulations issued on a particular issue over a given period of time, the text of the actual regulations.  The nice people over at Critical Juncture tweeted me showing interest in the idea and wondering what would be useful. I was thinking that in the package there could be commands such as getFedRegister and getMultiFedRegister that would do pretty much do what the API is set up to help now, except download the data into an R object rather than straight to JSON or CSV. More Ideas? Any other ideas for things that might be useful? Just leave them in the comments at my Tumblr site ."], "link": "http://christophergandrud.blogspot.com/feeds/3336488466438594566/comments/default", "bloglinks": {}, "links": {"http://criticaljuncture.org/": 1, "http://cran.r-project.org/": 1, "https://www.federalregister.gov/": 1, "http://christophergandrud.tumblr.com/": 2}, "blogtitle": "Christopher Gandrud"}, {"content": ["Posts I recently put up at my miscellaneous things blog :  Dutch election results mapped!  Research on optical illusions and primary visual cortex size."], "link": "http://christophergandrud.blogspot.com/feeds/6505757916403247425/comments/default", "bloglinks": {}, "links": {"http://christophergandrud.tumblr.com/": 3}, "blogtitle": "Christopher Gandrud"}, {"content": ["Setting up a beamer slideshow is tedious. Creating new slideshows with the same header/footer/style files every week for your course lectures is very very tedious. To solve this problem I created a simple bash shell script. When you run the script in your terminal it asks whether you want to create a \"Lecture\" or \"Seminar\" and what number you want it to have. Then it does the rest.  You can find the script and all of the necessary files here .  To create the README file I used knitr version 0.8's new engine='bash' option. This allows you to knit bash code into your Markdown file the same what you would R code. It's pretty simple. See the R Markdown file for more details. Please feel free to take and modify the files. Also, if you can help streamline them that would be great. Oh kind of related tip: If you want a bash command to show up over more than one line in your knitted document place a backslash ( \\ ) at the end of the line.  The beamer theme I use is based on something I hammered together awhile ago. See this post for more details."], "link": "http://christophergandrud.blogspot.com/feeds/162818696553282605/comments/default", "bloglinks": {}, "links": {"http://christophergandrud.blogspot.kr/": 1, "https://github.com/": 2, "http://yihui.name/": 1, "http://en.wikipedia.org/": 2, "https://raw.github.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["In a recent paper on Federal Reserve inflation forecast errors ( summary blog post , paper ) I wanted a way to easily compare the coefficients for a set of covariates (a) estimated from different types of parametric models using (b) matched and non-matched data. I guess the most basic way to do this would be to have a table of columns showing point estimates and confidence intervals from each estimation model. But making meaningful comparisons with this type of table would be tedious. What I ended up doing was creating a kind of stacked caterpillar plot . Here it is:    Comparing 95% Confidence Intervals     I think this plot lets you clearly and quickly compare the confidence intervals estimated from the different models. I didn't include the coefficient point estimates because I was most interested in comparing the ranges. The dots added too much clutter. I have a link to the full replication code at the end of the post, but these are the basic steps:  I estimated the models using MatchIt and Zelig as per Ho et al. (2007) . I created new objects from the results of each model. I used the confint command to find the 95% confidence intervals. I did some cleaning up and rearranging of the confidence intervals, mostly using Hadley Wickham 's melt function in the reshape package . The basic idea is that to create the plots I needed a data set with columns for the coefficient names , the upper and lower confidence interval bounds , what parametric model the estimates are from, and whether the data set was matched or not . I removed the Intercept and sigma2 estimates for simplicity. I made the graph using ggplot2 . The key aesthetic decisions that I think make it easier to read are: (a) making the lines a bit thicker and (b) making the bands transparent. I liked making the bands transparent and stacking them rather than showing different lines for each set of estimates because this halved the number of lines in the plot. Makes it much crisper.  The full code for replicating this figure is on GitHub Note: this code depends on objects that are created as the result of analyses run using other source code files also on the GitHub site."], "link": "http://christophergandrud.blogspot.com/feeds/837643766642054901/comments/default", "bloglinks": {}, "links": {"http://papers.ssrn.com/": 1, "https://github.com/": 1, "http://had.co.nz/": 2, "http://gking.harvard.edu/": 2, "https://docs.google.com/": 1, "http://support.sas.com/": 1, "http://christophergandrud.blogspot.kr/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["Soon to be Hurricane Isaac has thrown off mine and my coauthor Cassie Grafstr\u00f6m 's plans to present a paper at the APSA Annual Conference . The paper is on whether or not US Federal Reserve staff bias their inflation forecasts based on the president's party. In lieu of the actual presentation I thought that I would at least post our slides (below).  In a couple earlier posts I showed some graphs indicating that there might be a presidential partisan bias to Fed inflation forecasts. The bias would look like this:   Democratic Presidents: Inflation is over-estimated.  Republican Presidents: Inflation is under-estimated.  In our paper we try to find out if this is just a coincidence or if inflation forecasts are really biased by the president's party identification. After running many different models we find that, yes Fed staff predict inflation will be higher than it actually is during Democratic presidencies and lower than it is during Republican presidencies.  This figure from the paper gives you a sense of how big we predict the inflation forecast errors will be under Republican and Democratic presidents.    Simulated Inflation Forecast Errors      0 = no forecasting error   Inflation is expected to be about 10% higher than it actually is during Democratic presidencies. It is expected to be about 20% lower than it actually is during Republican presidencies. I'll write another post describing how we made this graph and a few others in the paper. Now for the slides from our ill-fated APSA presentation (you can find the full paper here ):"], "link": "http://christophergandrud.blogspot.com/feeds/8619978772412723697/comments/default", "bloglinks": {}, "links": {"http://www.apsanet.org/": 1, "http://christophergandrud.blogspot.kr/": 3, "http://www.noaa.gov/": 1, "http://www.hertie-school.org/": 1, "http://papers.ssrn.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["I've been in China working for a few weeks (where this blog is (oddly) blocked). So, I haven't been able to post much over the summer. To kick things off for the new (academic) year, I thought I might just re-post something good I saw on the Book of Saturday blog. I think it was posted via Kieran Healy 's blog.  Regardless of the origin, it's pretty funny (see especially the second box from the right, top row)."], "link": "http://christophergandrud.blogspot.com/feeds/6321096713606324265/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://kieranhealy.org/blog": 1, "http://phnk.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["In previous posts I described how to input data stored on GitHub directly into R . You can do the same thing with source code stored on GitHub . Hadley Wickham has actually made the whole process easier by combining the getURL , textConnection , and source commands into one function: source_url . This is in his devtools package. Imagine we have a .R source code file like this: # Make cars scatter plot library(ggplot2) Plot <- qplot(cars$dist, cars$speed) +    theme_bw() print(Plot)  It is hosted on GitHub with the URL: https://raw.github.com/christophergandrud/christophergandrud.github.com/master/SourceCode/CarsScatterExample.R So to run this source code directly in R all we need to type is: library(devtools) SourceURL <- \"https://raw.github.com/christophergandrud/christophergandrud.github.com/master/SourceCode/CarsScatterExample.R\" source_url(SourceURL)   There you go. You can also directly source GitHub gists (which are nice for sharing short bits of code) with the source_gist command."], "link": "http://christophergandrud.blogspot.com/feeds/686769136951546510/comments/default", "bloglinks": {}, "links": {"https://github.com/": 2, "http://christophergandrud.blogspot.de/": 1, "http://had.co.nz/": 1, "http://christophergandrud.blogspot.kr/": 1, "https://raw.github.com/": 1, "https://gist.github.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["Felix Salmon had a nice piece the other day about the Jonah Lehrer self-plagiarism affair . (Basically a journalist, Jonah Lehrer, copied some things he had published elsewhere and posted them on his New Yorker blog .) Felix uses the controversy surrounding this event to write up four useful blogging rules for print journalists who also blog. I think these rules are probably also useful for academic researchers who also blog. To paraphrase Salmon\u2019s rules: \u2018\u2018Hey Look at This\u2019\u2019: Blogging is about reading rather than writing. Point others to something interesting you read. This can serve a very important function in academia, where without blogs and the like, most research is confined to low readership journals and conference presentations.  Link, Do Not Repeat: Because any content on the internet is just a link away, you never have to repeat it. The need not to repeat frees researchers up to add onto others work. It also fits in well into an established culture of citing other\u2019s work.  Blogs are Interactions, Not Just Primary Sources: Read, generously link to, summarise, and fill in the gaps of other blogs and web content.  This is another key part of the same process as rule 2. A Blog Post is the Beginning, Not the End: Use a blog to develop ideas. This is the one I like the most. Even if no one read my blog I would still write it. Writing the blog has become part of my learning process. Writing a post makes me sharpen my ideas and, in the case of technical posts, my skills. I also regularly go back to previous posts to remember how to do something. Of course the social aspect of blogging about research ideas further increases the benefits of blogging for researchers. A recent example is when I posted about using GitHub to host data. Someone wrote to me with a problem they were having with my code. So I then learned about how to solve this problem. Commenters to my solution post then shared even more information about the issue. I now know how to solve this problem so that I can get my research done, other people can also find the solution, and I have a record if I forget what the solution was."], "link": "http://christophergandrud.blogspot.com/feeds/8797378863316120339/comments/default", "bloglinks": {}, "links": {"http://blogs.reuters.com/": 2, "http://jimromenesko.com/": 1, "http://christophergandrud.blogspot.kr/": 2, "http://www.newyorker.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["A reader of my most recent post tried the R code I had written to download the data set of electoral disproportionality from the GitHub repository. However, it didn\u2019t work for them. After entering disproportionality.data <- getURL(url) they got the error message: Error in function (type, msg, asError = TRUE) : SSL certificate problem, verify that the CA cert is OK. Details: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed  The Solution The problem seems to be that they didn\u2019t have a certificate from an appropriate signing agent (see the RCurl FAQ page (near the bottom) for more information. If you are really interested in SSL verification this page from redhat is a place to look). The solution to this problem is pretty straightforward. As the RCurl FAQ page points out you can use the argument ssl.verifypeer = FALSE to skip certificate verification (effectively a man-in-the-middle attack). So, if you get the above error message just use this new code: library(RCurl) url <- \"https://raw.github.com/christophergandrud/Disproportionality_Data/master/Disproportionality.csv\" disproportionality.data <- getURL(url, ssl.verifypeer = FALSE)     disproportionality.data <- read.csv(textConnection(disproportionality.data))  That should work. Question I didn\u2019t originally mention this issue, because I didn\u2019t get it when I ran the code on my Mac. When I tried the code on a Windows machine I was able to replicate the error. Does any reader know why Windows computers (or any other types) lack certificates from an appropriate signing agent needed to download data from GitHub? How can you get one?"], "link": "http://christophergandrud.blogspot.com/feeds/7558239337008312714/comments/default", "bloglinks": {}, "links": {"http://docs.redhat.com/": 1, "http://gb.redhat.com/": 1, "http://christophergandrud.blogspot.com/": 1, "http://www.omegahat.org/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["Update (15 June 2012): See this post for instructions on how to download GitHub based data into R if you are getting the error about an SSL certificate problem .  GitHub is designed for collaborating on coding projects. Nonetheless, it is also a potentially great resource for researchers to make their data publicly available. Specifically you can use it to: store data in the cloud for future use (for free), track changes , make data publicly available for replication, create a website to nicely present key information about the data, and uniquely: benefit from error checking by the research community. This is an example of a data set that I\u2019ve put up on GitHub.  How? Taking advantage of these things through GitHub is pretty easy. In this post I\u2019m going to give a brief overview of how to set up a GitHub data repository . Note: I\u2019ll assume that you have already set up your GitHub account. If you haven\u2019t done this, see the instructions here (for set up in the command line) or here (for the Mac GUI program) or here (for the Windows GUI program). Store Data in the Cloud Data basically consists of two parts, the data and description files that explain what the data means and how we obtained it. Both of these things can be simple text files, easily hosted on GitHub: Create a new repository on GitHub by clicking on the New Repository button on your GitHub home page. A repository is just a collection of files.  Have GitHub create a README.md file.  Clone your repository to your computer.  If you are using GUI GitHub, on your repository\u2019s GitHub main page simply click the Clone to Mac or Clone to Windows buttons (depending on your operating system). If you are using command line git.  First copy the repository\u2019s URL. This is located on the repository\u2019s GitHub home page near the top (it is slightly different from the page URL). In the command line just use the git clone [URL] command. To clone the example data repository I use for this post type:  $ git clone https://github.com/christophergandrud/Disproportionality_Data.git Of course you can choose which directory on your computer to put the repository in with the cd command before running git clone .  Fill the repository with your data and description file.  Use the README.md file as the place to describe your data\u2013e.g. where you got it from, what project you used it for, any notes. This file will be the first file people see when they visit your repository.  To format the README.md file use Markdown syntax. Create a Data folder in the repository and save your data in it using some text format. I prefer .csv . You can upload other types of files to GitHub, but if you save it in a text-based format others can directly suggest changes and you can more easily track changes.  Commit your changes and push them to GitHub.  In GUI GitHub click on your data repository, write a short commit summary then click Commit & Sync . In command line git first change your directory to the data repository with cd . Then add your changes with $ git add . . This adds your changed files to the \u2018\u2018staging area\u2019\u2019 from where you can commit them. If you want to see what files were changed type git status -s .  Then commit the changes with: $ git commit -m \u2018a comment describing the changes\u2019 Then push the committed changes to GitHub with: $ git push origin master For more information see this git reference page .  Create a cover site with GitHub Pages . This creates a nice face for the data repository. To create the page:  Click the Admin button next to your repository\u2019s name on its GitHub main page. Under \u2018\u2018GitHub Pages\u2019\u2019 click Automatic Page Generator . Then choose the layout you like, add a tracking ID if you like, and publish the page.   Track Changes GitHub will now track every change you make to all files in the data repository each time you commit the changes. The GitHub website and GUI program have a nice interface for seeing these changes.  Replication Website Once you set up the page described in Step 5, other researchers can easily download the whole data repository either as a .tar.gz file or .zip . They can also go through your main page to the GitHub repository. Specific data files can be directly downloaded into R with the RCurl package (and textConnection from the base package). To download my example data into R just type:  library(RCurl)  url <- \"https://raw.github.com/christophergandrud/Disproportionality_Data/master/Disproportionality.csv\"  disproportionality.data <- getURL(url)      disproportionality.data <- read.csv(textConnection(disproportionality.data)) Note: make sure you copy the file\u2019s raw GitHub URL. You can use this to directly load GitHub based data into your Sweave or knitr file for direct replication.  Improve your data through community error checking GitHub has really made open source coding projects much easier. Anybody can view a project\u2019s entire code and suggest improvements. This is done with a pull request . If the owner of the project\u2019s repository likes the changes they can accept the request. Researchers can use this same function to suggest changes to a data set. If other researchers notice an error in a data set they can suggest a change with a pull request . The owner of the data set can then decide whether or not to accept the change. Hosting data on GitHub and using pull requests allows data to benefit the kind of community led error checking that has been common on wikis and open source coding projects for awhile. Git Resources Pro Git : a free book on how to use command line git. Git Reference : another good reference for command line git. github:help : GitHub\u2019s reference pages."], "link": "http://christophergandrud.blogspot.com/feeds/4789387859220657563/comments/default", "bloglinks": {}, "links": {"https://github.com/": 1, "http://mac.github.com/": 1, "http://gitref.org/": 2, "http://pages.github.com/": 1, "http://daringfireball.net/": 1, "http://windows.github.com/": 1, "http://support.google.com/": 1, "http://4.blogspot.com/": 1, "http://christophergandrud.blogspot.kr/": 1, "http://git-scm.com/": 1, "https://help.github.com/": 2, "http://christophergandrud.github.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["Tools for using R / RStudio as a one-stop shop for research and presentation have been coming out quickly. I think this one has a good shot of being included in future releases of RStudio : The other day I ran across a new R package called slidify by Ramnath Vaidyanathan . In previous posts I had been messing around with Pandoc and deck.rb to turn knitr Markdown files into HTML presentations. Slidify has two key advantages over these approaches:  it can directly convert .Rnw files in R into slideshows, i.e. no toggling between R and the Terminal, there are lots of slideshow options ( deck.js , dzslides , html5slides , shower , and slidy ).  It\u2019s not on CRAN yet, but it worked pretty well for me. The syntax is simple.  In the Markdown document demarcate new slides with --- (it has to be three dashes and there can\u2019t be spaces after the dashes). When you want to convert your .Rnw into a presentation just type: library(slidify) slidify(\"presentation.Rnw\")  The default style is html5slides . The package isn\u2019t that well documented right now, but to change to a different style just use framework . For example:  slidify(\"presentation.Rnw\", framework = \"deck.js\")  I used slidify to put together a slideshow that advertises an intro applied stats course I\u2019m teaching next semester. The slideshow is here . (You can see that I\u2019m trying to attract social science students who are reluctant to take a stats class). I sloppily removed the default Slidify logo by deleting the images folder in the html5slides folder slidify creates. PS Oh, also you might notice that I\u2019m using github to host the course. I hope to blog about this in the near future."], "link": "http://christophergandrud.blogspot.com/feeds/6232507409284318466/comments/default", "bloglinks": {}, "links": {"http://www.w3.org/": 1, "https://dl.dropbox.com/": 1, "http://paulrouget.com/": 1, "https://github.com/": 1, "http://rstats.posterous.com/": 1, "http://ramnathv.github.com/": 1, "http://christophergandrud.blogspot.kr/": 2, "http://christophergandrud.blogspot.com/": 1, "http://imakewebthings.com/": 1, "https://code.google.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["I just noticed that Markus Gesmann has a nice post on using RStudio , knitr , Pandoc , and Slidy to create slideshows. After my recent attempt to use deck.rb to turn a Markdown / knitr file into a deck.js presentation I caved in and also decided to go with Pandoc and Slidy . For me, Slidy produced the cleanest slides of the three formats that Pandoc supports. The presentation is here and the source is here . The only thing I really disliked was having to use <br /> or something similar to keep the text from bunching up at the top of the slides, which looked strange when projected onto a screen. You can customise Slidy CSS files, but I haven\u2019t got around to that yet. In this post I don\u2019t want to duplicate what Markus Gesmann has already done. Instead, I wanted to mention two things that I noticed/thought about while making my presentation:  The new MathJax syntax implemented in RStudio 0.96.227 doesn\u2019t seem to work with Pandoc. It just renders latex as if it was part of the equation rather than the qualifier to the equation begin delimiter. To get around this I just used the regular old $ $ and $$ $$ syntax. It\u2019s pretty easy to host presentations with Dropbox . Just make sure all of your files are in the same folder in your Public folder. If you want output from knitr to go into and be retrieved from someplace else, you can use the desired base URL for these files by adding this code after the Pandoc title information:  ```{r setup, echo=FALSE} opts_knit$set(base.url = \"\") ```  Where base.url = \"\" includes the URL of the folder you want the output stored in.  All items in a folder in Dropbox\u2019s Public folder have the same base URL.  I learned about base.url from Yihui Xie\u2019s source code for his knitr /Markdown example on github . He uses it to save and retrieve figures from other folders on github.  Extra: Pandoc Code I used the following Pandoc code in the Terminal to convert the .md file to Slidy: pandoc -t slidy leg_violence_present1.md -o leg_violence_present1.html -s -i -S --mathjax"], "link": "http://christophergandrud.blogspot.com/feeds/3193068095208960411/comments/default", "bloglinks": {}, "links": {"http://www.w3.org/": 1, "http://blog.rstudio.org/": 1, "http://johnmacfarlane.net/": 1, "http://dl.dropbox.com/": 2, "http://yihui.name/": 1, "https://github.com/": 1, "http://lamages.blogspot.com/": 1, "http://christophergandrud.blogspot.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["There has been some interest in the recent release of RStudio 0.96 and especially the ability to use combine its knitr Markdown functionality with Pandoc to integrate R and a variety of different documents types. I just wanted to add two quick things (one mostly aspirational, the other useful) Aspirational: Markdown/Ruby/deck.js I am currently using this combination to put together a presentation based on a recent working paper . Maybe out of procrastination I decided to see if there was any way to use knitr /Markdown to write a deck.js presentation. I generally prefer deck.js to the three Pandoc HTML presentation types ( slidy , S5 , and dzslides ). Deck.js presentations are a pain to write, so it would be great if there was a program like Pandoc that could quickly convert a Markdown file into a deck.js presentation. I discovered that there kind of is. There is a ruby program called deck.rb . The Markdown syntax is really simple and would be familiar to Pandoc users (individual slides are demarcated with the first level header # ). After you install deck.rb in the terminal with the usual:  sudo gem install deckrb you can easily build presentations in the command line with:  deck myPresentation.md  However, I\u2019ve classified this as aspirational since it lacks a lot of functionality that Pandoc has, including:  There really aren\u2019t title slides. The slideshow opens as a locally hosted webserver, and the command to build a stand alone HTML presentation doesn\u2019t seem to work that well (hence no example included with this post). It only allows you to use the Swiss template. I couldn\u2019t figure out how to easily get MathJax support to display equations.  Maybe I won\u2019t use use deck.rb for this presentation, but I will keep an eye on any developments. Useful Tip: Command Line/Go2Shell Since I\u2019m on about the terminal and command line, I thought I might mention a small (free) program that is very helpful: Go2Shell . It is a little Mac application that only opens a new terminal window from the folder that you currently have open. Very useful for easily setting your terminal working directory when, for example, making Pandoc presentations."], "link": "http://christophergandrud.blogspot.com/feeds/8878778073281449387/comments/default", "bloglinks": {}, "links": {"http://www.w3.org/": 1, "http://www.ruby-lang.org/": 1, "http://paulrouget.com/": 1, "http://johnmacfarlane.net/": 2, "http://www.mathjax.org/": 1, "http://rstudio.org/": 1, "http://ssrn.com/": 1, "https://github.com/": 1, "http://meyerweb.com/": 1, "http://imakewebthings.com/": 1, "http://alicedev.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["This is a rare non- R /my research-based post.    From Wikipedia  I\u2019ve been reading D. Graham Burnett \u2019s The Sounding of the Whale: Science and Cetaceans in the Twentieth Century . One particularly interesting piece of information, in this generally very interesting book for anyone interested in whales, science, the history of science, conservation, regulation, international agreements \u2026 is that when the early 20th century officials at the British Colonial Office tried to make sure that the (fairly meagre) whaling restrictions around South Georgia Island were being enforced: [the official was given] a brisk lesson in South Georgia realpolitik: the [enforcement officer] \u2018occupies two rooms in a cottage owned by [the main whaling company] and boards at the managers mess,\u2019 placing him \u2018in a most delicate and difficult position\u2019 when it came time to deliver sanctions; his nearest ally was some 800 miles of rough sea away\u2013and he had no boat."], "link": "http://christophergandrud.blogspot.com/feeds/1849530488163610768/comments/default", "bloglinks": {}, "links": {"http://www.uchicago.edu/": 1, "http://3.blogspot.com/": 1, "http://en.wikipedia.org/": 1, "http://www.princeton.edu/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["As Markus Gesmann recently pointed out , the new version of RStudio (0.96) has some really nice features for creating dynamic reports with Yihui Xie \u2019s knitr . You can integrate not just R and LaTeX , but also R and Markdown (as well as some other formats). If you haven\u2019t used Markdown before, it\u2019s basically a really simplified syntax for writing web content, though it can easily be converted not just to HTML but also LaTeX and other formats with Pandoc . See this post by Yihui Xie for a discussion of how to make HTML presentations with knitr and Pandoc. These programs make it much easier to create HTML presentations that display interactive R output from packages like googleVis (like I did in an earlier post ). I\u2019ve been using RStudio \u2019s new features in the preview version for a few weeks and it has been really great. It has made creating web content much easier. I\u2019ve even decided to pretty much move my entire introductory data analysis course to the web because I can create lecture notes and assignments with nice syntax highlighting and R output integration (especially interactive output). I remember a few years ago saying to my PhD supervisor that I thought it would someday be standard for theses to be written in HTML. Maybe I need to revise that slightly: theses may be displayed in HTML, but written in Markdown or (more specifically) MultiMarkdown (which has footnote and BibTeX integration). Recommendation A small program I really recommend purchasing if you are using RStudio with Markdown is Marked . RStudio has a Markdown previewer, but its capabilities are a bit limited. Marked gives you nicer previews with multiple styles to choose from, word counts, hyperlink validation, and some other stuff that definitely justifies its $3.99 price. To use Marked with RStudio just drag the .Rnw or .md file you're working on in RStudio on top of the Marked icon. It will update any time you save or compile the files. (Oh, note I think Marked is Mac only. Also, I have no affiliation with RStudio or Marked , I just really like them.)"], "link": "http://christophergandrud.blogspot.com/feeds/2797088354708948337/comments/default", "bloglinks": {}, "links": {"http://johnmacfarlane.net/": 1, "http://rstudio.org/": 1, "http://fletcherpenney.net/": 1, "http://yihui.name/": 3, "http://daringfireball.net/": 1, "http://markedapp.com/": 1, "https://github.com/": 1, "http://lamages.blogspot.com/": 1, "http://christophergandrud.blogspot.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["In my previous post I briefly mentioned an early draft of a working paper ( HERE ) I've written that looks into the possible causes of violence between legislators (like the violence shown in this picture from the Turkish Parliament).    From The Guardian    In this post I'm going to briefly discuss how I used Zelig 's rare events logistic regression ( relogit) and ggplot2 in R to simulate and plot the legislative violence probabilities that are in the paper. In this example I am plotting simulated probabilities at fitted values on three variables:   Age of democracy ( Polity IV > 5) A dichotomous electoral proportionality variable where 1 is high proportionality, 0 otherwise (see here for more details) Governing parties' majority (as a % of total legislative seats. Data is from DPI .)    Background   I used King and Zeng's rare events logistic regression which they include in their R  package Zelig to study incidences of legislative violence because (a) I was interested in a dichotomous outcome--whether or not a legislature had an incident of violence in a given year and (b) fortunately legislative violence is fairly rare. There are only 88 incidences in my data set spanning 1981 to Spring 2011 and even fewer (72) when I constricted the sample to 1981-2009, because there is limited data on many of my dependent variables after 2009.   Why GGPLOT2?   If you are familiar with the Zelig package, you'll know that it already includes a capability to both simulate quantities of interest (for me it's probabilities of violence given various values of the covariates) and plot the results from these simulations with uncertainty estimates.   To do this, first run the basic Zelig model then use setx() to set the range of covariate fitted values you are interested predicting probabilities for (all others are set to their means by default). Then use sim() to simulate the quantities of interest. Finally, just use plot() on the Zelig object that  sim() creates. (See the full code at the end of the post.)   However these plots are . . . not incredibly visually appealing. Here is an example with various ages of democracy:      Plus, if you are not using base R plots in the rest of your paper, these types of plots will clash.    I used ggplot2 graphs in the rest of the paper so I wanted a way to plot simulated probabilities with ggplot2. Basically I wanted this:        Using GGPLOT2 and Zelig Simulation Output.   Once you have the Zelig object returned from sim() it is simply a matter of extracting the simulation results. The default is to run 1,000 simulations for each fitted value. Zelig stores these in qi$ev in the Zelig object. In this example the fitted values of democratic age (fitted at years 0 through 85) are in a Zelig simulation object that I called Model.DemSim. To extract the simulations of the predicted probabilities use: # Extract expected values from simulations Model.demAge.e <- (Model.DemSim$qi) Now turn the object Model.demAge.e into a data frame and use melt() from Reshape2 to reshape the data so that you can use it in ggplot2. # Create data.frame Model.demAge.e <- data.frame (Model.demAge.e$ev)  # Melt data Model.demAge.e <- melt (Model.demAge.e, measure = 1 : 86 ) Since the numbers in Variable actually mean something (years of democracy) the final cleanup stage is to remove the \u201cX\u201d prefixes attached to Variable. # Remove 'X' from variable Model.demAge.e$variable <- as.numeric ( gsub ( \"X\" , \"\" , Model.demAge.e$variable)) Now we can use Model.demAge.e as the source of data for geom_point() and stat_smooth() in ggplot! You might want to drop simulation results outside the 2.5 and 97.5 percentiles to keep only the middle 95%. The red bars in the Zelig base plots represent the middle 95%. Right now I prefer keeping all of the simulation results and simply changing the alpha (transparency) of the points. This allows us to see all of the results, both outliers and those within widely accepted, but still somewhat arbitrary 95% bounds. Here is the full code for completely reproducing the last plot above (which is also in the working paper). The last thing to mention is that subsetting the data with complete.cases() to keep only observations with full data on all variables is a crucial step to make before running zelig() ."], "link": "http://christophergandrud.blogspot.com/feeds/5868899623167771179/comments/default", "bloglinks": {}, "links": {"http://papers.ssrn.com/": 1, "http://www.co.uk/": 1, "http://had.co.nz/": 1, "http://gking.harvard.edu/": 2, "http://1.blogspot.com/": 2, "https://github.com/": 1, "http://4.blogspot.com/": 1, "http://go.worldbank.org/": 1, "http://www.systemicpeace.org/": 1, "http://christophergandrud.blogspot.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["Update (6 May 2012): I've updated the framework a little since I first wrote this post. I've refined the focuses on majoritarian vs. consensual systems rather than fairness. Please see the most updated version of the paper at SSRN .    From: The Guardian   If you've ever wondered why physical fights sometimes break out between legislators, like the one in the above picture from the Ukrainian parliament, you might be interested in a working paper that I just put together. The working paper is called: \"Two Sword Lengths: Losers' Consent and Violence in National Legislatures\". The title refers to the rumor that in the UK House of Commons the government and opposition benches are two sword lengths apart to prevent actual duels. You can find a PDF version of the paper here . My main findings are that new democracies and unfair legislatures are more likely to have violence. Legislative unfairness means that the allocation of seats is disproportional and a smaller proportion of legislators are part of the governing majority. In new democracies losers many not have developed a reasonable expectation that they can become winners some day. Unfair legislatures both increase the proportion of legislators that are losers as well as heightening their sense of loss. For example, if your party wins 30% of the vote but only 20% of the seats and that 10% means the difference between being in government or out, you might be rather irritated and maybe more likely to attack governing party legislators who try to pass a piece of legislative you really dislike. Here is a summary table from the working paper of my framework and some illustrative examples.   I have some interesting graphs in the paper and plan to discuss how I made them in future posts."], "link": "http://christophergandrud.blogspot.com/feeds/5863261059222858041/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://papers.ssrn.com/": 1, "http://www.co.uk/": 1, "http://1.blogspot.com/": 1, "http://dl.dropbox.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["So I was hunting around for some data on disproportional electoral outcomes (when the proportion of voters cast for political parties is not close to the proportion of legislative seats that they win). Michael Gallagher keeps an updated version of his Least Squares (or Gallagher) Index of electoral disproportionality on his website , however it is in PDF format; very inconvenient for using in any stats project. John Carey & Simon Hix have some nice data--that includes much of Gallagher's data and some countries he doesn't cover--in easy to use Stata format ( here ). This is the data from their recent Electoral Sweet Spot paper (see here ). However it only goes to 2003. I combined the best of these two data sets into one .csv file and am making it available so that hopefully others can use their research time for better things than copying and pasting data from a PDF file. You can easily import this data into R or Stata or whatever you may use. The data set is downloadable HERE . More details on how I combined the data can be found there as well. I couldn't stop myself from making a few descriptive figures with the data. The first is a map of average disproportionality between 2000 and 2011. The second plots disproportionality over time (you can see there hasn't been much change).  Gallagher Electoral Disproportionality Averaged Over Elections from 2000 through 2011    As always, the R code:"], "link": "http://christophergandrud.blogspot.com/feeds/25247813069101856/comments/default", "bloglinks": {}, "links": {"http://www.dartmouth.edu/": 1, "http://www.tcd.ie/": 1, "http://onlinelibrary.wiley.com/": 1, "https://github.com/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["I have been meaning to post this slideshow for awhile now. It gives a brief introduction to using R for scraping text from multiple websites. It includes some basic debugging, because R sometimes misses a website. Just click the arrows to change the slides. Enjoy!"], "link": "http://christophergandrud.blogspot.com/feeds/4799962060470911412/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Christopher Gandrud"}, {"content": ["I ran across OpenCPU today. If you have any intest in R and reproducible research this is definitely worth checking out. Also, it looks like I might want to explore the potential of embedding functions in websites. Hm . . . ."], "link": "http://christophergandrud.blogspot.com/feeds/4541604211040895373/comments/default", "bloglinks": {}, "links": {"http://opencpu.org/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["Since I'm in the depths of PhD thesis revisions I haven't had much time to do much other than update previous posts (see my Stata Country Standardizer Update ). Here is an update of an earlier post about possible partisan biases in US Federal Reserve staff inflation forecasts (these influence Federal Open Market Committee meetings where US monetary policy is largely made). The new graph below allows us to see even more of what has been going on over time. The partisan effect is less obvious than in the earlier graph, but is is clear that during this time period the big over estimations are during Democratic presidencies and the big (actually almost all) underestimations are during Republican ones. The effect would be even stronger if we took out the end of Reagan's first term and his second one, where Fed staff may not have fully adjusted their forecasting to reflect the Volker-Greenspan era of moderate inflation. For more details about the graph (sources, how 'error' is defined, etc.) see the earlier post . Greenbook Inflation Forecast Errors, by Presidential Party   Note, the shaded area indicates minimal error.  The R Code:"], "link": "http://christophergandrud.blogspot.com/feeds/6571328213822195506/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://christophergandrud.blogspot.com/": 3}, "blogtitle": "Christopher Gandrud"}, {"content": ["I just updated my Stata do-file for standardizing country names (see earlier post here ). The main update is that I've added World Values Survey country codes. The do-file now lives at its own Git here . I hope to have an R version of this in the near future. (I still like using data to merge together large cross-country data sets. For example, the full World Values Survey is a bit unwieldy in R.) Update 20 February 2012: I just ran across Vincent Arel-Bundock 's countrycode package for R. I haven't tried it out yet, but from reading the documentation it looks like countrycode does pretty much what my do-file does, but better, e.g. it includes more country coding schemes. Vincent Arel-Bundock is also the author of another R package I really like, WDI . WDI makes it easy to grab World Bank Indicators . I've used it a number of times in post for this blog."], "link": "http://christophergandrud.blogspot.com/feeds/3910439806343417401/comments/default", "bloglinks": {}, "links": {"http://christophergandrud.blogspot.com/": 1, "http://www-personal.umich.edu/": 1, "http://data.worldbank.org/": 1, "https://github.com/": 1, "http://www.worldvaluessurvey.org/": 1, "http://cran.r-project.org/": 2}, "blogtitle": "Christopher Gandrud"}, {"content": ["Here are the slides from a presentation I'm giving at Waseda University this weekend. The presentation is based on a paper I wrote with M\u00edche\u00e1l O'Keeffe on how policymakers choose responses to banking crises. We specifically look at South Korea and Ireland. It uses the unofficial LSE beamer class I created. The paper is here ."], "link": "http://christophergandrud.blogspot.com/feeds/2087506646531558884/comments/default", "bloglinks": {}, "links": {"http://christophergandrud.blogspot.com/": 1, "http://globalcoe-glope2.jp/": 1, "http://dl.dropbox.com/": 1}, "blogtitle": "Christopher Gandrud"}, {"content": ["I came across this piece on The Atlantic website by Eamonn Fingleton arguing that Japan's two decade long stagnation is largely a myth. I'm not willing, just yet, to completely go along with his thesis that this myth has been stoked by the Japanese Government to ease political pressure on their export-oriented economic model. (It would certainly be interesting if it were true. It is definitely the case that Japanese companies have become adept at dealing with potential US political pressure, e.g. assembling cars in the US.) However, as a semi-regular visitor to Japan I do find it hard to completely believe the stagnation story. The place just seems so clean and vibrant. If Japan is stagnating, then American cities like Detroit would do well to 'stagnate' also. Anyways, beyond the interesting stats on Japanese trade growth and the improvement in living standards since the 1980s in Eamonn's piece, I've been sceptical of the stagnation story based on official GDP numbers (he actually thinks these are underestimates, but let's leave that issue aside for the moment). If we control for stagnate Japanese population growth by looking at GDP growth per capita as opposed to overall GDP growth we can see that Japan's growth numbers aren't that different from the US's (the usual country comparison in the stagnation stories). Look at this graph using data from the World Bank : It shows the difference between Japanese and US annual overall GDP growth and per capita growth. Negative number indicate that the US grew more than Japan and vice versa. 0 means that GDP growth is the same for both countries. The key stagnation period to look at is from around 2000 to the present. Japan was supposedly stagnating while the US was booming. If we just look at overall GDP growth Japan did grow more slowly than the US. But if we look at growth on a per capita basis, there is basically no difference. Apart from 2009, Japan has basically done at least as well as the US since the end of the Asian financial crisis. Here is the R code to reproduce the graph:"], "link": "http://christophergandrud.blogspot.com/feeds/7529462768030883852/comments/default", "bloglinks": {}, "links": {"http://data.worldbank.org/": 1, "http://www.theatlantic.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Christopher Gandrud"}]