[{"blogurl": "http://gettinggeneticsdone.blogspot.com\n", "blogroll": [], "title": "Getting Genetics Done"}, {"content": ["If you need to catch up on all those years you spent not learning how to code ( you need to know how to code ), here are a few resources to help you quickly learn R and Python, and have a little fun doing it. \n \nFirst, the free online Coursera course Computing for Data Analysis just started. The 4 week course is being taught by Roger Peng , associate professor of biostatistics at Johns Hopkins, and blogger at Simply Statistics . From the course description: \n \n This course is about learning the fundamental computing skills necessary for effective data analysis. You will learn to program in R and to use R for reading data, writing functions, making informative graphs, and applying modern statistical methods. \nHere's a short video about the course from the instructor: \n \n \n \nNext, for quickly learning Python, there's the Python track on Codeacademy . Codeacademy takes an interactive approach to teaching coding. The interface gives you some basic instruction and prompts you to enter short code snippets to accomplish a task. Codeacademy makes learning to code fun by giving you short projects to complete (e.g. a tip calculator), and rewarding you with badges for your accomplishments, which allow you to \"compete\" with friends. \n \nOnce you've learned some basic skills, you really only get better with practice and problem solving. Project Euler has been around for some time, and you can find many solutions out there on the web using many different languages, but the problems are more purely mathematical in nature. For short problems perhaps more relevant, head over to Rosalind.info for some bioinformatics programming challenges ranging from something as simple as counting nucleotides or computing GC content , to something more difficult, such as genome assembly . \n \n Coursera: Computing for Data Analysis \n \n Codeacademy Python Track \n \n Rosalind.info bioinformatics programming challenges Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/3394391914899318887/comments/default", "bloglinks": {}, "links": {"http://www.jhsph.edu/": 1, "http://simplystatistics.org/": 1, "http://gettinggeneticsdone.blogspot.com/": 1, "http://rosalind.info/": 5, "http://www.codecademy.com/": 2, "https://www.coursera.org/": 2, "http://projecteuler.net/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["DESeq and edgeR are two methods and R packages for analyzing quantitative readouts (in the form of counts) from high-throughput experiments such as RNA-seq or ChIP-seq. After alignment, reads are assigned to a feature, where each feature represents a target transcript, in the case of RNA-Seq, or a binding region, in the case of ChIP-Seq. An important summary statistic is the count of the number of reads in a feature ( for RNA-Seq, this read count is a good approximation of transcript abundance ). \n \nMethods used to analyze array-based data assume a normally distributed, continuous response variable. However, response variables for digital methods like RNA-seq and ChIP-seq are discrete counts . Thus, both DESeq and edgeR methods are based on the negative binomial distribution. \n \nI see these two tools often used interchangeably, and I wanted to take a look at how they stack up to one another in terms of performance, ease of use, and speed. This isn't meant to be a comprehensive evaluation or \"bake-off\" between the two methods. This would require complex simulations, parameter sweeps, and evaluation with multiple well-characterized real RNA-seq datasets. Further, this is only a start - a full evaluation would need to be much more comprehensive. \n \nHere, I used the newest versions of both edgeR and DESeq , using the well-characterized Pasilla dataset , available in the pasilla Bioconductor package. The dataset is from an experiment in Drosophila investigating the effect of RNAi knockdown of the splicing factor, pasilla . I used the GLM functionality of both packages, as recommended by the vignettes, for dealing with a multifactorial experiment (condition: treated vs. untreated; library type: single-end and paired-end). \n \n \n \nBoth packages provide built-in functions for assessing overall similarity between samples using either PCA (DESeq) or MDS (edgeR), although these methods operate on the same underlying data and could easily be switched. \n \nPCA plot on variance stabilized data from DESeq: \n \n \n \nMDS plot from edgeR: \n \n \n \n \nPer gene dispersion estimates from DESeq: \n \n \n \nBiological coefficient of variation versus abundance (edgeR): \n \n \n \n \nNow, let's see how many statistically significant (FDR<0.05) results each method returns: \n \n \n \nIn this simple example, DESeq finds 820 genes significantly differentially expressed at FDR<0.05, while edgeR is finds these 820 and an additional 371. Let's take a look at the detected fold changes from both methods: \n \n \n \nHere, if genes were found differentially expressed by edgeR only, they're colored red; if found by both, colored green. What's striking here is that for a handful of genes, DESeq is (1) reporting massive fold changes, and (2) not calling them statistically significant. What's going on here? \n \nIt turns out that these genes have extremely low counts (usually one or two counts in only one or two samples). The DESeq vignette goes through the logic of independent filtering, showing that the likelihood of a gene being significantly differentially expressed is related to how strongly it's expressed, and advocates for discarding extremely lowly expressed genes, because differential expression is likely not statistically detectable. \n \nCount-based filtering can be achieved two ways. The DESeq vignette demonstrates how to filter based on quantiles, while I used the filtering method demonstrated in the edgeR vignette - removing genes without at least 2 counts per million in at least two samples. This filtering code is commented out above - uncomment to filter. \n \nAfter filtering, all of the genes shown above with apparently large fold changes as detected by DESeq are removed prior to filtering, and the fold changes correlate much better between the two methods. edgeR still detects ~50% more differentially expressed genes, and it's unclear to me (1) why this is the case, and (2) if this is necessarily a good thing. \n \n \n \n \n Conclusions: \n \nUnfortunately, I may have oversold the title here - this is such a cursory comparison of the two methods that I would hesitate to draw any conclusions about which method is better than the other. In addition to finding more significantly differentially expressed genes (again, not necessarily a good thing), I can say that edgeR was much faster than DESeq for fitting GLM models, but it took slightly longer to estimate the dispersion. Further without any independent filtering, edgeR gave me moderated fold changes for the extremely lowly expressed genes for which DESeq returned logFCs in the 20-30 range (but these transcripts were so lowly expressed anyway, they should have been filtered out before any evaluation). \n \nIf there's one thing that will make me use edgeR over DESeq (until I have time to do a more thorough evaluation), it's the fact that using edgeR seems much more natural than DESeq, especially if you're familiar with the limma package (pretty much the standard for analyzing microarray data and other continuously distributed gene expression data). Setting up the design matrix and specifying contrasts feels natural if you're familiar with using limma. Further, the edgeR user guide weighs in at 67 pages, filled with many case studies that will help you in putting together a design matrix for nearly any experimental design: paired designs, time courses, batch effects, interactions, etc. The DESeq documentation is still fantastic, but could benefit from a few more case studies / examples. \n \nWhat do you think? Anyone want to fork my R code and help do this comparison more comprehensively (more examples, simulated data, speed benchmarking)? Is the analysis above fair? What do you find more easy to use, or is ease-of-use (and thus, reproducibility) even important when considering data analysis? Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/3718639602353197277/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://genomebiology.com/": 1, "http://bioconductor.org/": 7, "http://1.blogspot.com/": 1, "http://genome.cshlp.org/": 1, "https://gist.github.com/": 1, "http://www.nature.com/": 1, "http://4.blogspot.com/": 3, "http://2.blogspot.com/": 1, "https://help.github.com/": 1, "http://bioinformatics.oxfordjournals.org/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["About a year ago I wrote a post about producing scatterplot matrices in R . These are handy for quickly getting a sense of the correlations that exist in your data. Recently someone asked me to pull out some relevant statistics (correlation coefficient and p-value) into tabular format to publish beside a scatterplot matrix. The built-in cor() function will produce a correlation matrix, but what if you want p-values for those correlation coefficients? Also, instead of a matrix, how might you get these statistics in tabular format (variable i , variable j , r, and p, for each i - j combination)? Here's the code (you'll need the PerformanceAnalytics package to produce the plot). \n \n \nThe cor() function will produce a basic correlation matrix. 12 years ago Bill Venables provided a function on the R help mailing list for replacing the upper triangle of the correlation matrix with the p-values for those correlations (based on the known relationship between t and r ). The cor.prob() function will produce this matrix. \n \nFinally, the flattenSquareMatrix() function will \"flatten\" this matrix to four columns: one column for variable i , one for variable j , one for their correlation, and another for their p-value (thanks to Chris Wallace on StackOverflow for helping out with this one). \n \n \n \nFinally, the chart.Correlation() function from the PerformanceAnalytics package produces a very nice scatterplot matrix, with histograms, kernel density overlays, absolute correlations, and significance asterisks (0.05, 0.01, 0.001): \n \n \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/4645337786345003486/comments/default", "bloglinks": {}, "links": {"https://stat.ethz.ch/": 1, "http://3.blogspot.com/": 1, "http://gettinggeneticsdone.blogspot.com/": 1, "http://stackoverflow.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["Recently published in Nucleic Acids Research : \n \n \n \nF. Zambelli, G. M. Prazzoli, G. Pesole, G. Pavesi, Cscan: finding common regulators of a set of genes by using a collection of genome-wide ChIP-seq datasets., Nucleic acids research 40 , W510\u20135 (2012). \n \n \n \n  \n Cscan web interface screenshot \n \n \n \n \nThis paper presents a methodology and software implementation that allows users to discover a set of transcription factors or epigenetic modifications that regulate a set of genes of interest. A wealth of data about transcription factor binding exists in the public domain, and this is a good example of a group utilizing those resources to develop tools that are of use to the broader computational biology community. \n \n \n \n \nHigh-throughput gene expression experiments like microarrays and RNA-seq experiments often result in a list of differentially regulated or co-expressed genes. A common follow-up question asks which transcription factors may regulate those genes of interest. The ENCODE project has completed ChIP-seq experiments for many transcription factors and epigenetic modifications for a number of different cell lines in both human and model organisms. These researchers crossed this publicly available data on enriched regions from ChIP-seq experiments with genomic coordinates of gene annotations to create a table of gene annotations (rows) by ChIP-peak signals, with a presence/absence peak in each cell. Given a set of genes of interest (e.g. differentially regulated genes from an RNA-seq experiment), the method evaluates the over-/under-representation of target sites for the DNA binding protein in each ChIP experiment using a Fisher's exact test. Other methods based on motif-enrichment (using position weight matrices derived from databases like TRANSFAC or JASPAR) would miss DNA-binding factors like the Retinoblastoma protein (RB), which lacks a DNA-binding domain and is recruited to promoters by other transcription factors. In addition to overcoming this limitation, the method presented here also has the advantage of considering tissue-specificity and chromatin accessibility. \n \n \n \n \nThe web interface is free and doesn't require registration: http://www.beaconlab.it/cscan \n \n \n \n Nucleic Acids Research: Cscan: finding common regulators of a set of genes by using a collection of genome-wide ChIP-seq datasets Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/2376368188321931074/comments/default", "bloglinks": {}, "links": {"http://nar.oxfordjournals.org/": 3, "http://www.beaconlab.it/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["The 20th annual ISMB meeting was held over the last week in Long Beach, CA. It was an incredible meeting with lots of interesting and relevant talks, and lots of folks were tweeting the conference, usually with at least a few people in each concurrent session. I wrote the code below that uses the twitteR package to pull all the tweets about the meeting under the #ISMB hashtag . You can download that raw data here . I then use ggplot2 to plot the frequency of tweets about #ISMB over time in two hour windows for each day of the last week. \n \n \n \n \n \nThe code below also tabulates the total number of tweets by username, and plots the 40 most prolific. Interestingly several of the folks in this list weren't even at the meeting. \n \n \n \n \nI'll update the plots above at the conclusion of the meeting. \n \n \n \nHere's the code below. There's a limitation with this code - you can only retrieve a maximum of 1500 tweets per query without authenticating via OAuth before you receive a 403 error. The twitteR package had a good vignette about how to use the ROAuth package to do this, but I was never able to get it to work properly. The version on CRAN (0.9.1) has known issues, but even when rolling back to 0.9.0 or upgrading to 0.9.2 from the author's homepage, I still received the 403 signal. So my hackjob workaround was to write a loop to fetch all the tweets one day at a time and then flatten this into a single list before converting to a data frame. You still run into the limitation of only being able to retrieve the first 1500 for each day, but #ISMB never had more than 1500 any one day. If you can solve my ROAuth problem, please leave a comment or fork the code on GitHub . \n \n \n \n \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/4572915542945236858/comments/default", "bloglinks": {}, "links": {"http://www.iscb.org/": 1, "https://twitter.com/": 1, "http://1.blogspot.com/": 2, "https://docs.google.com/": 1, "https://gist.github.com/": 2, "http://cran.r-project.org/": 2}, "blogtitle": "Getting Genetics Done"}, {"content": ["I saw this plot in the supplement of a recent paper comparing microarray results to RNA-seq results. Nothing earth-shattering in the paper - you've probably seen a similar comparison many times before - but I liked how they solved the overplotting problem using heat-colored contour lines to indicate density. I asked how to reproduce this figure using R on Stack Exchange , and my question was quickly answered by Christophe Lalanne . \n \nHere's the R code to generate the data and all the figures here. \n \n \nHere's the problem: there are 50,000 points in this plot causing extreme overplotting. (This is a simple multivariate normal distribution, but if the distribution were more complex, overplotting might obscure a relationship in the data that you didn't know about). \n \n \nI liked the solution they used in the paper referenced above. Contour lines were placed throughout the data indicating the density of the data in that region. Further, the contour lines were \"heat\" colored from blue to red, indicating increasing data density. Optionally, you can add vertical and horizontal lines that intersect the means, and a legend that includes the absolute correlation coefficient between the two variables. \n \n \n \n There are many other ways to solve an overplotting problem - reducing the size of the points, making points transparent, using hex-binning. \n \n Using a single pixel for each data point: \n \n \n \n \n Using hexagonal binning to display density (hexbin package): \n \n \n \n Finally, using semi-transparency (10% opacity; easiest using the ggplot2 package): \n \n \n \n Edit July 7, 2012 - From Pete's comment below, the smoothScatter() function in the build in graphics package produces a smoothed color density representation of the scatterplot, obtained through a kernel density estimate. You can change the colors using the colramp option, and change how many outliers are plotted with the nrpoints option. Here, 100 outliers are plotted as single black pixels - outliers here being points in the areas of lowest regional density. \n \n \n \n \n How do you deal with overplotting when you have many points? Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/5290724219896931536/comments/default", "bloglinks": {}, "links": {"http://www.biomedcentral.com/": 1, "https://twitter.com/": 1, "http://3.blogspot.com/": 2, "http://1.blogspot.com/": 1, "http://stats.stackexchange.com/": 1, "http://2.blogspot.com/": 3, "http://i.imgur.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["Thanks to the excellent work of Lucia Hindorff and colleagues\nat NHGRI, the GWAS catalog provides a great reference for the cumulative\nresults of GWAS for various phenotypes. Anyone\nfamiliar with GWAS also likely knows about dbGaP \u2013 the NCBI repository for genotype-phenotype\nrelationships \u2013 and the wealth of data it contains. While dbGaP is often thought of as a way to\nget access to existing genotype data, analysis results are often deposited into\ndbGaP as well. Individual-level data\n(like genotypes) are generally considered \u201ccontrolled access\u201d, requiring\nspecial permission to retrieve or use. Summary-level\ndata, such as association p-values, are a bit more accessible. There are two tools available from the dbGaP\nwebsite: the Association Results Browser and the Phenotype-GenotypeIntegrator (PheGenI) . These tools provide a search\ninterface for examining previous GWAS associations. \n \n \n \nThe Association Results Browser provides a simple table\nlisting of associations, searchable by SNP, gene, or phenotype. It contains the information from the NHGRI GWAS catalog , as well as additional associations from dbGaP deposited\nstudies. I\u2019ve shown an example below for\nmultiple sclerosis. You can restrict the\nsearch to the dbGaP-specific results by changing the \u201cSource\u201d selection. If you are looking for the impact of a SNP,\nthis is a nice supplement to the catalog. \nClicking on a p-value brings up the GaP browser, which provides a more\ngraphical (but perhaps less useful) view of the data. \n \n \n \n \n \n \n \n \n \n \n \n \n \nThe PheGenI tool provides similar search functionality, but\nattempts to provide phenotype categories rather than more specific phenotype\nassociations. Essentially, phenotype\ndescriptions are linked to MeSH terms to provide categories such as \u201cChemicals\nand Drugs\u201d, or \u201cHemic and Lymphatic Diseases\u201d. \nPheGenI seems most useful if searching from the phenotype perspective,\nwhile the association browser seems better for SNP or Gene searches. All these tools are under active development, and I look forward to seeing their future versions. Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/2553300851754406825/comments/default", "bloglinks": {}, "links": {"http://www.genome.gov/": 2, "http://www.nih.gov/": 6, "http://1.blogspot.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["I just read an interesting paper on pathogen discovery using next-generation sequencing data, recommended to me by Nick Loman. \n \nA previously described algorithm ( PathSeq, Kostic et al ) for discovering microbes by deep-sequencing human tissue uses computational subtraction, whereby the initial collection of reads is depleted of human DNA by consecutive alignment to the human reference using MAQ and BLAST. \n \n  \n The PathSeq method: computational subtraction by depleting complete read set of all reads mapping to the human reference. \n \n \nThe method described here , Rapid Identification of Nonhuman Sequences (RINS), uses and intersection-based workflow rather than computational subtraction. RINS first maps to a user-supplied custom reference (e.g. a collection of all known viruses and bacteria), thereby drastically lowering computational requirements and increasing sensitivity. Contigs are then assembled de novo, and the original reads are then mapped back onto assembled contigs, which increases specificity. \n \n  \n The RINS method - uses intersection rather than subtraction to identify non-human reads. \n \n \nThe authors of the RINS (intersection) paper noted similar sensitivity and specificity to the PathSeq (subtraction) method in a fraction of the time (2 hours on a desktop machine versus 13 hours on the cloud). \n \n Rapid Identification of Nonhuman Sequences in High Throughput Sequencing Data Sets Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/6919482616312474516/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://www.nature.com/": 1, "http://2.blogspot.com/": 1, "http://bioinformatics.oxfordjournals.org/": 3}, "blogtitle": "Getting Genetics Done"}, {"content": ["The ENCODE project continues to generate massive numbers of\ndata points on how genes are regulated. This\ndata will be of incredible use for understanding the role of genetic variation,\nboth for altering low-level cellular phenotypes (like gene expression or\nsplicing), but also for complex disease phenotypes. While it is all deposited into the UCSC\nbrowser, ENCODE data is not always the easiest to access or manipulate. \n \n \n \nTo make epigenomic tracks from the ENCODE project more\naccessible for interpretation in the context of new or existing GWAS hits, Luke\nWard and Manolis Kellis at the BROAD Institute have developed a database called\nHaploREG. HaploREG uses LD and SNP\ninformation from the 1000 Genomes project to map known genetic variants onto\nENCODE data, providing a potential mechanism for SNP influence. HaploREG will annotate SNPs with evolutionary\nconstraint measures, predicted chromatin states, and how SNPs alter the\nPositional Weight Matrices of known transcription factors. \n \nHere's a screenshot from SNP associated with HDL cholesterol levels showing summary information for several SNPs in LD at R2>0.9 in CEU. Clicking each SNP link provides further info. \n \n \n \n \n \n \n \nIn addition to providing annotations of user-submitted SNPs,\nHaploREG also provides cross-references from the NHGRI GWAS Catalog, allowing\nusers to explore the mechanisms behind disease associated SNPs. Check out the site here: http://www.broadinstitute.org/mammals/haploreg/haploreg.php \nand explore the functionality of any SNPs you might find associated in your\nwork. The more functional information we\ncan include in our manuscripts, the more likely they are to be tested in a\nmodel system. \n \n HaploReg: Functional Annotation of SNPs Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/7630085772238350565/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://genome.ucsc.edu/": 1, "http://www.broadinstitute.org/": 2}, "blogtitle": "Getting Genetics Done"}, {"content": ["A few folks have asked me how I get my news and stay on top of what's going on in my field, so I thought I'd share my strategy. With so many sources of information begging for your attention, the difficulty is not necessarily finding what's interesting, but filtering out what isn't. What you don't read is just as important as what you do, so when it comes to things like RSS, Twitter, and especially e-mail, it's essential to filter out sources where the content consistently fails to be relevant or capture your interest. I run a bioinformatics core, so I'm more broadly interested in applied methodology and study design rather than any particular phenotype, model system, disease, or method. With that in mind, here's how I stay current with things that are relevant to me. Please leave comments with what you're reading and what you find useful that I omitted here. \n \n RSS \n \nI get the majority of my news from RSS feeds from blogs and journals in my field. I spend about 15 minutes per day going through headlines from the following sources: \n \n Journals . Most journals have separate RSS feeds for their current table of contents as well as their advance online ahead-of-print articles. \n \n Bioinformatics - RSS feed of current and advance online publications \n Genome Research - current & advance \n Genome Biology - editors picks, latest, most viewed, most forwarded . (Hit the RSS icon under each tab). \n PLoS Genetics - new articles \n PLoS Computational Biology - new articles \n Nature Genetics - current TOC and AOP \n Nature Reviews Genetics - current TOC and AOP \n \n Blogs . Some of these blogs are very relevant to what I do on the job. Others are more personal interest. \n \n The OpenHelix Blog \n Ensembl blog \n Galaxy News \n Blue Collar Bioinformatics \n Homologus \n Golden Helix - our 2 SNPs \n Genomics Law Report \n R-bloggers (aggregates feeds from >350 blogs about R) \n Genomes Unzipped \n Jason Moore's Epistasis Blog \n 23andMe - the Spitoon \n \n Forums. \n \n Seqanswers - bioinformatics forum \n Seqanswers - RNA-Seq forum \n BioStar \n \n \n Mailing lists \n \nI prefer to keep work and personal email separate, but I have all my mailing list email sent to my Gmail because Gmail's search is better than any alternative. I have a filter set up to automatically filter and tag mailing list digests under a \"Work\" label so I can get to them (or filter them from my inbox) easily. \n \n \n Bioconductor (daily digest) \n Galaxy mailing lists . I subscribe to the -announce, -user, and -dev mailing lists, but I have a Gmail filter set up to automatically skip the inbox and mark read messages from the -user and -dev lists. I don't care to look at these every day, but again, it's handy to be able to use Gmail's search functionality to look through old mailing list responses. \n \n \n \n \n Email Alerts & Subscriptions \n \n \n \nAgain, email can get out of hand sometimes, so I prefer to only have things that I really don't want to miss sent to my email. The rest I use RSS. \n \n \n SeqAnswers subscriptions. When I ask a question or find a question that's relevant to something I'm working on, I subscribe to that thread for email alerts whenever a new response is posted. \n Google Scholar alerts . I have alerts set up to send me emails based on certain topics (e.g. [ rna-seq | transcriptome sequencing | RNA-sequencing ] or [ intitle:\"chip-seq\" ]), or when certain people publish (e.g. [\"ritchie md\" & vanderbilt]). I also use this to alert me when certain consortia publish (e.g. [\"Population Architecture using Genomics and Epidemiology\"]). \n PubMed Saved Searches using MyNCBI , because Google Scholar doesn't catch everything. I have alerts set up for RNA-seq, ChIP-Seq, bioinformatics methods, etc. \n GenomeWeb subscriptions . Most of these are once per week, except Daily Scan. I subscribe to Daily Scan, Genome Technology, BioInform, Clinical Sequencing News, In Sequence, and Pharmacogenomics Reporter. BioInform has a \"Bioinformatics Papers of Note\", and In Sequence has a \"Sequencing papers of note\" column in every issue. These are good for catching things I might have missed with the Scholar and Pubmed alerts. \n \n \n \n \n \n Twitter \n \n \n \n99.9% of Twitter users have way too much time on their hands, but when used effectively, Twitter can be incredibly powerful for both consuming and contributing to the dialogue in your field. Twitter can be an excellent real-time source of new publications, fresh developments, and current opinion, but it can also quickly become a time sink. I can tolerate an occasional Friday afternoon humorous digression, but as soon as off-topic tweets become regular it's time to unfollow. The same is true with groups/companies - some deliver interesting and broadly applicable content (e.g. 23andMe), while others are purely a failed attempt at marketing while not offering any substantive value to their followers. A good place to start is by (shameless plug) following me or the people I follow (note: this isn't an endorsement of anyone on this list, and there are a few off-topic people I follow for my non-work interests). I can't possibly list everyone, but a few folks who tweet consistently on-topic and interesting content are: Daniel MacArthur , Jason Moore , Dan Vorhaus , 23andMe , OpenHelix , Larry Parnell , Francis Ouellette , Leonid Kruglyak , Sean Davis , Joe Pickrell , The Galaxy Project , J. Chris Pires , Nick Loman , and Andrew Severin . Also, a hashtag in twitter (prefixed by the #), is used to mark keywords or topics in Twitter. I occasionally browse through the #bioinformatics and #Rstats hashtag. Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/7339258006539035636/comments/default", "bloglinks": {}, "links": {"http://genomesunzipped.org/": 1, "http://genomebiology.com/": 1, "http://bcbio.wordpress.com/": 1, "http://www.nature.com/": 2, "http://www.nih.gov/": 1, "http://wiki.psu.edu/": 2, "http://www.ensembl.info/": 1, "http://bioinformatics.oxfordjournals.org/": 1, "http://www.genomeweb.com/": 1, "http://compgen.blogspot.com/": 1, "http://www.biostars.org/": 1, "http://support.google.com/": 1, "http://blog.openhelix.eu/": 1, "http://www.homolog.us/blog": 1, "https://twitter.com/": 17, "http://genome.cshlp.org/": 1, "http://seqanswers.com/": 3, "http://www.bioconductor.org/": 1, "http://www.r-bloggers.com/": 1, "http://blog.goldenhelix.com/": 1, "http://www.ploscompbiol.org/": 1, "http://spittoon.23andme.com/": 1, "http://genomicslawreport.com/": 1, "http://scholar.google.com/": 1, "http://www.plosgenetics.org/": 1, "http://twitter.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["I'm a huge supporter of the Free and Open Source Software movement. I've written more about R than anything else on this blog, all the code I post here is free and open-source , and a while back I invited you to steal this blog under a cc-by-sa license. \n \nEvery now and then, however, something comes along that just might be worth paying for. As a director of a bioinformatics core with a very small staff, I spend a lot of time balancing costs like software licensing versus personnel/development time, so that I can continue to provide a fiscally sustainable high-quality service. \n \nAs you've likely noticed from my more recent blog/twitter posts, the core has been doing a lot of gene expression and RNA-seq work. But recently had a client who wanted to run a fairly standard case-control GWAS analysis on a dataset from dbGaP. Since this isn't the focus of my core's service, I didn't want to invest the personnel time in deploying a GWAS analysis pipeline, downloading and compiling all the tools I would normally use if I were doing this routinely, and spending hours on forums trying to remember what to do with procedural issues such as which options to specify when running EIGENSTRAT or KING , or trying to remember how to subset and LD-prune a binary PED file, or scientific issues, such as whether GWAS data should be LD-pruned at all before doing PCA. \n \n Golden Helix \n \nA year ago I wrote a post about the \" Hitchhiker's Guide to Next-Gen Sequencing \" by Gabe Rudy, a scientist at Golden Helix . After reading this and looking through other posts on their blog , I'm confident that these guys know what they're doing and it would be worth giving their product a try. Luckily, I had the opportunity to try out their SNP & Variation Suite (SVS) software (I believe you can also get a free trial on their website). \n \nI'm not going to talk about the software - that's for a future post if the core continues to get any more GWAS analysis projects. In summary - it was fairly painless to learn a new interface, import the data, do some basic QA/QC, run a PCA-adjusted logistic regression, and produce some nice visualization. What I want to highlight here is the level of support and documentation you get with SVS. \n \n Documentation \n \nFirst, the documentation. At each step from data import through analysis and visualization there's a help button that opens up the manual at the page you need. This contextual manual not only gives operational details about where you click or which options to check, but also gives scientific explanations of why you might use certain procedures in certain scenarios. Here's a small excerpt of the context-specific help menu that appeared when I asked for help doing PCA. \n \n \n \nWhat I really want to draw your attention to here is that even if you don't use SVS you can still view their manual online without registering, giving them your e-mail, or downloading any trialware. Think of this manual as an always up-to-date mega-review of GWAS - with it you can learn quite a bit about GWAS analysis, quality control, and statistics. For example, see this section on haplotype frequency estimation and the EM algorithm . The section on the mathematical motivation and implementation of the Eigenstrat PCA method explains the method perhaps better than the Eigenstrat paper and documentation itself. There are also lots of video tutorials that are helpful, even if you're not using SVS. This is a great resource, whether you're just trying to get a better grip on what PLINK is doing, or perhaps implementing some of these methods in your own software. \n \n Support \n \nNext, the support. After installing SVS on both my Mac laptop and the Linux box where I do my heavy lifting, one of the product specialists at Golden Helix called me and walked me through every step of a GWAS analysis, from QC to analysis to visualization. While analyzing the dbGaP data for my client I ran into both software-specific procedural issues as well as general scientific questions. If you've ever asked a basic question on the R-help mailing list, you know need some patience and a thick skin for all the RTFM responses you'll get. I was able to call the fine folks at Golden Helix and get both my technical and scientific questions answered in the same day. There are lots of resources for getting your questions answered , such as SEQanswers , Biostar , Cross Validated , and StackOverflow to name a few, but getting a forum response two days later from \"SeqGeek96815\" doesn't compare to having a team of scientists, statisticians, programmers, and product specialists on the other end of a telephone whose job it is to answer your questions. \n \n Final Thoughts \n \nThis isn't meant to be a wholesale endorsement of Golden Helix or any other particular software company - I only wanted to share my experience stepping outside my comfortable open-source world into the walled garden of a commercially-licensed software from a for-profit company (the walls on the SVS garden aren't that high in reality - you can import and export data in any format imaginable). One of the nice things about command-line based tools is that it's relatively easy to automate a simple (or at least well-documented) process with tools like Galaxy, Taverna, or even by wrapping them with perl or bash scripts. However, the types of data my clients are collecting and the kinds of questions they're asking are always a little new and different, which means I'm rarely doing the same exact analysis twice. Because of the level of documentation and support provided to me, I was able to learn a new interface to a set of familiar procedures and run an analysis very quickly and without spending hours on forums figuring out why a particular program is seg-faulting. Will I abandon open-source tools like PLINK for SVS, Tophat-Cufflinks for CLC Workbench, BWA for NovoAlign, or R for Stata? Not in a million years. I haven't talked to Golden Helix or some of the above-mentioned companies about pricing for their products, but if I can spend a few bucks and save the time it would taken a full time technician at $50k+/year to write a new short read aligner or build a new SNP annotation database server, then I'll be able to provide a faster, high-quality, fiscally sustainable service at a much lower price for the core's clients, which is all-important in a time when federal funding is increasingly harder to come by. Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/2518421525750074300/comments/default", "bloglinks": {}, "links": {"http://doc.goldenhelix.com/": 2, "http://biostars.org/": 1, "http://goldenhelix.com/": 2, "http://www.goldenhelix.com/": 2, "http://stackoverflow.com/": 1, "http://people.virginia.edu/": 1, "http://gettinggeneticsdone.blogspot.com/": 5, "http://seqanswers.com/": 1, "http://stats.stackexchange.com/": 1, "http://genepath.harvard.edu/": 1, "http://2.blogspot.com/": 1, "http://blog.goldenhelix.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["A few weeks ago I showed you how to convert gene IDs with BioMart . Yesterday I hosted a workshop on the Ensembl Genome Browser, given by Dr. Bert Overduin from EBI-EMBL. He gave several examples of very useful tasks that you can do very quickly and easily using BioMart. One, in particular, is something that I'm doing for a client in the core right now. \n \nLet's say you have a set of genes in one species and you want to know the orthologs in another species and gene expression probes in that species you can use to assay those orthologs. For example, Table 1 in this paper reports 25 gene expression probes that are dysregulated in humans when exposed to benzene. What if you only had the U133A/B Affymetrix probe IDs and wanted to know the gene names? What if you also wanted all the Ensembl gene IDs, names, and descriptions of the mouse orthologs for these human genes? Further, what are the mouse Affymetrix 430Av2 probe IDs that you can use to assay these genes' expression in mouse? All this can be accomplished for a list of genes in about 60 seconds using BioMart. See the video below. Watch it on Youtube in 1080p if you're having a hard time reading the text. \n \n \n \nIf you want to try this yourself, head to BioMart and copy the list of Affy probe IDs below: \n \n207630_s_at \n221840_at \n219228_at \n204924_at \n227613_at \n223454_at \n228962_at \n214696_at \n210732_s_at \n212371_at \n225390_s_at \n227645_at \n226652_at \n221641_s_at \n202055_at \n226743_at \n228393_s_at \n225120_at \n218515_at \n202224_at \n200614_at \n212014_x_at \n223461_at \n209835_x_at \n213315_x_at \n \n Youtube - Get Orthologs via BioMart Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/2953583732705292809/comments/default", "bloglinks": {}, "links": {"http://www.youtube.com/": 2, "http://www.nih.gov/": 1, "http://www.ac.uk/": 1, "http://gettinggeneticsdone.blogspot.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["If you're doing any kind of big data analysis - genomics, transcriptomics, proteomics, bioinformatics - then unless you've been on vacation the last few weeks you've no doubt heard about the NSF/NIH BIGDATA Initiative (here's the NSF solicitation and here's the New York Times article about the funding opportunity). The solicitation \"aims to advance core scientific and technological means of managing, analyzing, visualizing, and extracting useful information from large, diverse, distributed and heterogeneous data sets so as to: accelerate the progress of scientific discovery and innovation; lead to new fields of inquiry that would not otherwise be possible; encourage the development of new data analytic tools and algorithms; facilitate scalable, accessible, and sustainable data infrastructure; increase understanding of human and social processes and interactions; and promote economic growth and improved health and quality of life.\" \n \nNSF is holding a webinar to describe the goals and focus of the BIGDATA solicitation, help investigators understand its scope, and answer any questions potential PIs might have. \n \nThe Webinar will be held from 11am-noon EST on May 8, 2012. Register here . The webinar will also be archived here a few days later. \n \n NSF BIGDATA Webinar - May 8 2012 Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/5298163470641973318/comments/default", "bloglinks": {}, "links": {"https://mmancusa.webex.com/": 1, "http://www.nytimes.com/": 1, "http://www.nsf.gov/": 3}, "blogtitle": "Getting Genetics Done"}, {"content": ["I was reading through a paper on comparative ChIP-Seq when I found this awk gem that lets you get some very basic stats very quickly on next generation sequencing reads. To use, simply cat the fastq file (or gunzip -c ) and pipe that to this awk command: \n \n \n \n \n\n\n\n\n\n\n\n\n \n cat myfile.fq | awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' \n \n \n \nThe output would look something like this for some RNA-seq data downloaded from the Galaxy RNA-seq tutorial : \n \n \n \n 99115 60567 61.1078 ACCTCAGGA 354 0.357161 \n \n \n \nThis is telling you: \n \n \n \n The total number of reads (99,115). \n The number of unique reads (60,567). \n The frequency of unique reads as a proportion of the total (61%). \n The most abundant sequence (useful for finding adapters, linkers, etc). \n The number of times that sequence is present (354). \n The frequency of that sequence as a proportion of the total number of reads (0.35%). \n \n \nIf you have a handful of fastq files in a directory and you'd like to do this for each of them, you can wrap this in a for loop in bash: \n \n \n \n\n\n\n\n\n\n\n\n \n for read in `ls *.fq`; do echo -n \"$read \"; awk '((NR-2)%4==0){read=$1;total++;count[read]++}END{for(read in count){if(!max||count[read]>max) {max=count[read];maxRead=read};if(count[read]==1){unique++}};print total,unique,unique*100/total,maxRead,count[maxRead],count[maxRead]*100/total}' $read; done \n \n \n \nThis does the same thing, but adds an extra field at the beginning for the file name. I haven't yet figured out how to wrap this into GNU parallel, but the for loop should do the trick for multiple files. \n \n \n \nCheck out FASTQC for more extensive quality assessment. \n \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/502704650699715734/comments/default", "bloglinks": {}, "links": {"https://main.psu.edu/": 1, "http://www.ac.uk/": 1, "http://gettinggeneticsdone.blogspot.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["There were lots of interesting developments this month that didn't work their way into a full blog post. Here is an incomplete list of what I've been tweeting about over the last few weeks. But first I want to draw your attention to the latest manuscript for a new bioconductor package for doing RNA-seq in R. \n \n DEXSeq vs Cuffdiff . See the pre-publication manuscript from Simon Anders, Alejandro Reyes, and Wolfgang Huber: \" Detecting differential usage of exons from RNA-Seq data. \" DEXSeq is an R package by the same guys who developed the DESeq R package and the HTSeq python scripts. (Incidentally, both DESeq and DEXSeq are rare examples of bioconductor vignettes which are well developed and are a pleasure to read). I often use cufflinks/cuffdiff in the bioinformatics core was because many other tools and methods only allow you to interrogate differential expression at the gene level. Using cufflinks for transcriptome assembly enables you to interrogate transcript/isoform expression, differential splicing, differential coding output, differential promoter usage, etc. DEXSeq uses similar methodology as DESeq, but can give you exon-level differential expression, without going through all the assembly business that cufflinks does. In one of the supplementary tables in their pre-pub manuscript, they compare several versions of cuffdiff to DEXSeq on two datasets. Both of these datasets had biological replicates for treatment and control conditions. They compared treatment to controls, and found DEXseq gave you more significant hits than cuffdiff. Then they compared controls to other controls (ideally should have zero hits) and found cufflinks had way more hits. See p13, p23, tables S1 and S2. \n \n \n \n Proper comparison treatment vs control, # significant hits: \n \nDEXSeq: 159 \n \nCuffdiff 1.1: 145 \n \nCuffdiff 1.2: 69 \n \nCuffdiff 1.3: 50 \n \n \n \n Mock comparison controls vs controls, # significant hits: \n \nDEXSeq: 8 \n \nCuffdiff 1.1: 314 \n \nCuffdiff 1.2: 650 \n \nCuffdiff 1.3: 639 \n \n \n \nIn the UVA Bioinformatics core we strive for reproducibility, scalability, and transparency using the most robust tools and methodology available. It gives me pause to see such alarmingly different results with each new version and each new protocol of a particular tool. What are your thoughts and experiences with using Cufflinks/Cuffdiff, DESeq/DEXSeq, or the many, many other tools for RNA-Seq ( MISO , ExpressionPlot , EdgeR , RSEM , easyRNASeq , etc.)? Please share in the comments. \n \n \n \n Everything else: \n \n \n \n \n \n Webinar from @goldenhelixinc: Learning From Our GWAS Mistakes: From experimental design to scientific method https://t.co/KkxAn18p \n \n \n \n  Sequencing technology does not eliminate biological variability http://t.co/NI3acZgn \n \n \n \n  [bump] Questions on cutoff setting of FPKM value & know genes filtering in Cuffmerge result http://t.co/iKMZ7Dsd #bioinformatics \n \n \n \n \n  Very cool: DNAse-Seq+RNA-seq used to show DNaseI sensitivity eQTLs are a major determinant of gene expression variation http://t.co/nPo3xHVa \n \n \n  Beware using UCSC GTFs in HTSeq/CovergeBed for counting RNA-seq reads. \"transcript_id\" is repeated as \"gene_id\"! https://t.co/ADg1Pi6U \n \n \n  Google Scholar Metrics: top 20 journals in bioinformatics http://t.co/QYTf5pyT \n \n \n  A systematic eQTL study of cis-trans #epistasis in 210 HapMap individuals http://t.co/0YHlFQak \n \n \n  Identification of allele-specific alternative mRNA processing via RNA-seq http://t.co/fig9cLlH #bioinformatics @myen \n \n \n  prepub on arXiv + analysis tutorial/walkthrough + AWS EC2 AMI + git repo + ipython notebook = reproducible research done right http://t.co/GPNmpdJD \n \n \n  Altmetrics in the Wild: Using Social Media to Explore Scholarly Impact http://t.co/8xZuIHw9 \n \n \n  NSF-NIH Interagency Initiative: Core Techniques and Technologies for Advancing Big Data Science and Engineering http://t.co/W3LUdCsG \n \n \n  New approach from @MarylynRitchie lab to collapsing/combining: using biological pathways rather than positional info http://t.co/ywWj0MNn \n \n \n  The Transcription Factor Encyclopedia http://t.co/DjJUwR10 Paper: http://t.co/b0J8PXO6 \n \n \n  NF-kB: where did it come from and why? http://t.co/NLV1mBd0 \n \n \n  Cloud BioLinux: pre-configured and on-demand #bioinformatics computing for the genomics community. @myen http://t.co/3kCE0ktH \n \n \n  SCOTUS remands AMP v Myriad (BRCA) patent case to CAFC to consider in light of prometheus decision http://t.co/7CkTa4l0 \n \n \n  57 year experiment, Drosophila kept in dark for 1400 generations, many evolutionary changes (record longest postdoc!) http://t.co/wukq8fAf \n \n \n  IsoLasso: a LASSO regression approach to RNA-Seq based transcriptome assembly http://t.co/FN1sYM8f #bioinformatics \n \n \n  Complex disease genetics is complex. Imagine that. Hirschhorn, Visscher, & the usual consortium suspects: http://t.co/Bwopxlx6 \n \n \n  MAnorm: a robust model for quantitative comparison of ChIP-Seq data sets http://t.co/pffuHIlO #bioinformatics \n \n \n  Paper about SEQanswers forum published in #Bioinformatics http://t.co/SUlQ6O8c \n \n \n  num-utils - like awk, grep, sort, cut, etc for numbers http://t.co/bgkB5FMV \n \n \n  Nat Protocols: Differential gene & transcript expression analysis of RNA-seq w/ TopHat & Cufflinks http://t.co/U1ZpSE7V #bioinformatics \n \n \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/7527787395401244218/comments/default", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://www-huber.embl.de/": 1, "https://t.co/": 2, "http://bioconductor.org/": 2, "http://www.nature.com/": 1, "http://t.co/": 22, "http://blog.expressionplot.com/": 1, "http://precedings.nature.com/": 1, "http://www.bioconductor.org/": 2, "http://genes.mit.edu/": 1, "http://deweylab.wisc.edu/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["I get asked frequently how to convert from one gene identifier to another. This can be tricky, especially when relying on gene symbols, as Will pointed out in a previous post a few years ago. There are several tools that can do this, including DAVID and the previously mentioned new Biomart ID Converter, but I still prefer using the Ensembl Biomart for this because of its added flexibility and annotation. \n \nI've started putting together video screencasts for things like this, especially when several of the core's clients ask the same question. In this example, I'll show you how to quickly convert from the Affymetrix Mouse Gene 1.0 ST microarray probeset IDs to an Ensembl gene ID and gene symbol. \n \n \n \nYou can also do this programmatically in R using the biomaRt package in Bioconductor. \n \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/740064021957791900/comments/default", "bloglinks": {}, "links": {"http://david.ncifcrf.gov/": 1, "http://bioconductor.org/": 1, "http://gettinggeneticsdone.blogspot.com/": 2, "http://www.ensembl.org/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["GGD has a new look. I was inspired by Gina Trapani (Smarterware, Lifehacker) to remove any extra lines, links, and other \"ink\" that doesn't serve any purpose, and I hope the site appears cleaner and easier to read. I also wanted the extra horizontal space for larger images and avoid the dreaded side-scrolling in posts with lots of code like this one . The space will also be handy for short instructional screencasts I've been recording for my clients here in the core, which I'll start posting here soon. Leave a comment if anything appears broken. I hope you like the new look! Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/3238076547709235592/comments/default", "bloglinks": {}, "links": {"http://smarterware.org/": 1, "http://gettinggeneticsdone.blogspot.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["*Edit March 12* Be sure to look at the comments, especially the commentary on Hacker News - you can supercharge the find|xargs idea by using find|parallel instead. \n--- \n \nDo you ever discover a trick to do something better, faster, or easier, and wish you could reclaim all the wasted time and effort before your discovery? I've had this happen many times - learning how to use vim , learning about querying relational databases, switching from EndNote to Mendeley, etc. \n \nNow I wish I could reclaim all the time I spent writing perl code to do something that find|xargs can do much more easily. I'll lead you through an example using a problem I ran into and solved easily with find|xargs . \n \n The problem: \n \nI'm doing an RNA-seq experiment where I have three replicates {1,2,3} for two conditions {C,M}. I've run Tophat to align the raw reads to a reference genome, and saved the output of each run to a separate directory. Within those directories I have an alignment (accepted_hits.bam) and some other stuff. \n \n \n \nNow, I want to assemble transcripts separately for each alignment. All the alignments are 1 directory deep in the tophat output directory. Furthermore, I want to submit a separate job for each assembly onto our cluster using qsub. In a former life I would have wrapped a perl script around this to write shell scripts that the scheduler would run, and then write a second perl script that would submit all the jobs to the scheduler. \n \nHere's a command that will do it all in one go: \n \n PROCS=8; find `pwd` -name \"accepted_hits.bam\" | xargs -i echo qsub -l ncpus=$PROCS -- `which cufflinks` -p $PROCS -o {}-cufflinks -g genes.gtf {} | sh \n \nSo let's break this down one piece at a time to see what's going on here. \n \nFirst the find command. If I want to find all the files in the current directory recursively through any subdirectory, I can use the find command with the -name argument. You can see that find is searching \".\" (the current directory), and is returning the path (relative to \".\") for each file it finds. \n \n \n \nBut if I'm submitting jobs to a scheduler, I want to use the full absolute path. You can modify find's output by telling it to search in \".\" but give it the full path. Even better, tell find to search in `pwd` (those are backticks, usually above the tab key. The shell will run the pwd command, and insert that output into the find command. See below. \n \n \n \nNow, here's where xargs comes in. xargs lets you build new commands based on the standard input. In other words, you can build a command to run on each line that gets piped to xargs. Use the -i option to create a command for each individual line on the pipe, and use {} as a placeholder. The example below should help. \n \n \n \nSo here's whats going on. In the first step, I'm piping the output of find into xargs, and xargs is creating a new command that will echo \" somecommand {} \", where {} is a placeholder for what gets piped into xargs. So you can replace somecommand with anycommand -any -options -you -want , and echo all that back out to the STDOUT. \n \nThe second part simply pipes whatever is echo'd to the STDIN to the bash shell ( | sh ). So bash will run each command it receives on the pipe. Since somecommand doesn't exist, I'm getting an error. \n \nBelow, I'm building the command to run cufflinks on each alignment, and dump that output back out to a new directory based on the pattern of the alignment (bam) file name. But since in the next step I want to parallelize this on the cluster, and the cluster won't know that cufflinks is in my path, I need to tell it where to find cufflinks. I could give it the path, but I would rather use the backtick trick I showed you above to let the shell tell the shell where cufflinks resides by using `which cufflinks` . \n \n \n \nIn the final piece, I'm adding a few more components. \n \n \n \nFirst, I'm setting a shell variable called PROCS . I can access this variable later by using $PROCS . This is how many CPUs I want to allocate to each assembly job. The \";\" separates two commands. Instead of using xargs to build a cufflinks command to run at the shell directly, I'm using xargs to build a qsub command that will qsub these jobs to the cluster. To qsub something from the command line, the syntax is \n \n qsub -- myprog -o -p -t arg1 arg2 \n \nWhere are the PBS directives to qsub (like the number of processors I want, the amount of RAM I require, etc). myprog is the program you're running. -o, -p, -t are options to myprog, and arg1 and arg2 are the arguments to myprog. The two hyphens are required between the qsub options and the commands you actually want to run. \n \nSo in the command above, I'm using xargs to build up a qsub command, substituting $PROCS (8 here) for the number of CPUs I require, calling cufflinks from the full path using the backtick trick, telling cufflinks that I want $PROCS (8) processors, use genes.gtf for reference annotation based transcript (RABT) assembly, and run all that cufflinks stuff on the supplied alignment. \n \nIn summary, this one-liner will submit six 8-processor jobs. It sure beats writing perl scripts. There are a zillion other uses for piping together find with xargs. If you have a useful one-liner, please share it in the comments. Happy piping! Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/7704972572566399164/comments/default", "bloglinks": {}, "links": {"http://www.vim.org/": 1, "http://news.ycombinator.com/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["I get a lot of requests in the core about running a \"pathway analysis.\" Someone ran a handful of gene expression arrays, or better yet, ran an RNA-seq experiment (with replicates!). These, and many other kinds of high-throughput assays (GWAS, ChIP-seq, etc.) result in a list of genes and some associated p-value, fold change, or other statistic. \n \nHere's some R code to download public data from a study on susceptibility to colorectal cancer. I realize that this is an oversimplified design - that's not the focus here. You can read more about proper design and contrast matrices in the limma vignette . You can read more about the study and the samples in the paper or on the GEO page . \n \n \n \nA thoughtful and thorough analysis doesn't end with a list of genes and P-values. You spent way too much money on an experiment just to end up with a list of genes and p-values at some arbitrary cutoff. I often see investigators going down the list and cherry-picking genes that they think are important or might play a role, and trying to create a story involving those cherry-picked genes. I believe it's better to be more agnostic, taking a data-driven approach to framing results in a functional context. \n \nHow do you do this? This review recently published in PLoS Comp Bio is an excellent place to start: \n \n Khatri, P., Sirota, M., & Butte, A. J. (2012). Ten Years of Pathway Analysis: Current Approaches and Outstanding Challenges. PLoS Computational Biology, 8(2), e1002375. doi:10.1371/journal.pcbi.1002375 \n \nI wrote an evaluation of this paper at Faculty of 1000 - here's an expanded version. The paper gives an overview of the available methods for pathway analysis, and categorizes these methods into three functional classes: over-representation analysis, functional class scoring, and topology-based methods. The paper discusses the advantages and limitations of each approach, as well as future development directions. \n \n Over-representation analysis is what you would typically do with something like gene ontology annotations. The Gene Ontology (GO) project is an effort to describe gene products from all organisms using a consistent language (ontologies are formal representations of knowledge domains; GO does this with cell biology). A biological process is typically made up of a group of genes, as opposed to an individual gene alone. The idea of over-representation analysis is that if a biological process is abnormal in a given study, the co-functioning genes are more likely to be selected as a relevant group by the high-throughput screening technologies. For example, if 10% of the your genes selected by a microarray experiment are kinases, as opposed to 1% of the genes in the whole human genome (this is the gene population background) that are kinases, then you have significant over-representation in your genes for kinases. There are many, many tools for doing over-representation analysis using gene ontology terms, but they all rely on the same idea, typically using a hypergeometric test. You can create directed acyclic graphs like the one below, color coding nodes based on the statistical significance of the term overrepresentation in your gene list. See the paper above for a discussion of limitations of over-representation analysis, and also check out this paper on the use and misuse of gene ontology annotations . \n \n \n \n \n Functional Class Scoring . One of the biggest limitations of over-representation analysis is that you have to arbitrarily decide what you call significant and what you don't. If you use an FDR threshold of 0.05 and fold change cutoff of 2, you'll lose all those genes with a fold change of 1.95 or FDR 0.051, which are arguably just as important as the genes just under the arbitrary cutoff. Further, over-representation analyses only use the number of genes and ignore how strongly those genes are associated with whatever you're studying. Pathway analysis methods that the above review classifies as \"Functional Class Scoring\" methods use all the genes you have as well as their association statistics (e.g. fold change), and compute a running enrichment score for gene groupings (based on some functional knowledge like gene ontology or KEGG pathways). Gene Set Enrichment Analysis is one of the more popular approaches in this category. The plot below shows the kind of results you get from GSEA. From the GSEA documentation: \"The primary result of the gene set enrichment analysis is the enrichment score (ES), which reflects the degree to which a gene set is overrepresented at the top or bottom of a ranked list of genes. GSEA calculates the ES by walking down the ranked list of genes, increasing a running-sum statistic when a gene is in the gene set and decreasing it when it is not. The magnitude of the increment depends on the correlation of the gene with the phenotype. The top portion of the plot shows the running ES for the gene set as the analysis walks down the ranked list. The score at the peak of the plot (the score furthest from 0.0) is the ES for the gene set. Gene sets with a distinct peak at the beginning (such as the one shown here) or end of the ranked list are generally the most interesting.\" So this shows that you have significant enrichment in the cancer phenotype (labeled \"LL\") for genes involved in the KEGG Spliceosome pathway . \n \n \n \n \n \n Topology based methods. The above methods don't take into account how genes interact with each other (e.g. activation, inhibition, phosphorylation, direct/indirect interaction, ubiquitination, etc). Pathway topology methods this extra information in computing pathway-level statistics. I've recently started using the bioconductor SPIA package (Signaling Pathway Impact Analysis), which integrates lists of differentially expressed genes, their fold changes, and pathway topology, to identify pathways associated with condition you're studying. The code below runs SPIA on the analysis we did above. Make sure you successfully loaded all the R packages in the code above. \n \nYou can read more about the results SPIA is giving you in their paper and vignette . Briefly, each pathway is represented by a point, and the x and y axes are showing increasing evidence for the involvement of this pathway in your disease (the total \"perturbation\" in the pathway based on your gene expression data). \n \n \n \nIt's worth noting that all of the above mentioned methods have limitations. We don't fully understand biology, and our understanding of molecular networks and signaling pathways is still very low-resolution. We also don't have information about how different isoforms have different effects - which is something we'll get from RNA-seq experiments. Annotations are often incorrect and inaccurate, and we don't have very much cell-type specific or dynamic information about these pathways. Finally, the analysis of large gene lists with the current enrichment tools is still more of an exploratory data-mining procedure rather than a pure statistical solution and analytical endpoint. \n \nDespite the limitations, the kinds of methods discussed here and reviewed elsewhere (see below) are still very useful for extracting biological meaning and framing results from high-throughput experiments in a functional context. The bioinformatics core here at UVA can do any of the kinds of analyses discussed above. Please fill out a consultation request form and I'll be happy to talk to you about what kinds of insight you may be able to glean from your existing data or in an experiment you're planning. \n \nOther useful literature on the subject: \n \n \n \nHolmans, P., Green, E. K., Pahwa, J. S., Ferreira, M. a R., Purcell, S. M., Sklar, P., Owen, M. J., et al. (2009). Gene ontology analysis of GWA study data sets provides insights into the biology of bipolar disorder. American journal of human genetics , 85 (1), 13-24. doi:10.1016/j.ajhg.2009.05.011 \n \n \n \nHuang, D. W., Sherman, B. T., & Lempicki, R. a. (2009). Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists. Nucleic acids research , 37 (1), 1-13. doi:10.1093/nar/gkn923 \n \n \n \nKhatri, P., Sirota, M., & Butte, A. J. (2012). Ten Years of Pathway Analysis: Current Approaches and Outstanding Challenges. (C. A. Ouzounis, Ed.) PLoS Computational Biology , 8 (2), e1002375. doi:10.1371/journal.pcbi.1002375 \n \n \n \nRhee, S. Y., Wood, V., Dolinski, K., & Draghici, S. (2008). Use and misuse of the gene ontology annotations. Nature reviews. Genetics , 9 (7), 509-15. doi:10.1038/nrg2363 \n \n \n \nWang, K., Li, M., & Bucan, M. (2007). Pathway-Based Approaches for Analysis of Genomewide Association Studies. American journal of human genetics , 81 (6), 1278-1283. doi:10.1086/522374 \n \n \n \nYaspan, B. L., & Veatch, O. J. (2011). Strategies for Pathway Analysis from GWAS Data. Current protocols in human genetics / editorial board, Jonathan L. Haines ... [et al.] , Chapter 1 (October), Unit1.20. doi:10.1002/0471142905.hg0120s71 Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/910598373471278501/comments/default", "bloglinks": {}, "links": {"http://www.genome.jp/": 1, "http://f1000.com/": 1, "http://bioconductor.org/": 1, "http://1.blogspot.com/": 2, "http://www.geneontology.org/": 1, "http://www.nature.com/": 1, "http://www.bioconductor.org/": 2, "http://www.nih.gov/": 2, "http://4.blogspot.com/": 1, "http://bioinformatics.virginia.edu/": 2, "http://bioinformatics.oxfordjournals.org/": 1, "http://www.broadinstitute.org/": 1, "http://www.ploscompbiol.org/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["I direct the Bioinformatics Core at the University of Virginia, and I'm hiring. Visit this link on the UVA Jobs website for more information. Here's the description: \n \nThe University of Virginia Bioinformatics Core is seeking a full-time position as a bioinformatics analyst. The analyst will work with other core staff on grant-funded and chargeback-based projects to manage and analyze large-scale datasets produced by next-generation sequencing. The analyst will identify opportunities and implement solutions for managing, visualizing, analyzing, and interpreting genomic data, including studies of gene expression (RNA-seq and microarrays), pathway analysis, protein-DNA binding (e.g. ChIP-seq), DNA methylation, and DNA variation, using Affymetrix, Illumina, Nimblegen, Agilent, Roche 454, Ion Torrent, and other high-throughput platforms in both human and model organisms. The analyst will work closely with the core director to assist in experimental design and provide expert consultation, technical, and scientific support for UVA investigators, and assist in outreach and training activities. The analyst will organize large-scale sequence data sets, manipulate and format data with perl, python, or other scripting languages, use established software to assess quality and analyze data, schedule and run jobs on a high-performance computing cluster, use Unix or a scripting language to extract meaningful results from output, use software or genome browsers for visualization, and use established databases and techniques for annotating genetic variants and results from expression/DNA-binding experiments. The successful candidate will have a demonstrated ability to translate biological questions into technical designs, and to identify, prioritize, and execute bioinformatics tasks to meet project goals and deadlines. An M.S. in Bioinformatics, Genomics, Biostatistics, or a related field is required for this position. \n I'm Hiring - Bioinformatics Analyst in the UVA Bioinformatics Core Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://www.blogger.com/feeds/6232819486261696035/posts/default/1880125310682424368?v=2", "bloglinks": {}, "links": {"http://jobs.virginia.edu/": 2, "http://bioinformatics.virginia.edu/": 1}, "blogtitle": "Getting Genetics Done"}, {"content": ["I'm updating my CV and biosketch for a few grant applications, and for some time now, NIH has required you to include the PubMed Central ID for each article you publish that arose from NIH support. I only have a dozen or so papers indexed in PubMed, but I still wanted a way to do this automatically. If you have scores of publications, looking up all the PMCIDs could easily become a hassle. \n \n \n \nFirst, create an account at My NCBI . Under your bibliography, click \"Manage My Bibliography.\" Then click \"Add citation,\" then in the new window that comes up, select \"Citation from PubMed\" and hit the \"Go To PubMed\" button. \n \n \n \nNow the trick here is constructing a PubMed query that will get your publications only. There are lots of Stephen D. Turner's out there, so I had to get creative. This query construction tip comes to me by way of my colleague here at UVA, Aaron Mackey : \n \n \n \n \nFor many people, simple PubMed author searches suffice, e.g. \"Pearson WR[Author]\". For some, such name-based searches get it mostly right, but may include a few spurious false hits. For these cases, it's easy enough to exclude those false hits explicitly (e.g. \"Mackey AJ\"[Author] NOT 9850730[PMID] NOT 10730495[PMID] gets rid of the two AJ Mackey publications that are not, in fact, mine). For others, simple author searches do not suffice at all, but usually adding an institution and/or departmental affiliation does narrow the results sufficiently (e.g. for Jeff Smith, Biochemistry: \"Smith JS\"[au] AND \"University of Virginia\"[Affiliation] AND \"Biochemistry\"[Affiliation] identifies the 16 articles for which Jeff Smith is the senior author; Jeff could also add a few collaborative publications by adding those pubmed IDs to the search, i.e. adding \"OR 17482543[PMID]\" to the end of his query. \n \n \n \n \nWhen I did this for myself, I searched by author, AND (any of my institutional affiliations separated by OR's), but NOT (any of the PMIDs that were not mine, separated by OR's). Apparently there was once another Stephen D. Turner at UVA in the department of Urology. Here are the results returned by my unique query: \n \n \n \n\n\n\n\n\n\n\n\n \n \"Turner SD\"[Au] AND (\"James Madison\"[Affiliation] OR Vanderbilt[Affiliation] OR Hawaii[Affiliation] OR \"University of Virginia\"[Affiliation]) NOT (11514333[PMID] OR 11058553[PMID]) \n \n \n \nThe final step is clicking the \"Send to\" link at the top right, and sending the results of your query to My Bibliography. \n \n \n \n \n \n \n \nNow, when you are back at My NCBI, you should see a list of all your publications, complete with both the PMID and PMCID, ready to go in your biosketch. \n \n \n \n \n \n \n \nYou can then export this bibliography as text, or simply copy/paste. Finally, you have the option of making your bibliography public ( example ). \n \n \n \n \n \n \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/4291567172235685541/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://cphg.virginia.edu/": 1, "http://www.nih.gov/": 3}, "blogtitle": "Getting Genetics Done"}, {"content": ["Kevin White from the University of Chicago will be giving a special guest lecture at NCI next week on systems biology approaches to mine genomics data for biomarkers and therapeutic targets. The lecture will be available online as a videocast . \n \n \n Title : Genomic Networks in Development and Cancer: Resolving Biomarkers and Therapeutic Targets from a Cloud of Data \n \n \n \n Speaker : Kevin White, University of Chicago \n \n \n \n When : Tuesday February 14, 2012, 1:00pm EST \n \n \n \n Summary : \n \n \n \nSystems level approaches to construct abstract molecular networks can lead to predictions about genetic and biochemical functions in cells, organisms and in disease states. I will show examples of this approach from work in my laboratory. In one example we used an integrated experimental and computational approach to construct a large scale functional network in Drosophila melanogaster built around key transcription factors involved in the process of embryonic segmentation. Our network model is based on a combination of gene expression, transcription factor DNA binding site mapping, automated literature mining and protein-protein interaction mapping. We provide a strategy for reducing the dimensionality of the massive networks that result from such integrated whole genome analyses. \n \n \n \nUsing results from one factor in particular, we demonstrated that our approach can rapidly translate a finding in a model organism to the development of a therapeutic target in kidney cancer. In another example, we built a large scale network based on gene expression and genome-wide ChIP results for 40 transcription factors, including two dozen Nuclear Receptor (NR) class proteins. Using this NR network we identified novel prognostic signatures for breast cancer survival and recurrence, as well as new therapeutic leads. \n \n \n \nFinally, if time permits I will talk about how we are mining The Cancer Genome Atlas along with data from the Chicago Cancer Genomes Project using the Bionimbus Cloud in order to identify new tumor suppressors and panels of genetic markers capable of classifying cancer subtypes that correspond to patient outcome. \n \n \n \n NIH Videocast: Genomic Networks in Development and Cancer: Resolving Biomarkers and Therapeutic Targets from a Cloud of Data \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/7203047735418516532/comments/default", "bloglinks": {}, "links": {"http://videocast.nih.gov/": 2}, "blogtitle": "Getting Genetics Done"}, {"content": ["Title: A Backstage Tour of ggplot2 with Hadley Wickham \n Date: Wednesday, February 8, 2012 \n Time: 11:00AM - 12:00PM Pacific \n Presenter: Hadley Wickham, Professor of Statistics, Rice University \n \n Register here . \n \nI used ggplot2 extensively a few years ago, but reverted back to base graphics when ggplot2 was too slow for a project I was working on. But ggplot2 and plyr have improved much in the last few years, and I'm starting to pick it back up again. This webinar will give an overview of ggplot2, a preview of some of ggplot2's forthcoming new features, and will discuss ggplot2's internals and development over the last few years and how ggplot2 development is becoming easier. \n \nI received an email yesterday saying that the registration list is over 1000 long, so it's a good idea to sign into the webinar early to make sure you get a spot. Hit the link below to register and you'll get a link to the webinar. \n \n A Backstage Tour of ggplot2 with Hadley Wickham Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/7854899309726992677/comments/default", "bloglinks": {}, "links": {"http://gettinggeneticsdone.blogspot.com/": 1, "http://www.revolutionanalytics.com/": 2}, "blogtitle": "Getting Genetics Done"}, {"content": ["The winter Joint Techs meeting is next week in Baton Rouge. I'm not going, but I plan on participating via a netcast to see what's going on. Jim Bottum, Clemson's CIO, is moderating an entire day devoted to the topic Enhancing Infrastructure Support for Data Intensive Science. Of particular interest to me are the talks from 9:30-11am Tuesday January 24 from researchers and those supporting climatology, genomics, and the XSEDE projects. The afternoon of January 24 has some talks from academic and government labs who've successfully deployed methods to enhance their infrastructure support for data intensive science. Check out the full agenda for the day here . These sessions sound particularly relevant for those researching and supporting large-scale genomics and bioinformatics projects. \n \n Joint Techs Meeting Netcast Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/6536223172088465316/comments/default", "bloglinks": {}, "links": {"http://events.internet2.edu/": 3}, "blogtitle": "Getting Genetics Done"}, {"content": ["Lately I've been using the limma package often for analyzing microarray data. When I read in Affy CEL files using ReadAffy(), the resulting ExpressionSet won't contain any featureData annotation. Consequentially, when I run topTable to get a list of differentially expressed genes, there's no annotation information other than the Affymetrix probeset IDs or transcript cluster IDs. There are other ways of annotating these results (INNER JOIN to a MySQL database, biomaRt , etc), but I would like to have the output from topTable already annotated with gene information. Ideally, I could annotate each probeset ID with a gene symbol, gene name, Ensembl ID, and have that Ensembl ID hyperlink out to the Ensembl genome browser. With some help from Gordon Smyth on the Bioconductor Mailing list, I found that annotating the ExpressionSet object results in the output from topTable also being annotated. \n \nThe results from topTable are pretty uninformative without annotation: \n \n \nAfter annotation: \n \n \nYou can generate an HTML file with clickable links to the Ensembl Genome Browser for each gene: \n \n \n \n \n \n \nHere's the R code to do it: \n Getting Genetics Done by Stephen Turner is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License."], "link": "http://gettinggeneticsdone.blogspot.com/feeds/4779486207616584590/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "https://stat.ethz.ch/": 1, "http://www.bioconductor.org/": 1, "http://bioconductor.org/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "Getting Genetics Done"}]