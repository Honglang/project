[{"blogurl": "http://core-genomics.blogspot.com\n", "blogroll": [], "title": "CoreGenomics"}, {"content": ["We have a tradition in the lab that you bring cakes in on your birthday. Home-made is always encouraged and one of my favourite party bakes is this Millionaire's shortbread. Enjoy.    Mmmm, sweeet!!!  Make shortbread: 200g/7oz butter 115g/4oz caster sugar 285g/10oz plain flour Cream the butter and sugar, beat till fluffy, add flour to make a dough. Spread out in an even layer in a 22x30cm/9x12inch tin. Bake in a pre-heated oven at 180C/350F/gas mark 4 for 20 minutes or until golden-brown. Allow to cool to room temperature. Make caramel filling: 115g/4oz butter 115g/4oz caster sugar 400g/14oz can of condensed milk Put all the ingredients into a saucepan and gently melt the butter and dissolve the sugar. Once dissolved turn up the heat and bring to the boil for five minutes, stirring constantly. Do not touch this as it most closely resembles sweet napalm! Cool for one minute in a sink filled with cold water. Pour over the cool shortbread and allow to coll and set at room temperature. Make the topping: 115g/4oz plain chocolate Melt the chocolate and pour over the cooled and set caramel. Mark into fingers but do not cut until the chocolate is cooled and set. Cut into fingers or squares and serve.  Recipie from Leiths baking bible , buy it and never look back!"], "link": "http://core-genomics.blogspot.com/feeds/9218554608367373582/comments/default", "bloglinks": {}, "links": {"http://www.co.uk/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["In a few years hopefully every cancer patient in the UK will be screened for the most common somatic mutations. CRUK's Stratified Medicine Project has already tested over 5000 patients as part of a program to roll-out more uniform testing in the NHS. There has been an explosion of interest in genomic medicine driven along by the release of next-generation sequencing instruments like MiSeq and PGM, as well as the development of methods to assay small numbers of loci at very low cost and fast turnaround. Many academic centre's are working on NGS tests using amplicons or capture of gene panels or even exomes. We have been working with the Fluidigm AccessArray system for a number of years and using this with a MiSeq it is possible to sequence, in duplicate, 48 patients for 48 loci of 150-250bp at over 1000x coverage for just \u00a320 each. This is a very low cost compared to other medical tests and the potential of somatic screening is so great that I think it has to be made available to as many patients as possible. Of course sequencing more can often be better and with technologies like Nextera capture, Ampliseq, Haloplex, etc an awful lot can be sequenced nowadays pretty easily.One company that sems to be putting all the right pieces together is Foundation Medicine... Foundation Medicine: is a relatively new company making big strides in cancer genomic testing. They are trying to take recent advances in our understanding of cancer genomics to inform patient treatment, and aim to \" improve day-to-day care for patients by serving the needs of clinicians, academic researchers and drug developers to help advance the science of molecular medicine in cancer\". Find out more on their website www.foundationmedicine.com . About six months ago they released the Foundation One caner genome profiling test. This targets all known cancer drivers as well as genes somatically altered in Human cancer that have known therapeutic impact. The test uses and NGS capture-based assay and reports on genomic alterations including substitutions, insertions, deletions, copy number changes and some rearrangements. It can be run on any solid tumor with an input requirement of just 50ng FFPE DNA, and is run in their own CLIA labs.    The Foundation One test : Foundation Medicine are using in-solution capture of NGS libraries. They prepare sequencing libraries from very low amounts of starting material, and it is not clear to me which technology they are using (TruSeq, Rubicon, Nextera, or something else). Libraries are captured using Agilent SureSelect and sequenced on HiSeq. The poster: there is a very nice poster from 2012 ASCOmeeting available on their website. This presents the results from the first 304 commercial cases run on the assay, which sequences the all exons of 182 cancer genes (over 3000 exons) plus introns from 14 commonly rearranged cancer genes to >500x coverage. The poster has a nice graph showing how penetrant each gene is in cancer. It reads like the usual Top 10 list with, TP53, KRAS and PIK3CA in the top 3.   Commonly altered gens in Cancer identified by FoundationOne  A recent paper in European Urology presented the Foundation One test results from 45 Prostate cancer patients using 50ng of FFPE DNA used in hybrid capture and sequencing of targeted loci to over 900x coverage. They found mutations and alterations in AR, TMPRSS2:ERG fusions; and loss of or mutation in PTEN, TP53, RB, MYC, and PIK3CA. They also found alterations in the key DNA repair genes BRCA2 and ATM which they suggested could be targets for PARP inhibitors, and discovered an actionable rearrangement involving BRAF. In an earlier paper in Nature Medicine , they found ALK and RET mutations when testing 64 colorectal or non\u2013small cell lung cancers ( this is also available as a poster on their website ). Interestingly the earlier paper described a test analysing just over 2500 exons in 145 cancer-relevant genes with the same number (37) of introns from 14 commonly rearranged cancer genes. This suggests to me that Foundation Medicine understand fully the need to keep the test current and are watching the literature for new candidates to add. It does make me wonder though if smaller panels might be the best way to go for general screening of all patients first? The reports: There is not a copy of a Foundation One report available on their website but you can get a good idea of what is covered by watching the video on their website. The test is supported by a bioinformatic pipeline that sits on top of a curated database of research and clinical publications, as well as previous cases. The mutational profile is reported along with suggestions for targeted therapy and clinical trial opportunities.    A FoundationOne report  One of the challenges discussed in rolling cancer genome analysis out to the clinic has been teaching people how to interpret the data. Foundation Medicine is not the first to offer genomic data backed up by published research ( my 23andMe profile was pretty clearly explained ), but they have obviously seen that to make the test easy to adopt they need to make it easy to understand."], "link": "http://core-genomics.blogspot.com/feeds/3602923892622109118/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://core-genomics.co.uk/": 1, "http://1.blogspot.com/": 1, "http://www.foundationmedicine.com/": 3, "http://www.nature.com/": 1, "http://www.foundationone.com/": 2, "http://rubinlab.cornell.edu/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["The deadline for submitting your registration and abstract to ABGT closes tonight! As I said a few weeks ago this year the organisers are trying harder than ever to make sure the meeting reaches as wide an audience as possible. There are always grumblings from us but they really are working hard to please us!  You have until midnight tonight to register and send in your abstract if you want to present a talk or poster . After this everyone will have to wait until December 1st to see if they got a place. Then you can book your flights and buy some suncream.  Good luck to you. I am not going this year due to family commitments so I'll be reading blogs and listening (is that what you do) to Tweets to keep up with all the news.  Have fun."], "link": "http://core-genomics.blogspot.com/feeds/6438924275239750994/comments/default", "bloglinks": {}, "links": {"http://www.blogger.com/": 1, "http://core-genomics.co.uk/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["I finally saw an image of a 2500 flowcell so you can stop looking at the one I mocked up based on discussions with Illumina personnel just before AGBT. One thing immediately struck me and that was how much a HiSeq 2500 flowcell looks like an old fashioned razor blade? OK, you may have to squint to see the similarity!  I am sure many of you have heard of the razor and razor-blades business model? It is where the hardware (HiSeq 2500 in this analogy) is sold off pretty cheaply while the consumables (flowcells and SBS reagents) are marked up to bring in the real profits. Why then does a HiSeq 2500 cost $700,000? PS: I can't be the only person looking forward to real \"genomes-in-a-day\"? 20x coverage is for wimps! ;-)"], "link": "http://core-genomics.blogspot.com/feeds/1878328182614726784/comments/default", "bloglinks": {}, "links": {"http://2.blogspot.com/": 1, "http://core-genomics.co.uk/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["PCR was invented way back in 1985 and the first thermo-cyclers were released in Before this PCRs were done in tubes placed in separate water-baths. Polymerase was also added at each new cycle until someone worked out that Taq ( discovered in 1976 by University of Cincinnati researcher Alic Chien ) might be a good alternative for the standard DNA polymerase. The first PCR machine was produced by Cetus in 1988 called \"Mr Cycle\" but required the use of fresh enzyme after each cycle. In 1998 Perkin Elmer released the first automated thermal-cycler that all instruments we know and love today are based on.    My Cycle  You can see some really old instruments on the LifeTechnologies website , where they are giving away a free Veriti PCR machine for entrants into their competition. You can see old Hybaid, MJR, Perkin Elmer and other instruments. Look hard for the one you first used, mine was the MJR PTC-100. Work got a lot easier with the release of the Tetrad machines because fights no longer broke out over who had/not booked the machine before going home!   Screenshot from Life Tech competition website   PS: If you fancy running 230,400 PCRs in one go give the Soellex a try. A water-bath PCR machine that hod 600 384 well plates."], "link": "http://core-genomics.blogspot.com/feeds/3507573501847023070/comments/default", "bloglinks": {}, "links": {"http://resource.lifetechnologies.com/": 1, "http://classic.the-scientist.com/": 1, "http://www.douglasscientific.com/": 1, "http://marketing.appliedbiosystems.com/": 1, "http://www.nih.gov/": 1, "http://4.blogspot.com/": 2}, "blogtitle": "CoreGenomics"}, {"content": ["Design, replication, multiplexing. These are the three things I spend most of my time discussing in experimental design meetings. In my institute we hold three 30-minute design sessions every week where users talk to my group and our bioinformatics core about how best to run a particular experiment. I do believe our relatively short discussions have a big impact on the final experiments and the most common issues are the three I listed at the start. Of course we spend lots of time talking about the pro\u2019s and con\u2019s of different methods, RNA-seq vs arrays or mRNA-seq vs Ribozero RNA-seq for instance, but the big three get discussed even if it is clear what method should be used. I'd encourage everyone to think about experimental design as much as possible. Simply thinking about the next step in your experimental process is not enough. Take time to plan where your experiments are going and what are the most logical steps to get there. Then make sure each experiment is planned to make best use of your available resources. Even cheap experiments can end up being expensive in lost time. Don't save experimental design for more costly array or sequencing based projects! Design: This is important because it suggests that an experiment has had more than one person think about it more than once. Even \u201csimple\u201d experiments often have confounding factors that need to be considered; or require assumptions to be made about steps in the experiment where real data might turn out to be sorely lacking. Designing an experiment often means sitting down and listing all the issues that might affect the results and highlighting the things that can, or can\u2019t be done to mitigate of remove these issues. This can be done by anyone with sufficient experience of the experiment being performed. We find it is best done together over a cup of tea or lunch in an informal discussion, just like our design sessions!  Replicates: Replication is vital in almost all experiments. Only if an experiment is truly limited to being done once should replication be ignored. Most people can come up with multitudes of reasons as to why more replicates are a bad idea. However when confronted with data showing how increasing replicate numbers can make experiments more robust and more likely to find significant differences, many users are persuaded to add in four or even more replicates per group. Biological replication is king and technical replicates are often a waste of time. Be wary of pooling samples to make replicates appear tighter, you are losing information about the biological distribution of your data that might be meaningful.  We find four replicates is the minimum to consider for almost all experiments. Three works well but if one samples fails to generate results a whole experiment can be rendered useless. Four gives a big step up in the ability to detect differences between groups, five adds even more power but after six replicates many experiments start to tail off in this additional power. Unfortunately it is difficult to predict the number of replicates needed to get the best \u00a3:power ratio. It is easily done after the experiment is complete, and I have yet to go to a statistical seminar that does not put Fishers \u201c   statistical post mortem\" quote* up at some point to ram this home!  * Currently at number 3 in the famous statistician quotes chart!  Multiplexing: For me this is the one people seem to forget to think about. I think I am convincing many that the correct number of samples to multiplex in a single NGS run is all of your samples . Rather than run 4, 12 or 24 samples per lane and always stick to this I prefer to argue that having all the samples in a pool and running multiple-lanes makes the experiments more robust to any sequencing issues. If a lane has problems then there is still likely to be lots of data from all the other lanes in the run.  There are also some issues with demultiplexing low-plex pools on Illumina as the software struggles to identify clusters correctly if they are too similar. We have had users submit libraries for sequencing with just two or three samples pooled. These have failed to generate the usual yield of data and demultiplexing is horrible. There is nothing we can do and it has been frustrating explaining to users that there four carefully pooled libraries have all failed when if they had just mixed all the samples together in one super-pool and run 4 lanes everything would have been fine!  Putting it all together: When we plan experiments now I try to ask how many reads might be needed per sample for a specific application. Once this is fixed then we can decide on replicate numbers for the experiment. Finally we can work out how many lanes are likely to be needed given the variability of Illumina sequencing. If an extra lane is needed later on there is enough data to start analysis, but often we don\u2019t need more data and the sequencing becomes as efficient as possible.  PS: feel free to comment on any aspect of design discussed or missed here. Please don't ask me for help designing your projects though!"], "link": "http://core-genomics.blogspot.com/feeds/4574112120146124643/comments/default", "bloglinks": {}, "links": {"http://www.brainyquote.com/": 1, "http://stats.stackexchange.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["George Church is one of the \u201cgodfathers of genomics\u201d*. In one of his latest publications, Next-Generation Digital Information Storage in DNA he demonstrates how to use DNA as an information storage medium. He\u2019s not the first to do this and the supplemental information to the paper lists ten other references, but his is the best example so far.  *George Church, along with 15 others including; Walter Gilbert, Leroy Hood and John Sulston was one of attendees at the 1984 Alta conference where the Human Genome Project was conceived. He published the first method for sequencing of methylation sites in 1984, Genomic sequencing in PNAS. George is very open access, see his unathorised autobiography . In fact he is so open-access I wonder if he could be a godfather of that too!  The paper describes how the text, images and a JavaScript program from the book Regenesis were converted to DNA in a readily amplifiable and readable form. This is not something just anyone can read though; in the paper they used 170M PE100bp reads from a HiSeq lane. This makes it a very expensive book in a format not compatible with a Kindle!  How do you turn a book into a library: George used Agilent\u2019s programmable eArrays to make the DNA version of the book. After synthesis the oligo\u2019s were cleaved from the array into a pool that was PCR amplified with Illumina compatible primers ready for sequencing. You can buy an 8x60k eArray for the equivalent of about \u00a3100 per book.  How much sequencing do you need to do: The sequencing was 3000x fold coverage and geivn the aibtily of Hmunas to raed smrlcbaed text I suspect that level of redundancy is massive overkill. Reducing read lengths to PE75 and using slightly longer fragments (150 vs 115) would decrease the costs of sequencing. George used 54,898 115bp oligos each carrying an address and 12x8bit sequences, increasing this to 16x8 would result in a 151bp oligo and only require 41,000 fragments. Even low coverage sequencing could be completed on a MiSeq or PGM.   \"Encoding and decoding\" DNA from the paper As DNA read-lengths increase, especially out to the 100kb Oxford NanoPore presented, then reading become a matter of only a few reads. Georges book could be read by just 52x100kb ONT reads. Perhaps combining the oligo production with Craig Venter\u2019s artificial lifemethods would be the way to go?  Fancy giving it a try yourself, the code is available here, Bits2DNA.pl and some of you have sequencers ready to run in the lab.  PS: George Church did the experiments himself. His supplementary information is excellent, probably the best I have read for being able to actually repeat the experiment. It also appears that George has written like this for most of his research career, the methods in his 1984 paper are just as comprehensive and concise. I wish everyone (myself included) wrote this level of detail so succinctly.  PPS: if George Church is reading this then please accept an open invitation to coffee next time you are in Cambridge, UK."], "link": "http://core-genomics.blogspot.com/feeds/507060425229459367/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://www.amazon.com/": 1, "http://www.sciencemag.org/": 3, "http://bits2dna.pl/": 1, "http://arep.harvard.edu/": 2}, "blogtitle": "CoreGenomics"}, {"content": [], "link": "http://core-genomics.blogspot.com/feeds/7973555488133237044/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "CoreGenomics"}, {"content": ["A few weeks ago I wrote about an idea I had been thinking about for a while for using SNPs as additional content in capture or amplicon assays to provide patient identification at the same time as generating test data. I have many responses from others that would like to do this and the question I have been asked most is which SNPs should we be using.  I\u2019m afraid I can\u2019t answer that question and think a community discussion would be the best way forwards although I am not sure how best to kick-start that discussion. If you have any ideas do let me know! I\u2019ll ask CRUK\u2019s StratMed board for starters.  One person also pointed to a couple of papers and Sequenom\u2019s iPlex sample ID panel . The assay runs a 52SNP multiplex PCR on the MassArray sysytem. I think the biggest thing missing from the SPIA assay on Sequenom is the lack of coding SNP information. I suggested in the older post that using coding SNPs would allow the assay to be more widely used in research settings and even in future clinical RNA-seq based gene expression panels. PAm50-seq perhaps? I'm getting some designs done to test different methods in the lab. Feel free to send me suggestions on what criteria should be considered when choosing SNPs.   Paper 1:  Sanchez et al; A multiplex assay with 52 single nucleotide polymorphisms for human identification . The authors are members fo the EU SNP for ID project they r ecommended 52 SNP markers in t he publication where they describe their choice of 52 SNP markers and development and validation of a multiplex PCR assay. The project webist has a very nice tool to visualise SNP distribution across different populations .   Their SNP selction criteria were;  i)   Maximum 120bp amplicon size ii)    Minimum 0.28 MAF in one and 0.17 in at least three populations ii i)   Random distribution of SNPs iv )   100kb distance from neighboring marker SNPs or genes v)   flanking DNA sequence reliably reported and free from interfering poly- morphisms, such as nucleotide substitutions in potential primer binding sites.  They analysed SNPs using single-base extension (SBE) with ABI SNaPshot kits, products were detected on a capillary sequencer and analysed with GeneScan software.  Paper 2: Demichelis et al; SNP panel identification assay (SPIA): a genetic- based assay for the identification of cell lines . Used cell lines run on Affymetrix SNP genotyping arrays to identify candiate SNPs. They commented in their paper that they would have preferred to use whole genome genotype data from a larger number of samples to rank SNPs that best distinguish sampes and using an iterative design could define the \u201cmost accurate and parsimonious panel [] and how many [SNPs] are needed\u201d. They used cell lines typed on Affymetrix XBba 50k arrays. Their SNP selection criteria were;  i)    SNPs have assigned rs identifier ii)    SNPs are not located in intronic regions iii)   SNPs are also represented on the 10 K Affymetrix oligonucleotide array.  Their iterative testing generated an average of 30-40 SNPs as the optimum. They developed a SPIA test panel to run on Sequenom\u2019s MassArray which uses multiplex PCR as the method to target specific genomic loci. The SPIA analysis tool is written in R and available here . Their conclusion was that any 40 SNPs from their top 100 would produce a good SNP panel for Human identification but that more SNPs equals more confidence! They also showed that a SNP-ID panel can be used to monitor genetic drift of cell lines during passaging."], "link": "http://core-genomics.blogspot.com/feeds/4238420502362894229/comments/default", "bloglinks": {}, "links": {"http://spsmart.cesga.es/": 1, "http://core-genomics.co.uk/": 1, "http://www.snpforid.org/": 1, "http://www.sequenom.com/": 1, "http://www.nih.gov/": 2, "http://2.blogspot.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["I wanted to point my readers to BiteSizeBio as I have been writing for them for about a year now and my latest post has just been published on the NGS channel, A Short History of Sequencing Part 1 .  BiteSizeBio offers; \"brain food for biologists\". It offers short articles on all things biological from Analytical Chemistry and Bioinformatics to Statistics and Technologies and Techniques . There are tips on how to survive in the lab and fun stuff as well. New to BiteSizeBio are their blog channels dedicated to particular topics. My latest post is on the Next Generation Sequencing channel, there are others for Cloning & Expression , Microscopy & Imaging and Writing / Publishing / Presenting . I'd encourage you to also get involved by contributing your own articles or by submitting a tip to make life easier in the lab. I started writing my blog and the articles for BiteSizeBio because I wanted to do more writing and most of it is done in my spare time at home or during lunch at work. I hope it is helping when it comes to writing or reviewing papers for work, but even if it does not help there it is very enjoyable."], "link": "http://core-genomics.blogspot.com/feeds/5042499817454214828/comments/default", "bloglinks": {}, "links": {"http://bitesizebio.com/": 15, "http://4.blogspot.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["There has been a battle going on between Life Technologies and Illumina for NGS supremacy, especially over the personal sequencer market. I am sure you saw the \"PGM vs MiSeq\" videos on YouTube ( 1 , 2 & 3 )? If you didn't they are worth a look.  However I'm sorry to say Illumina still don't gift wrap!  Illumina and Life will always spend a fair proportion of income on marketing. In their 2011 annual reports they spent $261M and $1023M respectively on \"selling, general and administrative\" which I guess includes marketing somewhere. As a percentage of revenue this is 24% for Illumina and 29% for Life Technologies.  Where is the money being spent today: two recent marketing activities caught my eye, I don't prefer one over the other and they both target very different audiences. Both companies are doing much more than these two examples.  Illumina MiSeq grant: Illumina are hoping to tempt potential customers by awarding three grants to use the MiSeq system. Awardees will receive a MiSeq, 10 sequencing kits, a sample prep kit (Nextera or TSCA) and access to BaseSpace. It does not matter where you are based or if you have experience with NGs, just submit your idea and keep your fingers crossed.     Ion Torrent's Mini tour: The Ion Torrent bus is still touring the USA, what better to tour the UK than a Mini (shame their German now). The tour is coming to my Institute next week and due to the quick run time of the PGM you can watch a run being setup, listen to a seminar and talk to the sales team, have lunch (I am not sure if lunch is included) and see the results from the run. All in about 2.5 hours. It should be fun and I hope to see some of you there. Register for the 24th of September visit to the CRI.     12.30 Meet the PGM in the back seat of a Mini 12.45 Start the PGM run  13.00 \"Enjoy\" a technical seminar, no heckling about MiSeq or GS Junior please 15.00 See the results from the PGM run   I am not endorsing either of these particular marketing events. I hope you like them though because we are all paying for them with the instruments and kits we are buying! Think about how much of the fun at AGBT is sponsored by PacBio (Gold), Agilent & Roche (Silver) and Caliper, Illumina, Complete Genomics, NuGen, RainDance and Advanced Analytical (Bronze). Life Tech were sleeping over in the bus and sneaking into the bar!  PS: If Ion want to get rid of a Mini after the event I need a new car. I'd prefer them to follow my lead on a BMW R1200GS motorbike fitted out as a mobile sequencing lab though!"], "link": "http://core-genomics.blogspot.com/feeds/3104912353463837495/comments/default", "bloglinks": {}, "links": {"http://core-genomics.co.uk/": 1, "http://www.sequencingforall.com/": 3, "http://www.illumina.com/": 2, "http://www.youtube.com/": 3, "http://www.iontorrent.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["Illumina just announced that they have aquired the Cambridge based BlueGnome , the company produces their own arrays for genetic screening and the BlueFuse analysis package.  BlueGnome formed in 2002 to commercialise the BlueFuse microarray analysis software. This was the first software I know of that used Bayesian algorithms to generate intensity values for each spot   on the array. It worked very well and I was interviewd for a job selling it way back in about 2003 (I did not get the job).  In 2006 BlueGnome launched CytoChip for genetic screening and according to company stats over 100,000 have been run in 40 countries. In 2009 the first BlueGnome baby was born (I am not joking)! BlueGnome had developed a 24Sure , preimplantation genetic  screening (PGS) test to screen for all 24 chromosomes to enable selection of normal chromosome complement eggs. The technology allowed a woman to have her 14th and final IVF cycle which was successful due to the selection of an egg that formed a high-quality embryo  Illumina are buying BlueGnome to push into the IVF market. Illumina wants to be a big diagnostics player and recently released test kits for Cancer and other diseases. IVF and fertility is another obvious clinical step.  BlueGnome also sell a library of 26,000 BlueFish fluorescent in-situ hybridisation probes. Probes are labelled by BlueGnome and sent directyl to users for running in their own labs removing the need to produce probes. FISH can be an important tool in cancer diagnostics as well, something Illumina can't have failed to spot.  You can find out more by watching BlueGnome TV .  Hey, I thought Agilent made BlueGnome chips: Yup, they do (did?) According to a March GenomeWeb report Agilent manufactures all BlueGnome's oligo-arrays. Although the CytoCHIP and 24sure are made internally with BAC probes and spotting. Exactly what the deal meams for Agilent is difficult to say but it is another blow from Illumina. Agilent and Illumina used to have cosy co-marketing deals but are more often chasing at each others heels over technologies related to sample prep."], "link": "http://core-genomics.blogspot.com/feeds/5830153348730235787/comments/default", "bloglinks": {}, "links": {"http://www.cambridgebluegnome.com/": 4, "http://www.genomeweb.com/": 1, "http://investor.illumina.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["The 2013 AGBT meeting runs from the 20th to the 23rd of February and, as always, is in sunny Marco Island, Florida. This annual meeting is the one to watch for announcements around new genomics technologies. Last years hot topic was Oxford Nanopore\u2019s announcement of MinION (although no-one appears to have head anything since). What will be the number one story this year? Well the 2013 meeting has an increased focus on clinical applications of NGS, so perhaps GnuBio will wow everyone with their technology, or perhaps someone will speak about exciting methods for analysis of circulating tumour DNA? ;-)   Buy your lottery ticket soon:    Registration opens on October 1st and this year all applications will go into a clearing system to allocate the 1000 or so available spaces. One of the more common gripes from users has been the registration process. AGBT in 2011 & 2012 sold out in a matter of hours, the scientific equivalent of a a Stone Roses concert! This is not the conference organisers fault but rather shows how important the meeting is to users of NGS.  It perhaps also has something to do with being in Florida in February. Sun, sea and seqs anyone?  The organisers are going to limit the number of attendees from individual labs and institutes so expect arguments at Broad, Sanger and WashU in December. Registration includes the meeting, hotel costs and meals and the legendary AGBT party. Last year one post-doc was there in a home-made Storm Trooper costume that cost over $1000. I\u2019m not sure what the theme is this year but you can expect more nerdy fun.  A list of things to do at AGBT:  Enter the competitions: last year I won an iPad2. I entered almost every competition going and there were over 20 big prizes on offer including iPads, Apple TV\u2019s, etc. The odds are pretty good with around 1000 attendees and most not bothering to enter.  Attend the parties: last years parties were as good as usual. Lots of choice on offer and Agilents video wall was fantastic. A big headache with AGBT is the entertainment on offer, I am supposed to be working the next day for goodness sake! Oh well you can always sleep on the plane home.  Catch up in the bar: AGBT is a great place to catch up with colleagues you have not seen for ages. Everyone is in party mood and usually excited by the pre-AGBT announcements. Lots of ideas get floated around and who knows your next big collaboration might start over a beer.  Don\u2019t forget your bag: everyone at AGBT ends up with the same bag as they give out an exceptionally good one compared to most conferences (now I know where my $2400 registration fee goes). Wirte your name in your notebook or use a different bag for carrying round the meeting as it is too easy to get yours mixed up and lose all your notes.  Follow the conference on-line: MassGenomics , EdgeBio , Core-Genomics , Jonathan Eisen's blog , Omically Speaking , Pathogens Genes and Genomes all had coverage if you could not make it or get in and dont forget CrapBio, there #notAGBT announcement was the laugh of AGBT. Twitter will again have the #AGBT to follow.  Don\u2019t forget the suncream: It was hot last year and swimming in the sea was certainly the order of the day. It being February I did not even think to bring suncream but fortunately I think Nanostring gave out little pumps of SPF50. Who wants to go home with sunburn!  Blogging and tweeting: I tried blogging through the whole meeting last year and am not sure I\u2019ll try again. It is hard work making quickly scribbled notes understandable to a wider audience and it took too much time. Time I could have spent in the bar! There are very clear policies on blogging or tweeting at this years meeting although to go by last year most speakers are happy to get the coverage. I saw a large spike in readers of my blog and Core-Genomics received over 10,000 page views in February. Thanks to everyone that commented.  Changes to AGBT: One really annoying problem with AGBT (and all parallel session meetings) is the inevitable overrun of talks, stronger chairing and timekeeping would really help in moving between sessions to catch all the talks you want to hear. There were hundreds of posters last year; far too many even to walk past. It would be great if these were all online, downloadable and searchable a few weeks beforehand so I could arrange to speak with the person presenting the poster with specific questions. It would also help if they were collected together under different themes using tags of some kind. I\u2019d like to see more people coming from the developing world this year. NGS is not just for the rich (although it helps to have a few $Million). I\u2019d also like to have a session for core facility labs, perhaps run by ABRF . Talking to other core labs is always useful, even if it just helps to reassure me that I am not the only person facing particular issues.   See you there (I hope). PS: If anyone wants to share a room at next years meeting feel free to leave a comment. Double occupancy saves my budget a ton of cash and as my lab is funded by public donations I don\u2019t see any reason not to share. Please don\u2019t \u201capply\u201d if you snore!"], "link": "http://core-genomics.blogspot.com/feeds/1182212969374772216/comments/default", "bloglinks": {}, "links": {"http://core-genomics.co.uk/": 1, "http://massgenomics.org/": 1, "http://www.edgebio.com/": 1, "http://1.blogspot.com/": 1, "http://www.omespeak.com/blog": 1, "http://agbt.org/": 1, "http://www.abrf.org/": 1, "http://pathogenomics.ac.uk/blog": 2, "http://www.co.uk/": 1, "http://phylogenomics.co.uk/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["There has been lots of recent activity around using NGS gene resequencing in the clinic. Although clinical DNA sequencing has been an important tool for several decades the explosion in NGS methods for amplicon resequencing has made it feasible for just about any lab to do. Previous posts on this blog have discussed NGS amplicon methods and some of the tools needed to design amplicons.  It is not easy to say clear which technologie(s) will dominate in the clinical space, nor whether small targeted panels will be preferred over more comprehensive and larger panels, medical exomes or even whole genomes. But it does seem pretty clear that amplicon-sequencing is going to be a very important clinical tool. Why is patient ID important: As we are more easily able to sequence not just multiple genes but also multiple patients in s single NGS run it becomes very important to make sure results are not assigned to the wrong patient. Clinical molecular labs spend a huge amount of time and effort on making sure results don\u2019t get mixed up, but I thought the tests themselves could be improved to determine which patient results came from at the same time as the clinical results are being generated. Just add a large enough number of SNP loci to allow patient identification by comparison to a simple blood-based genotyping assay. SNP-seq for patient ID: I have been discussing using additional content in amplicon (and other) tests for a year or so, but have never found the time to get in the lab and demonstrate the idea. I asked our Tech-Transfer people about it and they said whilst it was a nice idea there was little that could be protected from an IP perspective. As I am not going to get time to work on it, and as it can\u2019t be protected easily I am hoping this blog will help stimulate discussion and someone will take the idea on board for their research. I call the method HuID-seq.  Comparison to STR profiling: There is already a gold-standard for Human Identification in the STR profiling used in forensic applications. Unfortunately the tests cannot be simply added to an NGS assay. What we need is a level of discrimination so that results cannot be sent to the wrong patient, in theory the HuID-seq could be set at a level significantly lower than forensic STRs. Today 13 STR loci are used in the United States Combined DNA Index System (CODIS) forensic kits. SNPs have lower resolving power and more are likely to be needed but before I get onto that a recent paper deserves a mention. In Biotechniques Bornman et al published an NGS based method that reproduces STR data very nicely (see Short-read, high-throughput sequencing technology for STR genotyping ). Potentially this could be added to current tests but I prefer SNPs for a number of reasons.  Why SNPs, which ones and how many: SNPs are a good choice because they are easily assayed by PCR amplification of by in-solution capture methods. SNPs are also already assayed by current NGS methods and perhaps most importantly if coding SNPs are used then RNA-seq data can also be used for HuID-seq. This will be important if array-based gene expression signatures are ported over to NGS. It also means that the HuID-seq method could be used to very good effect in research projects whatever the source of data. It is often important to quality control large experimental datasets to remove duplicate samples or wrongly assigned samples. In the supplementary information for the Nature paper The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups , the authors presented a novel eQTL-like approach to check that the same patients samples were used for whole genome gene-expression and SNP genotyping arrays. They termed the method BeadArray Diagnostic for Genotype and Expression Relationships (BADGER).  Which ones: There were around 250,000 SNPs present on both Affymetrix and Illumina arrays. 2000 could be used for identification, 100 for ethnicity and 200 for gender. All SNPs would allow mapping of samples onto other SNP or sequence based data, e.g. SNP arrays, ChIP-seq, RNA-seq and exomes or genomes. We looked carefully at which of these SNPs might be used in a HuID-seq method and came up with some requirements.  MAF 0.5 (0.4-0.6) Present on most widely used genotyping arrays. Coding SNPs Ability to predict identity Ability to predict ethnicity Ability to predict gender  How many: The number of SNPs is going to be higher than the STR loci, but it need not be very high and the \u201creal-estate\u201d taken up by such a HuID-seq panel need not be large in comparison to an NGS clinical test. SNPs need to be present very broadly in the population to be of any use, ideally using SNPs with MAF0.5 gives any individual a 50:50 chance of being homozygous for one allele or heterozygous. If this 50:50 chance holds true for all SNPs, and if the assay used is perfect then (i.e. no errors in genotyping) then just 4 SNPs give a >90% chance of uniquely identifying an individual. Increasing this to 8 results in a 99.5% and 20 SNPs gives a 99.9999% chance. Choosing a set of SNPS such that there is at least one per chromosome arm leads to a set of about 48. The final number of SNPs used could be determined by the requirement for unique identification, cost or complexity of PCR.  Community cohesion: It makes sense that if this approach is going to be used then everyone should use the same set of SNPs. The best way to do this would be to get people from different clinical and research backgrounds in a room to discuss the why\u2019s and wherefores\u2019 of different SNPs and to recommend a set to the community. If Life Tech, Illumina, Roche, or others get marketing too early on then we are likely to see multiple standards. This is exactly what we have in STR kits with the US and Europe using different sets of loci.  There are already some SNPs being used to QC data by the Broad but the GATK pages have been updated and the older page is now a dead link! LifeTech are launching an 8 SNP Ampliseq based Sample ID kit giving a resolution of 1:5000, you spike the SNP targeting reagents into your assay and go. Illumina are aslo collaborating on forensic products, Dr. Bruce Budowle at the Institute of Applied Genetics, University of North Texas discusses forensic NGS applications on You Tube with the IlluminaInc channel. It probably has not escaped your notice that a HuID-seq panel could be used for cell line authentication as well. This is something that often gets attention but is easily forgotten by PhD students and post-docs until too late. Making a test part of their ChIP-seq or RNA-seq experiment and comparing back to a reference database would be a simple ad-on."], "link": "http://core-genomics.blogspot.com/feeds/1285829671624993125/comments/default", "bloglinks": {}, "links": {"http://www.youtube.com/": 2, "http://www.nih.gov/": 3, "http://www.broadinstitute.org/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["One of the things I most like about my job is being allowed to think. NGS has provided a very fertile ground for innovation and lots of novel ideas have become products we use regularly today. It is sometimes to easy to forget that any of us can have ideas that are just as good. The missing bit is usually the drive to try and commercialise or patent the idea. I've tried several times with many ideas but have not been successful yet. In this post I will describe one idea I was sure was going to be the winning one, although in the end someone else had beaten me to it (gues who). The problem: Illumina NGS systems require very careful quantification of the final sequencing library to generate cluster densities that give the highest yield from a flowcell. Getting QT wrong means over-or under clustering both of which lead to a drop in yield. And over-clustering can mean zero yield or lower quality data. Illumina recommends qPCR (although the method suggested has a few flaws) and many labs have had success wth qPCR, Bioanalyser, QuBit and other QT methods.    Standard Illumina clustering  But wouldn't it be nice if we could just put an unquantified library into a flowcell and always get the correct cluster density? I thought so and came up with a method to do just that. The idea: basically I wanted to make sure only a certain number of discrete library molecules could hybridise to the flowcell surface for the initial hyb, but allow clustering to proceed normally afterwards. There are currently two oligos in the lawn on the flowcell surface, complementary to the ends of the Illumina adapters. I proposed adding a third \u201ccluster seeding\u201d oligo at a lower concentration. This oligo would have the same sequence as the current flowcell oligos but would be longer and include slightly more of the adapter sequence. Flowcells would be created with a similar lawn of primers such that the seed oligo is present at a spacing consistent with maximal yield and cluster requirements. Additionally the lawn oligos would be blocked by short complementary primers leaving only the 5\u2019 end of the seed oligo unblocked for hybridization with library molecules. A covalently attached library molecule would be produced by polymerase extension. The first round of denaturation would remove the original library molecule and all blocking oligos. Bridge amplification would then proceed as normal.  This method would hopefully allow high, low and/or variable concentration of libraries to be entered into the flowcell without quantification as only a certain fraction would be able to hybridise. The yield of the flowcells would be significantly less variable.   This would remove the need for Illumina customers (you and I) to perform quantification and significantly increase yields from runs.    Seed-oligo clustering idea  Who got there first: Unfortunately when searching through patents to see if anyone else had a similar idea we found this patent US20090226975 by Illumina. They describe a similar method with a clever use of a hairpin oligo so the blocking is removed enzymatically. Fundamentally the same outcome is achievable and quantification can be considered a thing of the past.    US20090226975 figure 3  Why are we all running lots of qPCR: I don't understand why this has not made it out into the current generation of kits. All the users I talk to agree that removing quantification would be a good thing. Even if it is done perfectly it still takes an hour to do so time can be saved. Come on Illumina, where is it? Hopefully reading this has spurred you on to move ahead with the good idea you had a couple of months ago and left on the \"back burner\". I'll follow this post with another one about a completely different idea that is about to be realised by Life Technologies instead! Ho hum, spending my millions will just have to wait!"], "link": "http://core-genomics.blogspot.com/feeds/6063877304677675372/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://1.blogspot.com/": 2, "http://www.google.com/": 2}, "blogtitle": "CoreGenomics"}, {"content": ["Yesterday Illumina released their clinical research NGS kits called TruSight . TruSight: There are currently five kits in development, Cancer, Autism, Cardiomyopathy, Inherited disease and Human Gene Mutation Database Exome. Custom kits are sure to follow. Kits are designed to run on MiSeq ad the lab workflow is based on Exome capture protocols recently released as a new product from Illumina. Combining Nextera with TruSeq (or any other) capture was a smart move (s ee a previous post on this blog ). The simplicity of the library prep much better fits labs than the more complex standard adapter-ligation protocol. In the tests we did in my lab when we first tried the kits we found there was enough library from a 50ng input to sequence a small capture kit (TruSight for instance) and still have enough left over to capture an exome or possibly even sequence the whole genome. GeneSight: Illumina also announced a partnership with Patners Healthcare on the GeneSight software. Aiming to make this the tool clinical researchers, medical geneticists and pathologists use for analysis. GeneSight is ready for MiSeq data, BaseSpace and the iPad MyGenome app, and is already FDA registered. It should be possible to provide a workflow from sample prep, through sequencing all the way to final analysis and interpretation. The press release mentions that geneSight has been used for over 24000 tests. If this takes off then expect to see graphs similar to ones showing how NGS yield has increased, but for the number of patients reported on by the MiSeq TruSight combo! Geneinsight has been around since 2005 ( this is a good paper describing it ) and aims to help with data analysis is several ways. It acts as a repository for data in the form of case histories and variant information. It facilitates clinical reporting. And it will update clinicians as new variants become clinically relevant. This last feature is likely to cause some headaches as patients may need to be told their prognosis has changed based on new information. The GeneInsight network also allows labs to share results and new findings increasing the ease with which variants might be understood to be clinically relevant. Most of us are going to have to wait though. As is all too often the case with launches like this we have the information on how exciting this is going to be but no release date. A few pilot sites and Illumina\u2019s own CLIA labs will be the first to access the package. Competition within Illumina: TruSight will go hand-in-glove or head-to-head with the TheraSeq product Illumina recently made a pre-release announcement about. TheraSeq uses the TruSeq custom amplicon approach to target small numbers of clinically relevant genes, see a TheraSeq overview for more details. Currently in development are kits for Non-small cell lung cancer, Advanced metastatic disease and Gastrointestinal stromal tumor. It will work from FFPE tissue and again is likely to run on MiSeq. If you want to steer Illumina's clinical developments then why not take the survey on the TheraSeq page ? One of the interesting questions relates to the size of test panels asking if responders prefer small or large panels, and whether these should be dictated by current standard-of-care or expand beyond it. They also ask for opinions on returning variants of unknown significance. Both hot topics in diagnostic panel discussions. Also on the TheraSeq page is a link to an overview of Cancer research papers that used Illumina technology. Summary: Illumina, and all the other NGS companies, know how important clinical is going to be in their future growth. This is unlikely to be just through instrument, consumable and test kit sales. Service provision could be an important revenue generator and this is likely to put Illumina into direct competition with the people they are selling to today. Although according to GenomeWeb Matt Posard said that \u201cIllumina does not intend to offer its TruSight assays as a diagnostic service, because that would compete directly with its customers."], "link": "http://core-genomics.blogspot.com/feeds/503749261009474576/comments/default", "bloglinks": {}, "links": {"http://www.illumina.com/": 4, "http://www.nature.com/": 1, "https://www.surveymonkey.com/": 1, "http://core-genomics.co.uk/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["I posted last week about the emerging field of Immunogenomics . Today I\u2019ve taken a brief look at what is happening in Radiogenomics. Whilst this field is not using NGS in such a comprehensive way I think it can only be a matter of time before it ramps up.  Radiotherapy is an important tool in treating cancer and the impact of genomics on the field was recognised by researchers in Cambridge and Manchester in 2004. Those researchers started the Radiogenomics: Assessment of Polymorphisms for Predicting the Effects of Radiotherapy study (RAPPER) and also helped found the International Radiogenomics Consortium .  The ultimate aim of this consortium is to individualise radiation dose prescription for patients maximising the impact on the tumour whilst minimising normal tissue damage for the patient. They aim to find genomic variants that can help predict how patients will respond to radiotherapy and allow tailoring of treatment. This is somewhat similar to pharmacogenomics approaches used for drugs like Warfarin where SNP genotyping can help establish the correct dose for individual patients. The consortium should make it easier to collect samples for genomic studies and also spur development of methods for radiogenomic research.  The consortium is likely to also learn a lot more about the biology of radiation-induced tissue and DNA damage. Whilst understanding how individuals may respond to radiotherapy is a primary goal, hopefully a better understanding of biology may lead to a list of genes that might be mutated in tumours making them more susceptible to radiotherapy as well.  The Radiogenomics Consotium conducted a GWAS in radiotherapy patients ( Independent validation of genes and polymorphisms reported to be associated with radiation toxicity: a prospective analysis study. Lancet Oncol. 2012 ) to address concerns over how underpowered previous research on late side-effects had been. Late side-effects can have serious impacts on patients and their treatment. This prospective study genotyped 92 SNPs (selected from previous studies) in 1600 breast and prostate cancer patients using the Fluidigm 96.96 Dynamic Arrays . None of the SNPs previously reported to have a significant associations with radiation sensitivity were confirmed. The consortium suggested that the previous associations were \u201cdominated by false-positive associations due to small sample sizes, multiple testing, and the absence of rigorous independent validation attempts in the original studies\u201d.  As the costs of sequencing continue to fall and as associations are found it is likely that NGS will become a more important tool for the consortium. Longitudinal studies of cancer patients can be incredibly revealing and comparison of cancer genome and normal genome with radiotherapy follow up data is likely to yield interesting results."], "link": "http://core-genomics.blogspot.com/feeds/4499537002640167478/comments/default", "bloglinks": {}, "links": {"http://www.fluidigm.com/": 1, "http://ccge.ac.uk/": 1, "http://www.nih.gov/": 2, "http://core-genomics.co.uk/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["The immune system is becoming easier to investigate as new methods based on nextgen sequencing are published. I am not an immunologist and the complexities of the immune system for me are stuck back in the days of my undergraduate training. And that was in the 90\u2019s!  Nature and the HudsonAlpha Institute are hosting the first Immunogenomics conference  next month bringing together scientists from many disciplines to learn about large-scale immune sequencing projects; epigenetics and the immune system and many other topics. Immunogenomics looks like it is going to make headlines next year.  There have been several papers describing HLA typing (e.g. Gabreil 2009 & Bentley 2009 using 454 and more recently Wang 2011 using HiSeq) and many groups are working on using next-gen methods to replace older tests.  A new product from Sequenta is aiming to make this kind of analysis simple to do for any user. The Lymphosight platform uses a multiplex PCR to amplify the IgH, IgK, TRB, TRG, TRD immune cell receptor loci, allowing each T or B cell to be characterised and counted. Immune cell proliferation in response to disease and other studies might be far easier to carry out using this new kit. With 100\u2019s of millions of reads coming from HiSeq, and eventually Proton, even fairly rare immune cells should be detectable in a high background.    Lymphosight workflow from Sequenta website  The company discuss a test they ran where sequences associated with a B cell tumour were diluted into a normal background at 1:1,000,000. They got very reproducible, quantitative results and a useful dynamic range that compares well to flow cytometry methods currently being used. They expect Lymphosight to be useful in monitoring of minimal residual disease.  David Haussler Director, Centre for Biomolecular Science and Engineering at UCSC said 'We can read genomes from your immune cells. They adapt throughout your lifetime so they can protect you from diseases. Reading those genomes will be important, and you\u2019re going to hear a lot about them next year.'   I recently visited TRON , a spin out from Mainz University Medical Center, where they are conducting translational research in the field of oncology and immunology. One of their aims is to take personalised immunogenomic markers and turn these into personalised Cancer vaccines. The head of TRON, Ugur Sahin just published a very interesting article in OncoImmunology where they describe using NGS to demonstrate a proof-of-concept for identification of immunogenic tumour mutations that are targetable by individualised vaccines. They analysed a melanoma cell line and found over 500 non-synonymous expressed somatic mutations, one third of which were immunogenic. From these they made long peptides of 27aa length and tested these for immune response. 11 of these immunogenic tumour-specific peptides effectively immunised mice against the Tumorigenic cell line (see figure 1 from their paper below).   I am sure Sequenta are hoping groups like these will be using Lymphosight to do perform their analysis of the Immune repertoire."], "link": "http://core-genomics.blogspot.com/feeds/825738439197207330/comments/default", "bloglinks": {}, "links": {"http://sequentainc.com/": 2, "http://3.blogspot.com/": 1, "http://www.nature.com/": 1, "http://www.nih.gov/": 4, "http://4.blogspot.com/": 1, "http://tron-mainz.de/": 1, "http://www.nytimes.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["Apologies to readers who might have been hoping for some posts over the past few weeks but I have been offline whilst holidaying in France and Spain.  One of my \"to-do's\" before I left was to respond to the \"change your password or you'll be locked out\" email from our IT manager. This was one thing that got missed at the end of the frantic Friday afternoon before leaving and subsequently I could not log on, even through webmail. However, this made for a lovely and uninterrupted holiday and I am sure made it easier to forget about work as there was little point to even try and get online. As a result though I have come back to over 800 emails. This got me wondering about what I might be writing in my out of office message next time I go away. I think it should go something like this... \"I am currently out-of-the-office on holiday until the 1st of September and will not have any access to email whilst away. If your message really is important then please send it again on the 2nd of September as I will be deleting every email I receive between now and my return to work. Sorry for any inconvenience.\" This would certainly make the task I now face much easier! PS: Normal posting will resume shortly."], "link": "http://core-genomics.blogspot.com/feeds/7606201878182122188/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "CoreGenomics"}, {"content": ["After the furore around the Loman et al paper it is interesting to read another comparison of NGS platforms. Lets face it most of us want to know either what we should be buying next, or if we bought the right thing in the first place.  Comparison papers help.  As do beers at AGBT!  The latest sequencer comparison paper: Mike Quails group at the Sanger published a comparison of PGM, MiSeq and PacBio (interesting choice of the third platform). They sequenced several small genomes that varied massively in GC content. It was interesting to me that these genomes are the routine test genomes for Mikes group, most of us would shudder if a user asked us to sequence something with 20% GC on HiSeq!  Table 1 is excellent reading and should help people in making purchasing decisions. Collecting all this information together needs to be done by each individual institute as prices can vary quite widely. But the table as it stands should allow anyone to make basic comparisons and also see what is missing that they might need to put greater effort into. In the paper they say that although the raw error rate is significantly different for the instruments compared, the affect on SNP calling is negligible given sufficient coverage. 15x appeared fine for the genomes tested. I\u2019d prefer to have seen this in the table as well, to act as a counter to claims around error rates from sales people! They compared most of the things you would want to when deciding what to buy (see the table for everything). The sequencing costs differ significantly per Gb at $500, $1000 and $2000 for MiSeq, PGM 318 and PacBio respectively. This compares to about $50 per GB on HiSeq.    Table 1 from the paper Most people considering PGM or MiSeq are after a fast sequencer and both will deliver. As we get used to sub-24 hour run times our users will notice how long library prep takes. As costs for sequencing continue to fall we\u2019ll also spend more time questioning the costs of library prep. The paper talked about the push from all companies to make library prep as simple as possible. However there was no mention on the cost of library prep. The genomes sequenced require only ?-? Gb of data and so ??? libraries would be needed per run. At $100 per sample the cost is X?times more than the sequencing. This is still an unmet need of the community, $1 sample prep for $10 genomes.  How did they do the comparison: Genomes sequenced included Bordetella pertussis (68% GC), Salmonella Pullorum (52% GC), Staphylococcus aureus (33% GC) and Plasmodium falciparum (19% GC). They made PCR-free or PCR amplified libraries for MiSeq PE150bp runs, or HiSeq PE75bp lanes allowing a direct comparison of the impact of PCR. Additionally they prepared Nextera libraries from three of the genomes sequenced (Bp, Sa & Pf) and whilst two produced \u201cremarkably even\u201d data the Pf genome was very biased. They made PGM libraries using physical shearing and \u201cFragmentase\u201d digestion using the Ion Xpress kits and showed both to be comparable. These were run on 316 chips for 65 cycles, generating mean read lengths of 120 base pairs. Standard PacBio libraries were prepared and sequenced using C1 chemistry on multiple SMRT-cells how many?  What did they find: PGM struggled with the very AT rich Pf genome, and the bias appeared to be partly in the library-prep. By tweaking the protocol and swapping the polymerase for a better one they demonstrated a significant improvement in results. Why don\u2019t all companies do this kind of testing before releasing products on us users, using the best polymerase or ligase available can make a huge difference.  Error rates were best for MiSeq, no surprise to Illumina users there. But there was no impact on true-SNP calling with PGM doing best at 15x genome coverage although it did produce more incorrect SNP calls. PGM and MiSeq correctly called 82% and 76% of SNPs and produced 1800 and 1300 incorrect SNP calls respectively. For Illumina MiSeq made more correct SNP calls than HiSeq or GAIIx and Nextera library prep worked as well as the standard protocol. Both MiSeq and PGM\u2019s built-in variant calling was inadequate; MiSeq reporter called 7% and Torrent suite called 1.5% of variants. SNP calling for PacBio was hampered by a lack of tools as most are designed for short-read data.  A word of caution: The paper is out-dated as are all comparisons and the authors are happy to acknowledge this. It takes time to perform an experiment like this, analyse it and finally write it up. C2 chemistry was used for PacBio and a new method has been described for magnetic loading of chips. MiSeq now has 500bp kits available and even more reads. PGM has error rate has improved. MiSeq has an upgrade being rolled out now for more and longer reads. To be fair to the non-Illumina platforms MiSeq is based on a pretty mature technology whilst Ion and PacBio should be given some time to catch-up (and perhaps overtake), some of the issues with the PGM and PacBio might be resolved by evolution.  GenomeWeb had comments from Ion, Illumina and PacBio. Ion and Illumina both said the comparison was fair. Ion clarified this by saying that the data showed what was possible in 2011 but that error rate was now just 0.4%. Whilst IlluminaLoman et al presented.  Mike also spoke to GenomeWeb and said that the same test genomes are still being run and that the results were as valid today as back in 2011. Significant improvements had come from PGM 200 cycle kits and the C2 chemistry for PacBio.  I am confident there will be more of these comparisons in the next few months. Expect at least one AGBT presentation and lots more discussion over beers.  See you at the bar perhaps?"], "link": "http://core-genomics.blogspot.com/feeds/8224720034913056226/comments/default", "bloglinks": {}, "links": {"http://www.biomedcentral.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["Sense About Science is an organisation that tries to provide expert advice on scientific matters to whoever needs it. They monitor the papers and news and produces an annual \u201cCelebrities and Science\u201d round up of the best and worst comments from people in the public eye. The organisation behind Sense About Science has come under some criticism for being pro-GM and a bit radical and certainly not everyone is a fan . But I enjoyed reading through their annual reviews and wanted to share a few of my favourite comments. The best for me was from Nicole \u2018Snooki\u2019 Polizzi who said \u201cthe oceans were salty because of all the whale sperm\u201d! See the bottom of this post for a selection from the last three years round-ups. Sense About Science scan many publications looking for comment; of course celebs and politicians don\u2019t always get it wrong but it is far easier to pick up on the crazies out there. There appear to be fewer celebrities who deny evolution or suggest \u201cfossil fuels\u201d aren\u2019t running out, whilst some politicians careers appear to be built on such claims. If you want to help out then you can sign up or email them with examples of bad science.  What do Scientists think of celebrity genomes? Jeff Barrett's web page at the Sanger has coverage of a debate on the value of celebrity genomes between Ewan Birney and Paul Flicek. This was part of a series of events at the Sanger institute looking at the relationship between society and personal genomics. Jeff chaired the debate and before starting the room was evenly split between those who agreed, disagreed or were undecided on the statement \u201ccelebrity genomes are a useful contribution to science and society\u201d.  The debate focused around how useful genomes from celebrities were in creating a dialogue between scientists and the public. Paul argued that celebrity genomes are no more important than non-celebrity genomes, so what makes celebrities qualified to speak about genomics? Ewan argued that celebrity genomes have contributed to science, even if only a little. At the end of the debate Jeff asked the audience to judge what impact they thought celebrity genomes had on science, 33% said positive, 62% said negative and 4% were undecided. He also asked the audience if they thought celebrity genomes had had an impact on society, 41% positive, 51% negative and 8% were undecided.  During the debate Paul talked about the impact celebrities can have as patient advocates using Michael J Fox and Parkinsons as an example. Celebrities have as much chance of developing cancer as any of us and as they get their cancer genomes sequenced and see a benefit from the \u201ctreatment\u201d they are uniquely placed to talk about the impact in a way that is going to get across to more people than coverage of a Nature paper on the BBC six o\u2019clock news will ever do.  We should be trying to engage with this as much as possible, shouldn\u2019t we? PS: If you are a celebrity (why wouldn\u2019t they be reading my blog?) and need some advice then help is just a phone call away, call sense about science on +44(0)20 7478 4380 . I can\u2019t promise they can say how many reads you\u2019ll need for your next exome sequencing experiment! PPS: If you want your celebrity genome sequenced there are plenty of labs in LA.   My pick of the best and worst from the annual round-ups. Positive:  Bonnie Tyler when questioned about trying acupuncture said \u201cI lost some weight but I was also on a more sensible diet at the same time which, if I\u2019m cynical, is more likely the reason for the weight loss.\u201d And Natascha McElhone\u2019s comments about tetanus after a visit to Angola: \u201cIt\u2019s completely preventable if you\u2019re inoculated against it.\u201d Negative:  Heather Mills \u201cmeat sits in your colon for 40 years and putrefies, and eventually gives you the illness you die of. And that is a fact.\u201d Roger Moore \u201ceating foie gras can lead to Alzheimer\u2019s, diabetes and rheumatoid arthritis. In short, eating foie gras is a tasty way of getting terminally ill.\u201d I don\u2019t eat foie gras on compassionate grounds but it is unlikely to be the cause of so many diseases, and I am not sure any of those Sir Patrick listed are actually terminal? Alex Reid gave out a horrible message about unprotected sex saying \u201cit\u2019s actually very good for a man to have unprotected sex as long as he doesn\u2019t ejaculate\u201d and \u201csemen has a lot of nutrition. A tablespoon of semen has your equivalent of steak eggs, lemons and oranges.\u201d Irresponsible nutter if you ask me! Julia Sawalha doesn\u2019t get inoculated or take anti-malarials but uses \u201c homeopathic alternatives, called \u2018nosodes\u2019\u201d and said \u201cI\u2019m the only one who never goes down with anything.\u201d Joanna Lumley , her AbFab co-star put the increase in cancer down to \u201cthe growth hormones in the food we eat, that try to make all the chickens, sheep and cows, more productive\u201d. Sarah Palin who\u2019s autobiography \u201cGoing Rogue\u201d says that she \u201cdidn\u2019t believe in the theory that human beings \u2014 thinking, loving beings \u2014 originated from fish that sprouted legs and crawled out of the sea or from monkeys who eventually swung down from the trees.\u201d Yikes how can such strong anti-evolution views be held by someone who (from a UK news coverage perspective) holds some power in the USA? Michelle Bachman , member of the US House of Representatives and Republican Presidential Candidate, told journalists that a woman had told him her daughter suffered mental retardation after receiving the HPV vaccine, and that this vaccination program has dangerous consequences. What is the likelihood she is a right-wing, pro-christian, pro-guns, anti-abortion Republican? These last two particularly disturb me. The first highlights how nuts some politicians are. The second because as the UK MMR scare showed, bad science can become mainstream fact and affect us all in a very negative way. We shouldn\u2019t believe everything we hear in the press, but politicians surely have an obligation to be careful about what they say."], "link": "http://core-genomics.blogspot.com/feeds/1141125733652121305/comments/default", "bloglinks": {}, "links": {"http://www.senseaboutscience.org/": 1, "http://wp.ac.uk/": 1, "http://www.co.uk/": 1, "http://4.blogspot.com/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["Eight years is a long time in NGS. I recently re-read a 2004 article in Pharmacogenomics 2004, and also found a EBI presentation from Clive Brown and Ewan Birney . Both of these were from a small company based in Cambridgeshire called Solexa. At the time of publication they had only just identified their first alpha-test site and the presentation talked about a prototype instrument ready for the end of 2004.    Prototpye GA1  Trademarks mentioned in the paper such as SMA-seq and TotalGenotyping have not, I suspect, been heard of by most Illumina sequencing users (including myself).   The paper describes where Solexa came from (Shankar Balasubramanian and David Klenerman's patents of 1998 spun out of Cambridge University Department of Chemistry). It mentions Solexa's demonstration version of \u201ca system that will allow rapid, base-by-base comparison of genomic DNA sequences\u201d and that this will produce \u201cfour or five orders of magnitude improvement over conventional sequencing\u201d. Read lengths of just 25-30bp are proposed, and a nice graph illustrates how just over 80% of the Human genome is uniquely mappable with these incredibly short reads.  Simon Bennett, business development director of Solexa at the time and author of the Pharmacogenomics paper suggests that Solexa will achieve the $1000 genome within the next ten years. That leaves us two more years to get to $1000 genomes. It does not seem unreasonable that we\u2019ll get there although more discussion today is about the cost of bioinformatics analysis!  What happened to single molecule sequencing: There is an overview of the Solexa Single Molecule ArrayTM technology that the paper suggest can analyse a Human genome in a single experiment. As described there were just 100,000 DNA molecules per cm2 compared to 100M cm2 today. The basic chemistry description is unchanged from current SBS, although only 25 bases were being sequenced at the time of publication.  It is only towards the end of the paper that Solexa\u2019s acquisition of Manteia\u2019s solid surface bridge-amplification technology, this is the clustering we know and love today. Up until this point Illumina had been focusing on single molecule sequencing. Without the acquisition of Manteia perhaps Solexa would have continued to chase single molecule sequencing and ended up like Helicos or Pacific BioSciences. As it stands clustering and SBS chemistry have been the bedrock of next-gen sequencing for the past five years.  Personally I\u2019d bet Illumina are still putting lots of effort into single molecule approaches, and not just by investing in companies like ONT. I\u2019d like to know if it would be possible to sequence single molecules on a HiSeq with a more sensitive camera (massive oversimplification I know)? Imagine 1000M single molecule reads! This might not be what we ultimately use for single-molecule but I think we can be certain there is a lot more coming for next-gen in the next eight years.  PS: Would SOLiD have been the dominant technology if Agencourt had bought Manteia instead? Perhaps we should have a genomics version of Marvel\u2019s \u201cWhat if\u201d comic books from the 80\u2019s?  PPS: The Illumina history lesson also taught me that we share half our genes with bananas!"], "link": "http://core-genomics.blogspot.com/feeds/7069239750323569233/comments/default", "bloglinks": {}, "links": {"http://2.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://www.nih.gov/": 1, "http://www.ac.uk/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["I have been using the Bioanalyser since its introduction in 1999. Originally intended for QC analysis of total RNA for microarray studies it quickly became a standard tool for many labs. Over the past few years we have run almost as many NGS libraries on DNA 1000 assays as we have RNA chips.  I think we are going to stop using it for all but a small proportion of libraries by next year.  The Bioanalyser has been a great tool for quality control of NGS libraries. Users can clearly see if they have prepared a high-quality library, if there is lots of adapter-dimer present and if the insert size is what they expected. Unforunately running the Bioanalyser is a bit of a pain once you have more than 12 or 24 libraries. In my lab we are now preparing 24, 48 and 96 libraries in each batch. QC of these has become too much work using current methods so we looked at alternatives. This included the Caliper LabChip GX , Shimazdu MultiNA , Agilent ScreenTape , Qiagen QIAxcel and Advanced Analytical\u2019s Fragment Analyser (see the bottom of this post for a full list of features).    From our analysis of the system features we asked for demonstrations of the Caliper and Advanced Analytical instruments. These two both appeared to give us the throughput and sensitivity we need, both systems worked well and I know of several labs using these instruments very successfully. However we decided not to invest in a high-throughput Bioanalyser.  Why not and what do we want from library QC: most users want sequence results as soon as possible and are happy with some libraries failing so for some the QC is seen as a bar that gets in the way of their science. My lab wants to satisfy all users and return the highest percentage possible of high quality sequencing runs. Generating 40M reads of a poor library is no use to anyone. With the introduction of 96 and 384 index kits from companies like Bioo Scientific and with Illumina finally catching up with the TruSeq HT kits I think we are ready to ditch gel-based analysis. Instead we will start using a QC pipeline that will use the data from a single lane analysis of up to 96 libraries. We can look at computed insert-size, verify quantification by checking pooling ratios, screen for adapter-dimer or contamination with other genomes and make sure duplication rates are not too high. Even with 96 samples we should get around 1-2M reads each, and some readers of this blog may remember when 1 M reads was considered enough for ChIP-seq analysis, let alone QC! There are also some hints that 1M reads might be acceptable for basic differential gene expression analysis of highly expressed transcripts. We\u2019ll be slowly retiring the Bioanalyser type analysis of libraries and using the qPCR quantification as a simple QC tool for pass/fail decisions. We might even get to a point that we only quantify the final pool after mixing equal volumes of all 96 libraries, such that cluster density is spot-on. Then we can use the sequence demultiplexing to indicate the actual balance of indexes to re-pool for the final high read number sequencing.  High Throughput Bioanalyser Platform Features  Caliper - Labchip GX   High throughput bioanalyser with 96 and 384 well compatibility  Asseses RNA quality and gives exact sizing and quantification of DNA fragments.  Can analyse 96 samples in less than 1 hour  RNA metrics are used to calculate the RGS value (RNA quality score) which has been validated to correlate with the agilent bioanalyser RIN score. This would be beneficial since users are already familiar with a RIN value for assessing RNA quality.  Resolution down to 5bp and sensitivity of 0.1 ng/ul  Can visualise the results on electropherogram or gel view similar to Agilent 2100.  Data can be viewed in tabular form which can be easily exported/uploaded onto our LIMS system.  High sensitivity kit also available  There is a barcode reader for sample tracking which would be important when running large numbers of samples.  Shimadzu Biotech \u2013 MCE 202 MultiNa   This is a microchip electrophoresis system for DNA/RNA analysis.  Reusable microchips are used which could reduce running and consumable costs.  120 samples can be run simultaneously across 4 separate microchips with 80 seconds per sample processing speed.  It can also perform automatic or manual reanalysis of the samples as seen with the agilent bioanalyser and can export the results in a csv. format.  lab901 Agilent - Screentape   The Lab901 ScreenTape system is a fully automated system for gel electrophoresis. The ScreenTape instrument loads, separates, images and analyses both DNA and RNA samples. It does this by loading each sample onto a screentape each of which contain 16 microgels which align to built in electrodes and imaging system.  Only 1 ul of sample is required and analysis takes 1 minute per sample. It is fully automated with prepacked reagents so there is no gel preparation or chip priming.  Different screentapes are available for DNA and RNA analysis.  For RNA analysis, quality is displayed as the screentape degradation value (SDV)  Qiagen- QIAxcel system   A microcapillary electrophoresis system, which is fully automated and can process up to 96 samples per run. Separation is performed in a capillary of precast gel cartridge which are reusable.  Sensitivity of 0.1ng/ul Resolution down to 3-5 bp.  Sample consumption is less than 0.1ul, although the minimum sample volume to load for analysis is 10ul.  96 samples can be processed in approximately 1 hour.  The data can be viewed as electropherogram or gel images.  Advanced Analytical \u2013Fragment Analyser   is a fluorescence-based capillary electrophoresis instrument for both sizing and quantifying nucleic acids (DNA and RNA).  Can run either 12 samples or 96 samples at a time  The instrument provides space for up to six 96-well plates  Can be used to quantify and qualify NGS fragments, RNA, genomic DNA and also for mutation detection, Microsatellite (SSR) analysis.  Various capillary lengths can be used, depending on the application, required resolution and desired speed of analysis. Longer arrays provide resolution down to 2 bp for fragments under 300 bp in length. Shorter arrays still provide good resolution with run times as fast as 15 minutes PROSize\u2122 software is used to analyse the data and this can be viewed as a gel view, electropherogram or a results table.  The data is exportable and can be linked to the LIMS."], "link": "http://core-genomics.blogspot.com/feeds/8052262291003741690/comments/default", "bloglinks": {}, "links": {"http://www.agilent.com/": 2, "http://www.qiagen.com/": 2, "http://1.blogspot.com/": 1, "http://www.biooscientific.com/": 1, "http://www.aati-us.com/": 2, "http://2.blogspot.com/": 1, "http://www.co.uk/": 2, "http://support.illumina.com/": 1, "http://www.caliperls.com/": 2}, "blogtitle": "CoreGenomics"}, {"content": ["A year ago I wrote a post about the explosion of different NGS acronyms . When I wrote it I was surprised to see over 30 different acronyms and suggested that part of this was authors wanting to make their work stand out, hopefully coining the next \u201cChIP-seq\u201d. In the past year more and more NGS acronyms have been published. I am partly responsible for one of these TAm-seq and understand better the reasoning for using acronyms. Once I have spoken to someone about the work we did in the STM paper I can simply refer to TAm-seq in future conversations. It might help if we as a community could agree on a naming convention to make searching for work using specific techniques easier. There are multiple techniques for analysis of RNAs and using the catch-all \u201cRNA-seq\u201d would allow much quicker PubMed searching. Of course we would need to add keywords around the particular technique being used, RNA-seq could encompass mRNA, ribosome removal, strand-specific, small, micro, pi, linc, etc, etc, etc. Here is a list of acronyms that we in the community could use to simplify things today. It would obviously need tidying up every year or so as new acronyms get added.  DNA-seq: Unmodified genome sequencing. RNA-seq: All things RNA. SV-seq: Structural-variation sequencing. Capture-seq: Exomes and other target capture sequencing. Amplicon-seq: Amplicon sequencing. Methyl-seq: Methylation and other base modification sequencing. IP-seq: Immuno-Precipitation sequencing. Let me know what you think Again, here is a link to the data ."], "link": "http://core-genomics.blogspot.com/feeds/8756576537747857308/comments/default", "bloglinks": {}, "links": {"http://stm.sciencemag.org/": 1, "https://docs.google.com/": 1, "http://core-genomics.co.uk/": 1}, "blogtitle": "CoreGenomics"}, {"content": ["One of my favourite columns in any scientific journal is Nature Methods \u201cPoints of View\u201d by Bang Wong . The column is focused on visualisation and presentation of scientific data and I\u2019d highly recommend it if you haven\u2019t already seen it.   Here is a link to Nature Methods and also a public Mendeley group (please feel free to join) so you can access the papers, Bang Wong's points of view . I'd be very interested in a hard-copy version, perhaps the articles expanded and collected into a book?   Data visualisation is improving all the time: In the March 2010 issue of Nature Methods the Creative Director of the Broad Institute, Bang Wong, was senior author on a paper highlighting some of the challenges we face in visualising complex data sets. The paper presents some of the developments over the past twenty years that today allow almost anyone to; create a phylogenetic tree, a complex pathway diagram or a transcriptome heat map. We are generating huge amounts of data and visual tools for interpretation are vital. Fortunately there are lots of people working on this.   Circos plots: I am always struck by how much data is conveyed in a circus plot, and these are becoming more complex as data sets grow. Can you imagine how many slides you would have needed to use just three or four years ago? The Circos tool was published in Genome Research in 2009. There is a Circos website and the New York Times had a great feature way back in 2007 highlighting what was possible with this new visualisation tool.       Points of view: The column covers many aspects of data visualisation and presentation. Some highlights for me are:   Colour: Spiralling through the colour wheel when choosing colours to use in figures can allow the same visual impact in both colour and black-and-white print. Adobe Illustrator and Photoshop allow you to simulate what Red:Green colour-blindness will do to your figures, and replacing red with magenta makes images accessible for all. Colour can be misleading and sometimes a simple black line will do.       Whitespace: Absence of colour is important. Many scientific presentations and posters covey too much information and don\u2019t have enough empty page to allow readers to see how the text should flow.     Typeface: The reason we use serif typeface in text is because the \u2018feet\u2019 help us follow the line of the text. A generalisation is that serif fonts should be used for large blocks of text (posters and papers) and sans serif fonts for smaller strings of text (presentation slides). Spacing of words and paragraphs can have a dramatic impact on the readability of a document.      Simplification: If your data is easy to read then people will read it. Sounds simple, but I am sure many of us have prepared posters with far too much information, that need lots of explanation, yet we get less than one minute with people in the poster session. Identifying your most important idea and focusing on that can help.   I\u2019d also recommend Bang\u2019s website http://bang.clearscience.info which has links to lots of interesting visualisation and scientific art as well. Enjoy.    PS: If the posts on my blog are not taking all this into account, or if you see a poster or presentation of mine that could be improved then let me know. Remember that feedback has to be constructve!"], "link": "http://core-genomics.blogspot.com/feeds/446388987196658972/comments/default", "bloglinks": {}, "links": {"http://bang.clearscience.info/": 2, "http://3.blogspot.com/": 4, "http://www.mendeley.com/": 1, "http://genome.cshlp.org/": 1, "http://www.nature.com/": 2, "http://2.blogspot.com/": 1, "http://circos.ca/": 1}, "blogtitle": "CoreGenomics"}]