[{"blogurl": "http://www.statalgo.com\n", "blogroll": [], "title": "statalgo"}, {"content": ["Reinforcement Learning is an approach to learning that attempts to maximize a cumulative reward based on a set of actions and states. The techniques are very popular within operations research and control theory. It does not fall under the traditional paradigms of supervised or unsupervised learning because correct inputs/outputs are never provided. The algorithm is instead presented with some notion of doing well or poorly at each step. \n There are several different methods for optimizing a reinforcement learning problem; I will be focusing on Dynamic Programming.  Dynamic programming (or DP) is a powerful optimization technique that consists of breaking a problem down into smaller sub-problems, where the sub-problems are not independent . This is useful both in mathematics (especially fields like economics and operations research) and computer science (where the algorithm's complexity can be substantially improved through methods such as memoization ). In this context, \"programming\" actually means \"planning\", as dynamic programming consists of forming a table of solutions. \n \n I give a brief overview of dynamic programming using R. I start by focusing on two well-known algorithm examples (fibonacci sequence and the knapsack problem), and in the next post I will move on to consider an example from economics, in particular, for a discrete time, discrete state Markov decision process (or reinforcement learning ). The goal is to both describe the algorithm design and give some sense for how it can be applied to actual problems. I'm interested in demonstrating this because it will be required for reproducing results from some of the primary trade optimization methods (such as Bertsimas and Lo ) as part of my series on Market Microstructure . \n Computational Design: Dependent Divide and Conquer \n A quick search of CRAN (including the Optimization view ) will reveal that there are no dynamic programming packages available. This is because dynamic programming is a design technique rather than a specific algorithm; it is specifically tailored to each given problem. \n From a design perspective, dynamic programming has a lot in common with divide-and-conquer, but is specifically designed to address problems where the subproblems are not independent. A nice summary of this distinction can be found in \"Introduction to Algorithms\" : \n Dynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems...divide-and-conquer algorithms partition the problem into independent subproblems, solve the subproblems recursively, and then combine their solutions to solve the original problem. In contrast, dynamic programming is applicable when the subproblems are not independent, that is, when subproblems share subsubproblems. In this context, a divide-and-conquer algorithm does more work than necessary, repeatedly solving the common subsubproblems. A dynamic-programming algorithm solves every subsubproblem just once and then saves its answer in a table, thereby avoiding the work of recomputing the answer every time the subsubproblem is encountered.\n \n There is a video lecture from the MIT course using this canonical algorithm textbook, including one of the authors: Charles Leiserson and Erik Demaine: \n \n \n Fibonacci Series \n The Fibonacci Sequence is one of the most popular introductory examples used in teaching programming (also one of the most popular pages on Rosetta Code ). This entails a function where the output is simply the sum of the preceding two values: \n \n Which results in a sequence that looks like: \n \n A classic approach to solve this problem is to use recursion: \n \nfib1 <- function(n) {\n print(paste(\"fib1(\", n, \")\", sep=\"\"))\n if(n == 0) return(0)\n else if(n == 1) return(1)\n else {\n return(fib1(n-1) + fib1(n-2))\n }\n}\n \n I have included a print statement at the top of the function so that it's easy to see how this function is called. Unfortunately, the computational complexity of this algorithm is exponential . As an example, for fib(5) , we end up calling fib(2) three times. \n The dynamic programming solution calls each value once and stores the values ( memoization , also see Hadley Wickham's memoise package for R ). This might seem overly simplistic when you consider the recursive algorithm, but in this case we need to loop and maintain state, which is a very imperative approach. \n \nfib2 <- function(n) {\n fibs <- c(0, 1)\n if(n > 1) {\n for (i in 2:n) {\n  fibs[i] = fibs[i - 1] + fibs[i - 2]\n }\n }\n return(fibs[n])\n}\n \n This second algorithm is linear time complexity, and its performance is noticeably better even on small values for n. \n \n> library(rbenchmark)\n> benchmark(fib1(20))\n  test replications elapsed relative user.self sys.self \n fib1(20)   100 15.15  1  13.64  0.02 \n> benchmark(fib2(20))\n  test replications elapsed relative user.self sys.self\n fib2(20)   100 0.05  1  0.03  0\n \n This demonstrates both the performance improvement that is possible, and also the dependence structure that is common in dynamic programming applications: the state of each subsequent value depends on prior states. \n This also demonstrates another important characteristic of dynamic programming: the trade-off between space and time . \n 0/1 Knapsack Problem \n How can we use this for optimization? The knapsack problem is a very well known example of dynamic programming that addresses this question. This is a resource allocation problem. We're given a set of items with a weight and a value. And we have a limited capacity to carry these items, so we want to maximize the value given the constraint. This is sometimes described as a story: a thief enters a house and wants to steal as many valuable items as possible but can only carry so much weight. \n \n Presented with this problem, we can think of several different approaches. We might start with a greedy strategy: rank all the items by value, and add them to the knapsack until we can't fit any more. This won't find the optimal solution, because there will likely be a combination of items each of which is less valuable, but whose combined value will be greater. Alternatively, we might simply iterate through every single combination of items, but while this finds the optimal solution, it also grows with exponential complexity as we have more items in the set. So we can use Dynamic Programming: \n Define a set of items each with a value and weight respectively. We want to solve for: \n \n Subject to: \n \n The dynamic programming solution follows these steps: \n \n Characterize the structure of an optimal solution. \n Recursively define the optimal solution to subproblems. \n Step backwards and find the global optimal value. \n \n \nknapsack <- function(w, v, W) {\n \n A <- matrix(rep(0, (W + 1) * (length(w) + 1)), ncol=W+1)\n for (j in 1:length(w)) {\n for (Y in 1:W) {\n  if (w[j] > Y)\n  A[j+1, Y+1] = A[j, Y+1]\n  else\n  A[j+1, Y+1] = max( A[j, Y+1], v[j] + A[j, Y- w[j]+1])\n }\n }\n\n return(A)\n}\n\noptimal.weights <- function(w, W, A) {\n\n amount = rep(0, length(w))\n a = A[nrow(A), ncol(A)]\n j = length(w)\n Y = W\n \n while(a > 0) {\n while(A[j+1,Y+1] == a) {\n  j = j - 1\n }\n j = j + 1\n amount[j] = 1\n Y = Y - w[j]\n j = j - 1;\n a = A[j+1,Y+1];\n }\n\n return(amount)\n}\n \n We can test this on a simple example of 7 items with different weights an values: \n \nw = c(1, 1, 1, 1, 2, 2, 3)\nv = c(1, 1, 2, 3, 1, 3, 5)\nW = 7\n\nA <- knapsack(w, v, W)\nbest.value <- A[nrow(A), ncol(A)]\nweights <- optimal.weights(w, W, A)\n \n We can look at the matrix A to see how dynamic program stores values in a table rather than recomputing the values repeatedly: \n \n> A\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,] 0 0 0 0 0 0 0 0\n[2,] 0 1 1 1 1 1 1 1\n[3,] 0 1 2 2 2 2 2 2\n[4,] 0 2 3 4 4 4 4 4\n[5,] 0 3 5 6 7 7 7 7\n[6,] 0 3 5 6 7 7 8 8\n[7,] 0 3 5 6 8 9 10 10\n[8,] 0 3 5 6 8 10 11 13\n \n Where the best total value when satisfying the weight constraint in the example. \n I should also point out that \"Mike's Coderama\" has a post covering this in Python . \n Conclusion \n Dynamic programming is a powerful technique both for finding optimal solutions to problems where steps have dependence, and at improving algorithm performance by trading off space for time. I will finish off this topic in the next post by looking in more detail at the topic of Bellman's Optimality Principle and at an example from economics. \n Resources \n I rely primarily on three sources for this analysis, all of which I highly recommend: \n \n Mario Miranda and Paul Fackler \"Applied Computational Economics and Finance\" . Gives a concise overview of computational methods for solving dynamics problems in economics and finance. \n Thomas Cormen, Charles Leiserson, Ronald Rivest, and Clifford Stein \"Introduction to Algorithms\" . This is a classic text on computer algorithms. \n Richard Sutton and Andrew Barto \"Reinforcement Learning: An Introduction\" ( html version available for free ) The most popular introductory text on the subject, lacking in some of the mathematical formalism that plagues other books on the subject.\n \n In addition, I would point out: \n \n \"In Pursuit of the Traveling Salesman: Mathematics at the Limits of Computation\" is an excellent recent chronicle of the effort to solve The Traveling Salesman Problem (TSP). \n Dynamic programming can suffer from the \"curse of dimensionality\", and methods exist such as approximate dynamic programming to address this problem: see \"Approximate Dynamic Programming: Solving the Curses of Dimensionality\" for treatment of this method."], "link": "http://www.statalgo.com/2012/10/29/reinforcement-learning-in-r-an-introduction-to-dynamic-programming/", "bloglinks": {}, "links": {"http://rosettacode.org/": 1, "http://ocw.mit.edu/": 1, "http://stackoverflow.com/": 1, "http://www.statalgo.com/": 1, "http://webdocs.ualberta.ca/": 1, "http://en.wikipedia.org/": 8, "https://sites.google.com/": 1, "http://web.mit.edu/": 1, "http://www.amazon.com/": 6, "http://cran.r-project.org/": 2}, "blogtitle": "statalgo"}, {"content": ["As in Hasbrouck (2007) , we continue our series on Market Microstructure with a simple theoretical model of bid/ask prices: the Roll model. Richard Roll ( faculty page ) developed the model in \"A simple implicit measure of the e!ective bid-ask spread\" (1984). \n This paper presents a method for inferring the effective bid-ask spread directly from a time series of market prices. The method requires no data other than the prices themselves...\n \n We start by making the following simplifying assumptions: \n \n All trading in the market is conducted through dealers. These dealers will post bid/ask prices at each point in time as and . \n Dealers are competing with each other so that the spread between the bid/ask prices is equivalent to the dealers' cost per trade. \n \n We consider the bid/ask by considering a arithmetic random walk as the mid-point of the bid-ask: \n \n Then the bid and ask are given by: \n \n \n We can infer the trade price as a function of the the mid-quote price , half of the bid/ask cost , and the sign of the trade (+1 for buy and -1 for sell): \n \n We can model this simply in R. First let's look at bid/ask prices assuming a constant cost : \n \nc = 1\ntime = 1:100\nepsilon = rnorm(time)\nprices = cumsum(epsilon)\nm_t = zoo(prices)\na_t = m_t + c\nb_t = m_t - c\n \n We can simulate trade prices by adding a normally distributed trade sign variable: \n \nq_t = sign(rnorm(time))\np_t = m_t + (c * q_t)\n \n We can see the trades bouncing back and forth between the bid and ask after running this simulation: \n  \n We can also use this model in the opposite direction, to infer transaction costs from actual prices. The roll model has two parameters that can be estimated, and . This is very useful as it implies that we can estimate transaction costs by only using transaction data (i.e. without knowing quoted spreads). Here, transaction costs are inferred from serial covariance of daily returns. \n \n We define the variance and covariance as and . Following on Hasbrouck (2007), we have that: \n \n \n So we can now solve for and as: \n \n \n So we just need to compute the variance and covariance of returns in order to estimate the expected cost. An example of estimating these values in R using daily data. \n \nreturns <- function(x) (x/lag(x))-1\nrets <- na.omit(returns(m_t))\ngamma_0 <- var(rets)\ngamma_1 <- cov(rets[1:length(rets)-1], lag(rets, na.pad=FALSE))\n\nsigma_2 <- gamma_0 + 2 * gamma_1\ncost <- sqrt(2 * gamma_1)\n \n The cost variable might now be compared against actual bid/ask data whether our assumptions are reasonable. There are some obvious aspects of this model that don't match reality. \n \n Ordinarily bid and ask prices stay at the same level for different periods of time. This model doesn't include any sense of size of orders, and hence doesn't capture this temporal aspect. \n The bid/ask spread is not a constant in actual trading. It also follows clear intraday patterns where it varies between the open, midday, and close times.\n \n There is a strong relationship between trades and changes in quotes, this dynamic is completely ignored in the Roll model. A quote level may change when market makers change their prices or someone trades and moves from one level of an order book to another.\n \n Trades do not occur with every quote. There can be extended periods of time where markets are being quoted without any trading taking place.\n \n \n Several existing empirical studies attempt to test the Roll model on real data and attempt to verify the validity of its estimates. Harris (1990) \" Statistical Properties of the Roll Serial Covariance Bid/Ask Spread Estimator \" provides a nice summary of the basic flaws: \n Unfortunately, the serial covariance estimator yields poor empirical results when used to estimate individual security spreads from a year of daily or weekly data. Estimated first-order serial covariances are positive for about half of all securities so that the square root in the estimator is not properly defined. Further, estimated spreads depend on the observation interval. In particular, daily estimates are smaller than weekly estimates. Both problems appear in Roll's empirical results.\n \n While the basic Roll model is not empirically successful, it is still appealing as a first model due to its simplicity, and it can be easily extended into more a realistic model (as a recent example, see Zhang and Hodges (2011) ). We will revisit extensions in later posts, but our next model will move on to consider market maker inventory models, starting with Garman (1976)."], "link": "http://www.statalgo.com/2012/10/07/market-microstructure-3-the-roll-model-1984/", "bloglinks": {}, "links": {"http://bunhill.ac.uk/": 1, "http://www.uh.edu/": 1, "http://www.statalgo.com/": 2, "http://en.wikipedia.org/": 1, "http://www.amazon.com/": 1, "http://www.jstor.org/": 1, "http://www.ucla.edu/": 1}, "blogtitle": "statalgo"}, {"content": ["What is market microstructure ? \n Market microstructure is about the fundamental forces involved in market exchange. Economics and finance tends to abstract itself away from the underlying details of trading and exchange, but market microstructure puts these details at the center of the analysis, and investigates how these change the basic supply and demand interactions. \n  \n In short, market microstructure aims to understand the foundations of markets. It is concerned with (1) market structure and design , (2) price formation and discovery , and (3) liquidity and transaction costs . While a disperate subject (\"market microstructure theory may appear an amorphous collection of models, with little in common but subject matter\", O'Hara p.2), there is fairly clear agreement on these subject areas: \n \n The NBER working group on Market Microstructure definition identifies these subjects: \"...theoretical, empirical, and experimental research on the economics of securities markets, including: the role of information in the price discovery process; the definition, measurement, control, and determinants of liquidity and transactions costs; and their implications for the efficiency, welfare, and regulation of alternative trading mechanisms and market structures.\" \n Maureen O'Hara wrote the foundational monograph on the subject ( \"Market Microstructure Theory\" ), in which she defines market microstructure as \"the study of the process and outcomes of exchanging assets under a specific set of rules. While much of economics abstracts from the mechanics of trading, microstructure theory focuses on how specific trading mechanisms affect the price formation process.\" (p.1) \n Nikolaus Hautsche identifies the central topics in market microstructure theory as \"price formation, price discovery, inventory, liquidity, transaction costs as well as information diffusion and dissemination in markets.\" ( Econometrics of Financial High-Frequency Data , p.19) \n \n The term was first used by Garman in his 1976 paper: \"Market microstructure\". Garman was interested in considering how spread prices would evolve given that supply and demand would arrive at different times. \n We depart from the usual approaches of the theory of exchange by (1) making the assumption of asynchronous, temporally discrete market activities on the part of market agents and (2) adopting a viewpoint which treats the temporal microstructure, i.e., moment-to-moment aggregate exchange behavior, as an important descriptive aspect of such markets. (Garman 1976, p. 257, quoted in Hasbrouck Empirical Market Microstructure ) \n Market microstructure is one of the most interesting set of ideas in economics and finance, and of immense practical relevance to practitioners. It it tangentially related to important theories such as the Law of One Price , Rational Expectations , and the efficient markets hypothesis . Much of the underlying logic of standard microeconomics and finance is that market structure doesn't matter . Microstructure theory is explicitly about these frictions and costs, and how they lead to equilibrium. \n Roadmap \n I will be considering this field progressively. Starting with some basic considerations of trading mechanisms, rules, and important regulations. This forms a foundation for all subsequent work. \n Following on Hasbrouck's Empirical Market Microstructure , I will start with a simple stochastic model -- the Roll Model -- introduced by Roll (1984). I will then move onto considering standard models microstructure models that consider market makers , or the sell side . There are currently two major types of models for explaining price formation: asymmetric information based models and inventory models. Inventory models were originally derived from Garman (1976). Asymmetric information models have received the most attention recently; there are two standard frameworks: the \"sequential trade framework\" by Glosten and Milgrom (1985) and the \"strategic trade framework\" developed by Kyle (1985). \n I will then consider models that apply to how the buy side executes orders. This is slightly more disparate, but covers topics on transaction costs (especially market impact models), optimal trading models, and algorithmic trading. The optimal trading framework is based primarily on Bertsimas and Lo (1998) and Almgren and Chriss (2000). \n Lastly, I will review some recent papers on high frequency trading, which covers a different paradigm from the original market maker model. \n There are two useful survey papers on the subject: \n \n \nBruno Biais, Larry Glosten and Chester Spatt \"Market Microstructure: A Survey of Microfoundations, Empirical Results, and Policy Implications\" (2005) \n \n Ananth Madhava \"Market microstructure: A survey\" (2000) \n Martin Sewell \"Market Microstucture\" (2007)"], "link": "http://www.statalgo.com/2012/08/05/market-microstructure-2-an-overview/", "bloglinks": {}, "links": {"http://web.org.cn/": 1, "http://www.cornell.edu/": 1, "http://dilbert.comh": 1, "http://www.upenn.edu/": 1, "http://en.wikipedia.org/": 4, "http://www.amazon.com/": 4, "http://finance.martinsewell.com/": 1, "http://amor.hu-berlin.de/": 1, "http://www.nber.org/": 1}, "blogtitle": "statalgo"}, {"content": ["I just finished reading \"Dark Pools\" by Scott Patterson. The book documents the foundation of the major electronic exchanges (from Island through BATS) and high-frequency trading firms, along with the related regulations. Ironically, the book says virtually nothing about actual dark pools (such as Goldman's Sigma X). Patterson makes this clear in the first note: \n The title of this book doesn't entirely refer to what is technically known in the financial industry as a \"dark pool\". Narrowly defined, dark pool refers to a trading venue that masks buy and sell orders from the public market. Rather, I argue in this book that the entire United States stock market has become one vast dark pool. Orders are hidden in every part of the market. And the complex algorithm AI-based trading systems that control the ebb and flow of the market are cloaked in secrecy. Investors -- and our esteemed regulators -- are entirely in the dark because the market is dark . (\"Dark Pools\" 339)\n \n \"Narrowly defined\"? This decision to immediately simply ignore the meaning of a well-defined type of trading venue provides a good analogy for the book overall: it provides a very juicy story about the world of high-frequency finance while gently stepping around real accuracy. An example is the common theme of the book of the dominance of AI-based models. This phrasing seems to demonstrate a clear lack of understanding for how most firms actually run their models; to the best of my knowledge, there is very little actual AI employed. \n  \nPatterson also wrote \"The Quants\" about the origins of the major quantitative hedge funds such as Renaissance Technologies and Citadel. In both books, Patterson tries to make a case for how Quants can be destructive forces in the markets. In \"The Quants\", Patterson actually makes the case that the quantitative funds such as Citadel were responsible for the crash of 2008, and almost brought the global markets to collapse. This book also suffers from needless sensationalism (presumably with the intention of selling more books), which is unfortunate given the otherwise great amount of excellent journalism. \n All that said, I personally highly recommend both books to anyone interested in a popular, entertaining story about quantitative trading. I'm unaware of a more readable history of electronic exchanges, high-frequency trading, or quantitative hedge funds. And while these are certainly not balanced accounts, they're far more balanced than most in the popular press. Patterson does a reasonably good job at wearing his biases on his sleeve. \n Market Microstructure \n Given the popular attention now given to high-frequency trading, I thought that it would be worthwhile to given a general introduction to the field of \"market microstructure\" . The goal will be to give an unbiased account of how the markets function at a micro level. \n I will give a brief treatment to the major topics as covered in the canonical texts on the subject: \n \n Larry Harris Trading and Exchanges: Market Microstructure for Practitioners \n Joel Hasbrouck Empirical Market Microstructure: The Institutions, Economics, and Econometrics of Securities Trading \n Maureen O'Hara \"Market Microstructure Theory\" \n Barry Johnson \"Algorithmic Trading and DMA: An introduction to direct access trading strategies\" \n \n The next set of posts will give a brief overview of the functioning of exchanges, introduce the standard sequential trade and strategic trade models, and provide a short lit review of some of the latest academic ideas on high-frequency quoting and trading (e.g. Easley, Lopez de Prado, and O'Hara et. al. on \"Flow Toxicity and Liquidity in a High Frequency World\" ). \n This will partly follow along with some of the material from Joel Hasbrouck's two courses at NYU on \"The Structure and Dynamics of Financial Markets\" and \"Market Microstructure\" , and Dale Rosenthal's course at UIC on \"Market Microstructure and \nElectronic Trading\" . I will also try to include R code along with the respective models so that interested readers can explore ideas further. \n As always, let me know if there are specific topics that you would like to see covered! \n [I should also point out that Quantivity had an excellent blog series on \"How to Learn Algorithmic Trading\" which is related (although more general in scope).]"], "link": "http://www.statalgo.com/2012/07/08/market-microstructure-and-high-frequency-trading-part-1/", "bloglinks": {}, "links": {"http://papers.ssrn.com/": 1, "http://quantivity.wordpress.com/": 1, "http://people.nyu.edu/": 2, "http://tigger.uic.edu/": 1, "http://en.wikipedia.org/": 2, "http://www.amazon.com/": 8}, "blogtitle": "statalgo"}, {"content": ["Linear regression , and ordinary least squares in particular, is one of the most popular tools for data analysis. Continuing on my series about using the Julia language for basic statistical analysis with a review of the most well known direction solutions to the least squares problem. \n The Least Squares approach to linear regression was discovered by Gauss and first published by Legendre, although there has been some historical controvesy over this point ( a translation of Legendre's original from 1804 ). It has grown to become the workhorse of most statistical analysis: most common estimators can be cast into this framework, it is very mathematically tractable, and has been used for a long period of time. \n From a machine learning standpoint, this is an example of supervised learning since we have data for the dependent (predicted) variable and the independent variable(s) (the predictors). This means that we will be training the model on some known data, and trying to find the best set of parameters to minimize the difference between our model and the observations. \n The notation will vary by field, but it is common in Statistics to denote the observations of the dependent variable by the vector , observations of the independent variables by a matrix , and either or more often as the model with a set of parameters . As such, our least squares minimization problem can be defined as: \n  \n We can simplify the notation further by adding a vector of ones to the matrix X to denote the intercept term, at which point we will want to minimize this term: \n  \n Those coming from a more numerical background will find this same problem defined as . \n The Least Squares solution is found my minimizing the sum of squares of the residuals (i.e. the difference between the prediction and the observations . Minimizing the Euclidean norm of the residuals, in the case of a classical linear model with i.i.d. residuals, is the best linear unbiased estimator (BLUE, from the Gauss-Markov Theorem ). This is a common simplifying assumption in statistical models, and there are many manipulations to variables that can enable this to hold. \n We will now start to focus on several direct methods for solving this problem, followed subsequently by iterative methods. R and Matlab have built-in methods for doing this, and they both take advantage of different characteristics of the input data (e.g. sparcity); as such, I would never advise using what I provide below in place of these functions, but only to get a better understanding of different methodologies. \n A Simple Motivating Example \n We start with a very simple example: 4 x and y data points: \n \nX = matrix(1:4)\ny = c(2, 1, 1, 1)\nlm(y ~ X)\n \n Similarly, the simple solution in Julia is available through the linreg() function which solves the problem efficiently with LAPACK: \n \nX = [1:4]; y = [2, 1, 1, 1];\nlinreg(X, y)\n \n Julia's linreg ( which uses LAPACK ) is much less rich than the infrastructure that supports lm in R. The only returned values are the coefficients. R's function accepts a formula, following on the design from the famous \"White Book\", \"Statistical Models in S\" from Chambers and Hastie. And it returns an lm.fit object, which provides a wealth of available data and features. \n I want to look into several different methods for solving the least squares problem. R's lm functions use C and LINPACK under the hood, so this shouldn't be considered a fair comparison of their statistical functions, but more as a comparison of the languages themselves. I also don't go into mathematical derivations but simply provide the Julia code. \n Normal Equations \n [Note: By convention, I will be putting semi-colon's ; after expressions even though it isn't critical because it prevents Julia from printing out the return value.] \n The normal equations result from defining the problem in terms of and finding the first order condition for a minimum. \n Note that I am adding a column of 1's to the input matrix, as this will represent the intercept in the regression equation. \n \nX = [1 1; 1 2; 1 3; 1 4]; y = [2 1 1 1]';\n(m, n) = size(X);\nC = X' * X; c = X' * y; yy = y' * y;\nCbar = [C c; c' yy];\nGbar = chol(Cbar)';\nG = Gbar[1:2, 1:2]; z = Gbar[3, 1:2]'; rho = Gbar[3, 3];\nbeta = G' \\ z\n \n This makes use of the Cholesky decomposition (you can also find an implementation of a parallel cholesky decomposition in Julia from Omar Mysore as part of a project for MIT 18.337 \"Parallel Computing\" ). \n [Note: This code is almost exactly valid in Matlab/Octave, with the only exception being that the returned values on line 2 should be in square parentheses and the matrix slicing on line 6 should be in round parentheses.] \n QR factorization \n The QR factorization solution breaks the matrix A into a matrix Q and an upper right triangle R. For this it can be convenient to break the Q matrix into two parts: Q1 corresponding to the part that intersects with R, and Q2 which corresponds to the zero part of R. \n \nX = [1 1; 1 2; 1 3; 1 4]; y = [2 1 1 1]';\n(m, n) = size(X);\n(Q, R) = qr(X);\nR1 = R[1:n, 1:n]; Q1 = Q[:, 1:n];\nbeta = R1 \\ (Q1' * y)\n \n SVD factorization \n The SVD approach can handle situations in which the columns of matrix A do not have full rank. \n \nX = [1 1; 1 2; 1 3; 1 4]; y = [2 1 1 1]';\n(m, n) = size(X);\n(U, S, V) = svd(X);\nc = U' * y; c1 = c[1:n]; c2 = c[n+1:m];\nz1 = c1 ./ S;\nbeta = V' * z1\n \n You can also find a nice discussion of this with Matlab in Jim Peterson's \"Numerical Analysis: Adventures in Frustration\" . \n SVD provides a more precise solution than QR, but it is considerably more expensive to compute. As a result, modern software uses QR most of the time, unless there are specific cases in which that methodology underperforms or doesn't hold. \n End notes \n We will look at iterative methods in the next post. Then spend some time looking at how we can characterize the \"goodness of fit\" and statistical significance in the traditional sense. And finally look at ways to avoid overfitting (e.g. regularization) and models other than OLS (including weighted-least squares and logistic regression). \n Some useful references on this subject: \n * Much of what I showed above was based on sections from \" Numerical Methods and Optimization in Finance \" by Gilli, Maringer, and Schumann. I highly recommend this text to anyone involved with optimization in finance as it provides a strong overview of most major areas, including computational considerations. \n* Another nice free resource is Steven E. Pav's \"Numerical Methods Course Notes\" , which includes examples of direct and iterative methods for least squares using Octave."], "link": "http://www.statalgo.com/2012/04/27/statistics-with-julia-least-squares-regression-with-direct-methods/", "bloglinks": {}, "links": {"http://www.ac.uk/": 1, "http://www.statalgo.com/": 4, "http://www.clemson.edu/": 1, "http://en.wikipedia.org/": 3, "http://beowulf.mit.edu/": 1, "http://www.amazon.com/": 2, "http://personal.ashland.edu/": 1, "http://projecteuclid.org/": 1}, "blogtitle": "statalgo"}, {"content": ["Linear algebra lies at the heart of many modern statistical methods. As such, continuing on my short series on using the Julia language for basic statistical analysis , I want to give a short review of some basic matrix and vector procedures which we will use subsequently when constructing some simple optimization routines. \n [Note: The relevant Julia manual section lists all the relevant functions and should be considered the primary source.] \n Linear algebra provides a mechanism for efficiently solving systems of equations. A single equation may be expressed as a row vector, such as: \n \n As: \n \n Similarly, systems of equations can be combined into a matrix: \n \n Which can then be solved more efficiently using various different theorems. \n I recommend Gilbert Strang's MIT course on Linear Algebra as a reference (as I did in an extremely short introduction to Linear Algebra with R ). \n \n Julia, like R, uses LAPACK (which makes use of BLAS ) for all the related linear algebra functionality. LAPACK (Linear Algebra PACKage) is a software library for numerical linear algebra, written in Fortran 90. Dirk Eddelbuettel wrote an excellent paper (highly recommended) and a related R package (gcbd) benchmarking different BLAS implementations. Dirk shows four benchmarks in his paper: matrix crossproducts, SVD decomposition, QR decomposition, and LU decomposition. I give examples of the latter three of these below as these are core linear algebra algorithms. \n Before starting, I wanted point out some recent news with the language: \n \n There is a now a public web repl hosted by Forio . This makes it easy to test out the language. \n For R developers, John Myles White published a quick language comparison . \n \n Vector/Matrix Creation \n I showed how to construct an Array in the last post on basic Julia syntax . \n A matrix in Julia is nothing more than a 2-D array. There are many ways of constructing these. Some examples: \n \n# Construction of arrays\nA = randn(10) # an array of size 10\nA = [1:10]\n\n# Construction of matrices\nM = [1 2 3; 4 5 6] # a 2x3 matrix\nM1 = randn(3, 3) # a random 3x3 matrix\nM2 = reshape(fill(1, 9), 3, 3) # a 3x3 matrix of all 1's\n\n# Some special matrices\nID = eye(10) # The identity matrix\n \n We can get various different parts of matrices with simple commands: \n \ntriu(M) # Upper triangle of matrix M\ntril(M) # Lower triangle of matrix M\ndiag(M) # The diagonal vector from matrix M\n \n And do basic matrix math as expected \n \nM1 + M2\nM1 - M2\nM1 * M2\nM1 \\ M2 # Matrix division using a polyalgorithm (i.e. optimized for the type of matrix).\ninv(M) # Inverse of matrix M, equivalent to 1/M\n \n Vector/Matrix Maths \n There are several important matrix decomposition (or factorization) methods which are worth exploring further and are available through LAPACK. These all seek to break a matrix into other important canonical forms. \n LU Decomposition \n The first method is LU decomposition , which was introduced by Alan Turing in 1948 in his paper \" Rounding-off errors in matrix processes \". \n LU decomposition is a key step in several fundamental numerical algorithms, including as one method for solving a system of linear equations (as we shall see later), inverting a matrix (an extremely expensive operation), or computing the determinant of a matrix. This can be expressed in matrix notation as: \n \n In Julia, we can compute the simple example on wikipedia : \n \n \nWith the lu function. \n \nA = [4 3; 6 3]\nA = [1 -2 3; 2 -5 12; 0 2 -10]\n(L, U, p) = lu(A)\nL \nU\n \n You will see that this matches the same output from R using the lu() function from Matrix: \n \nlibrary(Matrix)\nM = matrix(c(4, 6, 3, 3), nrow=2)\nexpand(lu(M))\n \n QR Decomposition \n QR decomposition breaks a matrix into an orthogonal matrix and an upper triangular matrix can also be used to solve linear least squares. This is usually represented in matrix notation as: \n \n Wikipedia shows several methods for solving for this simple matrix: \n \n Which we can solve in Julia using the qr() function: \n \nA = [12 -51 4; 6 167 -68; -4 24 -41]\n(Q, R, p) = qr(A)\nQ\nR\n \n Similarly, in R we can use the base function qr() (note that R uses LINPACK by default): \n \nx = t(matrix(c(12, -51, 4, 6, 167, -68, -4, 24, -41), nrow=3))\nqr.R(qr(x, LAPACK=TRUE))\nqr.Q(qr(x, LAPACK=TRUE))\n \n Singular Value Decomposition (SVD) \n One of the most common routines is a singular value decomposition ; this is also used in fitting a least squares model. \n \n where U is a unitary matrix, the matrix \u03a3 is a diagonal matrix with nonnegative real numbers on the diagonal, and the unitary matrix V* denotes the conjugate transpose of V. \n Once again, we can solve for the example given on the wikipedia article as: \n \n Which solves into: \n \n This can be solved using the svd() function in Julia: \n \nA = [1 0 0 0 2; 0 0 3 0 0; 0 0 0 0 0; 0 4 0 0 0]\n(U, S, V) = svd(A)\nU\nS\nV\n \n Cholesky Decomposition \n Lastly, we will need to use Cholesky factorization (or Choleski, depending on your background). This is a special case of LU factorization (symmetric), but it requires half the memory and half the number operations of an LU decomposition. This can be expressed in matrix notation as: \n \n And can be solve in Julia with the chol() function: \n \nA = [5 1.2 0.3 -0.6; 1.2 6 -0.4 0.9; 0.3 -0.4 8 1.7; -0.6 0.9 1.7 10];\nR = chol(A)\nR\n \n \n It should be noted that I am showing these functions because they are important to solving statistical methods. But these functions are not relevant in so far as we are interested in Julia for the performance of the language, since they rely on LAPACK rather than native methods. \n Now that we have briefly covered the notation for several matrix decompositions, we are ready to look at a least squares problem in the next post."], "link": "http://www.statalgo.com/2012/04/15/statistics-with-julia-linear-algebra-with-lapack/", "bloglinks": {}, "links": {"http://dirk.eddelbuettel.com/blog": 1, "http://julia.forio.com/": 1, "http://ocw.mit.edu/": 1, "http://www.johnmyleswhite.com/": 1, "http://www.statalgo.com/": 3, "http://micromath.wordpress.com/": 1, "http://en.wikipedia.org/": 9, "http://qjmam.oxfordjournals.org/": 1, "http://cran.r-project.org/": 2, "http://www.netlib.org/": 1, "http://julialang.org/": 1}, "blogtitle": "statalgo"}, {"content": ["Before running any statistical analysis with the Julia programming language, I thought it would be fruitful to start by giving a (very) brief introduction to the syntax and basic language features. \n The Julia manual is already very detailed, so that should be considered the first source; I am here only going to scratch the surface, and put things in perspective (relative to R and Python/Pandas). Julia's syntax mostly resembles Matlab, so users of that language will be immediately comfortable. \n But first, I would also be remiss if I didn't highlight two exciting recent developments: \n \n Doug Bates implemented GLM . Doug is the creator of important LME4 package in R , so he's really an expert on this subject. \n John Myles White has been falling in love with the language, and created a Julia version of Simulated Annealing . \n \n Installation \n The installation instructions are documented on the github page , but different builds are available for download here , including a windows build which was just made available thanks to Keno Fischer and Jameson Nash (available for download here ). . \n Julia is most readily available on Linux or Mac OS X. I am running Julia on Ubuntu. If you want full control over the language (i.e. use the source, Luke), then it may be easier to switch off Windows to Ubuntu (with Wubi ) or Debian (with http://goodbye-microsoft.com/). Or there's always the virtual machine approach using VMWARE and a Bagside application . \n Data Types \n Julia is \"typeless\" like other dynamic languages , but it comes equiped with a really powerful type system . This means that you don't have to declare a variable type, but that you can do so and can easily create your own types. Julia is dynamically typed , but it can achieve such good performance through type inference with JIT from LLVM. \n Just like R and Python, simply entering a number into the Julia REPL results in its immediate type inference without explicit declaration. \n \njulia> typeof(1)\nInt64\n\njulia> typeof(1.0)\nFloat64\n\njulia> typeof(\"hello world\")\nASCIIString\nMethods for generic function ASCIIString\nASCIIString(Array{Uint8,1},)\n \n Julia also has the all-import unknown datatypes: \n \njulia> 1/0\nInf\n\njulia> Inf\nInf\n\njulia> NaN\nNaN\n \n And these might themselves be considered numeric types: \n \njulia> NaN + 1\nNaN\n\njulia> NaN + \"a\"\n+(Float64, ASCIIString)\nno method +(Float64,ASCIIString)\n \n The mathematical focus of Julia is also immediately apparent by the ability to specify mathematical formulas without excess notation. \n \njulia> 1.5x^2 - .5x + 1\n13.0\n \n Also, similar to some Lisps, Julia supports imaginary and rational numbers (using the // operator). This is big deal, because beyond everything else, it means that you can avoid some floating point errors. \n \njulia> 2//3 + 1//3 + 2 == 3\ntrue\n \n Arrays \n Julia comes with a flexible array type, which can hold any number of dimensions. \n Most simple arithmetic/logic functions can be used in a vectorized form by affixing a \".\". \n \njulia> x = randn(10)\n[0.120646, 0.857561, 0.819921 ... -1.80995, -0.466323, -0.111218]\n\njulia> 2x\n[0.241292, 1.71512, 1.63984, -0.328591 ... -3.61991, -0.932645, -0.222436]\n\njulia> x * x\n*(Array, Array)\nno method *(Array{Float64,1},Array{Float64,1})\n\njulia> x .* x\n[0.0145555, 0.735411, 0.67227, 0.026993 ... 3.27594, 0.217457, 0.0123695]\n \n Another very useful feature is comprehensions . This is based on the set notation in mathematics, and it basically defines and function and then iterates over values within that function. \n \njulia> [ x^2 | x=1:10 ]\n{1, 4, 9, 16, 25, 36, 49, 64, 81, 100}\n \n Python users will find this very similar to list comprehensions . \n As in NumPy, a Matrix in Julia is just a 2-D array: \n \njulia> Matrix\nArray{T,2}\n \n Functions \n Functions in Julia are very similar to functions in R and Python. They can be declared in long form or on one line: \n \nfunction f(x,y)\n x + y\nend\n\nf(x,y) = (z = x + y; 2z)\n\njulia> f(3, 4)\n14\n \n You can also return values with the return command, particularly if there are multiple routes through a function. \n Julia also supports anonymous functions (equivalent to a lambda function in Python). \n \njulia> map(x -> x/2.0, [1,3,-1])\n[0.5, 1.5, -0.5]\n \n Functions also support multiple return values and varargs (using the ellipsis ...). Functions currently do not support default parameter values, although that's in the works. \n Julia also include methods which allow for multiple dispatch: \n Although it seems a simple concept, multiple dispatch on the types of values is perhaps the single most powerful and central feature of the Julia language. \n This allows for different behavior depending on the parameters passed to the function. This can readily be see by typing + at the console. \n \nsame_type_numeric{T<:Number}(x::T, y::T) = true\nsame_type_numeric(x::Number, y::Number) = false\n\njulia> same_type_numeric(1, 2)\ntrue\n\njulia> same_type_numeric(1, 2.0)\nfalse\n \n I have really glossed over many details (e.g. flow control and loops) as well as advanced language features (such as metaprogramming and parallel computing). But this should give a taste. All else is readily available in the manual . \n Next up, I will give a short review of some linear algebra in Julia, before starting to look at basic statistical analysis in the language."], "link": "http://www.statalgo.com/2012/04/04/statistics-with-julia-the-basics/", "bloglinks": {}, "links": {"http://www.ubuntu.com/": 1, "https://github.com/": 3, "http://www.johnmyleswhite.com/": 1, "http://www.python.org/": 1, "http://bagside.com/": 1, "http://en.wikipedia.org/": 6, "http://julialang.org/": 4, "http://groups.google.com/": 2, "http://cran.r-project.org/": 1}, "blogtitle": "statalgo"}, {"content": ["I first heard about the Julia programming language a little over a month ago, in the middle of February with their first blog post: \"Why We Created Julia\" . This was an exciting turn of events. \n We want a language that\u2019s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that\u2019s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.\n \n This is music to my ears. This is what I want too! The thing that's really exciting is that it actually looks like the language may deliver on these things. And after a very short period of time, it is gaining a significant amount of traction on the Julia developers list (an important indicator for whether a language will succeed). [I was also interested to see that one of the language creators, Stefan Karpinski , was a high school classmate.] \n I was recently reading the \"Steve Jobs\" biography and Jobs discussed one major realization after selling the Apply 1 : to go beyond the geeks, it would be necessary to include the full package, such as a monitor, keyboard, and power supply. Julia is still at this early stage: it must be built off github, and only supports Linux and OS X. But the documentation is already extensive. \n I use R and Python for all my research (with Rcpp or Cython as needed), but I would rather avoid writing in C or C++ if I can avoid it. R is a wonderful language, in large part because of the incredible community of users. It was created by statisticians, which means that data analysis lies at the very heart of the language; I consider this to be a major feature of the language and a big reason why it won't get replaced any time soon. Python is generally a better overall language, especially when you consider its blend of functional programming with object orientation. Combined with Scipy/Numpy, Pandas, and statsmodels, this provides a powerful combination. But Python is still lacking a serious community of statisticians/mathematicians. \n There are always other languages to consider. I've tried OCaml , Haskell , J , K , Q , along with Matlab and Mathematica. These are all great languages and platforms. But they are generally lacking something, by either being expensive and closed source or simply lacking features and community support. It wasn't too long ago when people were considering Clojure with Incanter as an alternative. But while clojure is a nice language (i.e. Lisp is a nice language), Incanter is not a serious option for replacing R. For starters: it's performance was worse for very basic operations. And it doesn't have anywhere near the amount of libraries for analysis. \n Julia and R \n My interest has continued to grow with the active involvement of Douglas Bates and Harlan Harris on the Julia discussion list. Bates also wrote a nice blog post showing a performance comparison vs. R and Rcpp . Some of the discussion has been taking place on the Julia developers list: \n \n \"R and statistical programming\" from Harlan on Feb 25 \n \"RFC: data frame proposal\" \n \n The addition of a real data frame, and appropriate handling of NA/NaN values, will be a serious addition to Julia. \n There has also been some discussion taking place on the R developers list . \n The question remains: Is Julia a viable option for statistics and machine learning at this stage? I'm going to start a short blog series exploring some simple analysis with the language over the next few weeks to try and explore the language a little further. My hope is to learn a little about the language and draw some attention to interesting new developments. \n [Note: I should also draw attention to Vince Buffalo's post on the same topic .]"], "link": "http://www.statalgo.com/2012/03/24/statistics-with-julia/", "bloglinks": {}, "links": {"http://karpinski.org/": 1, "http://vincebuffalo.org/": 1, "http://www.wisc.edu/": 1, "http://www.jsoftware.com/": 1, "http://groups.google.com/": 3, "http://julialang.org/blog": 2, "http://www.harris.name/": 1, "http://www.statalgo.com/": 1, "http://dmbates.blogspot.com/": 1, "http://caml.inria.fr/": 1, "http://r.nabble.com/": 1, "http://en.wikipedia.org/": 3, "http://www.haskell.org/": 1, "http://cython.org/": 1, "http://www.amazon.com/": 1, "http://dirk.eddelbuettel.com/": 1, "http://incanter.org/": 1}, "blogtitle": "statalgo"}, {"content": ["Readers of this blog will no doubt be eagerly following along with the continuing developments of open education at Coursera and Udacity . As a professed autodidact, I have been a long-time consumer of online education, especially through iTunes university, and I'm really excited to see where everything is going. \n I regretfully won't have time to fully explore the offerings over the next few months. I am signed up for Natural Language Processing , Probabilistic Graphical Models , Game Theory , Model Thinking , and Information Theory. Of these, Information Theory interests me the most since I have a background in the subject having studied Electrical Engineering (and signal processing). I started watching the lectures on the other classes and thus far continue to be extremely impressed. \n While I won't be posting R code to accompany all of these lectures (I'm close to finishing a few more in the Stanford Machine Learning series , on Neural Networks, and that's a higher priority for me), I did want to point out a few relevant resources: \n \n For Natural Language Processing, there are two excellent books that are available for free online: \"An Introduction to Information Retrieval\" by Manning, Raghavan, and Sch\u00fctze (2009) (book website: http://nlp.stanford.edu/IR-book/) and \"Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit\" by Bird, Klein, and Loper (2009). The course itself primarily uses \"Speech and Language Processing\" by Jurafsky and Martin. Another good book that I've reviewed in the past is \"Foundations of Statistical Natural Language Processing\" by Manning and Sch\u00fctze (1999) (book website: http://nlp.stanford.edu/fsnlp/). R users can look at the NLP task view for further materials. \n For Probabilistic Graphical Models, the most widely used book is Probabilistic Graphical Models: Principles and Techniques by Friedman and Koller (2009). This is the also the primary (optional) text for the course as it is being taught by none other than Daphne Koller . R has a wealth of resources on this subject available in the graphical models task view . \n \n As always, let me know if you come across other interesting material."], "link": "http://www.statalgo.com/2012/03/15/openeducation/", "bloglinks": {}, "links": {"http://ai.stanford.edu/": 1, "http://www.nltk.org/": 1, "http://nlp.stanford.edu/": 1, "http://www.pgm-class.org/": 1, "http://www.modelthinker-class.org/": 1, "http://www.statalgo.com/": 1, "http://www.game-theory-class.org/": 1, "http://www.amazon.com/": 3, "https://www.coursera.org/": 1, "http://www.nlp-class.org/": 1, "http://www.udacity.com/": 1, "http://cran.r-project.org/": 2}, "blogtitle": "statalgo"}, {"content": ["We considered the problem of overfitting as model complexity increase in the prior post . Now we look at one way to control for this problem: regularization. The basic idea is to penalize each the model, essentially saying that we don't entirely believe the fit that falls out of our optimization. Since we are fitting to a sample of the data, overfitting will mean that the resulting model doesn't generalize well: it won't fit well to new datasets since they are unlikely to match the training data exactly. \n [This is just a short post on regularization to show how it can help improve the generalization of a model.] \n Regularization and Ridge Regression \n Continuing with the polynomial regression example from PRML 1.1, we now look at adding a penalty term to the error function. This will discourage the parameters from reaching large values during the optimization. Our old loss function for linear regression and logistic regression was: \n  \n Now adding the penalty term, it becomes: \n  \n Notice again that the loss function is identical for linear regression and logistic regression; what differs is the hypothesis function . [Note: If you are following along with PRML, then will notice that Bishop refers to this as the error function and parameters are labeled instead of .] \n This particular form of regularization, using a quadratic penalty term, is known as ridge regression . \n We can minimize the loss function as before using gradient descent, or using an explicitly solution from linear algebra. I have implemented these solutions but not posted them for the time being because the performance of the gradient descent solution is appalling. The closed form solution is already implemented in R in the MASS package, in the lm.ridge function. This function does not have a prediction function, so I have implemented this here. \n In the last post, we saw before how increasing the model complexity resulted in a poor fit on the out-of-sample data. The more complex model is overfit to the training dataset. Here we can see the same diagram using ridge regression. At high model complexity, the fit still remains roughly constant because these additional terms are penalized. \n  \n \n I won't expand on regularization at this stage, although I will commit the gradient descent solution to the github project. I will expand further on these topics (looking at other regularization models such as Lasso) in later posts when I continue with ESL. For now, we will start moving onto neural networks in the next post."], "link": "http://www.statalgo.com/2011/11/16/stanford-ml-5-2-regularization/", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1, "http://www.statalgo.com/": 2}, "blogtitle": "statalgo"}]