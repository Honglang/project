[{"blogurl": "http://bit-player.org\n", "blogroll": [], "title": "bit-player"}, {"content": ["The invented etymology of posh\u2014which says it\u2019s an acronym for \u201cport out, starboard home\u201d\u2014is utterly bogus. Nevertheless, when I book airline seats I always try to get a window on the shady side. Photography is easier with the sun at your back. This morning, however, I was on the sunny side southbound out of Boston. The photographic result was fairly curious. \n \n You might think you\u2019re looking at a snowy landscape here, with bits of two lakes intruding into the left and right edges of the frame. But the lakes are actually islands, and the white field between them is the sea, with the specular reflection of the sun in the upper lefthand corner. Under these lighting conditions, it seems that small variations in angle or texture cause huge differences in brightness. It\u2019s Snell\u2019s Law in action. \n A detail from a second frame\u2014two was all I caught\u2014shows the interference patterns in the boat wakes even more clearly. \n \n Here is the uncropped and unenhanced version of the first image. (Uncropped but with grossly reduced pixel count.) \n \n The two islands are part of an archipeligo that extends southwest from Woods Hole on Cape Cod, between the mainland and Martha\u2019s Vineyard. I believe these two are Pasque Island on the left and Nashawena Island on the right. Wikipedia tells me that almost all of these islands, which are known as the Elizabeths, are owned by the Forbes family. How posh!"], "link": "http://bit-player.org/2012/on-the-sunny-side", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1}, "blogtitle": "bit-player"}, {"content": ["Less than two years ago, I was reading and writing about a new kind of passive circuit element\u2014the memristor, a companion to the resistor, the capacitor and the inductor. (See the bit-player coverage , a bit-player followup and my American Scientist column .) The principal actors in this story were Leon Chua of Berkeley, who formulated the theory of the memristor 40 years ago, and R. Stanley Williams of Hewlett-Packard, who announced a working device in 2008. \n I haven\u2019t been keeping up with memristor news lately, but I did notice that Williams has been making the rounds with a talk on \u201cMott Memristors, Spiking Neuristors and Turing Complete Computing.\u201d When he came to Harvard last week, I went over to listen in. I discovered that my 20-month-old article is totally out of date. \n The memristor I wrote about in 2011 was based on ion drift in titanium dioxide. A layer of TiO 2 was fabricated with a conductive doped region and an insulating undoped region. Current flowing through the TiO 2 would shift the boundary between the two regions and thereby switch the memristor between conducting and insulating states. The device was bipolar: Currents in opposite directions had opposite effects. It was also nonvolatile: The resistance state would persist even after the current ceased. \n The memristor that Williams discussed last week is still recognizable as a member of the same family, but just barely so. Again the material is TiO 2 or another transition-metal oxide, but there is no mention of doping. And, again, electric currents cause a change in resistance, but the underlying mechanism is quite different. Williams described the device as a cylinder of insulating oxide with a narrow conductive channel down the middle; current flowing through the channel heats the cylinder from the inside out, causing a phase transition that converts surrounding layers of material to the conducting state. Specifically, the heating induces a Mott transition, in which localized electron clouds begin to overlap, bringing on a sudden increase in conductivity. \n During the talk I didn\u2019t hear Williams say anything about reversing the phase transition, but other sources indicate that the high-resistance state is restored by applying a second, larger current. Note that this memristor is not a bipolar device: The switching actions can be triggered by currents flowing in either direction. \n A heat-induced phase transition sounds like an unlikely mechanism for a modern computing element. When I heard the idea explained, I couldn\u2019t help thinking of computing with a toaster, whose glowing red wires seem ruinously power-hungry, and awfully slow. But Williams says the Mott memristor can be both fast and efficient, because the active volume is very small, with a typical dimension of 30 nanometers. Even though the temperature swing may be as much as 800 Kelvin, the switching time is nanoseconds, and the energy dissipated is femtojoules. \n It\u2019s a little disconcerting to see the basic physical principles of the memristor changing so drastically before the device even reaches the market. But I suppose there\u2019s precedent in the early history of the transistor. The transistor announced by Bell Labs in 1948 was a bipolar, point-contact device made from a block of germanium; the world now runs on field-effect transistors imprinted on a silicon surface. \n Which brings us to the big, obvious question I wasn\u2019t able to answer two years ago. Will the memristor become the transistor of the 21st century, transforming electronics and computing? Or will it fade away like so many other once-promising technologies, like magnetic bubbles or superconducting Josephson junctions? Or could it find a niche market, the way charge-coupled devices have? \n It\u2019s well-known\u2014and it may even be true\u2014that most innovations fail to dislodge the incumbent technology. That\u2019s a reason for betting against the memristor regardless of its particular merits and handicaps. No doubt it\u2019s a smart bet. Nevertheless, I would like to say one thing in support of efforts to develop the memristor and other novelties like it. The mainstream in digital electronics is still focused on taking devices whose operation we understand at micrometer scale and trying to reproduce the same behavior in devices a thousand times smaller. It seems worthwhile trying the opposite strategy as well: Looking at what happens \u201cnaturally\u201d at nanoscale, and trying to build something out of it. \n After the talk, I dug up a few papers with more details on the recent developments. Two are by members of the Williams group: \n Alexandrov, A. S., A. M. Bratkovsky, B. Bridle, S. E. Savel\u2019ev, D. B. Strukov and R. Stanley Williams. 2011. Current-controlled negative differential resistance due to Joule heating in TiO 2 . Applied Physics Letters 99:202104. ( Preprint. ) \n Strukov, Dmitri B., Fabien Alibart and R. Stanley Williams. 2012. Thermophoresis/diffusion as a plausible mechanism for unipolar resistive switching in metal\u2013oxide\u2013metal memristors. Applied Physics A DOI 10.1007/s00339-012-6902-x. ( Preprint. ) \n The third is a slightly earlier discussion of Mott memristors by a Korean-UCSD collaboration: \n Driscoll, Tom, Hyun-Tak Kim, Byung-Gyu Chae, Massimiliano Di Ventra and D.N. Basov. 2009. Phase-transition driven memristive system. ( arXiv preprint. )"], "link": "http://bit-player.org/2012/remember-the-memristor", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://www.americanscientist.org/": 1, "http://bit-player.org/": 2, "http://www.ucsb.edu/": 2}, "blogtitle": "bit-player"}, {"content": ["In my latest American Scientist column I\u2019m packing spheres again . The goal this time is to maximize the number of contact points where spheres touch each other. For example, four spheres in a tetrahedral cluster have six contacts; this is the most possible, since each sphere touches all the rest. Over the past few years groups at Harvard and Yale have solved the maximum-contact problem for clusters of up to 11 spheres. Beyond that, nobody knows. \n You can read all about it in your choice of PDF or living HTML , or you might even hunt down one of the rare and precious paper copies of the magazine. \n The American Scientist column focuses on the work of the Harvard and Yale groups. Here I\u2019m going to talk about my own adventures with contact-counting while creating illustrations for the article. \n Geomag . When I visited some of the people behind the new contact-counting results, the first thing I noticed was their toys. They play with Geomag , a Swiss-made construction kit consisting of plastic-coated magnetic rods and half-inch polished steel balls. As soon as I got home, I bought a set for myself. \n The pair of images below, showing two depictions of a regular tetrahedron, should make clear how clusters of spheres map onto Geomag models. \n \n Assume all spheres have a radius of 1/2; then two tangent spheres have a center-to-center distance of 1. In the diagram at right I emphasize this fact by drawing a rod of unit length between the centers of all spheres that are in contact. The Geomag model at left reproduces this skeleton of rods, omitting the spheres themselves. \n Natalie Arkus, who initiated the recent contact-counting work when she was a graduate student at Harvard (she is now at Rockefeller University) told me that the Geomag models were more than just a visual aid in her work. They were a tool for discovering new configurations and deciding which candidate structures are geometrically feasible. \n Spinners . Having a 3D model to turn over in your hands is definitely the best way to understand the geometry of sphere clusters. Unfortunately, there is no <geomag> HTML tag that would allow me to embed a model in this web page. \n A familiar alternative for 3D visualization is the stereoscopic pair: \n \n I\u2019m not a big fan of this technique. Not everyone is adept at fusing the images without the help of a viewing device. And even when you can achieve the stereo illusion, it doesn\u2019t help much in seeing what\u2019s on the back side of the object. \n My own favorite trick for 3D visualization is to make the object twirl. It\u2019s no secret that the human visual system does a better job of interpreting three-dimensional structures from a moving image than from a still one. This strategy is of no use in the print edition of the magazine, but I wanted to take advantage of it in the web edition and here on bit-player. If you haven\u2019t done so already, hover your mouse pointer over the image above right; it should start spinning. \n \u201cShould,\u201d I said. It works fine in Chrome and Safari and Opera. It even works in Internet Explorer. It works in Firefox version 3.6. But newer versions of Firefox give erratic results. Typically, the animation plays once but then dies. This bug was reported to Mozilla ages ago, but it persists in the latest Firefox update (16.0.1). Sorry for the rant, but it\u2019s not like we\u2019re dealing with bleeding-edge HTML5 technology here. The spinning sphere clusters are animated GIFs, a file format that goes back to 1989. \n Coordinates . The first task in creating these illustrations is finding the coordinates of the sphere centers. Both the Harvard and the Yale groups have published extensive lists of coordinates, but I elected to roll my own. I wanted the animated figures to twirl around an axis of symmetry (in those cases where such an axis exists), and I found it easier to build up the geometry from scratch rather than making transformations from some other coordinate system. \n Calculating coordinates is generally easy in these clusters because they all share an important property: Each sphere touches at least three other spheres. Suppose you want to add a fifth sphere to the tetrahedral cluster illustrated above. The only way the new sphere can touch three others is if it nestles into the central dimple in one of the four triangular faces of the tetrahedron. You can find the coordinates of the fifth center point by solving three simultaneous equations, defining distances to the three vertices of the triangular face. The equations have two solutions, defining points above and below the plane of the triangle. In the tetrahedron, one of those points is already occupied, and so the new sphere must be placed at the other solution point, on the opposite side of the plane of the triangle. \n Having attached a fifth sphere to the cluster, you can use the same method to add a sixth, a seventh, and so on. In clusters made up entirely of regular tetrahedra, the process is especially straightforward. With other geometries it\u2019s not always quite so obvious how to proceed, but with one exception I was able to construct all the clusters of interest by such step-by-step methods. (For the exception, see below under The Problematic Eight .) \n Making Movies . With the coordinates in hand, assembling the diagrams is not complicated. At each sphere-center coordinate, draw a semitransparent sphere of radius 1/2. Then, to represent the Geomag-like skeleton of bonds, draw much smaller opaque spheres (radius 1/20) at each point, and add opaque tubes connecting the centers of spheres that are touching. How does the algorithm know which spheres are touching? Simply run through all \\(n(n-1)/2\\) pairs of spheres and select those whose euclidean distance is close enough to 1. (\u201cClose enough\u201d allows for some imprecision in the case of coordinates determined by numerical rather than analytic methods.) \n To make the cluster twirl, I generate a series of views, rotating the object by some fixed, small angle between snapshots. If the object has rotational symmetry, there\u2019s no need to twirl through a full 360 degrees. The tetrahedron, for example, makes do with a 120-degree subset. \n All this was done in Mathematica, which has a rich set of three-dimensional graphics routines. Rotating a 3D object, for example, is a built-in function; all the trig and the linear algebra happens behind the scenes, without human intervention. Another Mathematica command exports the whole series of rotated views as a prepackaged GIF file. The entire makeSpinner procedure is a seven-line program. Nifty! On the other hand, I had to struggle to defeat some of the Mathematica-knows-best automation. It seems a 3D graphic is always sized so that its 2D projection fills the allotted rectangular region on the screen. This may be a good idea most of the time, but it induces nausea with a rotating object. Try hovering on the octahedron at left. I found a directive (SphericalRegion \u2192 True) that I thought would cure this behavior, but it didn\u2019t. If anyone reading this knows the right way to mark it up, please do let me know. I wound up with a kludge: I surrounded each cluster with a large, invisible sphere centered at the origin, fooling the Mathematica drawing routines into thinking that the cluster has constant width. \n The Mathematica source code for building all the illustrations and animations is available here as a Mathematica notebook . \n The Wiggler . Once I had all of those pirouetting purple bunches of grapes, I realized that the same animation apparatus could be adapted to show other kinds of motion. \n Among all the sphere packings I\u2019ve met in the course of this project, one of the most pivotal\u2014and I choose that word carefully\u2014is a nine-sphere configuration that exhibits a degree of torsional flexibility. Even though every sphere in the cluster touches at least three others, and therefore cannot move as an individual, the overall structure can twist without breaking any bonds. \n \n Animating this kind of motion is a little more complicated than displaying a rigid rotation of the entire cluster. At each time step, the two spheres at the top of the structure are twisted in opposite directions; then all the rest of the coordinates have to be recalulated to accommodate the change. \n Note that the animation shows an exaggerated range of motion. The structure can twist only infinitessimally if you insist on maintaining all center-to-center distances at exactly 1. The animation was drawn with a tolerance of &pm1 percent, which allows rotation of &pm4 degrees before bonds start breaking. \n The Fivefold Way . If you spend any time playing with Geomag models, you are sure to stumble upon the structure shown at right, which consists of four tetrahedra joined along faces. It looks as if you might be able to add one more bond to close the gap, creating a solid of five joined tetrahedra. But it doesn\u2019t work. The gap is slightly too wide. (By the way, I consider this a defect of our universe. We shouldn\u2019t have to put up with such untidyness.) If you remove the central strut from the cluster of four tetrahedra, allowing the top and bottom spheres to move apart slightly, the gap closes and you arrive at a new cluster with exact fivefold symmetry, the pentagonal dipyramid, shown at left. \n After the magazine had gone to press, I realized that the transition between these two structures could also be made clearer by an animation. Hover on the figure below to see it in action: \n \n The Arkus Conjecture . The reversible transition between a tetratetrahedron and a pentagonal dipyramid requires breaking one bond and forming one new bond. Here\u2019s another example of a single-bond exchange, where an octahedron is transformed into a cluster of three joined tetrahedra. \n \n Arkus points out that all known maximum-contact packings are connected by some sequence of such single-bond moves. She conjectures that this fact will remain true for all larger clusters as well. If the conjecture is true, it will become much easier to catalogue the clusters for each value of n . \n The Problematic Eight . At n =8 we encounter this peculiar structure, which can be understood as two pentagonal dipyramids smushed together at right angles: \n \n This object is stumped all my efforts to determine the coordinates by a step-by-step procedure, working from a known triangle to a fourth vertex, then a fifth, and so on. Making the Geomag model was easy, but to generate the Mathematica graphic I had to solve for five variables simultaneously. \n Turn It Up to 12 . The Harvard and Yale groups identified all configurations that maximize the contact number for clusters with up to 11 spheres. Beyond this limit is terra incognita. Nevertheless, there\u2019s a 12-sphere solution that looks like a very plausible candidate. \n \n At the upper left is the nine-sphere \u201cwiggler,\u201d with 21 contacts. Then each of the subsequent frames, proceding from left to right and top to bottom, adds a single node and four more contacts. The final state, with 12 spheres and 33 contacts, is also shown at right in a spinning diagram. Is this structure the best that can be achieved with 12 spheres? For now the only reliable way to answer this question is to catalogue all other 12-sphere candidate clusters, and that would be a formidable undertaking. \n I should add that this dense 12-sphere cluster is not a subset of the Kepler packing (hexagonal close-packed or face-centered cubic), and it cannot be extended to fill all of three-dimensional space. \n Overflow . In American Scientist, my column this month spilled over two extra pages beyond my usual allotment. Nevertheless, there are still topics that I wasn\u2019t been able to fit in. Some pointers: \n \n In the 1970s, M. R. Hoare and J. McInnes published a pioneering study of \u201csmall atomic clusters,\u201d anticipating some of the recent results. They surveyed clusters with up to 13 atoms but considered only clusters that could be formed by combining tetrahedra and octahedra, ignoring various \u201cnew seeds\u201d such as the pentagonal dipyramid. Reference: Hoare, M. R., and J. McInnes. 1976. Statistical mechanics and morphology of very small atomic clusters. Faraday Discussions of the Chemical Society 61:12\u201324. \n In 1995 Neil J. A. Sloane, R. H. Hardin, T. D. S. Duff and John Horton Conway suveyed \u201cminimal-energy\u201d clusters of up to 32 identical spheres. They defined the lowest-energy configuration as the one that minimizes the second moment of the spatial distribution, or in other words the sum of the squares of the distances from the center of mass. Although this criterion is quite different from the contact-counting method, many of the same clusters turn up. Reference: Sloane, N. J. A., R. H. Hardin, T. D. S. Duff and J. H. Conway. 1995. Minimal-energy clusters of hard spheres. Discrete and Computational Geometry 14:237\u2013259. \n Conway and Sloane are also the authors of the standard reference work on the mathematics of sphere packing: Conway, J. H., and N. J. A. Sloane. 1999. Sphere Packings, Lattices, and Groups . 3rd edition. New York: Springer. I can also enthusiastically recommend a less-formal account of packing problems by Tomaso Aste and Denis Weaire: The Pursuit of Perfect Packing , 2008, second edition. New York: Taylor & Francis. \n If I ever get a chance to return to this topic, I would like to take a closer look at the work of Salvatore Torquato and his colleagues at Princeton, who study dense clusters in a context akin to the Newton kissing-number problem. For example, see: Hopkins, Adam B., Frank H. Stillinger and Salvatore Torquato. 2011. Densest local sphere-packing diversity. II. Application to three dimensions. Physical Review E 83:011304. ( arXiv preprint ) \n \n Thanks . For helpful conversations and other kinds of guidance, my thanks to Rob Hoy of the University of South Florida, Corey O\u2019Hern of Yale, Vinny Manoharan of Harvard and Natalie Arkus of Rockefeller University."], "link": "http://bit-player.org/2012/dancing-with-the-spheres", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://support.mozilla.org/": 1, "http://www.geomagworld.com/": 1, "http://www.americanscientist.org/": 2, "http://bit-player.org/": 3}, "blogtitle": "bit-player"}, {"content": ["Nate Silver\u2019s fivethirtyeight blog for the New York Time s applies computational statistics to U.S. presidential politics. A recent post discusses the possibility of a tie vote in the Electoral College. \n \n If the votes on November 6 should come out according to the map above, we\u2019ll have a 269-to-269 deadlock, leaving it to the House of Representatives to choose the next president. Silver ran 25,001 simulated elections, and the result shown above appeared 137 times. Eight other tied arrangements also turned up, though more rarely. The overall probability of an even split was about 0.6 percent. (The simulations were based on polls taken before last week\u2019s Obama-Romney debate.) \n Looking at Silver\u2019s maps, I began to muse about a purely combinatorial question: Setting aside all considerations of political plausibility, in how many different ways can the Electoral College vote come out a tie? To put it another way, how many two-colorings of the U.S. map give each party 269 Electoral College votes? \n I now have an answer to this question, which I\u2019ll give below. But first, for the benefit of my non-U.S. readers, I should say a word about that curious institution the Electoral College . It was the wisdom of the Founders that the POTUS should not be elected directly by the people but rather by the states, with each state getting as many votes as it has senators (two each) plus members of the House of Representatives (from 1 to 53, depending on population). Since 1964 the District of Columbia (which isn\u2019t a state, but I\u2019m going to call it one anyway) has also had three votes. Thus there are 51 voting states, with vote allocations ranging from 3 to 55, and the total number of votes is 538. \n \n \n California 55 \n Maryland 10 \n Utah 6 \n \n \n Texas 38 \n Minnesota 10 \n Nebraska 5 \n \n \n Florida 29 \n Missouri 10 \n New Mexico 5 \n \n \n New York 29 \n Wisconsin 10 \n West Virginia 5 \n \n \n Illinois 20 \n Alabama 9 \n Hawaii 4 \n \n \n Pennsylvania 20 \n Colorado 9 \n Idaho 4 \n \n \n Ohio 18 \n South Carolina 9 \n Maine 4 \n \n \n Georgia 16 \n Kentucky 8 \n New Hampshire 4 \n \n \n Michigan 16 \n Louisiana 8 \n Rhode Island 4 \n \n \n North Carolina 15 \n Connecticut 7 \n Alaska 3 \n \n \n New Jersey 14 \n Oklahoma 7 \n Delaware 3 \n \n \n Virginia 13 \n Oregon 7 \n District of Columbia 3 \n \n \n Washington 12 \n Arkansas 6 \n Montana 3 \n \n \n Arizona 11 \n Iowa 6 \n North Dakota 3 \n \n \n Indiana 11 \n Kansas 6 \n South Dakota 3 \n \n \n Massachusetts 11 \n Mississippi 6 \n Vermont 3 \n \n \n Tennessee 11 \n Nevada 6 \n Wyoming 3 \n \n \n Generally, each state\u2019s votes are cast as a winner-take-all block (but see the note on Maine and Nebraska at the end of this post). \n I assume here that only two parties receive Electoral College votes. Thus any partitioning of the elector list into two subsets\u2014red and blue\u2014is a possible election outcome. The number of distinguishable outcomes is simply the product of all these binary choices: 2 51 , or 2,251,799,813,685,248. We want to know how many of these cases yield exactly 269 red and 269 blue votes. It turns out there are 16,976,480,564,070 ways of arriving at a drawn election, which is roughly 0.75 percent of the total. \n Let me emphasize that this number is of no political consequence; in particular, it says nothing about the probability of a tie\u2014at least not unless the states choose their votes by flipping fair coins. I can\u2019t see where the number holds any notable mathematical significance either. What attracted my interest was simply the challenge of computing it. \n Doing it the direct and obvious way\u2014enumerating all 2 51 partitions and summing the votes for each case\u2014is not utterly unthinkable. Amazingly enough, we live in a world where you can count to a quadrillion and live to tell the tale. But doing so would require more programming effort\u2014or more patience or more hardware\u2014than I\u2019m willing to invest for the sake of idle curiosity. \n It\u2019s interesting that some bigger problems are actually easier. Suppose we keep the number of electors constant at 538 but we subdivide the states into smaller territories, each of which gets fewer votes. In the limiting case there are 538 regions with one vote each. Now the problem is immensely larger: There are 2 538 ways of two-coloring a map with 538 regions. Nevertheless, with very little fuss we can calculate the number of tied configurations, without having to count them all one by one. The number is: \n $$\\binom{538}{269} = \\frac{538!}{(269!)^2} \\approx 3 \\times 10^{160}.$$ \n (The exact value is 30937469012875859932471852213149074559 46754979248232932201743920079668590495281866215749684213 30476315882464242716565222046883627439960129220374560486 58575478800.) \n Is there some similar mathematical magic that will solve the original Electoral College problem? Not quite so neatly, as far as I know. But the same underlying principle offers a measure of help. \n Let\u2019s focus for a moment on the eight states in the Electoral College that get three votes each. Considering just those states in isolation, they have 2 8 = 256 possible red-blue configurations, but only nine different ways of contributing to the election outcome. If red wins none of the states, it gets zero votes. If red wins any one of the states, it gets three votes, and there are eight ways for this to happen. Winning any two of the states yields six votes, and there are \\(\\binom{8}{2} = 28\\) combinations with this result. In the map above, red wins five of the three-vote states (Alaska, Montana, North Dakota, South Dakota and Wyoming) while blue wins three (Delaware, the District of Columbia and Vermont). This combination is one of 56 that produce a 15\u20139 vote split in favor of red. By weighting the vote sums according to the appropriate binomial coefficients, we can deal with just nine cases instead of 256. \n The same trick works for the five states that get four votes apiece, the three states that have five votes each, and so on. A neat way of organizing the computation is to count through all the possible values of a mixed-radix number (where each digit has its own radix, independent of its neighbors). For the Electoral College data, the mixed-radix number has a digit for each set of states that are allocated the same number of votes; the radix of this digit is one more than the number of states in the set. The number of 19 digits overall, with radices ranging from 2 to 9. \n \n We can survey the full spectrum of election outcomes by cycling through all the values of this number, starting at 0000000000000000000 and counting up to 1122121111443236358. For each value we calculate the corresponding vote total s : \n $$s = \\sum_i k_i m_i$$ \n Then we use s as an index into an array H where we keep track of how many ways the total s can be achieved. We increment H s by the appropriate number of combinations: \n $$H_s = H_s + \\prod_i \\binom{r_i}{k_i}$$ \n With this algorithm, the number of configurations for which we need to sum up the votes is 6,270,566,400, which is a lot less than 2 51 . Even allowing for the extra work of calculating binomial coefficients, the mixed-radix strategy reduces the size of the computation by a factor of 10,000 or so. For a Lisp program running on a laptop, it becomes a job of hours rather than years. \n Since I was riffling through all of those 6,270,566,400 configurations anyway, I figured I might as well record not just the number of ties but the full spectrum of vote totals. It\u2019s no surprise that the result looks like a binomial distribution: \n \n What did surprise me a little is the smoothness of the curve. I was expecting a bit of lumpiness because of the minimum allocation of three votes and because a few states constitute large, indivisible blocks of votes. If you zoom in on the extreme tails of the distribution, there is indeed some jaggedness: \n \n votes:  0 1 2 3 4 5 6 7 8 9 10\n combinations: 1 0 0 8 5 3 34 43 36 122 201\n \n There\u2019s just one way that a party can receive zero votes; winning either one or two votes is impossible; seven votes are more likely than either six or eight. But fluctuations of this kind become undetectable toward the middle of the distribution. \n Is there some other approach that would improve on the mixed-radix algorithm? Almost surely. But I would bet against the possibility of a polynomial-time algorithm. Counting Electoral College ties is a variation on another problem for which I harbor a special fondness, called the number partitioning problem (NPP). I was introduced to the NPP by my friend Stephan Mertens , and I wrote about it in a 2002 American Scientist column . The NPP asks whether a set of positive integers has at least one partitioning into two sets that have the same sum. In this form, the problem is NP-complete. The counting version of the problem is surely no easier. \n Finally, I have to confess that I haven\u2019t really computed all the ways that the Electoral College can wind up with a tied vote. The snag is that pesky pair of states Maine and Nebraska, which do not necessarily award all their votes to the winner of the state contest. Instead they allocate one elector to the winner in each Congressional district, with the two senatorial electors going to the winner of the entire state. Thus Maine could be viewed as comprising three voting territories with allocations of 1, 1 and 2 electoral votes; Nebraska breaks down into four districts with 1, 1, 1 and 2 votes. This fragmentation of the vote expands the combinatorial challenge of counting ties. Another complication is even more troublesome: The intrastate voting territories are not independent. Unless vote counting is even more treacherous than it seems, you can\u2019t lose all of a state\u2019s Congressional districts but win the state overall. In my calculations I\u2019ve simply ignored the possibility of split votes in Maine and Nebraska; perhaps someone else would like to patch this up. \n Yet another caveat is that the Electoral College is not just a mathematical artifice but also a human institution. The electors are people who pledge to vote according to the mandate of their state, but they are not compelled to do so. An elector who ignores his or her instructions and votes for Mickey Mouse may well be punished after the fact, but the vote for Mickey stands, nonetheless. \n Update 2012-10-10 : I wrote: \u201cIs there some other approach that would improve on the mixed-radix algorithm? Almost surely.\u201d Well, I got that right, and I\u2019ve never felt quite so chagrinned about being correct. An anonymous commenter suggested trying dynamic programming, and soon after that Iain Murray posted an elegant, terse Python program based on that principle. His program answers the how-many-ties? question in milliseconds. It\u2019s hard to fathom how I could have spent several days noodling over the tie-counting problem and missed a solution that\u2019s so clearly superior. Especially since I\u2019d seen it before. I mentioned above that Stephan Mertens introduced me to the number-partitioning problem. His lovely book The Nature of Computation (co-authored with Cristopher Moore) includes a detailed discussion of a dynamic-programming algorithm for the NPP. \n One final point: Although Murray\u2019s program is fast, it is not polynomial-time (nor does he make any claims to that effect). The anonymous commenter correctly states that the running time of the algorithm is proportional to the product of the number of states and the number of electors. But the size of the input is the logarithm of this number. Thus the running time is an exponential function of the number of bits in the input."], "link": "http://bit-player.org/2012/college-ties", "bloglinks": {}, "links": {"http://www.americanscientist.org/": 1, "http://www-e.uni-magdeburg.de/": 1, "http://homepages.ac.uk/": 1, "http://www.archives.gov/": 1, "http://www.nature-of-computation.org/": 1, "http://fivethirtyeight.nytimes.com/": 1, "http://www.potus.com/": 1}, "blogtitle": "bit-player"}, {"content": ["Lots of digital cameras come with a default file-naming scheme of IMG_ nnnn , where nnnn is a four-digit number assigned sequen\u00adtially, starting with 0001. The four images above are photos I made with four different cameras over a span of more than a decade, but they all have the same filename: IMG_1134.JPG. As you might guess, name collisions cause occasional trouble when I decide to merge folders. I could easily fix the problem at the source\u2014all of the cameras allow for an override of the default naming scheme\u2014but I\u2019ve never bothered. And I\u2019m not the only one. \n The other day it occurred to me that the prevalence of IMG_ nnnn filenames offers a way to \u201crandomly\u201d select a sample of images from public archives such as Flickr. Here\u2019s a collage of results produced by searching Google Images with the query string \u201cIMG_1134\u2033: \n \n Below is a similar collage generated by running the same search query on Flickr: \n \n And finally I offer a specimen of what Bing Images delivers for \u201cIMG_1134\u2033: \n \n In what sense are these selections random? The rationale is this: If we choose the 1,134th picture taken with many cameras by many photographers, I wouldn\u2019t expect much correlation in the subject matter of those images. And indeed the samples above do seem to cover a pretty wide range. \n On the other hand, I wouldn\u2019t try to argue that the method produces a totally unbiased sample. The most serious problem is that the list of images returned by a search engine is not itself randomized. Google and Bing rank images according to the estimated prominence of the web pages they appear on. Flickr offers three sorting orders\u2014\u201drelevant,\u201d \u201crecent\u201d and \u201cinteresting\u201d\u2014but does not have a \u201crandom\u201d option. \n In the samples above I dealt with this issue in various ad hoc ways. For the Google Images sample I made the selections based on computer-generated pseudorandom numbers. In the case of Flickr I took every seventh image. On Bing I scrolled down past the first few thousand thumbnail images and then copied an arbitrary rectangular region of the screen. These are feeble attempts at randomization; no doubt the selections still favor higher-ranked or more popular images. \n Another source of bias is that we\u2019re only seeing images that people chose to save and to share. The pictures you take with your thumb on the lens are likely to be deleted; unflattering portraits of your spouse will not be uploaded. \n Still another bias is subtler and more interesting: Not all nnnn \u2018s are the same. A camera that gets used for a few weeks when it\u2019s fresh out of the box and then gathers dust in the closet over subsequent months and years will never reach the IMG_9000s, and maybe not even the IMG_1000s. To test this hypothesis I ran a search on Google Images for each string of the form \u201cIMG_ nn 00,\u201d from \u201cIMG_0100\u2033 through \u201cIMG_9900.\u201d For each search string I recorded the number of hits reported by Google: \n \n Over much of the range, the number of images decays in a way that looks vaguely logarithmic\u2014but then something weird happens at about nnnn = 6000. I suspect that the weirdness comes from a glitch in Google\u2019s classification and counting algorithms rather than from the habits of the worldwide community of photographers. To test this idea I reran the same script on Flickr, with these results: \n \n The scale is different but the shape of the distribution is very similar\u2014without any spiky weirdness at the high end. A logarithmic curve fits quite well. \n As the number of IMG_ nnnn images falls off with increasing nnnn , the nature of the images may also change. In particular, one might suppose that photographers who practice their craft enough to make several thousand images would produce more accomplished and more interesting results. In other words, by looking at the higher-numbered images, we may weed out the dilettantes. \n I grew curious about IMG_0001. What kinds of pictures do people take when they first unpack a new camera, charge up the battery, and begin clicking the shutter? I expected a lot of self-conscious and self-referential images like these: \n \n Obviously I found a few, but I had to sift through several hundred Flickr IMG_0001 thumbnails to come up with these four. (There were also a few pictures of camera boxes and packaging.) To my surprise, the vast majority of IMG_0001 photos did not look at all like test images made with a new toy. Not unless people unpack and test their new cameras inside the Hagia Sophia or on the south rim of the Grand Canyon or at a basketball game . \n So what\u2019s going on with the 0001s? Perhaps through some metadata mixup, multiple images are all getting labeled \u201cIMG_0001,\u201d even though that\u2019s not the filename coming out of the camera. But if that\u2019s the case, \u201cIMG_0001\u2033 ought to be an outlier, with substantially more exemplars than neighboring file names IMG_0002, IMG_0003, etc. The data indicate otherwise: \n \n An alternative explanation is that photographers frequently reset the camera\u2019s file counter to 0001. Here\u2019s still another possibility: Maybe some of those IMG_0001 pictures are not the camera\u2019s first image but its 10,000th. I\u2019ve never gone all the way around with a camera, so I don\u2019t know exactly what happens when the counter turns over. Is IMG_9999 followed by IMG_0000? (I note that a Flickr search for \u201cIMG_0000\u2033 returns 2,430 results. It\u2019s the tiny leftmost lollipop in the graph above.) \n \u2022  \u2022  \u2022 \n Even if searching for IMG_ nnnn offers only a crude level of randomization, I think it may still hold some promise of showing us what average or typical photographs look like. How many are portraits and how many are landscapes? Taken indoors or out? What fraction of all photos show children? Pets? What are the proportions of men and women? \n Based on a casual perusal of a few thousand photos, I was led to make some tentative observations: \n \n The genre of family snapshots\u2014children\u2019s birthday parties, vacation trips to Disney World\u2014is not nearly as prominent in these collections as I would have expected. I guess those pictures are all on Facebook. \n Food, on the other hand, is a much more popular subject than I ever would have guessed. In a sample of 100 Bing images, 21 showed comestibles of some kind. Is it really true that a fifth of all the world\u2019s pixels are being used to show what we ate for lunch? \n Vehicles are not quite as everpresent as meals, but there sure are a lot of cars, bikes, trains, trams and aircraft to be seen. It seems a lot of people want to show off their ride. \n \n Addendum : Here\u2019s the data behind the three graphs above."], "link": "http://bit-player.org/2012/img_1134", "bloglinks": {}, "links": {"http://bit-player.org/": 1, "http://www.flickr.com/": 3}, "blogtitle": "bit-player"}, {"content": ["Five years ago I wrote about a rumored proof of the abc conjecture, an idea from the 1980s that stands at the juncture between the additive and the multiplicative parts of number theory. Now there\u2019s more abc news, and this time it\u2019s not just a rumor. Shinichi Mochizuki of Kyoto University has released a series of four long papers in which he claims to provide a proof. I have nothing useful to say about the proof itself\u2014the waters are deep and dark out where Mochizuki swims\u2014but I\u2019m going to take the occasion for some dabbling in the shallow end of the pool. \n So what\u2019s the abc conjecture? I gave an account in my earlier post , but here I want to try a different approach. Let\u2019s make a game of it. \n You choose a positive integer a , and I\u2019ll choose a larger integer b that\u2019s relatively prime to a . In other words, a < b and no integer n > 1 divides both a and b . Now we calculate two quantities. The first is the sum a + b = c . (I note that c must also be relatively prime to both a and b . Agreed?) The second quantity is called the radical of a, b, c, which I\u2019m going to designate R ; it\u2019s the product of all the distinct primes in a, b and c . To compute the radical we list all the prime factors of a, b and c, merge the three lists, cast out duplicates until each prime appears just once, and finally multiply. \n Now for the payoff: If R \u2265 c , you win. If R < c, the triple {a, b, c} is called an abc -hit, and I win. \n Suppose you pick a = 5 and I reply with b = 9, for a sum of c = 14. The distinct prime factors of these numbers are 2, 3, 5 and 7, and so the radical R = 2 \u00d7 3 \u00d7 5 \u00d7 7 = 210. Since R is larger than c , I lose. But if you pick 5 and I pick 27, we have c = 5 + 27 = 32 whereas R = 2 \u00d7 3 \u00d7 5 = 30. This is an abc -hit, and I\u2019m the winner. \n In a sense, this game is very much in your favor, because abc -hits are rare. There are 4,950 integer pairs with a < b \u2264 100, and among them 3,043 pairs are relatively prime; but only six of those pairs produce an abc -hit: \n \\[\\begin{align} \n\t1 + 8 &= 9 & R &= 6\\\\ \n\t1 + 48 &= 49 & R &= 42\\\\ \n\t1 + 63 &= 64 & R &= 42\\\\ \n\t1 + 80 &= 81 & R &= 30\\\\ \n\t5 + 27 &= 32 & R &= 30\\\\ \n\t32 + 49 &= 81 & R &= 42 \n\t \\end{align}\\] \n The same information is presented graphically below. Maroon dots are abc -hits (and their symmetrical counterparts after inter\u00adchanging a and b ); gray dots are all the other relatively prime pairs. \n \n For a broader view, here are the 1,318 abc -hits with a < b \u2264 10 6 (and their reflections). I didn\u2019t compute all these hits myself; the data come from the ABC@Home project, where volunteers have been donating cpu cycles to the task for several years. \n \n Given the rarity of abc -hits, it looks like you have an excellent chance of avoiding them. If we both pick random a \u2018s and b \u2018s in the range from 1 to 1,000,000, you win with probability 1 \u2013 10 \u201310 . Before you lay down any heavy wagers on this game, however, consider that I too have an advantage. I play second. If for every a there is some b that yields an abc -hit, then in principle I can always win. All the same, I won\u2019t be making any big bets either. Although it may well be true that there\u2019s at least one abc -hit for every a , I don\u2019t have the computational resources to find a winning b in every case. For instance, if you choose a = 330, I\u2019m stuck. The ABC@Home team has not yet found a b that makes an abc -hit with a = 330; if such a b exists, it must be larger than 999,999,999,999,999,670. \n So that\u2019s the basic abc game. Let\u2019s fiddle with the rules a little to make it more interesting. \n Not all abc -hits are equally impressive. Sometimes R is much smaller than c , but in other cases the difference is slight and the ratio c/R is barely greater than 1. A few examples suggest the range of possibilities: \n \\[\\begin{align} \n\t459 + 3125 &= 3584 & R &= 3570 & c/R &= 1.004\\\\ \n\t5 + 27 &= 32 & R &= 30 & c/R &= 1.067\\\\ \n\t1 + 4374 &= 4375 & R &= 210 & c/R &= 20.833\\\\ \n\t2 + 6436341 &= 6436343 & R &= 15042 & c/R &= 427.891 \n\t \\end{align}\\] \n Suppose we adjust the rules of the game so that I win only if I can find a \u201chigh-impact\u201d abc -hit, where R is dramatically less than c . Before playing, we agree on a number h , which can be any real number equal to or greater than 1. Then you pick your a , I pick my b , and we calculate the sum c and the radical R , just as before. But in this version of the game, I win only if R h < c . (A mathematically equivalent formulation says that I win if (log c )/(log R ) > h .) \n If h = 1, our new game is identical to the old one (since R 1 = R ). If we assume that an abc -hit exists for every a , and if I have unlimited computing power, then I should always be able to win with h = 1. But everything changes when h > 1. This is where the abc -conjecture comes into the picture. The conjecture asserts that only finitely many abc -hits have R h < c whenever h has any value greater than 1\u2014no matter how slight the excess over 1. An immediate consequence is that you win the game. If there are only finitely many \u201chigh-impact\u201d abc -hits for our chosen value of h , then there must be infinitely many a \u2018s for which I can\u2019t find a winning b . \n For the record, here\u2019s a statement of the abc conjecture without the apparatus of the two-player game: \n Given any real number h > 1, there are at most finitely many triples {a, b, c} with radical R(a,b,c) where R h < c . \n Note that the definition requires the exponent h to be strictly greater than 1. It says there can\u2019t be an infinity of abc -hits with R 2 < c or R 1.5 < c or even R 1.0000001 < c . But it doesn\u2019t rule out an infinity of abc -hits with R 1 < c , and in fact such an infinity is known to exist. (A presentation by Frits Beukers gives a constructive proof, generating an infinite sequence of abc -hits with a = 1.) \n I\u2019d like to dwell for just a moment on the strangeness of this statement. Think of h as a parameter controlling the size of the pores in a mathematical sieve. We set h to some specific value and then run all the abc -hits through the sieve, keeping those with (log c )/(log R ) > h , and discarding the rest. If we start with h = 1.5, we might find we retain just a few hits. As we reduce the value of h , the size of the collection grows, but (if the abc conjecture is true) the number of qualifying triples remains finite as long as h is greater than 1, even infinitesimally so. And then, when h = 1, the sieve suddenly catches an infinity of abc -hits. So now we can ask: For how many of those infinitely numerous hits is (log c )/(log R ) equal to 1? The answer is: 0. \n I don\u2019t mean to suggest there\u2019s anything wrong with the mathematics of this situation. It\u2019s just another hint that this paradise Cantor created for us is a mighty weird place to live. \n One more digression. The defining property of an abc -hit is R < c . What about the case of R = c ? I would love to exhibit a specimen of such a triple, but I haven\u2019t been able to find one. Do they exist? I see no obvious reason why they shouldn\u2019t, and I suspect they\u2019re just very rare. After all, there are many ways for R to be less than c and many ways for R to be greater than c but there\u2019s only one way for R to be equal to c . My search for low-hanging fruit (examining all triples with c \u2264 25,000) came up empty. I have also sifted through 14 million triples recorded by the ABC@Home project, which cover all territory up to c = 10 18 . In this data set I found no triples with R = c . However, I don\u2019t know whether that\u2019s because no R = c triples exist in this range or because the ABC@Home software was set to look only for R < c . \n \u2022  \u2022  \u2022 \n My abc games require nothing but simple arithmetic. Not so Mochizuki\u2019s work on the abc conjecture. \n Mochizuki is professor in the Research Institute for Mathematical Sciences in Kyoto. His Ph.D. is from Princeton, where he studied under Gerd Faltings, the Fields Medalist who proved the Mordell Conjecture. \n In August Mochizuki posted four manuscripts on his web site at Kyoto University. (Links to PDFs: Part I , Part II , Part III , Part IV .) They carry the collective title \u201cInter-Universal Teichm\u00fcller Theory,\u201d and together they amount to 512 pages. Part IV is where the claimed proof of the abc conjecture appears. \n I have made a sincere attempt to extract some meaning from these documents. In the case of Part IV, I have \u201cread\u201d the entire manuscript, in the sense that I have cast my eyes over each line of text and tried to apprehend the sense of the words and the symbols, seeking help in reference works and the web when necessary. Nothing of worth has come from this endeavor. I\u2019m usually pretty good at reading things I don\u2019t understand, but Mochizuki has defeated me. \n I haven\u2019t yet found anyone else who claims to fully understand the proof, but several readers have made much more progress than I have. The best commentary so far is on Mathoverflow . Other sources are Jordan Ellenberg\u2019s blog Quomodocumque , a Google+ note by John Baez and Peter Woit\u2019s blog Not Even Wrong . \n Note: The paragraph above beginning \u201cI\u2019d like to dwell for just a moment\u2026\u201d has been changed since this article was first published. Resisting the powerful urge to bury my embarrassing mistakes, I reproduce below the original version: \n I\u2019d like to dwell for just a moment on the strangeness of this statement. Suppose we go through all the abc -hits and for each one write down the value of h = (log c )/(log R ). Then, assuming the abc conjecture is true, we have these results: \n \n How many hits have h \u2265 1? Answer: infinitely many. \n How many hits have h > 1? Answer: finitely many. \n How many hits have h = 1? Answer: zero. \n \n \n Addendum 2012-09-12: When I began this piece, I said I was going to dabble in the shallow water. Now I\u2019m in well over my head. As Stevie Smith said: Not waving but drowning. \n In the comments, Jim Lyon takes issue with my statement about the \u201cstrangeness\u201d of the abc conjecture. Lyon writes: \n If N(h) is the number of hits for value h, it\u2019s not that strange that N(h) is finite when h > 1, but infinite when h = 1. \n To some extent, strangeness is subjective. What a person finds strange or surprising is the mirror image of what feels familiar and expected. Is it strange that the integers and the rationals can be put into one-to-one correspondence but the real numbers form a larger set? Well, I think I understand the proof of that fact, and yet, when I focus on the fact itself, my brow still furrows. Is it strange or remarkable that the infinite series 1/1 + 1/4 + 1/9 + 1/16 . . . converges but 1/2 + 1/3 + 1/5 + 1/7 . . . does not? What about the Banach-Tarski procedure for turning a pea into a planet ? Is there anyone for whom that notion is a comfy piece of mental furniture? \n The thing is, though, if strangeness is just a matter of unfamiliarity, it can\u2019t be sustained indefinitely. And, indeed, after wrestling with these ideas for a few days and nights, I find the sense of strangeness fading. I\u2019ve changed my mind and I tend to agree with the Jim Lyon statement quoted above. \n A simple model helps to clear away some of the sense of mystery. Take the set of all fractions 1/ n with n in 1, 2, 3, 4, . . . Now we can count all the fractions whose value is greater than some real number m . For 0 < m \u2264 1, there is a finite set of fractions greater than m . As we reduce m , the set becomes larger, and at m = 0 the set of qualifying fractions is infinite. And yet there are no fractions in the set with 1/ n = 0. This is the same behavior observed with abc -hits as h is reduced to 1, and there\u2019s nothing very mysterious about it. \n But perhaps this model explains too much. If the set of abc -hits had the same structure as the set of 1/ n fractions, there would be no need for an abc conjecture. The truth of the proposition would be easy to establish. \n In another comment, Craig asks: \n Is anything known, even empirically, about the asymptotic behavior of the number of solutions as h approaches 1? (Aside from the fact that it goes to infinity.) Like, does it go as 1/( h -1)? 1/( h -1)^2? exp[1/( h -1)]? \n I suspect this is actually a deep and difficult question. If we had a definitive answer, surely that would be at least a head start toward resolving the abc conjecture. \n In any case, here is some empirical data that might or might not shed light on the limiting behavior. For a sample of 100,000 hits drawn from the ABC@Home file, I calculated \u03b5 = h \u20131 = ((log c )/(log R )) \u20131. Then I plotted the cumulative distribution of the \u03b5 values, using six bins of exponentially increasing width. The first bin includes counts for all \u03b5 > 0, the second for all \u03b5 > e \u20135 &approx 0.007, the third for all \u03b5 > e \u20134 &approx 0.018, and so on. \n \\[\\begin{align} \n\t\\textit{bin} & \\quad \\textit{count}\\\\ \n\t0 & \\quad 100000\\\\ \n\te^{-5} & \\quad 84532\\\\ \n\te^{-4} & \\quad 63839\\\\ \n\te^{-3} & \\quad 30514\\\\ \n\te^{-2} & \\quad 4748\\\\ \n\te^{-1} & \\quad 48\\\\ \n\t \\end{align}\\] \n The same data in graphic form: \n \n The sample consists of the first 100,000 consecutive abc hits beginning with the first hit having c \u2265 10 9 . I avoided the beginning of the file because the smallest hits seem to have peculiar statistics. Still, I\u2019m not sure whether this is a reasonably fair sample, or even what it means to sample fairly from an infinite data set."], "link": "http://bit-player.org/2012/the-abc-game", "bloglinks": {}, "links": {"https://plus.google.com/": 1, "http://bit-player.org/": 2, "http://www.ac.jp/": 4, "http://www.columbia.edu/": 1, "http://mathoverflow.net/": 1, "http://www.uu.nl/": 1, "http://quomodocumque.wordpress.com/": 1, "http://abcathome.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "bit-player"}, {"content": ["For something like 80 years, a traffic light was an ordinary incandescent light bulb mounted behind a colored glass lens. Then, in 1999, LED signal lights began appearing at street corners; the light bulb was replaced by an array of small light-emitting diodes shining through a diffusing lens. LEDs are more efficient than incandescent bulbs, and they last longer. Moreover, having many LEDs instead of one tungsten filament offers the safety advantage of a gradual failure mode. As I wrote in my Field Guide to the Industrial Landscape : \u201cIf one small emitter burns out, the rest continue to shine.\u201d \n That sentence was written when LED traffic lights were still very new, and I had never actually seen one with a failed emitter. Now that the LED signals have several years of service behind them, I\u2019ve begun to notice a lot of defects, and they don\u2019t look like what I expected. Instead of random, isolated blemishes on the face of the signal, I\u2019m seeing large scars and ragged craters. Oddly shaped blocks of emitters are all failing at once. \n This is a pattern that turns up with notable frequency: \n \n This one has three blocks of dead pixels: \n \n And here almost half the emitters have gone dark: \n \n All three of the examples above are green signal lights, which seem to fail much more often than red and yellow ones. But here\u2019s an example of a red light with more than 20 blacked-out pixels and several more dim ones: \n \n Finally, in another green light, a quite different and more orderly pattern of defects appears: \n \n For me these images provoke a couple of questions. First, why do we so often see signal faces with a dozen or more LED failures, while single-pixel outages seem to be very rare? Second, what accounts for the distinctive shapes of the dark regions? \n No doubt some diligent googling or wikipedia-wrangling would turn up some answers, but sometimes it\u2019s just more fun to speculate in ignorance. In any case, I have a hypothesis about the first question. The idea comes from childhood experience with Christmas-tree lights. LEDs are generally low-voltage devices, but the available power supply in the traffic light is 120 volts. The cheapest and easiest way to deal with this mismatch is to wire groups of LEDs in series, thereby dividing the voltage. A consequence is that if any one LED in a group fails to an open-circuit state, the entire chain goes dark. \n As for the geometric question, I haven\u2019t got a clue. If I were laying out the circuit board, I would surely wind up with some collection of reasonably tidy, compact, symmetrical clusters. The patterns on exhibit here are not tidy, compact or symmetrical. The last of the images above might be explicable: Wiring together every other element in a row would be a good way of reducing the visual impact of a failure. But all the other patterns are incomprehensible. \n Of course my whole Christmas-lights hypothesis may be totally screwy. I welcome alternative hypotheses, and even Google-Wiki-derived answers. \n As a final bit of fun, here\u2019s a video of an LED traffic signal shot at 1,000 frames per second, and slowed down by a further factor of 2, showing the 120-hertz flicker from the AC power supply, including the fade at the end of the green cycle."], "link": "http://bit-player.org/2012/stop-lights", "bloglinks": {}, "links": {"http://industrial-landscape.com/": 1}, "blogtitle": "bit-player"}, {"content": ["David G. Hays, a pioneer of computational linguistics, describes an experiment he performed in 1956 at the RAND Corporation: \n The experiment strips conversation down to its barest essentials by depriving the subject of all language except for two pushbuttons and two lights, and by suggesting to him that he is attempting to reach an accord with a mere machine. We brought two students into our building through different doors and led them separately to adjoining rooms. We told each that he was working with a machine, and showed him lights and pushbuttons. Over and over again, at a signal, he would press one or the other of two buttons, and then one of two lights would come on. If the light that appeared corresponded to the button he pressed, he was right; otherwise, wrong. The students faced identical displays, but their feedback was reversed: if student A pressed the red button, then a moment later student B would see the red light go on, and if student B pressed the red button, then student A would see the red light. On any trial, therefore, if the two students pressed matching buttons they would both be correct, and if they chose opposite buttons they would both be wrong. \n We used a few pairs of RAND mathematicians; but they would quickly settle on one color, say red, and choose it every time. Always correct, they soon grew bored. The students began with difficulty, but after enough experience they would generally hit on something. Some, like the mathematicians, chose one color and stuck with it. Some chose simple alternations (red-green-red-green). Some chose double alternations (red-red-green-green). Some adopted more complex patterns (four red, four green, four red, four green, sixteen mixed and mostly incorrect, then repeat). The students, although they were sometimes wrong, were rarely bored. They were busy figuring out the complex patterns of the machine.\n \n (The quotation is from \u201cLanguage and Interpersonal Relationships,\u201d Daedalus , Vol. 102, No. 3, Summer 1973, pp. 203\u2013216.) \n I too have met a few mathematicians who might be described as always correct but soon bored. In this case, however, I don\u2019t think the mathematicians are to blame. The game they were asked to play seems like the dullest possible use of this experimental apparatus, which actually has rich possibilities. \n Wouldn\u2019t it have been more fun to put the players at crossed purposes? Let one try to match the color of the button to the color of the light, while the other player strives to create a mismatch. This game, more commonly played by matching pennies, has no strategy that allows both competitors to be boringly correct. In 1956\u2014the same year as the RAND experiments\u2014D. W. Hagelbarger of Bell Labs wrote a computer program to play the penny-matching game. In 9,795 games against human opponents (some of them probably mathematicians) the program scored 5,218 wins and 4,577 losses, well above chance level. \n The RAND experiment, with its deliberate confusion of human and computational agents, also summons up thoughts of the Turing test. Turing\u2019s paper on the \u201cimitation game\u201d was published in 1950, but Hays does not allude to it. And, after all, in his experiment the computer was a mere fiction, introduced to distract the participants from the true nature of the proceedings. But I\u2019m curious about the outcome of a Turing test conducted over this minimalist channel of communication. If you were sitting at that console, with nothing but two buttons and two light bulbs to express yourself, how would you persuade an interrogator that you are a person rather than a machine? What conversational strategy could you adopt that a computer program could not master just as well, at least over a limited number of exchanges? \n If the conversation goes on long enough, of course, the thinness of the channel hardly matters. With two buttons to choose between, you can communicate one bit of information in each exchange, and in principle that stream of bits can encode any message. Nevertheless, getting the conversation started seems like an intriguing problem. Without prompting or prearrangement, how would the two parties\u2014human or otherwise\u2014bootstrap a language? Would they reinvent Morse code, or some ad hoc equivalent? Maybe they\u2019d start like this: \n \n00000010101010000000000 \n00101000001010000000100 \n10001000100010010110010 \n10101010101010100100100 \n \u2026 \n (but I doubt it)."], "link": "http://bit-player.org/2012/crossed-conversations", "bloglinks": {}, "links": {}, "blogtitle": "bit-player"}, {"content": ["Let\u2019s suppose you are a client of the notorious accounting firm Dewey, Cheetham & Howe. You want them to compute your income tax, but you don\u2019t trust them with the confidential details of your financial life. This impasse seems insurmountable. Surely there is no way for an accountant to calculate your income tax without knowing your income. More generally, you can\u2019t apply an algorithm to data unless you have access to the data. \n Or so you might think. Consider the computing scheme known as fully homomorphic encryption. With this protocol you encrypt your data and send the ciphertext off to the untrustworthy accountants, who cannot decrypt it or otherwise learn anything about its content. Nevertheless, they can perform computations on the data, producing results that are also encrypted. When they send back the output of the computation, you apply your private decryption key, getting the same answers you would have obtained if the entire operation had been conducted in the open. In my opinion this is one of the most amazing magic tricks in all of computer science. \n If you\u2019d like to learn more about homomorphic encryption, I can recommend a place to start: my column in the new issue of American Scientist . Get it now in HTML or PDF . \n One further note about the column: For crypto buffs, there\u2019s an Easter egg in the first illustration."], "link": "http://bit-player.org/2012/computing-with-encrypted-data", "bloglinks": {}, "links": {"http://www.americanscientist.org/": 2}, "blogtitle": "bit-player"}, {"content": ["I was doing some reading in the history of cryptography when I came upon a reference to a 1996 article by Solomon W. Golomb . Golomb always has something interesting to say, so I had to go off and find and the paper, especially since the title was intriguing and a little mysterious: \u201cOn Factoring Jevons\u2019 Number.\u201d That would surely be William Stanley Jevons, 19th-century economist and statistician. I\u2019ve run into Jevons before\u2014he gets two chapters in Stephen Stigler\u2019s Statistics on the Table \u2014but I never knew he had a number. \n Golomb\u2019s paper lives behind a paywall, so I\u2019ll quote at some length from the introductory paragraph: \n In his book The Principles of Science: A Treatise on Logic and Scientific Method , written and published in the 1870\u2032s, William S. Jevons observed that there are many situations where the \u201cdirect\u201d operation is relatively easy, but the \u201cinverse\u201d operation is significantly more difficult. One example mentioned briefly is that enciphering (encryption) is easy while deciphering (decryption) is hard. In the same section of Chapter 7: Induction titled \u201cInduction an Inverse Operation\u201d, much more attention is devoted to the principle that multiplication of integers is easy, but finding the (prime) factors of the product is much harder. Thus, Jevons anticipated a key feature of the RSA algorithm for public key cryptography, though he certainly did not invent the concept of public key cryptography. \n Golomb calls attention to the following passage from the Jevons book (p. 123 in the second edition, issued in 1874): \n Can the reader say what two numbers multiplied together will produce the number 8,616,460,799? I think it unlikely that anyone but myself will ever know; for they are two large prime numbers, and can only be rediscovered by trying in succession a long series of prime divisors until the right one be fallen upon. \n Obviously Jevons did not reckon on rapid progress in computing machinery. In our gigahertz age, even the crudest version of the trial-division algorithm finds the two prime factors of that 10-digit number in milliseconds. And of course Jevons was wrong in suggesting that trial division is the only method possible. \n Golomb\u2019s critique of Jevons\u2019s claim is even harsher: \n With a 10-place hand-held calculator, using only one memory location, and only the operations of subtraction, square and square-root, it took me less than six minutes to factor Jevons\u2019 number\u2026. This success led me to consider how easy or difficult it would have been for someone in the 1870\u2032s, using only hand calculation, to have succeeded in finding this factorization. I concluded that at most a few hours, and quite possibly less than an hour, would have been sufficient. \n Just in case anyone would like to put Golomb\u2019s assertion to the test, I\u2019ll refrain from giving the factors here. Sharpen your pencils! \n Golomb was not the first to crack the Jevons number. Derrick Lehmer the elder presented a factorization at a 1903 meeting of the American Mathematical Society. He added this note to the published account of the talk: \u201cI think that the number has been resolved before, but I do not know by whom.\u201d \n All of these results tend to make Jevons look a bit of a fool for so badly misjudging the difficulty of his factoring challenge. And his reputation is somewhat doubtful for another reason as well. Economists make fun of his theory that business cycles and sunspot cycles are causally connected. (Let us quietly ignore the close correlation between the most recent sunspot minimum and the financial unpleasantness of 2008\u201309.) \n I\u2019m not going to take it on myself to rehabilitate Jevons, but I can report that I\u2019ve spent a few days browsing in his Principles of Science , and I find it charming. Jevons was one of those frightfully prolific Victorian scribblers. He lived only to age 46, but in his short life he published at least a dozen substantial works. Principles of Science runs to almost 800 pages. His aim in this volume is to assemble a complete mental toolkit for doing science, starting with logic (Jevons was an early champion of Boole\u2019s Laws of Thought ), and proceeding through probability, measurement, experiment and on to various aspects of theory-building (generalization, classification, analogy). A theme that runs through the whole narrative is the importance of inductive inference, which Jevons sees as an inverse problem\u2014how to deduce causes from effects. \n Consider the moment when Jevons was writing. Darwin\u2019s Origin of Species had been published 15 years before. The debate over corpuscular and undulatory theories of light was in full cry. Phlogiston was long gone, but the luminferous ether was still permeating the universe. The apparent conflict between the antiquity of the earth and the measured flux of heat from the planet\u2019s interior was a great puzzlement. (It would not be resolved for another 20 years, with the discovery of radioactivity.) Thermodynamics was beginning to emerge as a science. Reading a contemporary\u2019s account of these developments offers a certain voyeuristic excitement. Jevons didn\u2019t yet know how these stories were going to turn out. \n There\u2019s a fair amount of math in the book, but even more noteworthy is how much math isn\u2019t in the book. In my perusal of the text, I found not one differential equation or even a use of elementary calculus. The square root of \u20131 appears just once. Instead of analysis, there is a strong emphasis on areas we would now call discrete mathematics, especially combinatorics and probability, as well as the interface between logic and the foundations of arithmetic. I was quite surprised by this slant. I tend to think of discrete math as a modern enthusiasm, inspired in part by the rise of computer science. \n Maybe Jevons should be considered a modern in this respect. He certainly seems to enjoy counting and calculating things. His approach to symbolic logic begins with the enumeration of all possible propositions with a given number of terms. Elsewhere he estimates the number of English words with various combinations of letters, and the number of chemical compounds with a given number of elemental constituents. He performs 20,480 coin tosses to test the law of large numbers. He correctly calculates that 2^2^2^2^2 has 19,729 decimal digits. This is a person who would have been thrilled to have access to the sort of computing machinery we now take for granted. He built an early digital device of his own\u2014a Logic Machine, sometimes called the Logical Piano, for evaluating Boolean formulas. \n \n Jevons seems to have a particular fascination with permutations, combinations and factorials, bringing them up even in contexts where you might not expect them to have much bearing. For example, there\u2019s this curious definition of Euler\u2019s number: \n At the base of all logarithmic theory is the mysterious natural constant commonly denoted by e , or \u03b5, being equal to the infinite series $$1 + \\frac{1}{1} + \\frac{1}{1 \\cdot 2} + \\frac{1}{1 \\cdot 2 \\cdot 3} + \\frac{1}{1 \\cdot 2 \\cdot 3 \\cdot 4} + \\cdots,$$ and thus consisting of the sum of the ratios between the numbers of permutations and combinations of 0, 1, 2, 3, 4, &c. things. [p. 330] \n The formula \\(e = \\sum{1/n!}\\) is standard, but what\u2019s that about ratios of permutations and combinations? Do they have anything to do with the value of e ? (I\u2019m not even sure what ratios are being summed.) \n Another instance: \n It is worth noting that this Law of Error, abstruse though the subject may seem, is really founded upon the simplest principles. It arises entirely out of the difference between permutations and combinations, a subject upon which I may seem to have dwelt with unnecessary prolixity in previous pages. [p. 383] \n Is the law of error\u2014the normal distribution\u2014entirely a matter of permutations and combinations? \n And Jevons returns to the same topic a third time at the very end of his final chapter, as he is summing up his views on life, the universe and everything. \n There formerly seemed to me to be something mysterious in the denominators of the binomial expansion (p. 190), which are reproduced in the natural constant \u03b5, or $$1 + \\frac{1}{1} + \\frac{1}{1 \\cdot 2} + \\frac{1}{1 \\cdot 2 \\cdot 3} + \\cdots$$ and in many results of mathematical analysis. I now perceive, as already explained (pp. 33, 160, 383), that they arise out of the fact that the relations of space do not apply to the logical conditions governing the numbers of combinations as contrasted to those of permutations. [p. 769] \n I have pursued the internal references that Jevons cites here, but I confess that what he formerly found mysterious is still wholly mysterious to me. If it makes sense to anyone else, I hope you\u2019ll enlighten me. \n One final digression. The full text of Principles of Science is available on Google Books . I started out reading it there, but eventually decided that ink and paper still have some advantages when it comes to 800-page tomes. So, being lucky enough to have library privileges, I went spelunking through the dim stacks of the Widener Library at Harvard and borrowed a copy. I soon realized it was the very copy that had been scanned by Google Books. The evidence lies in the distinctive marginal notes made by the volume\u2019s original owner. For example, on page 194 and 195 we find Jevons and his pencil-wielding reader disagreeing over the value of 2^2^2^2. (Jevons is correct; the penciled corrections are not.) \n A signature and stamp at the front of the book reveal the identity of the marginalist. He was George F. Swain, who held the Gordon McKay professorship at Harvard from 1909 until his death in 1931. There\u2019s a biography among the publications of the National Academy of Sciences (but the link seems wonky; I had to hunt down Google\u2019s cached copy). Swain was a civil engineer (now an extinct species at Harvard) who seems to have signed off on just about every railroad bridge in the Commonwealth of Massachusetts and a few other bridges farther afield (e.g., the Golden Gate). But evidently Swain also had more abstract interests. In the NAS biography a former student comments: \u201cLogical reasoning was constantly emphasized, and I well remember his earnest recommendation that we procure copies of Jevon\u2019s [sic] \u2018Logic\u2019 and master its contents.\u201d \n Reading over the shoulder of Professor Swain adds one more layer to the palimpsest. At the bottom we have Jevons himself, deeply embedded in the world of late-19th-century science, speaking familiarly of his contemporaries Professor Maxwell and Dr. Joule and Mr. Venn. Then Swain enters with his itchy annotator\u2019s pencil, calling attention to those passages that still resonated 50 years later\u2014and showing occasional impatience with ideas that no longer seemed so compelling. And we have our own knowing, modern perspective, you and I and Solomon Golomb, with all our computational power tools. \n Update 2012-09-15 : As mentioned above, Derrick Lehmer\u2019s 1903 paper on the Jevons number includes the footnote: \u201cI think that the number has been resolved before, but I do not know by whom.\u201d Josh Jordan may have now answered the \u201cby whom\u201d question. He searched Google Books for \u201c8616460799\u2033 and turned up an 1889 article in Nature by Charles J. Busk that presents a factoring algorithm and illustrates its application with the Jevons number. Busk claims that his scheme is \u201cdifferent from any previously tried,\u201d but Jordan points out that in fact Busk has reinvented Fermat\u2019s method, based on expressing an odd product of two factors as a 2 \u2013 b 2 = ( a + b )( a \u2013 b ). Fermat described the method in 1643. \n But perhaps we shouldn\u2019t be too hard on Charles Busk. A little more Googling turns up an 1898 discussion of Busk\u2019s ideas in Mathematical Questions and Solutions from the Educational Times (the MathOverflow of the 19th century). None of the other contributors mention the Fermat precedent either."], "link": "http://bit-player.org/2012/the-jevons-number", "bloglinks": {}, "links": {"http://www.tandfonline.com/": 1, "http://books.nap.edu/": 1, "http://books.google.com/": 1, "http://csi.usc.edu/": 1, "http://goo.gl/": 1, "http://www.google.com/": 1, "http://projecteuclid.org/": 1}, "blogtitle": "bit-player"}, {"content": ["My late friend Stan Ulam used to remark that his life was sharply divided into two halves. In the first half, he was always the youngest person in the group; in the second half, he was always the oldest. There was no transitional period. \n \u2014Gian-Carlo Rota \n \n When I crossed over the great Ulam divide, I had the same sensation: One day precocious, the next superannuated. I suppose I must be exaggerating, or else deluding myself: The transformation from young twerp to old fart can\u2019t really have happened overnight. Nevertheless, certain demographic trends can make the aging process seem more rapid than you might expect, especially when you judge your own age by looking at the people around you. As someone born in the middle of the last century, during an extraordinary population spurt, I grew up ridiculously fast. \n My calendrical age, A C , is the number of years since I was born. Let\u2019s define my relative age , A R , as the percentage of people in the world population whose calendrical age is less than mine. In other words, A R is my percentile in the world population age structure. (What about ages greater than 100? I\u2019ll worry about that when I get there.) In the graph below, the wine-colored curve labeled \u201cborn 1950\u201d shows my trajectory through age space, plotting A R as a function of A C for a person born at midcentury. \n \n At the tender age of 10, I was already older than a fourth of all the people on the planet. I was in my early twenties when I crossed the median line\u2014a relative age of 50\u2014and thus moved from the junior to the senior side of life. When I got to A C = 30, I was at A R = 62, older than almost two-thirds of the population. Now, having reached my calendrical sixties, I\u2019m in my relative nineties! But I guess age has its consolation: No matter how much longer I live, I can\u2019t get much older, relatively speaking. \n The population data underlying this curve come from United Nations estimates and projections , the same source I used in an earlier discussion of the population pyramid . Those numbers show the pyramid becoming more bullet-shaped as we move through the present century; by 2100 it begins to look more like an onion dome, with a base narrower than the midsection. At that point the youngest cohorts are no longer the largest, and the population has almost ceased growing, with about 10 billion people occupying the earth. \n \n With these shifts in age structure, future generations will have their Ulam moments later in life, and perhaps the transition from youth to age will seem less abrupt. In the A R -vs.-A C graph above, the green curve traces a trajectory for my granddaughter born near the turn of the millennium. She won\u2019t reach the population median until she\u2019s in her mid-thirties, and going from A R 40 to A R 60 will take her almost twice as long as it took me. \n The blue line in the graph is for the limiting case of a population with a perfectly uniform age structure, with equal numbers in every cohort. In this hypothetical society, everyone lives until their 100th birthday; then, as soon as they blow out the candles, they die. This population structure offers the mathematical convenience that calendrical age and relative age are exactly equal. \n Quick quiz: Describe a society where the relative-age curve is concave upward. \n \u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u2022\u00a0\u00a0\u00a0\u00a0\u00a0\u2022 \n I was led to these thoughts on aging and its demographic context by a reading of Joel Cohen \u2018s 1995 book How Many People Can the Earth Support? If you want an answer to that question, I must send you to the book itself, which treats this contentious subject thoroughly, thoughtfully and with exceptional grace and good humor. (And don\u2019t skip the notes and appendices, especially the appendix on mathematics in demography, which is labeled \u201cadult entertainment.\u201d) \n Here I want to pause over something that Cohen mentions in passing. He calls it Methuselah\u2019s choice. \n Discussions of population tend to focus on the birth rate and the death rate, which need to be in equilibrium if the total population is to remain fixed. But there\u2019s a third factor that connects these two rates: longevity. In a stationary population\u2014where the total number of people and the age structure are unchanging\u2014birth rate, death rate and lifespan are all tightly constrained. Specifically, the average life expectancy at birth must be equal to the reciprocal of the birth rate or the death rate. This fact is easy to verify in our society of centenarians: Each year 1/100th of the people die, an equal number are born, and the life expectancy at birth is exactly 100 years. If we reduced the lifespan to 50 years, the birth rate and death rate would have to double to maintain equilibrium. \n One way to clarify the influence of lifespan is to measure human fecundity not by counting births but by taking the product of births and life expectancy. Thus the unit of measure is person-years, a quantity that better represents how much spacetime each of us takes up in the world. Ten million births of future centenarians contribute a billion person-years to the human presence. \n The reciprocal relationship of birth rate and lifespan suggests the possibility of trading one for the other. Cohen frames the question as a social choice: \n \n People who live in a stationary population must choose: they can have a long average length of life together with a low birth rate, or they can have a high birth rate together with a short life expectancy\u2026. \n Methuselah\u2019s choice has a corollary. If people in a stationary population prefer long life and choose low fertility, like today\u2019s rich countries, there will be many more old people and many fewer young ones than if they choose high fertility and short lives. \n \n Note that Methuselah\u2019s choice is not the same as the Malthusian choice. Malthus warns us about the danger of letting births exceed deaths in a world of finite resources. Cohen\u2019s Methuselah tells us we still have a choice to make even when births and deaths are balanced. We have to choose our lifespan. \n The idea that family size is a matter for deliberate planning is no longer a novelty, but the idea that lifespan is also an elective choice still seems strange. If you think of it not as a collective societal decision but as a strictly individual one, the whole business becomes rather nasty. Suppose you are allocated a birthright of 100 person-years to be bestowed upon your offspring as you see fit. You can choose to have a single child who will live to blow out the 100 candles, or two kids who will die at 50, or a dozen who will survive only as far as age 8. Under this regime I suppose we could all agree that the cheaper-by-the-dozen choice is monstrous. But does it follow that the opposite extreme of maximizing lifespan is the optimum trade-off? This seems to be the direction we\u2019re heading. The consequence of that preference, as Cohen makes clear, is a world with more crabby old geezers and fewer bright young things. \n Although I\u2019m in no great hurry to make my own departure from life, I do think the future of humanity might be rosier if we spent a greater proportion of our available person-years in childhood and fewer in dotage, if we devoted more resources to teaching the young and fewer to propping up the old. But how do we get there from here, even if we could all agree on the most desirable compromise between fertility and longevity? Calling it a choice implies that we have some means to mold the world to our preferences; I don\u2019t see such a mechanism ready at hand."], "link": "http://bit-player.org/2012/methuselahs-choice", "bloglinks": {}, "links": {"http://esa.un.org/": 1, "http://bit-player.org/": 1, "http://www.rockefeller.edu/": 1}, "blogtitle": "bit-player"}, {"content": ["Beyond digital photography lies computational photography, which holds out the promise of extracting more bits from every photon. When I wrote about this idea a few years ago, I had no hands-on experience with computational cameras. The first widely available \u201clight-field camera,\u201d called the Lytro , was announced last fall. Mine finally arrived last week. So far I\u2019ve taken only a few dozen pictures, so this is a very preliminary report. But the camera itself is a very preliminary product, so perhaps it\u2019s appropriate to treat it as a preview of things to come. \n Here are a couple of pictures to play with. First, more magnet balls , with evidence that I\u2019ve finally figured out how to stack them up in something resembling a hexagonal close-packed configuration. \n \n And some flowers, which seem to be the front-running subject matter for Lytro photos. These are orchids at a street-corner stand, with extra inventory on display for mother\u2019s day. \n \n I trust that you\u2019ve figured out the trick: Clicking on a point in the image refocuses on the depth of the scene at that point. (You can also double-click to zoom.) \n How is this magic accomplished? The Stanford doctoral dissertation of Ren Ng sets forth the basic idea. Ng is the founder and CEO of Lytro Inc. \n This dissertation introduces a new approach to everyday photography, which solves the longstanding problems related to focusing images accurately. The root of these problems is missing information. It turns out that conventional photographs tell us rather little about the light passing through the lens. In particular, they do not record the amount of light traveling along individual rays that contribute to the image. They tell us only the sum total of light rays striking each point in the image. To make an analogy with a music-recording studio, taking a conventional photograph is like recording all the musicians playing together, rather than recording each instrument on a separate audio track. \n \n \n In this dissertation, we will go after the missing information. With micron-scale changes to its optics and sensor, we can enhance a conventional camera so that it measures the light along each individual ray flowing into the image sensor. In other words, the enhanced camera samples the total geometric distribution of light passing through the lens in a single exposure. The price we will pay is collecting much more data than a regular photograph. However, I hope to convince you that the price is a very fair one for a solution to a problem as pervasive and long-lived as photographic focus. In photography, as in recording music, it is wise practice to save as much of the source data as you can. \n So we have a radically different kind of camera here. It has a lens, but there\u2019s no focusing ring. Focusing is left to the computational post-processing of the image. \n The after-the-fact refocusing is most dramatic in close-up shots, with a wide ratio between the distances of near and far objects. You can see the effect by clicking around in this image of pine tree putting out some exuberant spring growth. \n \n The camera is capable of coming in even closer, as in this macro shot of some lichens and mosses on rocks in western Massachusetts. (The gray stalks are wo or three millimeters tall.) \n \n Getting much of a focal range is harder when you\u2019re dealing with more distant subjects. This was my best attempt to catch a red-wing blackbird in a marsh below Danehy Park in Cambridge. \n \n When you pull back even farther, the camera optics have enough depth of field that everything is in focus at once. Ordinarily, that high depth of field would be a virtue, but it strangely takes the fun out of Lytro pictures. \n \n The gallery on the Lytro web site includes lots more examples, including some classic pull-focus tricks like spider webs and raindrops on windowpanes. \n \u2022  \u2022  \u2022 \n Just a few years ago, the immediate reaction to the Lytro camera would have been: \u201cHow do you refocus after you print your pictures? You can\u2019t click on paper.\u201d Times change. The Lytro makes more sense in an age when people share their pictures on Facebook or Twitter instead of printing and framing them. Yet I still have misgivings about Lytro\u2019s scheme for distributing and publishing images. The Lytro photographs displayed above are not hosted on bit-player.org, as this text is; they are embedded <iframe> tags, providing a window onto content hosted at lytro.com. That\u2019s the only way I can post them here. This is an annoyance to a curmudgeonly control freak like me; I want to retain possession of my own work, as well as control its presentation. \n There is no fundamental reason the images have to be hosted at lytro.com. The refocusing algorithms are not running on the Lytro servers. What\u2019s embedded in the iframes is essentially a stack of images with different focal points, along with a Flash application that responds to clicks by displaying the appropriate image from the stack. (The use is Flash is another annoyance. Lytro evidently has a non-Flash version of the software, since the photos are viewable with Flashless devices such as the iPad, but there\u2019s no readily accessible way to choose the non-Flash version on other platforms.) \n The camera itself has an unconventional tubular design, but it fits the hand well enough and has a satisfying heft and solidity. The only physical controls are power, zoom and the shutter botton. The one severe problem with the camera hardware is that the viewing screen is too small (1 square inch) and too coarse; also, it\u2019s useless in bright sunlight. Often, you can\u2019t see what you\u2019re about to photograph, and afterwards you can\u2019t see what you\u2019ve captured until you upload the image to a computer. \n The software that runs on the computer has its own issues. For now it is Macintosh-only; a Windows version is promised, but there\u2019s no mention of Linux. When importing images, the software hogs the CPU, setting off a tremendous whoosh of fan noise. And once you have the images loaded into the software, there\u2019s not actually much you can do with them, other than add metadata or send them to the Lytro web site. There are no tools for cropping, correcting colors, sharpening, etc. You can export a JPEG version, but it\u2019s of course merely a static pixel array. (The JPEGs are 1080 by 1080 pixels, with quality in the range you\u2019d expect from a good cell-phone camera.) \n The full light-field photos are stored in 16-megabyte \u201c.lfp\u201d files, but there\u2019s no public documentation on the format of those files. As far as I know, no software other than Lytro\u2019s own can read the files. If there are any plans for, say, a Photoshop plugin or a software development kit, they are not discussed publicly. \n Is the Lytro the first chapter in the future of photography, or a novelty that will fade after a year or two? I suspect the answer will depend on how quickly Lytro is able to develop and release new features for the software and new models of the camera. At the moment, what the camera offers is a single trick: focusing after rather than before you press the shutter button. It\u2019s a neat trick, but probably not neat enough to support a whole new photographic infrastructure. The light-field technique could offer more. Lytro has promised that a future version of the software will allow control not just of focus but of depth of field, so that you can choose a version of the image in which everything is in focus at once. There are still more possibilities, even including shifting the camera\u2019s apparent point of view after the picture is taken. But it remains to be seen whether such features can be brought to market before people lose patience or interest."], "link": "http://bit-player.org/2012/light-field-photography", "bloglinks": {}, "links": {"http://www.lytro.com": 1, "http://www.americanscientist.org/": 1, "http://www.lytro.com/": 2, "http://bit-player.org/": 1}, "blogtitle": "bit-player"}, {"content": ["They come out of the can in a gleaming 6 \u00d7 6 \u00d7 6 cubic crystal. It took me a day to figure out how to get them back into the can. But that\u2019s not the deepest mystery about these curiously powerful little ferromagnetic balls. \n \n A web search turns up lots of sites that sell the magnets under various brand names, and a few more web pages that warn of their dangers . There are also videos and picture galleries of interesting constructions, such as polyhedra or a M\u00f6bius band. But I haven\u2019t been able to find anything on the questions that intrigue me most: For a set of N magnet balls, what is the ground-state configuration\u2014the geometric arrange\u00adment of lowest energy? How about the state of lowest free energy ? Informally: Given a handful of magnet balls, what is the shape they most \u201cwant\u201d to assume? \n You can get some rough intuition about these matters by using your fingertips. Take a random clump of magnet balls and try to pull them apart. How much force do you have to apply when you tug in various directions? How does the cluster break apart? I find that the balls usually peel off in long strings of pearls, going directly from a three-dimensional aggregate to a one-dimensional chain. This behavior is not entirely surprising. After all, the magnets are dipoles, and so they can reduce their total energy by lining up north-south-north-south-north-south\u2026. \n Once you have a long chain, you can reduce its energy a little bit further by connecting head to tail to form a closed loop. But then, when you play around with the resulting bracelet of beads, you soon discover that the circular configuration is not at the bottom of the energy spectrum. The loop\u2014if it\u2019s long enough\u2014tends to collapse on itself, with the strands on opposite sides zipping together in the middle, forming what RNA chemists would call a double-ended hairpin structure. \n \n Note the alignment of the beads in the zippered region: The arrangement is rectilinear, as in a square lattice. By bringing together more strands of beads, we can grow this zippered arrangement into a fully two-dimensional, planar pattern. However, when you try this experiment with half a dozen short strands, you quickly discover that there\u2019s more than one way of combining them. The dipoles give each strand an orientation, and so adjacent chains can be either parallel or antiparallel. The rectilinear habit of the zippered loop comes from antiparallel alignments. Parallel strands assume a quite different pattern, with triangular or hexagonal symmetry. It\u2019s the difference between Kansas and Tennessee: \n \n It might look as though you could convert one of these arrangements into the other just by squeezing and skewing, but that\u2019s not the case at all. If you could see the north and south poles of each ball magnet, they would look something like this: \n \n Which of these arrangements lives in the deeper energy well? Kansas is stabilized by a multitude of local rectangular flux loops, which link adjacent antiparallel rows. (There may be weak longer-range attractions as well.) The parallel strands in Tennessee, in contrast, form one big global flux tube, with highly favorable interactions within the fabric of the layer but with nothing to help close the flux loops that exit one end of the state and re-enter the other. \n Both kinds of planar sheets are happy to roll up into hollow cylinders. The resulting tubes (which can also be made by stacking rings) are notably sturdy, stable and stiff. More than any other structures I\u2019ve discovered in playing with the magnet balls, the tubes seem to have the quality that Buckminster Fuller used to call tensegrity . The Tennessee roll-up is slightly stronger than the Kansas model. (I\u2019ve tried constructing hemispherical endcaps for the tubes, without success.) \n What about a fully three-dimensional, space-filling lattice? We already know about the simple cubic lattice, because that\u2019s the configuration that comes out of the shipping container. But my attempts to build multiple layers of the hexagonal close-packed lattice have all failed. A two-layered Tennessee will not lie flat. Or, looking at it another way, magnetic cannonballs cannot be stored in the classic Keplerian heap. Even a tetrahedral pile with just four balls is violently unstable and spontaneously rearranges itself into a linear chain or a flat square. \n Looking at all these forms, I see an analogy with carbon chemistry. [Warning: half-baked ideas ahead!] According to the analogy, the cubic lattice of magnet balls is like diamond. Not that it has the same geometry as diamond, but it is the most symmetrical arrangement, and the only one that fills three-dimensional space. The Tennessee pattern with its hexagonal symmetry is analogous to graphene or graphite\u2014a substance that is actually more stable than diamond but has reduced symmetry. Continuing in this scheme, the Tennessee roll-up has to be a buckytube. \n \u2022  \u2022  \u2022 \n Whatever the true identity of the lowest-energy configuration, it\u2019s a state you would expect to see emerge spontaneously only after an eternity of gradual cooling toward zero temperature. For those of us with a shorter attention span, the state of lowest free energy might be of greater interest. Let\u2019s define free energy as \n A = U \u2013 TS \n where U is the ordinary internal energy\u2014the stuff we were trying to minimize in the paragraphs above\u2014 T is the temperature and S is entropy. In this context, temperature is not what the thermometer in the room reads. It\u2019s a measure of how vigorously we agitate the system of balls, for example by shaking them in a box. As for the entropy, let\u2019s think of it as counting the number of microstates per macrostate. The free-energy formula, as I understand it, implies the following: If we take random samples from a population at temperature T , then the configurations we\u2019re most likely to see are those that balance the imperatives to minimize U and to maximize S . The value of T determines the relative weight assigned to energy and entropy. \n I tried the obvious experiment. I put some magnet balls in a Tupperware box and shook vigorously. The result was noisy and uninformative. The magnetic forces are so strong that it would take a whole lot of shaking to have any observable effect. Indeed, I think the box might disintegrate before the cluster of magnet balls did. \n So I tried a different approach. I looked at very small clusters, typically two or three balls, and watched what happened when they collided. I arranged the collisions by having the balls slide or roll down the walls of a large china bowl, meeting at the bottom. In effect, the height and steepness of the walls play the role of temperature in this procedure. Here are my notes from the first series of experiments, in which each pairing was repeated 20 times: \n \n I thought I noted a bias toward small, open rings. To explore this possibility a little further, I tried colliding individual balls with progressively larger rings or ringlike clusters. \n \n These results also suggested a preference for symmetrical n -gons, especially pentagons and hexagons, with diminishing effects as n gets smaller than 5 or larger than 6. Again it\u2019s tempting to interpret the results in light of organic chemistry, where the bond angles of carbon atoms favor 5- or 6-member rings (cyclopentane, cyclohexane, benzene); smaller rings are very hard to make, while larger ones are floppy and fragile. \n But perhaps I\u2019m a little too eager to believe these chemical analogies. After all, when you start with ring-shaped ingredients, you might expect to get ring-shaped products. Here\u2019s what I saw after colliding various small linear chains: \n \n The preference for rings has largely disappeared; noncyclic clusters predominate. Bashing together five-bead strands produces quite a zoo of exotic shapes, hinting at still more diversity as the size of the molecules increases. \n Performing these experiments is tedious, and the results are probably not trustworthy. When dumping balls into a bowl, many factors are hard to control, and some of them cannot easily be randomized either. An important example is the impact geometry when two chains meet in a collision. Coming together end-to-end might well produce a different outcome than meeting broadside. \n Rather than work on refining my experimental techniques, I would like to try simulating the system. Doing all this inside a computer allows for greater control, better statistics and better randomness; besides, we can do some tricks that would be difficult in the physical world, such as turning magnetism on and off whenever it\u2019s convenient. However, creating an accurate simulation looks difficult and messy. Magnetic forces are harder to calculate than the simple inverse-square-law forces of most n -body simulations. Moreover, the program might also have to include friction and angular momentum. (Some clusters of beads can roll, whereas others slide.) \n I\u2019ve been able to find just one example of such a program, mentioned in a thread at Physics Forum. Perhaps there are others. \n I have the persistent sense that I am retracing the footsteps of others, but I have not been able to spot their tracks. The arXiv, the American Journal of Physics and the IOP journals all seemed like good prospects, but my searches have come up empty. I\u2019ll be grateful for any pointers."], "link": "http://bit-player.org/2012/statistical-mechanics-of-magnet-balls", "bloglinks": {}, "links": {"http://www.uspirg.org/blog": 1, "http://www.physicsforums.com/": 1}, "blogtitle": "bit-player"}, {"content": ["The Kepler conjecture\u2014the one about stacking cannonballs or oranges\u2014is now the Hales theorem, though with a cruelly lingering asterisk.\u00a0Thomas C. Hales announced his proof in 1998, and it\u00a0was published in 2005 and 2006 ( links ). But the referees\u00a0were unable to fully verify all the details, including some 5,000 nonlinear optimization problems solved by computer. Hales has continued to work on refinements to the proof, both simplifying the arguments and\u00a0exploring formal methods of verification.\u00a0 \n \n Johannes Kepler in 1610, an age of fanciful extravagance in collars and mustaches; from Wikipedia . \n \n Over the years I\u2019ve read parts of Hales\u2019s proof, but I realized the other day that I had never looked at Johannes Kepler\u2019s much earlier contribution to this discussion. It turns out that Kepler\u2019s essay on the subject is a little gem, an amiable work by an affable fellow, showing a lively mind at play.\u00a0The conjecture appears in The Six-Cornered Snowflake , a pamphlet pub\u00adlished in 1611\u00a0and offered as a New Year gift to Johann Matth\u00e4us Wacker von Wackenfels, who was then Kepler\u2019s patron. Apparently Wacker was more than a moneybags;\u00a0he had studied law in Strasbourg and Geneva, took a doctoral degree in Padua, and had literary interests. Kepler\u2019s essay includes a lot of learned banter addressed to Wacker, suggesting the two men may have been genuine friends. Some of the jokes involve bilingual puns\u2014Latin and German.\n \n Kepler\u2019s main subject, as you might well guess from his title, is the hexagonal symmetry of snowflakes. He writes: \n \n There must be some definite cause why, whenever snow begins to fall, its initial formations invariably display the shape of a six-cornered starlet. For if it happens by chance, why do they not fall just as well with five corners or with seven? Why always with six\u2026? \n \n \n The idea we now know as the Kepler conjecture is introduced in one of several speculative attempts to solve this puzzle. Kepler describes the familiar stack-of-cannonballs geometry for packing spheres and then makes a bold claim about it, phrased not as a conjecture but as a fact that stands on its own, without need of demonstration: \u201cThe pack\u00ading will be the tightest possible, so that in no other arrangement could more pellets be stuffed into the same container.\u201d The only alter\u00adnative he considers is the simple cubic lattice. He does not calculate the actual density of either lattice (\\(\\pi/\\sqrt{18}\\) vs. \\(\\pi/6)\\). In the woodcut above, Kepler deconstructs a tetrahedral heap of 35 spheres into five horizontal layers; he clearly understood that the packing could be extended throughout an unbounded volume of space. \n From the cannonball packing, Kepler passes on to other themes, including the arrangement of seeds in a pomegranate, the hexagonal cells of honeycombs and the symmetries of flowers (often fivefold); this last subject presents an opportunity for a further digression on the golden ratio and the Fibonacci numbers. But when Kepler finally returns to the snowflake, none of these ideas offers much help, and he is left with still more questions: \n I will grant that, as flakes fall from above through steamy air, some incrustation on the plumes can occur from the vapor that comes in contact with them. But why at six points? What is the origin of the number six? Who carved the nucleus, before it fell, into six horns of ice? What cause is it that prescribes in that surface, which is now in the very act of condensing, six points in a circle for six prongs to be welded to them? \n \n At this point Kepler comes up with a charming and highly creative idea, which also happens to be utterly preposterous. He proposes that snowflakes have six points because space has three dimensions: \n While these starlets are falling, they consist of three feathered diameters, joined crosswise at one point, with their six extremities equally distributed in a sphere; consequently they fall on only three of the feathered prongs, and tower aloft with the remaining three, opposite those on which they fall, on the same diameters prolonged, until those, on which they rested, buckle, and the remainder, until then upright, sag onto the level in the gaps between them. \n \n In other words, a pristine snowflake is not a planar hexagonal shape but a three-dimensional structure rather like a toy jack, with components aligned along three Cartesian coordinate axes. The flake collapses into a flat, six-pointed configuration only when it falls to earth. According to this theory, the number six is not something arbitrary or accidental but a direct consequence of the way the universe is put together. \n Kepler did not have the vocabulary of Cartesian coordinate systems to describe his idea\u2014Descartes would not invent it until 20 years later, incidentally after reading The Six-Cornered Snowflake \u2014and so Kepler had to adopt a more roundabout description: \n But is this perhaps the cause of the three diameters, that there is the same number of dimensions in animals? After all they have upper and lower parts, front and back, left and right. \n \n Almost half of Kepler\u2019s essay is given to the defense of this three-dimensional hypothesis, and then, at the end, all of his speculation is spoiled by a conflict with cold, wet reality: \n For as I write it has again begun to snow, and more thickly than a moment ago. I have been busily examining the little flakes. Well, they have been falling, all of them, in radial pattern, but of two kinds: some very small with prongs inserted all the way round\u2026 But scattered among them were the rarer six-cornered starlets of the second kind, and not one of them was anything but flat, whether it was floating or coming to earth, with the plumes in the same plane as their stem. \n \n In the end Kepler abandons his inquiry without reaching any conclusion. He throws the problem out for the chemists and physicists to solve\u2014which they did, three centuries later. \n Scientific discourse has changed a great deal in the past 400 years. When Hales wrote up his proof of the Kepler conjecture, he did not include jocular asides to his program officer at the National Science Foundation. And we don\u2019t see a lot of published papers these days that end with the admission, \u201cI have not yet got to the bottom of this.\u201d The negative capability that Kepler embraces is part of what makes his essay so appealing. \n I read the Kepler essay in an edition published in 1966, which prints the Latin text and an English translation on facing pages: \n Kepler, Johannes. 1966. The Six-Cornered Snowflake . Edited and translated from the Latin by Colin Hardie, with essays by L. L. Whyte and B. F. J. Mason. Oxford, U.K.: Oxford University Press. \n There\u2019s also a more recent edition, which I have not seen, from a small publishing house in Philadelphia: \n Kepler, Johannes. 2010. The Six-Cornered Snowflake . Translation by Jacques Bromberg, with essays by Owen Gingerich and Guillermo Bleichmar. Philadelphia, Penna.: Paul Dry Books . \n Update : Paul Dry Books has kindly sent me a copy of their edition, which is a charming small volume. In addition to the Gingerich and Bleichmar essays, it has extra illustrations and a poem by John Frederick Nims typeset in stellate verses."], "link": "http://bit-player.org/2012/keplers-snowflake", "bloglinks": {}, "links": {"http://pauldrybooks.com/": 1, "http://www.co.uk/": 1, "http://en.wikipedia.org/": 1, "https://sites.google.com/": 1}, "blogtitle": "bit-player"}, {"content": ["Still more on World3 and The Limits to Growth . Two weeks ago I gave a talk at Harvard on all this, and the video is now online . Look for the Brian Hayes\u2013March 30 link near the bottom of the page. \n The slides are here . \n And, by the way, the slides were done with deck.js , a JavaScript-and-HTML framework I had never tried. Both preparing the slides and presenting them went very smoothly. Running the talk from within a browser has obvious advantages when you\u2019re talking about Internet things; you don\u2019t have to break out of Powerpoint or Keynote to go to a web page. Using Mathjax for TeX stuff is effortless. So is posting your slides on the web. On the other hand, navigation may not be obvious at a glance. Use the arrow keys. The M and G keys also do helpful things."], "link": "http://bit-player.org/2012/world3-the-video", "bloglinks": {}, "links": {"https://www.harvard.edu/": 1, "http://imakewebthings.com/": 1, "http://bit-player.org/": 2}, "blogtitle": "bit-player"}]