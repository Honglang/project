[{"blogurl": "http://systematicinvestor.wordpress.com\n", "blogroll": [], "title": "Systematic Investor"}, {"content": ["Regime Detection comes handy when you are trying to decide which strategy to deploy. For example there are periods (regimes) when Trend Following strategies work better and there are periods when Mean Reversion strategies work better. Today I want to show you one way to detect market Regimes. \n To detect market Regimes, I will fit a Hidden Markov Regime Switching Model on the set of simulated data (i.e. Bull / Bear market environments) I will use the excellent example from the Markov Regime Switching Models in MATLAB post and adapt it to R. \n The idea behind using the Regime Switching Models to identify market states is that market returns might have been drawn from 2 or more distinct distributions. As a base case, for example, we may suppose that market returns are samples from one normal distribution N(mu, sigma) i.e. \n \nReturns = mu + e, e ~ N(0, sigma)\n \n Next we may suppose that market returns are samples from two normal distributions (i.e. returns during Bull market may be ~ N(mu.Bull, sigma.Bull) and returns during Bear market may be N(mu.Bear , sigma.Bear) i.e. \n \nReturns = mu + e, e ~ N(0, sigma) \nmu = mu.Bull for Bull regime and mu.Bear for Bear regime and\nsigma= sigma.Bull for Bull regime and sigma.Bear for Bear regime\n \n Fortunately we do not have to fit regimes by hand, there is the RHmm package for Hidden Markov Models at CRAN that uses the Baum-Welch algorithm to fit Hidden Markov Models. \n Next, let follow the steps from the Markov Regime Switching Models in MATLAB post. \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\t#*****************************************************************\n\t# Generate data as in the post\n\t#****************************************************************** \n\tbull1 = rnorm( 100, 0.10, 0.15 )\n\tbear = rnorm( 100, -0.01, 0.20 )\n\tbull2 = rnorm( 100, 0.10, 0.15 )\n\ttrue.states = c(rep(1,100),rep(2,100),rep(1,100))\n\treturns = c( bull1, bear, bull2 )\n\n\n\t# find regimes\n\tload.packages('RHmm')\n\n\ty=returns\n\tResFit = HMMFit(y, nStates=2)\n\tVitPath = viterbi(ResFit, y)\n\n\t#Forward-backward procedure, compute probabilities\n\tfb = forwardBackward(ResFit, y)\n\n\t# Plot probabilities and implied states\n\tlayout(1:2)\n\tplot(VitPath$states, type='s', main='Implied States', xlab='', ylab='State')\n\t\n\tmatplot(fb$Gamma, type='l', main='Smoothed Probabilities', ylab='Probability')\n\t\tlegend(x='topright', c('State1','State2'), fill=1:2, bty='n')\n \n  \n The first chart shows states (1/2) determined by the model. The second chart shows the probability of being in each state. \n Next, let\u2019s generate some additional data and see if the model is able to identify the regimes \n \n\t#*****************************************************************\n\t# Add some data and see if the model is able to identify the regimes\n\t#****************************************************************** \n\tbear2 = rnorm( 100, -0.01, 0.20 )\n\tbull3 = rnorm( 100, 0.10, 0.10 )\n\tbear3 = rnorm( 100, -0.01, 0.25 )\n\ty = c( bull1, bear, bull2, bear2, bull3, bear3 )\n\tVitPath = viterbi(ResFit, y)$states\n\n\t#*****************************************************************\n\t# Plot regimes\n\t#****************************************************************** \n\tload.packages('quantmod')\n\tdata = xts(y, as.Date(1:len(y)))\n\n\tlayout(1:3)\n\t\tplota.control$col.x.highlight = col.add.alpha(true.states+1, 150)\n\tplota(data, type='h', plotX=F, x.highlight=T)\n\t\tplota.legend('Returns + True Regimes')\n\tplota(cumprod(1+data/100), type='l', plotX=F, x.highlight=T)\n\t\tplota.legend('Equity + True Regimes')\n\t\n\t\tplota.control$col.x.highlight = col.add.alpha(VitPath+1, 150)\n\tplota(data, type='h', x.highlight=T)\n\t\tplota.legend('Returns + Detected Regimes')\t\t\t\t\n \n  \n The first 300 observations were used to calibrate this model, the next 300 observations were used to see how the model can describe the new infromation. This model does relatively well in our toy example. \n To view the complete source code for this example, please have a look at the bt.regime.detection.test() function in bt.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/11/01/regime-detection/", "bloglinks": {}, "links": {"http://blogs.mathworks.com/": 2, "http://systematicinvestor.wordpress.com/": 2, "https://github.com/": 1, "http://feeds.wordpress.com/": 1, "http://r-forge.r-project.org/": 1}, "blogtitle": "Systematic Investor"}, {"content": ["I first read about the Couch Potato strategy in the MoneySense magazine . I liked this simple strategy because it was easy to understand and easy to manage. The Couch Potato strategy is similar to the Permanent Portfolio strategy that I have analyzed previously. \n The Couch Potato strategy invests money in the given proportions among different types of assets to ensure diversification and rebalances the holdings once a year. For example the Classic Couch Potato strategy is: \n \n 1) Canadian equity (33.3%) \n 2) U.S. equity (33.3%) \n 3) Canadian bond (33.3%) \n \n I highly recommend reading following online resources to get more information about the Couch Potato strategy : \n \n MoneySense \n \n Couch Potato Portfolio: Introduction \n Couch Potato Portfolio: Meet the potato family \n \n Canadian Couch Potato \n \n Couch Potato FAQ \n Model Portfolios \n \n AssetBuilder \n \n Introducing: The Couch Potato Building Blocks \n The Couch Potato Portfolios \n \n \n Today, I want to show how you can model and monitor the Couch Potato strategy with the Systematic Investor Toolbox . \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\n\t# helper function to model Couch Potato strategy - a fixed allocation strategy\n\tcouch.potato.strategy <- function\n\t(\n\t\tdata.all,\n\t\ttickers = 'XIC.TO,XSP.TO,XBB.TO',\n\t\tweights = c( 1/3, 1/3, 1/3 ), \t\t\n\t\tperiodicity = 'years',\n\t\tdates = '1900::',\n\t\tcommission = 0.1\n\t) \n\t{ \n\t\t#*****************************************************************\n\t\t# Load historical data \n\t\t#****************************************************************** \n\t\ttickers = spl(tickers)\n\t\tnames(weights) = tickers\n\t\t\n\t\tdata <- new.env()\n\t\tfor(s in tickers) data[[ s ]] = data.all[[ s ]]\n\t\t\n\t\tbt.prep(data, align='remove.na', dates=dates)\n\t\n\t\t#*****************************************************************\n\t\t# Code Strategies\n\t\t#******************************************************************\n\t\tprices = data$prices \n\t\t\tn = ncol(prices)\n\t\t\tnperiods = nrow(prices)\n\t\n\t\t# find period ends\n\t\tperiod.ends = endpoints(data$prices, periodicity)\n\t\t\tperiod.ends = c(1, period.ends[period.ends > 0])\n\t\n\t\t#*****************************************************************\n\t\t# Code Strategies\n\t\t#******************************************************************\n\t\tdata$weight[] = NA\n\t\t\tfor(s in tickers) data$weight[period.ends, s] = weights[s]\n\t\tmodel = bt.run.share(data, clean.signal=F, commission=commission)\n\t\t\n\t\treturn(model)\n\t} \t\n \n The couch.potato.strategy() function creates a periodically rebalanced portfolio for given static allocation. \n Next, let\u2019s back-test some Canadian Couch Potato portfolios: \n \n\t#*****************************************************************\n\t# Load historical data\n\t#****************************************************************** \n\tload.packages('quantmod')\t\n\tmap = list()\n\t\tmap$can.eq = 'XIC.TO'\n\t\tmap$can.div = 'XDV.TO'\t\t\n\t\tmap$us.eq = 'XSP.TO'\n\t\tmap$us.div = 'DVY'\t\t\t\n\t\tmap$int.eq = 'XIN.TO'\t\t\n\t\tmap$can.bond = 'XBB.TO'\n\t\tmap$can.real.bond = 'XRB.TO'\n\t\tmap$can.re = 'XRE.TO'\t\t\n\t\tmap$can.it = 'XTR.TO'\n\t\tmap$can.gold = 'XGD.TO'\n\t\t\t\n\tdata <- new.env()\n\tfor(s in names(map)) {\n\t\tdata[[ s ]] = getSymbols(map[[ s ]], src = 'yahoo', from = '1995-01-01', env = data, auto.assign = F)\n\t\tdata[[ s ]] = adjustOHLC(data[[ s ]], use.Adjusted=T)\t\n\t}\n\t\t\n\t#*****************************************************************\n\t# Code Strategies\n\t#****************************************************************** \n\tmodels = list()\n\t\tperiodicity = 'years'\n\t\tdates = '2006::'\n\t\n\tmodels$classic = couch.potato.strategy(data, 'can.eq,us.eq,can.bond', rep(1/3,3), periodicity, dates)\n\tmodels$global = couch.potato.strategy(data, 'can.eq,us.eq,int.eq,can.bond', c(0.2, 0.2, 0.2, 0.4), periodicity, dates)\n\tmodels$yield = couch.potato.strategy(data, 'can.div,can.it,us.div,can.bond', c(0.25, 0.25, 0.25, 0.25), periodicity, dates)\n\tmodels$growth = couch.potato.strategy(data, 'can.eq,us.eq,int.eq,can.bond', c(0.25, 0.25, 0.25, 0.25), periodicity, dates)\n\t\n\tmodels$complete = couch.potato.strategy(data, 'can.eq,us.eq,int.eq,can.re,can.real.bond,can.bond', c(0.2, 0.15, 0.15, 0.1, 0.1, 0.3), periodicity, dates)\t\n\t\n\tmodels$permanent = couch.potato.strategy(data, 'can.eq,can.gold,can.bond', c(0.25,0.25,0.5), periodicity, dates)\t\n\t\t\n\t#*****************************************************************\n\t# Create Report\n\t#****************************************************************** \n\tplotbt.custom.report.part1(models)\n \n  \n I have included a few classic Couch Potato portfolios and the Canadian version of the Permanent portfolio. The equity curves speak for themselves: you can call them by the fancy names, but in the end all variations of the Couch Potato portfolios performed similar and suffered a huge draw-down during 2008. The Permanent portfolio did a little better during 2008 bear market. \n Next, let\u2019s back-test some US Couch Potato portfolios: \n \n\t#*****************************************************************\n\t# Load historical data\n\t#****************************************************************** \n\ttickers = spl('VIPSX,VTSMX,VGTSX,SPY,TLT,GLD,SHY')\n\t\n\tdata <- new.env()\n\tgetSymbols(tickers, src = 'yahoo', from = '1995-01-01', env = data, auto.assign = T)\n\t\tfor(i in ls(data)) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)\t\n\t\t\n\t\t# extend GLD with Gold.PM - London Gold afternoon fixing prices\n\t\tdata$GLD = extend.GLD(data$GLD)\n\n\t#*****************************************************************\n\t# Code Strategies\n\t#****************************************************************** \n\tmodels = list()\n\t\tperiodicity = 'years'\n\t\tdates = '2003::'\n\t\n\tmodels$classic = couch.potato.strategy(data, 'VIPSX,VTSMX', rep(1/2,2), periodicity, dates)\n\tmodels$margarita = couch.potato.strategy(data, 'VIPSX,VTSMX,VGTSX', rep(1/3,3), periodicity, dates)\n\tmodels$permanent = couch.potato.strategy(data, 'SPY,TLT,GLD,SHY', rep(1/4,4), periodicity, dates)\n\t\t\n\t#*****************************************************************\n\t# Create Report\n\t#****************************************************************** \n\tplotbt.custom.report.part1(models)\n \n  \n The US Couch Potato portfolios also suffered huge draw-downs during 2008. The Permanent portfolio hold it ground much better. \n It has been written quite a lot about Couch Potato strategy, but looking at different variations I cannot really see much difference in terms of perfromance or draw-downs. Probably that is why in the last few years, I have seen the creation of many new ETFs to address that in one way or another. For example, now we have tactical asset allocation ETFs , minimum volatility ETFs , income ETFs with covered calls overlays . \n To view the complete source code for this example, please have a look at the bt.couch.potato.test() function in bt.test.r at github . \n Some additional references from the Canadian Couch Potato blog that are worth reading: \n \n An Interview with the Original Couch Potato \n Scott Burns Interview: Part 2 \n How Often Should You Rebalance? \n How to Lower Your Rebalancing Costs \n Does Rebalancing Boost Returns?"], "link": "http://systematicinvestor.wordpress.com/2012/10/26/modeling-couch-potato-strategy/", "bloglinks": {}, "links": {"http://www.invescopowershares.com/": 1, "http://www.mebanefaber.com/": 1, "http://www.moneysense.ca/": 9, "http://assetbuilder.com/": 2, "http://systematicinvestor.wordpress.com/": 4, "http://feeds.wordpress.com/": 1, "http://canadiancouchpotato.com/": 8, "https://github.com/": 1}, "blogtitle": "Systematic Investor"}, {"content": ["Today I want to show a simple example of how we can value a company using Discounted Cash Flow (DCF) analysis . The idea is to compute the company\u2019s Intrinsic Value based on the discounted future cash-flows. To compute future cash-flows I will use the historical Free Cash Flow growth rate. To compute present value of these cash flows I will use a conservative 9% discount rate. If you want to read more about the Discounted Cash Flow (DCF) analysis , I recommend following references: \n \n Discounted Cash Flow \u2014 How Much is A Stock Really Worth? \n Apple (AAPL) Valuation \n DiscountedCashFlows Excel Workbook \n \n First let\u2019s load historical prices and fundamental data for Apple (AAPL) using the Systematic Investor Toolbox . \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\t#*****************************************************************\n\t# Load historical fundamental and pricing data\n\t#****************************************************************** \n\tload.packages('quantmod') \n\ttickers = spl('AAPL')\n\ttickers.temp = spl('NASDAQ:AAPL')\n\t\n\t# get fundamental data\n\tdata.fund <- new.env()\n\tfor(i in 1:len(tickers))\n\t\tdata.fund[[tickers[i]]] = fund.data(tickers.temp[i], 80, 'annual')\n\t\t\t\n\t# get pricing data\n\tdata <- new.env()\n\tgetSymbols(tickers, src = 'yahoo', from = '1970-01-01', env = data, auto.assign = T)\n\t\tfor(i in ls(data)) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)\t\t\t\n\n\t# prepare data\n\tfund = data.fund[[tickers[1]]]\n\tfund.date = date.fund.data(fund)\t\t\t\n\tprice = Cl(data[[tickers[1]]]['1995::'])\n \n Next let\u2019s extract fundamental data for Discounted Cash Flow (DCF) analysis \n \n\t#*****************************************************************\n\t# Extract Inputs for DCF Valuation\n\t#****************************************************************** \t\t\t\t\n\t# Free Cash Flows\n\tFCF = get.fund.data('free cash flow', fund, fund.date)\n\t\n\t# Invested Capital\n\tIC = get.fund.data('invested capital', fund, fund.date)\n\t\t\n\t# Sales\n\tSALE = get.fund.data('total revenue', fund, fund.date)\n\n\t# Common Equity\n\tCEQ = get.fund.data('total equity', fund, fund.date)\n\n\t# Common Shares Outstanding\n\tCSHO = get.fund.data('total common shares out', fund, fund.date)\n\n\t# Growth Rate\n\tCROIC = FCF/IC\n\t\n\t# Average inputs\n\tg = runMean(CROIC, 5)\n\tcash = runMean(FCF, 5)\n \n Next I created a simple function to estimate company\u2019s Intrinsic Value using Discounted Cash Flow (DCF) analysis . \n \n\t#*****************************************************************\n\t# Helper function to compute Intrinsic Value\n\t#****************************************************************** \t\t\t\t\n\tcompute.DCF.IV <- function(cash, eqity, shares, g, R) {\n\t\tif( cash <= 0 ) return(NA)\n\t\t\n\t\tif( len(R) == 1 ) R = rep(R, len(g))\n\t\t\n\t\tvalue = eqity + sum(cash * cumprod(1 + g) / cumprod(1 + R))\n\t\treturn( value / shares )\n\t}\n \n Finally, let\u2019s compute AAPL\u2019s Intrinsic Value and create plots \n \n\t#*****************************************************************\n\t# Compute Intrinsic Value, assumptions:\n\t# Company will grow for the first 3 years at current Growth Rate\n\t# slowed down by 20% for the next 4 years, and slowed down by a further 20% for the next 3 years\n\t# and finally 3% growth for the next 10 years\n\t#\n\t# The Discount Rate is 9%\n\t#\n\t# http://www.oldschoolvalue.com/blog/stock-analysis/apple-aapl-valuation/\n\t#****************************************************************** \t\t\t\t\n\tdcf.price = NA * g\n\ti.start = which(!is.na(g))[1] \n\t\n\tfor(i in i.start : nrow(g)) {\n\t\t# Create Growth Rate scenario: \t\t\n\t\tg.scenario = c(rep(g[i],3), rep(g[i],4)*0.8, rep(g[i],3)*0.8*0.8, rep(3/100,10))\n\t\t\n\t\t# Compute Intrinsic Value\n\t\tdcf.price[i] = compute.DCF.IV(cash[i], CEQ[i], CSHO[i], g.scenario, 9/100)\n\t}\n\t\n\t#*****************************************************************\n\t# Create Plots\n\t#****************************************************************** \n\tplota(price, type='l', log = 'y', col='blue', main=tickers[1],\n\t\tylim=range(price,dcf.price,na.rm=T))\n\tplota.lines(dcf.price, type='s', col='red', lwd=2)\n\tplota.legend('Close,Intrinsic Value', 'blue,red', list(price, dcf.price))\t\n\n\t\n\tplota(g, type='b', col='blue', pch=0, main='Growth Rate')\t\n\n\n\tplota(cash, type='b', col='blue', pch=0, main='Free Cash Flows')\t\n \n  \n  \n  \n The Intrinsic Value calculations are highly sensitive to your assumptions about the company\u2019s Growth Rate and Discount Rate used in the Discounted Cash Flow (DCF) analysis . \n AAPL has experienced the amazing Growth Rate over the last 5 years and the big question is whether AAPL will be able to maintain this Growth Rate in the future. If yes, then the stock price can easily reach new highs as suggested by the Discounted Cash Flow (DCF) analysis . \n To view the complete source code for this example, please have a look at the fundamental.dcf.test() function in fundamental.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/10/19/company-valuation-using-discounted-cash-flows/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.oldschoolvalue.com/blog": 1, "http://www.independent-stock-investing.com/": 1, "https://github.com/": 1, "http://en.wikipedia.org/": 6, "http://systematicinvestor.wordpress.com/": 5}, "blogtitle": "Systematic Investor"}, {"content": ["I want to address comments that were asked in my last post, Permanent Portfolio \u2013 Simple Tools , about Permanent Portfolio strategy. Specifically: \n \n The impact of transaction costs on the perfromance and \n Create a modified version of risk allocation portfolio that distributes weights across 3 asset classes: stocks(SPY), gold(GLD), and treasuries(TLT), and only invests into cash(SHY) to fill the residual portfolio exposure once we scale the SPY/GLD/TLT portfolio to the target volatility \n \n The first point is easy, to incorporate the transaction cost into your back-test just add commission=0.1 parameter to the bt.run.share() function call.For example, to see the dollar allocation strategy perfromance assuming 10c a share commission, use following code: \n \n# original strategy\nmodels$dollar = bt.run.share(data, clean.signal=F)\n\n# assuming 10c a share commissions\nmodels$dollar = bt.run.share(data, commission=0.1, clean.signal=F)\n \n The second point is a bit more work. First, let\u2019s allocate risk across only to 3 asset classes: stocks(SPY), gold(GLD), and treasuries(TLT). Next, let\u2019s scale the SPY/GLD/TLT portfolio to the 7% target volatility. And finally, let\u2019s allocate to cash(SHY) the residual portfolio exposure. \n \n\t#*****************************************************************\n\t# Risk Weighted: allocate only to 3 asset classes: stocks(SPY), gold(GLD), and treasuries(TLT)\n\t#****************************************************************** \t\t\t\t\n\tret.log = bt.apply.matrix(prices, ROC, type='continuous')\n\thist.vol = sqrt(252) * bt.apply.matrix(ret.log, runSD, n = 21)\t\n\tweight.risk = weight.dollar / hist.vol\n\t\tweight.risk$SHY = 0 \n\t\tweight.risk = weight.risk / rowSums(weight.risk)\n\t\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = weight.risk[period.ends,]\n\tmodels$risk = bt.run.share(data, commission=commission, clean.signal=F)\n\n\t#*****************************************************************\n\t# Risk Weighted + 7% target volatility\n\t#****************************************************************** \t\t\t\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = target.vol.strategy(models$risk,\n\t\t\t\t\t\tweight.risk, 7/100, 21, 100/100)[period.ends,]\n\tmodels$risk.target7 = bt.run.share(data, commission=commission, clean.signal=F)\n\n\t#*****************************************************************\n\t# Risk Weighted + 7% target volatility + SHY\n\t#****************************************************************** \t\t\t\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = target.vol.strategy(models$risk,\n\t\t\t\t\t\tweight.risk, 7/100, 21, 100/100)[period.ends,]\n\t\t\t\t\t\t\n \t\tcash = 1-rowSums(data$weight)\n\t data$weight$SHY[period.ends,] = cash[period.ends]\n\tmodels$risk.target7.shy = bt.run.share(data, commission=commission, clean.signal=F)\n \n  \n  \n  \n The modified version of risk allocation portfolio performs well relative to other portfolios even after incorporating the 10c transaction cost. \n To view the complete source code for this example, please have a look at the bt.permanent.portfolio3.test() function in bt.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/10/10/permanent-portfolio-transaction-cost-and-better-risk-parity/", "bloglinks": {}, "links": {"http://systematicinvestor.wordpress.com/": 4, "https://github.com/": 1, "http://feeds.wordpress.com/": 1, "https://systematicinvestor.wordpress.com/": 1}, "blogtitle": "Systematic Investor"}, {"content": ["The Barron\u2019s article Still Too Pricey by Andrew Bary looks at the share price of the Facebook and based on the P/E ration valuation metrics concludes that even at the current prices, stock is overvalued. I want to show how to do this type of fundamental analysis using the Systematic Investor Toolbox . \n First let\u2019s load historical prices and earnings per share (EPS) for Facebook and a few stocks in the technology sector: LinkedIn, Groupon, Apple, and Google. \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\t#*****************************************************************\n\t# Load historical fundamental and pricing data\n\t#****************************************************************** \n\tload.packages('quantmod') \n\ttickers = spl('FB,LNKD,GRPN,AAPL,GOOG')\n\ttickers.temp = spl('NASDAQ:FB,NYSE:LNKD,NASDAQ:GRPN,NASDAQ:AAPL,NASDAQ:GOOG')\n\t\n\t# get fundamental data\n\tdata.fund <- new.env()\n\tfor(i in 1:len(tickers)) {\n\t\t\tcat(tickers[i],'\\n')\n\t\t\tdata.fund[[tickers[i]]] = fund.data(tickers.temp[i], 80)\n\t}\n\t\n\t# get pricing data\n\tdata <- new.env()\n\tgetSymbols(tickers, src = 'yahoo', from = '1970-01-01', env = data, auto.assign = T)\n\t\tfor(i in ls(data)) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)\t\t\t\n \n Next, let\u2019s combine fundamental and pricing data and create P/E ratio for all stocks. \n \n\t#*****************************************************************\n\t# Combine fundamental and pricing data\n\t#****************************************************************** \t\t\t\t\n\tfor(i in tickers) {\n\t\tfund = data.fund[[i]]\n\t\tfund.date = date.fund.data(fund)\n\t\t\t\n\t\t# Earnings per Share\t\t\n\t\tEPS = 4 * get.fund.data('Diluted EPS from Total Operations', fund, fund.date)\n\t\tif(nrow(EPS) > 3)\n\t\t\tEPS = rbind(EPS[1:3], get.fund.data('Diluted EPS from Total Operations', fund, fund.date, is.12m.rolling=T)[-c(1:3)])\n\t\t\n\t\t# merge\t\n\t\tdata[[i]] = merge(data[[i]], EPS)\n\t}\n\n\tbt.prep(data, align='keep.all', dates='1995::')\n\t\n\t#*****************************************************************\n\t# Create PE\n\t#****************************************************************** \n\tprices = data$prices\n\t\tprices = bt.apply.matrix(prices, function(x) ifna.prev(x))\n\t\t\n\tEPS = bt.apply(data, function(x) ifna.prev(x[, 'EPS']))\n\t\n\tPE = ifna(prices / EPS, NA)\n\t\tPE[ abs(EPS) < 0.001 ] = NA\t\n \n Please note that for very small EPS, the P/E ratio will be very big; therefore, I set P/E to NA in such cases. \n The hard part is done, not let\u2019 plot P/E ratios for all companies. \n \n #*****************************************************************\n # Create Report\n #******************************************************************  \n plota.matplot(PE)\n\n plota.matplot(PE, type='b',pch=20, dates='2012::')\n\t\t\n plota.matplot(EPS)\n\t\n plota.matplot(prices)\n \n P/E ratios for all companies: \n \n P/E ratios for all companies in 2012: \n \n Earnings per share (EPS) for all companies: \n \n Prices for all companies: \n \n From these charts I would say it is too early to decide if Facebook is overvalued based on historical P/E ratio basis only, because we only have 3 financial statements, not enough to make an informed conclusion. You might use project one year (FY1) and two year (FY2) earnings estimates to make a better decision. \n What is interesting in these charts is how LinkedIn is managing to sustain its astronomical P/E ratio? \n I have previously shown examples of how to get and use fundamental data. Here are links for your reference: \n \n Multiple Factor Model \u2013 Building Fundamental Factors \n Multiple Factor Model \u2013 Fundamental Data \n \n To view the complete source code for this example, please have a look at the fundamental.fb.test() function in fundamental.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/10/07/weekend-reading-facebooks-pe-ratio/", "bloglinks": {}, "links": {"http://systematicinvestor.wordpress.com/": 7, "https://github.com/": 1, "http://feeds.wordpress.com/": 1, "http://online.barrons.com/": 1}, "blogtitle": "Systematic Investor"}, {"content": ["I have previously described and back-tested the Permanent Portfolio strategy based on the series of posts at the GestaltU blog. Today I want to show how we can improve the Permanent Portfolio strategy perfromance using following simple tools: \n \n Volatility targeting \n Risk allocation \n Tactical market filter \n \n First, let\u2019s load the historical prices for the stocks(SPY), gold(GLD), treasuries(TLT), and cash(SHY) and create a quarterly rebalanced Permanent Portfolio strategy using the Systematic Investor Toolbox . \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\t#*****************************************************************\n\t# Load historical data\n\t#****************************************************************** \n\tload.packages('quantmod')\t\n\ttickers = spl('SPY,TLT,GLD,SHY')\n\t\t\n\tdata <- new.env()\n\tgetSymbols(tickers, src = 'yahoo', from = '1980-01-01', env = data, auto.assign = T)\n\t\tfor(i in ls(data)) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)\n\t\t\n\t\t# extend GLD with Gold.PM - London Gold afternoon fixing prices\n\t\tdata$GLD = extend.GLD(data$GLD)\n\t\n\tbt.prep(data, align='remove.na')\n\n\t#*****************************************************************\n\t# Setup\n\t#****************************************************************** \t\t\n\tprices = data$prices \n\t\tn = ncol(prices)\n\n\tperiod.ends = endpoints(prices, 'quarters')\n\t\tperiod.ends = period.ends[period.ends > 0]\t\t\n\t\tperiod.ends = c(1, period.ends)\n\t\t\t\t\t\n\n\tmodels = list()\n\t\n\t\n\t#*****************************************************************\n\t# Dollar Weighted\n\t#****************************************************************** \t\t\t\n\ttarget.allocation = matrix(rep(1/n,n), nrow=1)\n\tweight.dollar = ntop(prices, n)\n\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = weight.dollar[period.ends,]\n\tmodels$dollar = bt.run.share(data, clean.signal=F)\n \n Now let\u2019s create a version of the Permanent Portfolio strategy that targets the 7% annual volatility based on the 21 day look back period. \n \n\t#*****************************************************************\n\t# Dollar Weighted + 7% target volatility\n\t#****************************************************************** \t\t\t\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = target.vol.strategy(models$dollar,\n\t\t\t\t\t\tweight.dollar, 7/100, 21, 100/100)[period.ends,]\n\tmodels$dollar.target7 = bt.run.share(data, clean.signal=F)\n \n Please note that allocating equal dollar amounts to each investment puts more risk allocation to the risky assets. If we want to distribute risk budget equally across all assets we can consider a portfolio based on the equal risk allocation instead of equal capital (dollar) allocation. \n \n\t#*****************************************************************\n\t# Risk Weighted\n\t#****************************************************************** \t\t\t\t\n\tret.log = bt.apply.matrix(prices, ROC, type='continuous')\n\thist.vol = sqrt(252) * bt.apply.matrix(ret.log, runSD, n = 21)\t\n\tweight.risk = weight.dollar / hist.vol\n\t\tweight.risk = weight.risk / rowSums(weight.risk)\n\t\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = weight.risk[period.ends,]\n\tmodels$risk = bt.run.share(data, clean.signal=F)\n \n We can also use market filter, for example a 10 month moving average, to control portfolio drawdowns. \n \n\t#*****************************************************************\n\t# Market Filter (tactical): 10 month moving average\n\t#****************************************************************** \t\t\t\t\n\tperiod.ends = endpoints(prices, 'months')\n\t\tperiod.ends = period.ends[period.ends > 0]\t\t\n\t\tperiod.ends = c(1, period.ends)\n\n\tsma = bt.apply.matrix(prices, SMA, 200)\n\tweight.dollar.tactical = weight.dollar * (prices > sma)\t\n\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = weight.dollar.tactical[period.ends,]\n\tmodels$dollar.tactical = bt.run.share(data, clean.signal=F)\n \n Finally, let\u2019s combine market filter and volatility targeting: \n \n\t#*****************************************************************\n\t# Tactical + 7% target volatility\n\t#****************************************************************** \t\t\t\t\n\tdata$weight[] = NA\n\t\tdata$weight[period.ends,] = target.vol.strategy(models$dollar.tactical,\n\t\t\t\t\t\tweight.dollar.tactical, 7/100, 21, 100/100)[period.ends,]\n\tmodels$dollar.tactical.target7 = bt.run.share(data, clean.signal=F)\n\t\t\n\t\t\t\n\t#*****************************************************************\n\t# Create Report\n\t#******************************************************************  \n\tplotbt.custom.report.part1(models)  \n\t\n\tplotbt.strategy.sidebyside(models)\t\n \n  \n  \n The final portfolio that combines market filter and volatility targeting is a big step up from the original Permanent Portfolio strategy: the returns are a bit down, but draw-downs are cut in half. \n To view the complete source code for this example, please have a look at the bt.permanent.portfolio2.test() function in bt.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/10/05/permanent-portfolio-simple-tools/", "bloglinks": {}, "links": {"http://systematicinvestor.wordpress.com/": 3, "http://gestaltu.blogspot.ca/": 1, "https://github.com/": 1, "http://feeds.wordpress.com/": 1, "https://systematicinvestor.wordpress.com/": 5}, "blogtitle": "Systematic Investor"}, {"content": ["I recently came across the \u201cAn early Halloween for gold traders\u201d article by Mark Hulbert. I have discussed this type of seasonality analysis in my presentation at R/Finance this year. \n It is very easy to run the seasonality analysis using the Systematic Investor Toolbox . \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n #*****************************************************************\n # Load historical data\n #****************************************************************** \n load.packages('quantmod')\n ticker = 'GLD'\n \n data = getSymbols(ticker, src = 'yahoo', from = '1970-01-01', auto.assign = F)\n  data = adjustOHLC(data, use.Adjusted=T)\n  \n #*****************************************************************\n # Look at the Month of the Year Seasonality\n #****************************************************************** \n month.year.seasonality(data, ticker)\n \n  \n This confirms that October have been historically bad for Gold, but we used only 8 years of history because GLD only started traded in 2004. \n To get a more complete picture, there is a long history of Gold prices at the Bundes Bank . I found this data source used at the Wikiposit . \n I created a helper function bundes.bank.data.gold() function in data.r at github to download prices from the Bundes Bank site. \n \n #*****************************************************************\n # Load long series of gold prices from Bundes Bank\n #****************************************************************** \n data = bundes.bank.data.gold()\n\n #*****************************************************************\n # Look at the Month of the Year Seasonality\n #****************************************************************** \n month.year.seasonality(data, 'GOLD', lookback.len = nrow(data))\n \n  \n The October have been historically bad for Gold using longer time series as well. \n Next I would recommend looking at the daily Gold\u2019s performance in October to get a better picture. You might want to use the Seasonality Tool for this purpose. Please read the Historical Seasonality Analysis: What company in DOW 30 is likely to do well in January? post for a case study on how to use the Seasonality Tool . \n To view the complete source code for this example, please have a look at the bt.october.gold.test() function in bt.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/09/29/weekend-reading-gold-in-october/", "bloglinks": {}, "links": {"https://github.com/": 2, "http://www.bundesbank.de/": 2, "http://www.marketwatch.com/": 1, "http://feeds.wordpress.com/": 1, "http://wikiposit.org/": 1, "http://systematicinvestor.wordpress.com/": 5, "http://www.systematicportfolio.com/": 2}, "blogtitle": "Systematic Investor"}, {"content": ["I want to show the example of calling the Minimum Correlation Algorithm from Excel. I will use RExcel to connect R and Excel and will create a small VBA cell array function to communicate between Excel and R. \n I have previously discussed the concept of connecting R and Excel in the \u201cCalling Systematic Investor Toolbox from Excel using RExcel & VBA\u201d post. Please read this post for the instructions to setup RExcel . \n Following is a screen shot of the complete workbook: \n  \n You can download the MinimumCorrelation.xls workbook and experiment with it while you keep reading. \n I created the \u201cMinimumCorrelation\u201d cell array function in VBA. Do not forget to use CTRL+SHIFT+ENTER to enter \u201cMinimumCorrelation\u201d function into your workbooks . The \u201cMinimumCorrelation\u201d function will send historical price information from Excel to the R environment. It will next execute the R script to construct weights using the Minimum Correlation Algorithm, and finally it will collect R calculations of portfolio weights and transfer them back to Excel. \n Here is the R script that calls min.corr.portfolio() function. I created a VBA function \u201ccreate_rcode\u201d to create this file automatically for this example. \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)          \n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/   \n###############################################################################\nif(!exists('min.corr.portfolio')) {           \n setInternet2(TRUE)               \n con = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))   \n  source(con)                \n close(con)                 \n}                    \n\n #*****************************************************************   \n # Setup                  \n #*****************************************************************   \n n = ncol(hist_prices)              \n hist = na.omit(hist_prices / mlag(hist_prices) - 1)       \n                    \n # create historical input assumptions          \n ia = list()                 \n  ia$n = n                \n  ia$risk = apply(hist, 2, sd)           \n  ia$correlation = cor(hist, use='complete.obs', method='pearson')  \n  ia$cov = ia$correlation * (ia$risk %*% t(ia$risk))      \n\n # portfolio allocation              \n weights = min.corr.portfolio(ia, null)          \n dim(weights)=c(1,n)               \n \n Next, the \u201cMinimumCorrelation\u201d cell array function in VBA that calls min.corr.portfolio() function in R: \n \n'Minimum Correlation Algorithm\nPublic Function MinimumCorrelation(ByRef r_values As Range) As Variant\n ' Start R connection\n RInterface.StartRServer\n\n ' Write R code to file\n create_rcode\n\n ' Put Historical Asset Prices into R\n RInterface.PutArray \"hist_prices\", r_values\n \n ' Executes the commands in filename\n RInterface.RunRFile r_filename\n   \n ' Get Portfolio Allocation determined by the Minimum Correlation Algorithm into Excel\n MinimumCorrelation = RInterface.GetArrayToVBA(\"weights\")\nEnd Function\n \n The complete working copy of the MinimumCorrelation.xls workbook. \n Please do not forget to use CTRL+SHIFT+ENTER to enter \u201cMinimumCorrelation\u201d function into your workbooks ."], "link": "http://systematicinvestor.wordpress.com/2012/09/27/calling-minimum-correlation-algorithm-from-excel-using-rexcel-vba/", "bloglinks": {}, "links": {"http://systematicinvestor.wordpress.com/": 5, "http://feeds.wordpress.com/": 1, "http://rcom.ac.at/": 2}, "blogtitle": "Systematic Investor"}, {"content": ["The Minimum Correlation Algorithm is a heuristic method discovered by David Varadi . Below I will benchmark the execution speed of 2 versions of the Minimum Correlation Algorithm versus the traditional minimum variance optimization that relies on solving a quadratic programming problem. \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\t#*****************************************************************\n\t# Setup test input assumptions\n\t#*****************************************************************\n\tload.packages('quadprog,corpcor')\n\t\n\tn = 100\n\thist = matrix(rnorm(1000*n), nc=n)\n\t\n\t# 0 <= x.i <= 1\n\tconstraints = new.constraints(n, lb = 0, ub = 1)\n\t\tconstraints = add.constraints(diag(n), type='>=', b=0, constraints)\n\t\tconstraints = add.constraints(diag(n), type='<=', b=1, constraints)\n\n\t# SUM x.i = 1\n\tconstraints = add.constraints(rep(1, n), 1, type = '=', constraints)\t\t\n\t\t\t\t\t\t\n\t# create historical input assumptions\n\tia = list()\n\t\tia$n = n\n\t\tia$risk = apply(hist, 2, sd)\n\t\tia$correlation = cor(hist, use='complete.obs', method='pearson')\n\t\tia$cov = ia$correlation * (ia$risk %*% t(ia$risk))\n\t\t\t\t\n\t\tia$cov = make.positive.definite(ia$cov, 0.000000001)\n\t\tia$correlation = make.positive.definite(ia$correlation, 0.000000001)\n\t\t\n\t#*****************************************************************\n\t# Time each Algorithm \n\t#*****************************************************************\t\t\t\t\n\tload.packages('rbenchmark')\t\t\t\n\n\tbenchmark(\n\t\tmin.var.portfolio(ia, constraints),\n\t\tmin.corr.portfolio(ia, constraints),\n\t\tmin.corr2.portfolio(ia, constraints),\n\t\t\n\t\t\n\tcolumns=c(\"test\", \"replications\", \"elapsed\", \"relative\"),\n\torder=\"relative\",\n\treplications=100\n\t)\n \n I have run the code above for n=10 (10 assets), n=100 (100 assets), n=500 (500 assets), n=1000 (1000 assets) \n[Please note that for n=1000 I have only run 5 replication] \n \nn=10 (10 assets)\n      replications elapsed relative\n min.var.portfolio   100 0.02  1.0\n min.corr2.portfolio   100 0.02  1.0\n min.corr.portfolio   100 0.03  1.5\n\n\nn=100 (100 assets)\n      replications elapsed relative\n min.corr2.portfolio   100 0.07 1.00\n min.corr.portfolio   100 0.25 3.57\n min.var.portfolio   100 0.31 4.42\n\n\nn=500 (500 assets)\n      replications elapsed relative\n min.corr2.portfolio   100 2.18 1.00\n min.corr.portfolio   100 9.59 4.39\n min.var.portfolio   100 139.13 63.82\n\n\nn=1000 (1000 assets) \n      replications elapsed relative\n min.corr2.portfolio   5 0.25  1.00\n min.corr.portfolio   5 1.39  5.56\n min.var.portfolio   5 113.27 453.08\n \n For small universe (i.e. n ~ 100) all algorithms are fast. But once we attempt to solve 500 or 1000 assets portfolio allocation problem, the minimum variance algorithm is many times slower than the both versions of the minimum correlation algorithm. \n So if we are considering a 500 assets weekly back-test for the 10 yrs the run-times in seconds (i.e. 52*10*single tun-time): \n \n       elapsed\n min.corr2.portfolio   11.3\n min.corr.portfolio   49.8\n min.var.portfolio   723.4\n \n To view the complete source code for this example, please have a look at the bt.mca.speed.test() function in bt.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/09/26/minimum-correlation-algorithm-speed-comparison/", "bloglinks": {}, "links": {"http://systematicinvestor.wordpress.com/": 1, "http://cssanalytics.wordpress.com/": 1, "https://github.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Systematic Investor"}, {"content": ["Today I want to follow up with the Minimum Correlation Algorithm Paper post and show how to incorporate the Minimum Correlation Algorithm into your portfolio construction work flow and also explain why I like the Minimum Correlation Algorithm. \n First, let\u2019s load the ETF\u2019s data set used in the Minimum Correlation Algorithm Paper using the Systematic Investor Toolbox . \n \n###############################################################################\n# Load Systematic Investor Toolbox (SIT)\n# http://systematicinvestor.wordpress.com/systematic-investor-toolbox/\n###############################################################################\nsetInternet2(TRUE)\ncon = gzcon(url('http://www.systematicportfolio.com/sit.gz', 'rb'))\n source(con)\nclose(con)\n\n\t#*****************************************************************\n\t# Load historical data for ETFs\n\t#****************************************************************** \n\tload.packages('quantmod,quadprog')\n\ttickers = spl('SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD')\n\n\tdata <- new.env()\n\tgetSymbols(tickers, src = 'yahoo', from = '1980-01-01', env = data, auto.assign = T)\n\t\tfor(i in ls(data)) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)\t\t\t\t\t\t\t\n\tbt.prep(data, align='keep.all', dates='2002:08::')\n \n Next, I created the portfolio.allocation.helper() function to ease the back-testing of portfolio construction algorithms: \n \n\t#*****************************************************************\n\t# Code Strategies\n\t#****************************************************************** \t\n\t\n\tobj = portfolio.allocation.helper(data$prices, periodicity = 'weeks',\n\t\tmin.risk.fns = list(EW=equal.weight.portfolio,\n\t\t\t\t\t\tRP=risk.parity.portfolio,\n\t\t\t\t\t\tMV=min.var.portfolio,\n\t\t\t\t\t\tMD=max.div.portfolio,\n\t\t\t\t\t\tMC=min.corr.portfolio,\n\t\t\t\t\t\tMC2=min.corr2.portfolio),\n\t\tcustom.stats.fn = 'portfolio.allocation.custom.stats'\n\t) \n\t\n\tmodels = create.strategies(obj, data)$models\n \n Please note that I assigned acronyms to various portfolio allocation algorithms in the code above. \n For example, the Minimum Correlation Algorithm\u2019s acronym is MC and the actual function that does all computations is min.corr.portfolio() function. \n Next let\u2019s create various summary reports to see the performance and allocations of different algorithms: \n \n #*****************************************************************\n # Create Report\n #******************************************************************  \n\t# Plot perfromance\n\tlayout(1:2)\n\tplotbt(models, plotX = T, log = 'y', LeftMargin = 3)\t \t\n\t\tmtext('Cumulative Performance', side = 2, line = 1)\n\t\t\n\tout = plotbt.strategy.sidebyside(models, return.table=T)\n\n\t# Plot time series of components of Composite Diversification Indicator\n\tcdi = custom.composite.diversification.indicator(obj,plot.table = F)\t\n\t\tout = rbind(colMeans(cdi, na.rm=T), out)\n\t\trownames(out)[1] = 'Composite Diversification Indicator(CDI)'\n\t\t\t\t\n\t# Portfolio Turnover for each strategy\n\ty = 100 * sapply(models, compute.turnover, data)\n\t\tout = rbind(y, out)\n\t\trownames(out)[1] = 'Portfolio Turnover'\t\t\n\n\t# Plot and compare strategies across different metrics\n\tperformance.barchart.helper(out, 'Sharpe,Cagr,DVR,MaxDD', c(T,T,T,T))\n\t\n\tperformance.barchart.helper(out, 'Volatility,Portfolio Turnover,Composite Diversification Indicator(CDI)', c(F,F,T))\n\n\t\n\t# Plot transition maps\n\tlayout(1:len(models))\n\tfor(m in names(models)) {\n\t\tplotbt.transition.map(models[[m]]$weight, name=m)\n\t\t\tlegend('topright', legend = m, bty = 'n')\n\t}\n\t\n\t# Plot transition maps for Risk Contributions\n\tdates = index(data$prices)[obj$period.ends]\n\tfor(m in names(models)) {\n\t\tplotbt.transition.map(make.xts(obj$risk.contributions[[m]], dates), \n\t\tname=paste('Risk Contributions',m))\n\t\t\tlegend('topright', legend = m, bty = 'n')\n\t}\n \n  \n  \n  \n  \n  \n  \n Looking at the summary statistics, you mind wonder what makes the the Minimum Correlation Algorithm different and why do I like it? \n The overall characteristics of the Minimum Correlation Algorithm makes it attractive for me. Specifically it has reasonable perfromance, limited draw-down, low turnover and high composite diversification score. The combination of these attractive properties does differentiate the Minimum Correlation Algorithm from others. \n In the next post, I will show the speed benchmarks for the Minimum Correlation Algorithm. \n To view the complete source code for this example, please have a look at the bt.mca.test() function in bt.test.r at github ."], "link": "http://systematicinvestor.wordpress.com/2012/09/24/minimum-correlation-algorithm-example/", "bloglinks": {}, "links": {"http://systematicinvestor.wordpress.com/": 9, "https://github.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Systematic Investor"}]