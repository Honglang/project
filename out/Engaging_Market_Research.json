[{"blogurl": "http://joelcadwell.blogspot.com\n", "blogroll": [], "title": "Engaging Market Research"}, {"content": ["A number of marketing researchers use the orthoplan procedure in SPSS to generate fractional factorial designs. It is not surprising, then, that I received a number of questions concerning the recent article in the Journal of Statistical Software by Hideo Aizaki on \u201cBasic Functions for Supporting an Implementation of Choice Experiments in R.\u201d To summarize their issues, why doesn\u2019t it work like orthoplan in SPSS, and can you answer in 300 words or less? Actually, I added the 300 words or less comment since \u201cmoaning when I started to go into detail\u201d sounds worst. For example, say that you wanted to generate a fractional design for five factors with the following levels: 4x4x3x3x2. The simplest syntax for orthoplan might be \u201corthoplan factors = a(1,2,3,4), b(1,2,3,4), c(1,2,3), d(1,2,3), e(1,2).\u201d And, SPSS would generate 16 different combinations of the orthogonal main-effects design (aka fractional factorial).  So, you go to the above article, copy the example, and change the code to conform to your example.  # Package library(\"support.CEs\")  # Example 1: Unlabeled Design rotation.design(attribute.names = list(  A = 1:4,  B = 1:4,  C = 1:3,  D = 1:3,  E = 1:2), nalternatives = 2, nblocks = 1, row.renames = FALSE, randomize = TRUE, seed = 987) des1  We seem to need more code, but not that bad. However, this code will generate the full factorial design with 288 combinations. What happened to the fractional design with 16 combinations?  It\u2019s a long story about how orthoplan works, but I will get to the bottom line. You should change the two 3-level factors to 4-level factors and rerun the code with the following two changes: C = 1:4 and D = 1:4. You will get your 16 combinations. Now simply recode all the 4\u2019s to 3\u2019s for C and D, and you have your orthoplan-like design in less than 300 words. The designs before and after the coding look like this:     before recoding    after recoding     A   B   C   D   E    A   B   C   D   E   1   4   3   2   1   1    4   3   2   1   1   2   3   2   4   1   2    3   2   3   1   2   3   1   3   4   2   2    1   3   3   2   2   4   4   1   3   2   2    4   1   3   2   2   5   2   4   3   1   2    2   4   3   1   2   6   3   3   3   3   1    3   3   3   3   1   7   1   2   3   4   1    1   2   3   3   1   8   4   4   4   4   1    4   4   3   3   1   9   3   4   1   2   1    3   4   1   2   1   10   4   2   1   3   2    4   2   1   3   2   11   2   2   2   2   1    2   2   2   2   1   12   2   1   4   3   1    2   1   3   3   1   13   1   1   1   1   1    1   1   1   1   1   14   2   3   1   4   2    2   3   1   3   2   15   3   1   2   4   2    3   1   2   3   2   16   1   4   2   3   2    1   4   2   3   2    Why? Read the IBMSPSS algorithm to see how orthoplan works. I prefer this pdf because it is easier to read. Plot spoiler: the above recode is how SPSS does it."], "link": "http://joelcadwell.blogspot.com/feeds/6100199579347751223/comments/default", "bloglinks": {}, "links": {"http://www.springanalytica.com/": 1, "http://publib.ibm.com/": 1, "http://www.jstatsoft.org/": 1}, "blogtitle": "Engaging Market Research"}, {"content": ["Suppose that you accepted my argument from the last two posts on halo effects and bifactor models . As you might recall, I argued that when respondents complete rating scales, they predominating rely on their generalized impression with a more minor role played by the specific features that the ratings were written to measure. Consequently, we see a sizable general factor accounting for a substantial amount of the variation between respondents. This general factor is important because much of human judgment and decision making is based on one\u2019s overall emotional response (approach vs. avoidance). But what if you needed to drill down into the specific features?  A good illustration is the work being done by National Institute of Health (NIH) over the last decade on the Patient Reported Outcome Measurement Information System ( PROMIS ). For example, when measuring the upper extremity physical function of children , they get very specific and ask about the occurrence of everyday activities: undoing Velcro, using a mouse with the computer, buttoning shirts, pouring liquids from a pitcher, and cutting paper with scissors. In order to construct these scales and analyze such data, PROMIS has turned to Item Response Theory (IRT). Neither dichotomous (checklist) nor ordinal (frequency of occurrence) responses can be analyzed with statistical procedures, like factor analysis, as if they were continuous.  OK, so you get the first part of the title, but what about \u201cDeveloping your intuition?\u201d This sounds like it belongs in a self-help book. But I have found that it is very difficult to learn item response theory unless you understand the motivation behind it. Perhaps it is because IRT is not a single statistical model, but a family of increasing complex models and estimation techniques. At times it seems like one is reading an old encyclopedia entry with heading after heading dealing with one more complex topic after another. There are one-parameter, two-parameter, and three-parameter models. Then, there are polytomous items, and nonparametric models, and linear logistic test models, and all the different estimation techniques (including Bayesian IRT). And when you think you got it, someone tells you about multidimensional IRT. But placing measurement on a firm foundation is too important for us to wait. So let\u2019s see if we can outline a framework that makes IRT seem reasonable and within which we can begin to place all the different models and approaches.  Measurement Scales Derived from Relative Comparisons  Harder substances scratch softer substances, as every elementary student is taught. Given an unknown substance, I can evaluate its relative hardness by attempting to scratch it with an ordered set of standards of increasing hardness (e.g., gypsum < quartz < diamond). This is the rationale for the Mohs scale . It yields an ordinal scale because all I know is relative hardness.  How about physical fitness? Could we not identify a set of standardized physical tasks of increasing difficulty and measure a person\u2019s physical fitness as the most difficult task they were able to pass? This is the idea underlying the Guttman scale . If I can order the tasks, then a person will pass every task until they fail and then not pass any subsequent tasks. The only problem is that human behavior is variable, so that it is possible for us to see random variation and inconsistent performance. By replacing the deterministic Guttman scale with a probabilistic response, we can deal with random variation and focus on the likelihood of passing. This is the approach taken by item response theory.  Ultimately, the goal is to get both criterion-reference and norm-referenced measurements . If we include physical tasks that have real world implications (e.g., walking up stairs, lifting heavy luggage into an overhead bin, running for a cab), we will know something about what the person can and cannot do (criterion-referenced). In addition, we have learned where the person places relative to others in the sample (norm-referenced).  But since I am a marketing researcher, perhaps we could substitute brand strength for physical fitness and talk about brand equity or brand health. Specifically, a strong brand or a healthy brand should be able to pass several tests. It should have a favorable image, be in the consideration set during purchase deliberations, be bought, have satisfied customers, and get recommended by its users.  It\u2019s All in the Response Pattern Matrix  I have generated some data consistent with the rank ordering of these five brand tests in order to show you the R code and the output from an item response model. I have provided all the R code needed to generate some data and run the analysis at the end of the post in an appendix. In addition, here is a link to a Journal of Statistical Software article by Dimitris Rizopoulos. He works through all the same code when analyzing five items from the LSAT (Section 3.1). I deliberately created a \u201cmatching\u201d example so that you could see two worked examples from different perspectives. Rizopoulos more fully discusses the code and the output, while I try to \"develop your intuition.\"  First, let us look at the frequency table for 200 respondents who gave Yes/No answers to the five brand strength tests. There are five binary variables, so there are 32 possible response patterns. We only see 21 patterns because the brands tests are not independent. That is, if we had seen all 32 possible combinations with equal or balanced cell frequencies, we would have concluded that the 5 tests were independent and would have stopped our analysis. If it helps, you can think of this response pattern matrix as a 2x2x2x2x2 factorial design or as a contingency table of the same dimensions.     Favorable   Consider   Purchase   Satisfied   Recommend   Percent   Total Score   Latent Score   1   0   0   0   0   0   18.0%   0   -1.07   2   0   0   1   0   0   4.5%   1   -0.50   3   0   1   0   0   0   5.0%   1   -0.50   4   1   0   0   0   0   13.5%   1   -0.50   5   0   1   0   1   0   0.5%   2   -0.08   6   0   1   1   0   0   4.0%   2   -0.08   7   1   0   0   0   1   1.5%   2   -0.08   8   1   0   0   1   0   1.0%   2   -0.08   9   1   0   1   0   0   2.5%   2   -0.08   10   1   1   0   0   0   5.0%   2   -0.08   11   0   1   1   1   0   0.5%   3   0.31   12   1   0   0   1   1   0.5%   3   0.31   13   1   0   1   1   0   3.5%   3   0.31   14   1   1   0   0   1   2.0%   3   0.31   15   1   1   0   1   0   4.0%   3   0.31   16   1   1   1   0   0   6.0%   3   0.31   17   1   0   1   1   1   0.5%   4   0.72   18   1   1   0   1   1   1.5%   4   0.72   19   1   1   1   0   1   3.5%   4   0.72   20   1   1   1   1   0   7.5%   4   0.72   21   1   1   1   1   1   15.0%   5   1.26    Note that the two profiles with the largest percentages of respondents are all No's (the first row with 18.0% or 36 of the 200 respondents) and all Yes's (the last row with15.0% or 30 respondents). This means that 18% received the lowest possible score and cannot be differentiated further without adding another brand strength test (something easier than favorable image, like awareness). Similarly, at the other end of the scale we find 15% with the highest possible score. If we wanted to separate this 15%, we would need to add a more severe test than recommendation (e.g., continue to purchase after major price increase).  Items vary in their difficulty, and as a result, are best at measuring individual difference near their location on the scale. On the one hand, easy items with lots of respondents saying \u201cYes\u201d separate individuals at the lower end of the scale. On the other hand, difficult items with lots of respondents saying \u201cNo\u201d separate individuals at the higher end of the scale.  Before we proceed, we need to assure ourselves that the five tests are all tapping one underlying dimension. Obviously, the tests were selected because we believed that they were all measures of brand strength. Strong brands deliver consistent value that customers are willing to pay for. When selecting the tests, I was thinking of the purchase funnel , a theory about the steps in the purchase process. Brands with favorable images tend to make it into the consideration set, but they are not always purchased. Everyone who buys is not necessarily satisfied with their purchase. Even satisfied customers don\u2019t always recommend. It is this \u201cfunneling process\u201d that makes each of the steps increasingly difficult for the brand to pass.  So, I have a solid theoretical basis for believing that these tests tap the same underlying individual difference dimension. Why the stress on the \u201cindividual difference\u201d modifier? The brand strength that we are concerned with varies over customers. This can be confusing because at times brand strength is measured over several different brands and used as a brand characteristic. Perhaps you need to recall your experimental design class where you studied two types of designs, between-subject and within-subject, and their combination as mixed designs . There are many research areas where we gather lots of data from subjects with the intent to learn something about how the individual operates. In IRT we have the extended Rasch model ( eRm package ), where the items are systematically varied according to a design and the effects of item features estimated. But that is not what we are doing here. We are looking at perceptions of a single brand. Some respondents have a favorable opinion of the brand, and others do not. Brand strength is the dimension that differentiates the favorable respondents from the not favorable respondents.  Now, what about empirical evidence for the unidimensionality of these five brand strength tests? Principal component analysis should help. The first principal component accounts for over 50% of the total variation and is 3.5 times the size of the second principal component. There are more tests that the ltm package will run (e.g. modified parallel analysis), but the magnitude of the first principal component is probably good enough for this example.  We can also get a sense of the data by looking at the above table. It is ordered by difficulty of the item and by the latent trait score associated with each response pattern. The last column is the latent trait score, an index of brand strength associated with each response profile. The adjacent column is the total score calculated as the number of Yes\u2019s to the five brand tests. As one moves from left to right across the table, the number of Yes\u2019s (=1) decreases. The percentage of Yes\u2019s falls from 67.5% for favorable to 54.5% to 47.5% to 34.5% to 24.5% for recommend. As one moves down the table, brand strength increases, as does the number of Yes\u2019s in each row.  Allow me to deal with one side issue. If this were a \u201ctrue\u201d funnel, I would fix the order of the questions from favorable image to recommendation and screen respondents so that no one who did not purchase would be asked about satisfaction. But I wanted to show the type of response patterns that you tend to see in survey data. As you can see in the R code at the end of this post, I generated random data as if this were a checklist with order randomized separately for each respondent and no branching or screening between items. For example, look at row 15 with 4% or 8 respondents. These 8 respondents indicate that they would be satisfied with the brand, but would not purchase it. Is this an inconsistent response pattern? Is it random variation? Or perhaps the brand is too expensive for them? I would be satisfied driving a luxury car (check \u201cYes\u201d under Satisfied), if I could afford it (check \u201cNo\u201d under Purchase). If it is the case that Purchase measures affordability, in addition to brand strength, then we might wish to remove that item from our scale.  They key to achieving an intuitive sense of what an IRT is trying to accomplish is the ability to \u201csee\u201d the relationship between the response pattern matrix and the underlying latent trait. Once you get that, it becomes much easier. Items tap different locations along the latent trait. How do we know that? Fewer respondent give positive responses to more severe tests of the latent trait. You need to \u201cpicture\u201d the response pattern matrix and see the number of 0\u2019s increasing and the number of 1\u2019s decreasing. Look at the above table again. I have used yellow and green coloring so that you get the sense that the green is the valley toward which \u201cYes\u201d responses flow.  But don\u2019t forget that there are two parts: items and persons. Latent traits are dimensions of individual differences. How are respondents separated by their response patterns? Can you see it in the response pattern matrix? What happens as you move down the table? More and more respondents begin to say \u201cYes\u201d to the more stringent tests of brand strength. What does it mean for a respondent to perceive a brand as being a strong brand? They give more Yes\u2019s, true, but lots of respondent say \u201cYes\u201d to the easy tests. Only those respondents with the most positive brand perceptions say \u201cYes\u201d to the hardest tests for a brand to pass.  Output from an Item Response Theory Analysis  If you recall, my intent was to develop your intuition and not review all of IRT. Keeping with that goal, I will only run one type of IRT model and then try to relate the output to the response pattern matrix.  We will use the one of the many IRT packages in R. I selected the latent trait model package, ltm, because it is both comprehensive and relatively easy to use. More importantly, Dimitris Rizopoulos has gone out of his way to provide extensive support, both in articles and in presentations. He has written a lot, and it is all worth reading.  We will run one of the more common IRT models, the two-parameter logistic model. To understand what the two parameters are, we look at the item characteristic curves. These are logistic functions, one for each of the 5 brand tests. Each one shows the likelihood of checking \"Yes\" as a function of the respondent's score on the underlying latent variable, called ability for historical reasons (the first applications were in educational testing).   The curves follow the same ordering that we saw in the response pattern matrix with favorable (1) < consider (2) < purchase (3) < satisfied (4) < recommend (5). They are defined by their location and slope.      Location  Slope   Item 1  Favorable  -0.57  2.36   Item 2  Consider  -0.15  1.98   Item 3  Purchase  0.09  1.65   Item 4  Satisfied  0.46  3.30   Item 5  Recommend  0.83  2.65   The location estimates nicely separate the five brand tests. They indicate the score on the latent variable that would yield a 50-50 chance of saying \"Yes\" to each item. As we noted when we looked at the response pattern matrix, these five brand tests do not do a good job of differentiating respondents at the very top or bottom of the scale. You should remember that 18% gave the lowest possible responses and 15% gave the highest possible responses. We see this in the location parameter that range from only -0.57 to 0.83. We would need to add easier items (location below -.057) and harder items (location above 0.83) in order to differentiate among these two large groups. I should note that I am interpreting these location parameters as if they were z-scores with a mean of 0 and a standard deviation of 1. Although one always needs to check the distribution of latent scores in their particular study, t his is not a bad \u201crule of thumb\u201d when you have many items and a normally distributed latent variable.  The slope parameters indicate how quickly the probability of saying \"Yes\" changes as a function of changes in the latent trait. Item 4 measuring Satisfaction has the steepest slope. In fact, its steepness causes the item curves to overlap, meaning that there is a range of the latent trait where the likelihood of saying Satisfied (4) is greater than the likelihood of saying Purchase (3). This is not a happy result. We would like for the five logistic curves to be parallel (same slope) or at least for them not to overlap. That is, we would like for Satisfaction to be a more severe test of brand strength than Purchase for everyone regardless of their level of brand strength.  Perhaps these differences in slope are due to random variation? What if we constrained the five slopes to equal the same value, would we be able to reproduce our observed response pattern matrix as well as when we allow the slopes to be different? Sounds like a likelihood ratio test using the anova() function, and with four degrees of freedom (i.e., five different slope estimates reduced to one common slope), we find a p value of 0.232. The location estimates change little when the slopes are constrained to be equal.      Location  Slope   Item 1  Favorable  -0.58  2.25   Item 2  Consider  -0.16  2.25   Item 3  Purchase  0.07  2.25   Item 4  Satisfied  0.49  2.25   Item 5  Recommend  0.87  2.25   The common slope seems a reasonable compromise, and the logistic curves are now parallel.   Respondents and Items on the Same Scale  Our goal from the beginning was to find some way to combine the five separate brand strength tests into a single index. Looking at the response pattern matrix, we knew that we had only 21 of the 32 possible combinations of the five dichotomous Yes/No tests. Each of 21 response profiles could have yielded a different latent variable score. Had we rejected the test for the equality of the five slope parameters, we would have had 21 different latent trait scores. This is an important point. When you look back to the response pattern matrix, you see all 21 possible response profiles along with the total score (number of Yes\u2019s) and the latent trait score from the IRT model with all the slopes constrained to be equal.   There are duplicate latent scores for different response patterns because all five tests have equal slopes. In fact, there is a unique latent score for each total score. In this particular case, the relationship between the total score and the latent score appears to be linear. However, this will not generally occur, and the relationship between the observed total score and the unobserved latent score is often not linear.   The slope indicates how well the item is able to discriminant among the respondents. A flat slope tells us that the probability of saying \"Yes\" changes slowly with increases or decreases in the latent variable.  A steep slope shows that the likelihood of \"Yes\" changes quickly over a small interval of the latent trait. The contribution that each item makes to the latent score depends on its slope (see section on scoring in this Wikipedia entry).  Respondents get latent scores, and items get latent scores too. Both items and respondents can be placed on the same scale. Had we more items and had those additional items filled in the gaps on both the lower and upper ends of the scale, our latent scores would looked more like z-scores and would have looked like the distribution of the underlying latent variable.  The table below summarizes these results and shows where the five tests fall in relation to the respondents.       Total Score   Latent Score   Percent   Item Score       0   -1.07   18.0%                -0.58   Favorable     1   -0.50   23.0%                -0.16   Consider     2   -0.08   14.5%                0.07   Purchase     3   0.31   16.5%                0.49   Satisfied     4   0.72   13.0%                0.87   Recommend     5   1.26   15.0%        Conclusions: Going beyond the Mathematics  Individual Difference Dimensions . The dimensions that IRT uncovers are between-person. Different respondents see the same brand differently. We can aggregate respondents and calculate the \"brand strength\" of many different brands. But now the analysis ignores individual differences among the respondents and focuses on the brands. It is important to maintain the distinction between brand-level and individual-level analyses.  Moreover, it is so easy to forget that what differentiates people at the lower end of the scale may not be what differentiates people at the upper end of the scale. Mathematical proficiency is assessed using arithmetic problems in the lower grades and algebra in the higher grades. As we have seen, different brand tests are necessary to measure low and high levels of brand strength.  Response Pattern Matrix . Although we can look item by item, we learn much more about individuals when we examine their pattern of responses across an array of items tapping different portions of the underlying trait. If it helps, think of it as a form of triangulation. We also learn about the items. Sometimes what we learn is that one or more items simply do not belong in the dimension and need to be removed. Often we learn that we have not adequately measured the entire range of our latent construct and need additional items, especially at the upper and lower ends of the scale.  Of course, the response pattern matrix becomes unwieldy as the number of items increase. For example, with 10 items the number of possible combinations is over a thousand and simply too large to be of any help. However, regardless of the IRT model or the number of items, you will be able to interpret the results because you have an intuitive understanding of the connection between the response patterns and the IRT parameters.  Criterion-referenced interpretation . I can do more with your response profile than just locate your performance in comparison to others, although such norm-referenced information is important. If I am careful, I can select brand tests corresponding to milestones that I wish to achieve. Knowing how favorable my brand perceptions are is valuable on its own because it provides diagnostic information in that it might explain why the brand is not considered during purchase. What moves a customer at the low end of the brand strength scale is likely to be different than what moves a customer at the upper end of the same scale.   Appendix: R code to generate data and run ltm  #use orddata package to generate random data  library(orddata) #probabilities for each brand test (location) prob <- list( c(35,65)/100, c(45,55)/100, c(55,45)/100, c(65,35)/100, c(75,25)/100 ) #slope for each logistic curve loadings<-matrix(c( .6, .6, .6, .6, .6), 5, 1, byrow=TRUE) #creates correlation matrix as input cor_matrix<-loadings %*% t(loadings) diag(cor_matrix)<-1 #generates 200 random ordinal observations ord<-rmvord(n = 200, probs = prob, Cor = cor_matrix) #calculates first principal component library(psych) principal(ord,nfactors=1)$value library(ltm) ord<-ord-1 descript(ord) #likelihood ratio test anova(rasch(ord), ltm(ord ~ z1)) #two-parameter logistic model fit<-ltm(ord ~ z1) summary(fit) #item characteristic curves plot(fit) #calculates latent trait scores pattern<-factor.scores(fit) #constrains slopes to be equal fit2<-rasch(ord) plot(fit2) summary(fit2)  scores2<-factor.scores(fit2)"], "link": "http://joelcadwell.blogspot.com/feeds/6449453091836339566/comments/default", "bloglinks": {}, "links": {"http://www.nihpromis.org/": 1, "http://www.jstatsoft.org/": 2, "http://joelcadwell.blogspot.com/": 2, "http://1.blogspot.com/": 1, "http://www.nih.gov/": 1, "http://en.wikipedia.org/": 6, "http://4.blogspot.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Engaging Market Research"}, {"content": ["As promised in Halo Effects and Multicollinearity (my last post), I will show how to run a confirmatory factor analysis in R to test our bifactor model. In addition, I will include a dependent variable and fit a structural equation model to illustrate how the general and specific components in a rating contribute to an outcome such as overall satisfaction.  The Bifactor Model: Exploratory Factor Analysis  I introduced the bifactor model to explain how a rating, even a rating of something relatively concrete, such as the fairness of an airline's ticket prices, reflects both the respondent's specific knowledge of the airline's prices and the respondent's feelings about the airline in general. I noted that this is \"bias\" if your intention was to use the customer as an informant to learn about an airline's actual ticket pricing, but it is \"brand equity\" if instead you wanted to learn about customers' perceptions.  Let's return to our airline satisfaction data and look again at the bifactor model diagram from the previous post (shown below). When providing 12 ratings of their last flight, respondents seem to distinguish, to some extent, between the aircraft's physical features (F1*), the service they received from the staff (F2*), and the ticketing process (F3*). But the greatest impact came from g, the generalized impression or brand affinity.  The factor loadings associated with each path are estimates from an exploratory factor analysis using the omega function in the psych package. Although the factor analysis is exploratory, the bifactor model does impose some structure in its attempt to reproduce the observed 12x12 correlation matrix. Specifically, it is assumed that the ratings were produced by orthogonal latent variables. One latent variable, g, has 12 slots for factor loadings, one for each item. Moreover, we had to specify the number of factors in advance, and these factors were restricted to be uncorrelated (no arcs between circles in the above diagram). As we will see, confirmatory factor analysis frees us to impose more interesting constraints. However, the model still needs to be identified.  Please allow me to make one last point about how factor loadings reproduce observed correlations before we leave the exploratory bifactor model. Looking at the bottom of the above diagram, we see that the correlation between Flight Options and Easy Reservation has two parts. One part is due to impact of g: 0.73*0.62=0.4526. The second part comes from F3*: 0.38*0.34=0.1292. These estimates of the factor loadings are those that most closely reproduce the observed correlation of 0.59 between Flight Options and Easy Reservation. In this case, the observed correlation is 0.59, and the reproduced correlation from the bifactor model is 0.5818 (=0.73*0.62 + 0.38*0.34).  The Bifactor Model: Confirmatory Factor Analysis  We have already seen the major elements of a confirmatory factor analysis. We start with a factor model representing the data generation process, for example, the diagram above. It shows the number of observed variables (boxes) and the number of latent variables (circles). It specifies that the latent variables are uncorrelated (absence of arcs between circles). It indicates that the observed variables are the effects and the latent variables are the causes (arrow pointing from circles to boxes). However, to leave some white space, we have not included the unique variances for each observed variable (one arrow pointing to each box indicating how much variation is unique to that observed variable).  We will use lavaan for a number of reasons but primarily because its syntax mimics the way we just talked about specifying the model.  bifactor <- ' general.factor =~ Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices + Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft + Courtesy + Friendliness + Helpfulness + Service ticketing =~ Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices aircraft =~ Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft service =~ Courtesy + Friendliness + Helpfulness + Service '   The name of the model statement is \"bifactor.\" A single quote starts and ends the model statement. We have four latent variables, each given a name and tied to the observed variables through the =~ operator. Thus, the general.factor lists all 12 observed variables using the + sign to link them together. This type of code should seem familiar to those using formula in R functions like lm().  To run the bifactor model, we add the following two lines:  modelfit <- cfa(bifactor, data=ratings[,1:12]), orthogonal=TRUE) summary(modelfit, fit.measures = TRUE, standardize = TRUE)   \"cfa\" is the function in lavaan for confirmatory factor analysis, and we pass the model statement (bifactor), the observed data set with the first 12 ratings, and a option (orthogonal=TRUE) to keep the latent variables orthogonal.  I will present the factor loadings in matrix format in order to highlight the hierarchical structure of the bifactor model.          g       ticketing       aircraft     service     Easy_Reservation   0.753   0.311     Preferred_Seats   0.697   0.398     Flight_Options   0.618   0.386     Ticket_Prices   0.644   0.436     Seat_Comfort   0.793    0.303    Seat_Roominess   0.725    0.366    Overhead_Storage   0.702    0.435    Clean_Aircraft   0.741    0.337    Courtesy   0.776     0.303   Friendliness   0.756     0.366   Helpfulness   0.835     0.435   Service   0.811     0.337   We find the same pattern that we saw in the exploratory factor analysis. There is a strong \"halo\" or generalized impression in all the ratings, but more so for the items with secondary loadings on service than the items with secondary loadings on ticketing. This is all consistent with our original hypothesis that respondents would base their ratings on brand affinity, if they could, and would not try to retrieve more detailed information about the brand from memory unless they were forced to do so.  Because this is a confirmatory factor analysis, we ought to look and see how well our model fits. Fortunately, David Kenny has updated recently his page on Measuring Model Fit . You will find a complete and succinct discussion of all the different indices (and a comprehensive tutorial on SEM as well). The lavaan package prints several of these tests and goodness-of-fit indices when you ask for a summary.  I have little to add to Kenny's overview, except to suggest that you always look at the residuals before you fail to reject the model. Remember that we are trying to reproduce the observed correlations after imposing the restrictions in our bifactor model. If we were successful, the residual correlations ought to be close to zero. The command, residuals(modelfit, type=\"cor\"), will get us what we want.       Esy_Rs   Prfr_S   Flgh_O   Tckt_P   St_Cmf   St_Rmn   Ovrh_S   Cln_Ar   Cortsy   Frndln   Hlpfln   Servic   Easy_Reservation   0.00              Preferred_Seats   0.00   0.00             Flight_Options   0.01   -0.01   0.00            Ticket_Prices   -0.01   0.01   0.00   0.00           Seat_Comfort   -0.01   0.01   0.00   0.02   0.00          Seat_Roominess   0.00   0.00   0.00   -0.03   0.00   0.00         Overhead_Storage   -0.01   0.01   0.01   0.00   0.00   0.00   0.00        Clean_Aircraft   0.01   0.00   0.00   -0.02   0.00   0.01   0.00   0.00       Courtesy   0.01   -0.02   0.01   0.02   0.00   0.01   0.00   0.00   0.00      Friendliness   0.01   -0.03   0.01   0.01   -0.01   0.00   0.00   0.01   0.01   0.00     Helpfulness   -0.01   0.00   -0.01   -0.01   0.01   0.01   0.00   -0.01   -0.01   0.00   0.00    Service   0.01   0.01   -0.01   0.01   -0.02   0.00   0.00   0.00   -0.01   0.00   0.01   0.00   Although the typeface is small, so are the residual correlations. Of course, you also need to look at the other measures of fit, including the chi-square test and the fit indices. You ought to read Kenny's summary of the controversy surrounding these indices. Because the chi-square test can be significant even when the model seems to fit, it has become common practice to rely on goodness-of-fit indices ranging from 0 (worst) to 1.0 (best). With this data, we find indices in the high 0.90's, which most judge as good or excellent. Thus, I can find nothing in any of these results that suggests that I do not have a reasonable fit for the bifactor model.  The Bifactor Model: Structural Equation Modeling  To summarize what we have done, we took 12 ratings that were highly correlated and \"factored\" them into four independent scores, one general and three specific latent variables. In a previous post , we learned that those 12 ratings were so interconnected that we could not disentangle them and assess their relative contribution to the prediction of outcomes like overall satisfaction, retention, and recommendation. But now we have four uncorrelated latent variables, so our ratings have been untangled. Let's add an endogenous outcome variable and run a structural equation model.  Our first thought might be to group all three outcome variables together as manifestations of the same latent variable. After all, satisfaction, retention, and recommendation are all measures of customer loyalty. The three observed variables do have substantial correlations of 0.63, 0.56, and 0.69. However, when we first looked at the driver analysis for these three outcomes, we ran separate regression equations and found different drivers for each variable (see Network Visualization of Key Driver Analysis ). Therefore, we will run three separate models, one for each outcome variable.  The lavaan code changes very little, which is why the package is so easy to use. Here is the code for Overall Satisfaction.  bifactor <- ' general =~ Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices + Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft + Courtesy + Friendliness + Helpfulness + Service ticketing =~ Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices aircraft =~ Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft service =~ Courtesy + Friendliness + Helpfulness + Service Satisfaction ~ general + ticketing + aircraft + service ' satfit <- sem(bifactor, data=ratings[,c(1:12,13)], orthogonal=TRUE) summary(satfit, fit.measures = TRUE, standardize = TRUE) inspect(satfit, \"rsquare\")  The model statement stays the same, except we have added a line for the regression equation predicting Satisfaction from the four latent variables. In addition, we changed the function from cfa to sem and included rating #13 (Satisfaction) in the data list.  We can see a clear pattern in the structural equation parameters shown below. General impression dominates the equations but becomes less important as we move from satisfaction to retention to recommendation. Although I have not included the standard errors, all the coefficients above 0.10 are significant.        Satisfaction      Fly_Again      Recommend          Standardized Regression Coefficients    g   0.756   0.735   0.670    ticketing   0.063   0.305   0.322    aircraft   0.094   0.227   0.384    service   0.186   0.046   -0.027        R-squared   0.619   0.687   0.701          Percent Contribution to R-squared    g   92%   79%   64%    ticketing   1%   14%   15%    aircraft   1%   7%   21%    service   6%   0%   0%   Because the latent variables are independent, the R-squared is equal to the sum of the squared beta weights. This allows us to calculate the percent contribution that each predictor makes. Service has minimal influence, and then only on satisfaction. Ticketing becomes more important when thinking about flying again with the same airline. The physical features of the aircraft are given more consideration when making a recommendation. Still, there is substantial \"halo\" or \"unattached affect\" impacting all the outcome measures. It is \"as if\" respondents started with brand affinity and then made different \"adjustments\" for each outcome measure (Kahneman's anchoring and adjustment heuristic ).  [If you want to see more R code for running structural equation models using the sem and lavaan packages, including bifactor models, check out Module 9 of Do it yourself Introduction to R .]  Now, do we know the one thing that should be done?  This was the question motivating our original inquiry. Our client wanted to know \"what is the one thing that they should do to improve customer satisfaction, to reduce churn, and to generate positive word of mouth.\" We have come a long way in our understanding of the structure underlying these 15 ratings (12 features + 3 outcomes). We learned how generalized affect or brand affinity has a major impact on every airline perception, including the outcome measures. But we also learned that customers make outcome-specific adjustments: overall satisfaction moves toward service perceptions, retention is more closely associated with ticketing, and recommendation depends more on the perceived physical characteristics of the aircraft.  So what is the one thing that we should do? Having identified g as the dominant driver, we could look to the factor loadings of individual items on g and pick the items having the highest correlations with g. These would be the service-related items, especially Helpfulness. But do we believe that this is a \"causal\" connection? Our bifactor model points the arrow in the opposite direction. Generalized affect \"causes\" variation in self-report measures of Helpfulness. The measurement model is reflective and not formative . The data generation process is straightforward. A respondent is asked to rate Helpfulness. Because they can, they take the easiest path and respond using System I thinking (Kahneman's \"thinking fast\"). They do not reflect about what constitutes helpfulness. They do not try to remember how many and to what extent they have experienced those constituent elements that might fall under the \"helpfulness\" heading. To be honest, it is not clear what the questionnaire writer meant by helpfulness, not to the respondent and probably not to the writer.  If our goal is to learn what interventions work to improve outcomes, we could replace the Helpfulness rating with a checklist of flight attendant behaviors. Did the flight attendant ask if you needed anything? Did they offer to assist you with your carry-on luggage? True, such items are binary and would require an item response or item factor analysis to score. But such items cannot be answered using generalized affect alone. A respondent is forced to \"think slow\" and retrieve specific memories of what did and did not happen. Moreover, the airline now has at least the hope that with an appropriate analysis they can know how each action impacts an outcome variable.  Not Just for Halo Effects  Bifactor modeling is an impressive technique. It starts with a bunch of overlapping variables with considerable shared variation and yields a much smaller number of underlying orthogonal dimensions with relatively clear interpretations. The bifactor model works particularly well with self-reports of human perception because our first response is to \"think fast\" (the general factor) and then, if needed, to \"think slow\" (the specific factors).  My proposal is that we stop asking questions that predominantly measure generalized impressions. However, as I noted above, if we ask about occurrences using a checklist, we can no longer use structural equation models that assume continuous observed variables. While the next version of lavaan will provide support for categorical observed responses, for now, we would need the r package mirt (multidimensional item response theory). It is called \"item response\" because the earliest work was done in educational measurement trying to understand the relationship between the examinee's ability and the likelihood of correctly answering individual test items. Some prefer to call this item factor analysis. We might think of item factor analysis as extending factor analysis with continuous manifest variables to factor analysis with categorical manifest variables. \"Manifest\" is added because we are talking about the observed variables. The r package mirt contains a bfactor() function for dealing with binary variables (e.g., correct/incorrect, yes/no, and did it/did not). An example is an achievement test with several reading passages followed by a small battery of questions for each passage (called a \"testlet\"). This is a bifactor model since all the reading passages are measuring the same general factor, but there is additional covariation due to each specific reading passage.  The mirt function bfactor() also handles ordered categories (e.g., never, sometimes, often, always) that ought not be treated as equal interval continuous scales. For example, a personality psychologist might want to measure a multifaceted construct, like extraversion, using a frequency scale with ordered categories. Extraversion is multifaceted in that it has many possible manifestations, such as, gregarious, assertiveness, warmth, and others. The bifactor model works well with such multifaceted construct because it separates the general from the more specific components (see this link for an example and references).  Conclusions  The large loadings from our general factor suggest that every rating is predominately a measure of brand affinity. It is, as if, we kept asking the same question over and over again, \"How much do you like the brand?\" The bifactor model offers us a means for assessing the extent of such \" exaggerated emotional coherence .\" But the bifactor model does not create the data that we have failed to collect.  Affinity is extremely important to a brand, nevertheless, one cannot design an intervention strategy based on it alone. Marketing efforts to improve satisfaction, retention, and recommendation require us to learn about the likely impact of specific features and services. Even with differentiated measurements, it is difficult to infer what will happen when we do something from what we have observed to be associated in survey data (see Judea Pearl ). Perhaps we can make a start in this direction by moving toward more rigorous measurement standards in marketing research practice."], "link": "http://joelcadwell.blogspot.com/feeds/5375233830592229926/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://www.jstatsoft.org/": 2, "http://joelcadwell.blogspot.com/": 3, "http://davidakenny.net/": 2, "http://www.unt.edu/": 1, "http://onlinelibrary.wiley.com/": 1, "http://en.wikipedia.org/": 1, "http://books.google.com/": 1, "http://ftp.ucla.edu/": 1}, "blogtitle": "Engaging Market Research"}, {"content": ["In the last post, The Relative Importance of Predictors , I showed how difficult it can be to assess the independent contribution that each predictor makes to the overall R-squared when the predictors are highly correlated. We spent some time looking at one example where the predictors were ratings from an airline satisfaction study. As is common in such studies, all the pairwise correlations tended to be sizable and suggested the presence of a strong first principal component, what some might call a halo effect.  Perception is Reality  Unfortunately, the term \"halo effect\" has too often been associated with measurement bias. Of course, it is a bias in the sense that perceptions do not reflect actual behavior, as was noted first by Thorndike in l920. But it does reflect how customers truly feel about the brands they buy and use. Human perceptions are more consistent than behavior, whether it be the person perceptions or brand perceptions. As they say, perception is reality. In his book Thinking, Fast and Slow Daniel Kahneman argues that there are two systems underlying human thinking: a relatively fast, intuitive, and associative System 1 and a slower, more deliberative, and effortful System 2. Then he uses the interplay between these two systems to explain the heuristics and biases that have been uncovered in cognitive psychology (e.g., framing, anchoring, and substitution) and behavioral economics (e.g., prospect theory). \"Exaggerated emotional coherence\" is the term Kahneman uses for the halo effect. It is part of every customer satisfaction rating. It is not simply \"measurement bias\" because it impacts not only the ratings but the actual purchase behavior of real customer in the marketplace. Loyal customers, for example, may well be \"biased\" in that they perceive that their brand delivers more consistent value than it actual does. But that \"bias\" produces revenue for the company and is encouraged at every touch point.  Structural Ambiguity  We are taught that rating items should be written to measure one and only one thing. This is how we avoid ambiguity. For example, we ought to be able to write an item that measures only whether an airline's ticket prices are seen as reasonable and nothing else. But price perception is more complex than looking at ticket prices from different airlines. True, there is a price-sensitive segment going online to flight booking sites and selecting the airline with the lowest price ticket. But what about frequent fliers who go directly to the airline website and so love their reward programs that they discount pricing differences and overlook defects. We cannot assume that respondents know much about the pricing practices of competitive airlines that they never fly. Nor can we ignore the effects of cognitive dissonance that make our memories more positive than our experiences. In the end, we do not have single ratings but a complex associative network of brand beliefs that serve brand purchase and usage, not reality. Perhaps if we looked beyond the individual item to the response patterns across multiple items, we could discover a statistical technique that separates generalized impression from the more specific features of products and services. Let's look again at our airline satisfaction data. [A good, short overview of the dual process model and its effects can be found in the article \" Associative processes in intuitive judgment \" by Morewedge and Kahneman.]  Multifaceted Factor Analysis (Bifactor Model)  If you were to run a web search, please enter the term \"bifactor model.\" You will find references to confirmatory factor analyses (we can use the R package lavaan ), to factor rotations ( GPArotation ), to exploratory factor analysis ( psych ), and to multidimensional item response theory ( mirt ). Historically, the name \"bifactor\" was introduced to distinguish the technique from those seeking a simple structure. A factor structure is simple when every item loads on one and only one factor.  However, I prefer the name \"multifaceted\" because even seemingly unidimensional constructs may be composed of several highly interconnected, but conceptual distinct, facets. What is customer loyalty? It is satisfaction and recommendation and retention and willingness to pay more and so on. Often, we will see satisfaction, retention, and recommendation combined into a single loyalty measure. But, as we saw in my previous post, Network Visualization of Key Driver Analysis , these three measures have different drivers. Hence, customer loyalty is a multifaceted construct - a set of interpretable subscales and a total score with a different meaning. [If you want to read more about the underlying psychometric theory, check out the writing of Steven P. Reise. \" Bifactor Models and Rotations \" is a good place to start.] We will restrict ourselves for now to exploratory factor analyses of the airline satisfaction data. The next post will outline how to run and interpret the confirmatory bifactor model.  The figure below shows a traditional principal axis factor analysis with oblique rotation. The R code using the package psych is shown in the appendix at the end of this post. The boxes are observed ratings, and the circles are latent variables. Each ratings loads on only one factor (simple structure), as indicated by the lack of multiple arrows to any of the boxes. And the factors are correlated as shown by the arcs between the circles. Thus, PA1 is our service factor, and the correlations among the first four boxes are due to the service factor. However, there is also a sizable, but not as large, correlation between Courtesy and Overhead Storage. We can see this by following the path from PA1 to Courtesy (0.71) and the path from Overhead Storage to PA3 (0.88) and the path between PA1 and PA3 (0.8).  One should remember that a factor model is a hypothetical structure with factor loadings and factor correlations estimated in order to reproduce the observed correlation matrix. If the only sizable correlations were among ratings loading on the same factor, the factors would not be correlated. But as we showed in the previous post, all the ratings are correlated with even the smallest pairwise correlation above 0.40. Thus, in order to maintain a simple structure with each rating loading on only one factor, the factors must be correlated. This is the price we pay for simple structure. Why are the factors correlated? Well, if the items are correlated because they all measure the same underlying latent variable, then the factors must be correlated because they all measure the same higher-order underlying construct. And thus, we have a hierarchical factor model, which looks like the following diagram.   The circle with \"g\" is the higher-order factor responsible for the lower-order factor correlations. Originally the letter \"g\" was selected to stand for general intelligence. The oblique and hierarchical factor models are essentially the same, but now the correlations among the first-order factors are due to g rather than being unexplained correlations. Unfortunately, g can be difficult to explain in the hierarchical model because it is restricted to impact the items only through the low-order factors. It might make more sense to have g directly impact the item, but then we would have a bifactor model. So we are ready for our multifaceted bifactor model. First, we abandon simple structure. Our observed ratings are multifaceted. We want them to load on more than one factor. In return we get orthogonal factors. Here is the diagram.  We have generalized impression g contributing to variation in all the observed ratings, more so for Helpfulness than Flight Options. This is a common pattern that I have found repeatedly in customer satisfaction data. The loading for g is an inverse function of the specificity of the rating. We see that here to some extent with g decreasing as we move from Helpfulness and Service to Flight Options and Ticket Prices, although all the items have substantial loadings on g.  In addition to g, we see the more specific feature factors that we found earlier in the oblique and hierarchical models. All the ratings are intercorrelated to some degree because they tap a generalized impression. But, in addition to this baseline correlation, some ratings are even more highly interrelated because they also measure one of the three more specific feature factors: aircraft, service, or ticketing. Finally, there are no arrows or arcs connecting any of the circles because all the latent variables are independent. If the bifactor model \"fits\" the data, then our estimated loadings shown in the above diagram will reproduce the correlation matrix among the ratings without any covariation among the factors. In this particular example, all the items have higher loadings on g than they have on their specific feature factors. This is not unexpected given that all the ratings were highly correlated and the first principal component accounted for 62% of the total variation. Of course, this will not always be the case. In general, when respondents can provide a rating by making a simple inference or association from their generalized impression, they will do so because it is easier than making the effort to retrieve specific memories and then taking the time to combine those memories into a response. [see Norbert Schwarz Cognitive Aspects of Survey Methodology for a review]  Conclusions  I have suggested that the analysis of customer satisfaction data ought to begin with the realization that satisfaction is a multifaceted construct. I am not speaking of an overall satisfaction rating, such as, \"Overall, how satisfied are you with your ____?\" This is an ambiguous question, which is why you get a different response if you ask it before or after a battery of more detailed satisfaction questions. When I speak of generalized impression, I am referring to the general factor that emerges from a factoring of all the rating items in a detailed battery of specific product and service features.  It is the pattern of responses to all the rating items that will tell you something about both the generalized impression and the specific feature components. Generalized impression is not obtained by asking the respondent to make an inference or generate a summary judgment. Instead, it is derived from the correlations among all the satisfaction ratings across the breadth of the customer's interaction with the product or service. On the one hand, customer satisfaction has well-defined components. That is, the product or service can be decomposed into its parts or factors, and each part has its own distinguishable subscale score. On the other hand, customer satisfaction is a separate construct, not simply a summary measure, but its own entity. In the marketing literature, we tend to speak of this view of customer satisfaction as brand attachment or brand relationship. I have argued that the \"halo effect\" or \"exaggerated emotional coherence\" is not measurement bias, but a real entity that can be thought of as an orienting response, a predisposition toward avoidance or approach, or an initial affective response. It deserves its own score. In the bifactor model that score is g. To be clear, I am not arguing that measurement bias does not exist and does not impact the way that a scale might be used. There are cultural and individual differences in scale usage. Scale usage heterogeneity is real, but it cannot explain g in the bifactor model. Halo effects are ubiquitous and robust. You find them with ratings, with rankings, with selection of best or top three, with behaviorally anchored scales, with ordered scales, with categorical scale, with scales created using item response theory, and hopefully you get the point. Method variation is not so robust an effect. In contrast, method variation is what you observe when you try to measure the same trait using different methods and fail to find consistency (e.g., multitrait-multimethod analysis). Method variation is responsible for the fact that an overall satisfaction rating is more highly correlated with a battery of more specific satisfaction ratings when overall satisfaction is asked after the battery than when it is asked before the battery. Method variation is the factor structure that you see when you group your items together in a questionnaire and ask them all at the same time. Order effects, however, are as strong as halo effects. But order effects are not method variation either. They are also due to the interplay of System 1 and 2 thinking (e.g., priming or context effects). And, like the halo effect, order effects impact marketplace decisions. They are not measurement artifacts, but the real thing that companies use to make money by discounting and controlling the retail shelf. No matter how hard I try to resist, that 50% discount looks like a great deal even though I know that they never intended to sell it at its \"original\" price. Similarly, I cannot bring myself to pay the extra money for the national brand when it sets on the shelf next to the discount store brand. Order effects, like halo effects, operate both in the marketplace and in our surveys. Neither should be dismissed for both contain valuable information.  Overview of Next Post  In order to realize the full potential of the bifactor model, we will need a confirmatory factor model. In the next post, I will show how a structural equation model can be used with a battery of satisfaction ratings and outcome measures like overall satisfaction, retention, and recommendation. It is a type of key driver analysis, although the goal is not to identify the most important rating item but to better understand the data generation process. The bifactor model can be fit and the estimates can tells us the relative contribution that is made by the general factor g and the specific product/service factors to the outcome measures.  Appendix with R code to create the three diagrams:  I used William Revelle's psych package to create the diagrams. He has done all the work, all I needed to do was add a main title. This is another must-visit website. Revelle provides a comprehensive introduction to R , an unfinished psychometric book with a number of well-written chapters (see chapter 6 for bifactor), and a detailed application of bifactor models in How Important is the General Factor of Personality?  library(psych) # runs a principal-axis factor analysis (fm=\u201dpa\u201d) # with oblique rotation (rotate=\u201doblimin\u201d) pa<-fa(ratings[,1:12], nfactors=3, rotate=\"oblimin\", fm=\"pa\") # creates the diagram with arrows for factor loadings greater than .3 fa.diagram(pa, cut=.3, digits=2, main=\"Oblique Factor Model)        \") # runs and creates diagram for a hierarchical factor model # sl=FALSE overrides default hier<-omega(ratings[,1:12], nfactors=3, sl=FALSE) omega.diagram(hier, digits=2, main=\"Hierarchical Factor Model\", sl=FALSE) #runs and creates diagram for bifactor model # default is Schmid-Leiman bifactor model bifactor<-omega(ratings[,1:12], nfactors=3) omega.diagram(bifactor, digits=2, main=\"Bifactor Model\")"], "link": "http://joelcadwell.blogspot.com/feeds/1607666709188880168/comments/default", "bloglinks": {}, "links": {"http://www.jstatsoft.org/": 1, "http://joelcadwell.blogspot.com/": 2, "http://lavaan.ugent.be/": 1, "http://personality-project.org/": 3, "http://1.blogspot.com/": 1, "http://deepblue.umich.edu/": 1, "http://www.cmu.edu/": 1, "http://www.nih.gov/": 1, "http://en.wikipedia.org/": 2, "http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://cran.r-project.org/": 3}, "blogtitle": "Engaging Market Research"}, {"content": ["What's the one thing we need to do?  Marketing researchers are asked this question frequently whenever they analyze customer satisfaction data. A company wishing to increase sales or limit churn wants to focus only on the most important determinants of those outcomes. Given the limitations imposed by the available customer survey data, this strategic question is transformed quickly into a methodological one concerning how to assess the relative importance of predictors in a regression equation. The problem is that the predictors are all highly intercorrelated, making the \"one thing\" hard to identify. But search the net for \"customer satisfaction multicollinearity,\" and you will find a number of sites claiming to have found the solution. Let's take a closer look.  The Gold Standard = Rating-Based Conjoint Analysis  The concept of relative importance comes from experimental design where we are able to piece together components any way we want. Consider the typical product design study. Products are bundles of attributes, and attributes are collections of levels. For example, a credit card company may be thinking of how best to configure attributes like interest rate, grace period, fees, customer support, and reward programs. They might want to test several levels of interest rate, different time limits for the grace period, a bunch of different fees, and so on. By systematically varying these attribute levels according to an experimental design, they can generate descriptions of hypothetical products that are presented one at a time to respondents who provide ratings of their preferences for all the product configurations. Each of these attributes is a building block, an independent component that can be manipulated in the design of a credit card, both in the study and in the marketplace. This is an essential point to understand when we look at multiple regression with observational data, where the variables are not independent and not directly manipulated. In a conjoint study relative importance is defined as percentage contribution. Changing the interest rates will impact the preference rating, so will varying the grace period, the fee structure, customer-support options, and the reward program. We sum the effects of all the attributes to get total variation, and then we divide the effect of each attribute by the total variation to get percent contribution. The attribute with the largest percent contribution is where we have the most leverage. If you are familiar with ANOVA, you will recognize the above as the partitioning of the total sum-of-squares into components associated with each factor and a residual within-group. Because the factors are independent, the partitioning is unique. If you are familiar with unbalanced designs where the factors are no longer orthogonal, you will know that there is no longer a unique partitioning of the total sum-of-squares. Percent contribution now depends on the order of entry, which makes interpretation much more difficult.  Warning #1 : Percent contribution is dependent on the variation across the attribute levels. Thus, I can make any attribute more important by increasing the range (e.g., going from a 2% spread between the highest and lowest interest rates to a 5% spread will make interest rate appear more important). I need to be careful about generalizing from specific attribute levels to the attribute in general. That is, we like to make summary statements about the effect of variables in general when we are actually speaking only of the impact of specific values of the variable.  Warning #2: Relative contribution makes sense in a rating-based conjoint analysis where the effects are assumed to be linear or where the attribute levels can be transformed so that the effects are made linear. Choice-based conjoint is not linear, and thus relative contribution is not constant but varies with values of all the predictors.  Warning #3: Finally, we must remember that all our data come from an experimental survey. Behavioral intentions are not marketplace behaviors. Moreover, no matter how hard we try, we cannot mimic the purchase context exactly so that generalization from our survey to the real world might be limited. For example, repeated measures on the same respondent always cause us trouble. Daniel Kahneman warned about this in his Nobel Prize lecture (pp. 473-474). The danger is that we create the effects through obtrusive measurement procedures. Still, after all those warnings, one can see the advantage of an experimental design that decomposes a product into its building blocks and assesses the impact of varying independently each attribute level. Unfortunately, this is not the case with observational studies, as we shall see next.  Relative Importance of Predictors from Observational Studies  R has a package for calculating relative importance. Ulrike Gr\u00f6mping, who maintains the CRAN Task View for Design of Experiments, has written an R package called relaimpo. More importantly, she has a website with references to everything you need to know about relative importance. Dr. Gr\u00f6mping clearly understands the limitations imposed when one uses observational data. She supplies the software along with all the necessary caveats. You should read her description of the package in the Journal of Statistical Software . But also take a look at her article and comment in The American Statistician . Finally, there is a reference for those of you interested in comparing how linear regression and random forest measure variable importance.  Although this package is full of features, the basics are very easy to run. Let's use the data set from an earlier post called Network Visualization of Key Driver Analysis . I will not repeat the analysis from that previous post, but I do want to remind the reader than this was an airline satisfaction study. Respondents were asked to rate their satisfaction with 12 components of their last flight, including ticketing, the aircraft, and customer service. Respondents also indicated their overall satisfaction, their likelihood to recommend, and their willingness to fly again with the airline. As one might expect, these ratings are highly correlated. We can see this clearly below in a correlation plot from the R package psych . The function is cor.plot(), and an example is given on page 15 of the previous link. There appears to be three factors (service, aircraft, and ticketing) as indicated by the by the 12x12 squares embedded in a 3x3 block pattern. The consistent shades of blue throughout the correlation plot suggests the presence of a strong first principal component, which in this data set accounts for almost 62% of total variation.   Most data analysts would assume that the last three ratings were outcomes that would serve as the dependent variables in regression analyses with the 12 more specific ratings as predictors. And this was the analysis that was previously run. Let's review just one of these dependent variables, say overall satisfaction, in more detail. The multiple regression is easy to run using the linear model function in R. First, we standardize the rating scores so that the regression coefficients will be standardized weights. scaled_ratings<-data.frame(scale(ratings)) ols.sat<-lm(Satisfaction~ Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices + Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft + Courtesy + Friendliness + Helpfulness + Service, data=scaled_ratings) summary(ols.sat)  What is the one thing that we ought to do? That is, what is the most important determinant of customer satisfaction? Here are the multiple regression coefficients. The multiple R-squared is 0.59.     Estimate   Std. Error   t value   Pr(>|t|)    (Intercept)   0.00   0.02   0.00   1.00    Easy_Reservation   0.05   0.03   1.61   0.11    Preferred_Seats   0.04   0.03   1.22   0.22    Flight_Options   0.05   0.03   1.98   0.05   *   Ticket_Prices   0.04   0.03   1.47   0.14    Seat_Comfort   0.09   0.03   2.71   0.01   **   Seat_Roominess   0.07   0.03   2.22   0.03   *   Overhead_Storage   0.02   0.03   0.77   0.44    Clean_Aircraft   0.10   0.03   2.97   0.00   **   Courtesy   0.06   0.03   1.91   0.06   .   Friendliness   0.15   0.04   4.13   0.00   ***   Helpfulness   0.13   0.04   3.46   0.00   ***   Service   0.14   0.04   3.75   0.00   ***   Friendliness has the highest coefficient, but Service and Helpfulness have coefficients almost as high and clearly overlap given the sizes of the standard errors. This is actually a convenient result because it would be difficult for the airline to increase customer perceptions of Friendliness without appearing to be Helpful at the same time. Although tongue-in-cheek, this last comment points to the problem of using observational data to make policy recommendations. We cannot directly change customer perceptions of friendliness, and whatever we do to become more friendly will have effects on more than just friendliness. Friendliness is not an independent component that can manipulated like interest rates on a credit card. It helps when interpreting regression coefficients to remember what effect is being measured. It is not the relationship between Friendliness and Satisfaction. That comes from a simple regression of Satisfaction on Friendliness. In this equation the regression coefficient indicates the effect of Friendliness controlling for the other predictors, as if Friendliness were entered last. What is the effect of Friendliness after controlling for Courtesy, Helpfulness, and Service? If all four ratings are reflective indicators of the same underlying latent variable, then we have a serious misspecification.  And what about relative importance? Can relative importance help us identify the one thing that we should do? We can calculate percent contribution when we have an experimental design with independent attributes. Can we calculate something similar when the data are observational and the predictors are correlated? The R package, relaimpo, implements several reasonable procedures from the statistical literature to assign something that looks like a percent contribution to each correlated predictor. We will look at just one of these, an averaging of the sequential sum-of-squares obtained from all possible orderings of the predictors. Gr\u00f6mping calls this \"lmg\" after the authors Lindeman, Merenda, and Gold. Marketing researchers are more familiar with another version of this same metric called Shapley Value Regression. Since we already have the output from our multiple regression above stored in ols.sat, we only need two lines of code. library(relaimpo) calc.relimp(ols.sat, type = c(\"lmg\"), rela = TRUE)  Here are our relative importance or contribution percentages from all possible orderings of the predictors.   Easy_Reservation   7.3%   Preferred_Seats   5.9%   Flight_Options   5.1%   Ticket_Prices   5.3%   Seat_Comfort   9.2%   Seat_Roominess   7.5%   Overhead_Storage   6.3%   Clean_Aircraft   8.4%   Courtesy   9.2%   Friendliness   11.6%   Helpfulness   12.1%   Service   12.1%   Do we now know what is the one thing we need to do? Fortunately, relaimpo automatically prints out some diagnostic information (shown below).     Average coefficients for different model sizes    1X   2Xs   3Xs   4Xs   5Xs   6Xs   7Xs   8Xs   9Xs   10Xs   11Xs   12Xs   Easy_Reservation   0.59   0.36   0.25   0.19   0.15   0.12   0.10   0.09   0.07   0.07   0.06   0.05   Preferred_Seats   0.54   0.30   0.20   0.15   0.11   0.09   0.08   0.06   0.06   0.05   0.04   0.04   Flight_Options   0.50   0.26   0.18   0.13   0.11   0.09   0.08   0.07   0.07   0.06   0.06   0.05   Ticket_Prices   0.52   0.28   0.18   0.13   0.11   0.09   0.07   0.06   0.06   0.05   0.05   0.04   Seat_Comfort   0.63   0.42   0.31   0.25   0.20   0.17   0.15   0.13   0.12   0.11   0.10   0.09   Seat_Roominess   0.59   0.36   0.25   0.19   0.16   0.13   0.11   0.10   0.09   0.08   0.08   0.07   Overhead_Storage   0.56   0.32   0.21   0.15   0.12   0.09   0.07   0.06   0.05   0.04   0.03   0.02   Clean_Aircraft   0.60   0.38   0.28   0.22   0.18   0.16   0.14   0.13   0.12   0.11   0.10   0.10   Courtesy   0.63   0.41   0.30   0.24   0.19   0.16   0.13   0.11   0.10   0.08   0.07   0.06   Friendliness   0.66   0.46   0.36   0.30   0.26   0.23   0.21   0.19   0.18   0.17   0.16   0.15   Helpfulness   0.68   0.50   0.40   0.33   0.28   0.25   0.22   0.19   0.17   0.16   0.14   0.13   Service   0.68   0.49   0.39   0.33   0.28   0.25   0.22   0.20   0.18   0.17   0.15   0.14   We have already calculated the standardized regression coefficients when all 12 predictors are entered into the equation. These coefficients are the last column in the above table (12Xs). Had we entered only one variable at a time? That information is in the first column (1X). Since the standardized regression coefficient for a simple regression with only one predictor is the correlation coefficient, the first column gives us the correlations of each rating with Overall Satisfaction. Moreover, the table provides a complete history of what happens as we add more predictors to the regression equation. What have we learned? Obviously, the coefficients vary as more predictors enter the equation, but the rankings stay relatively constant. All the ratings are correlated with satisfaction, but some more than others. The last three rows are always ranked as first, second, or third, although they trade positions as more variables are added to the equation. The first few rows with the ratings for ticketing are always toward the bottom of the rankings. We find sizable correlations with satisfaction across all the ratings. Some might call this a halo effect (measurement bias), and others would call it brand equity (positive/negative affect toward the airline). Regardless of what you call it, the ratings may be too intertwined to pick a single winner. Relative importance is no solution to multicollinearity. This is what we learned from the previous post. The network visualization (reproduced below) uses a different metaphor, but reaches the similar conclusions.   This picture shows Satisfaction on the far left with edges to the ratings with the highest relative importance. The nodes do not represent separate components that can be manipulated independently. \"Press\" on Friendliness (implement service changes that increase customers perceptions of friendliness), and you will change the ratings of satisfaction and all the other ratings connected to Friendliness. This assumes, of course, that we have a causal relationship. Ultimately, this last point is an issue that every driver analysis needs to address. Seeing is not doing, as Judea Pearl would remind us. Our data are observations, but our goal is intervention. Companies are looking for ways to improve those outcome measures. In fact, they are looking for the one thing that they need to do. Yet, we did not collect the relevant data. We could have asked about specific events under the control of the airlines that would have required respondents to recall what did and did not occur. Instead, we asked for broader summary judgments that can be made by accessing a generalized impression of the flight. No wonder we see such strong correlations among all the ratings and find it so difficult to separate the effects of our predictors."], "link": "http://joelcadwell.blogspot.com/feeds/4755526238793525038/comments/default", "bloglinks": {}, "links": {"http://bayes.ucla.edu/": 1, "http://www.nobelprize.org/": 1, "http://1.blogspot.com/": 1, "http://joelcadwell.blogspot.com/": 1, "http://prof.beuth-hochschule.de/": 1, "http://2.blogspot.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Engaging Market Research"}, {"content": ["Thinking Strategically about Customer Heterogeneity  Ironically, market segmentation, whose motto is \"one size does not fit all,\" seems to rely almost exclusively on one definition of what constitutes a segment. Borrowing its definition from cluster analysis, segments are groupings of objects such that similar objects are gathered together and separated from dissimilar gatherings of other similar objects. \"Birds of a feather flock together\" and stay away from birds of a different feather.  If there were a single feature responsible for the groupings, then we could describe each group by the name of this single feature. But this is a difficult task when there are lots of respondents measured on lots of variables. In fact, just assessing the similarity among respondents becomes difficult when each respondent is described by a long profile of measurements (especially when there is a mixture of quantitative and qualitative variables). In such cases, we describe segments using the average of all the object features that we used to measure similarity. Yet the goal of \"one size does not fit all\" is not to look for clusters. It is to look for heterogeneity among customers, and then exploit those differences by finding opportunities for product and service differentiation. Clustering is one way to structure a large multivariate data set. However, it is not the only way. Early adopters, as they evolve into new segments, are often not uniform or homogeneous in their needs or responses to questionnaire items. Price sensitive customers may never form a compact group but remain a continuum of more or less attraction toward lower costs. Cluster analysis focuses on groupings within the cloud of individual respondents. Archetypal analysis, on the other hand, searches the periphery for concentrations of more extreme individuals. Cluster analysis describes its segments using the \"average\" member as the prototype. Archetypal analysis uses extreme exemplars to describe the frontiers.  Archetypal analysis was introduced in 1993 by Cutler and Breiman. Their first example in this technical report was a question of how many sizes are needed to fit all.  \"For instance, a data set ... consists of 6 head dimensions for 200 Swiss soldiers. The purpose of the data was to help design face masks for the Swiss Army. A natural question is whether there are a few 'pure types' or 'archetypes' of heads such that the 200 heads in the data base are mixtures of the archetypal heads.\" This is a segmentation, but not in the traditional sense of a segment described as the average of all its members. Everyone must have a mask that covers their face. So the mask must be big enough to cover every one's face. But one can still wear a mask that is somewhat larger than their face. Consequently, the pure types must be located at the periphery. Hair stylists also have a lot to say about the shapes of faces. Faces are oblong, round, oval, square, and so on. Should we describe each of these types by its prototype (average) or by its caricature (archetype)? Of course, the answer is \"it depends.\"  If the goal is classification accuracy and the clusters are compact (e.g., a mixture of multivariate normal distributions), then the average is likely to be a good segment descriptor. However, the average performs less well as the clusters become elongated, and it does not serve us well if our goal is a set of contrastive categories to assist in decision making. Let's return to our hair stylist trying to help a client decide how to cut her hair. Here are the face shapes.   These are clear segments needing different hair styles and also different makeup (both are multi-billion dollar a year markets). The round face does not look like the rectangular face, but not because we have a compact cluster of round faces that is well-separated from the rectangular faces. The differences are due to ratios of length and width at different locations along the face. We could easily imagine the round face \"morphing\" into the rectangular because both faces represent pure types on the periphery of the distribution of facial shapes. Actual faces are mixtures of these ideal types. We should note again that pure types or archetypes are a form of contrastive categorization. We define our categories in terms of idealized types in order to magnify the contrast with competing options. The marketer seeking to find and exploit customer heterogeneity is not a scientist searching for the most accurate taxonomy. The customer trying to make sense out of a confusing array of varying products and brands is looking for distinctions that will help them make a purchase. Product differentiation and customer heterogeneity are co-creations, and both marketers and customers are willing to fudge a little in the service of action. Consequently, customers tend to see products and their users as a bit more extreme than they actually are. And marketers see market segments as more distinct and well-separated than the numbers show. This is the nature of human perceptions. Categorization gravitates toward black and white, premium and economy, self and full service, big and small businesses, and a seemingly unending list of dualisms. Of course, there can be more than two categories as in packages with basic, value, and luxury options. But the same tendency for the competing categories to push away from each other can be found so that the segments appear more extreme than they actually are. We have several programs for running K-means and latent class analysis. Any of these will identify well-formed and separated clusters using the average to describe the clusters. R offers the package archetype when we believe that the clusters are best defined by contrastive categories.  Usage Segmentation Comparing K-means and Archetypes  Let's look at an example. We will use a frequency of usage scale to measure how often a respondent uses each of 10 features. The usage scale has 7 categories ranging from 1=never, 3=once a month, 5=once a week, and 7=once a day or more often.  The 10 features can be divided into two groups tapping two distinct types of usage. There are five features tapping the first usage type, which we will call A1, A2, A3, A4, and A5. The other five features are called B1, B2, B3, B4, and B5 because they measure the second usage type. Percentages for 200 respondents on each of the 7-point usage scale values are shown below.    1   2   3   4   5   6   7    A1   61.5%   6.0%   5.5%   6.0%   2.5%   5.5%   13.0%    A2   55.5%   7.0%   4.0%   6.5%   4.0%   3.5%   19.5%    A3   53.0%   7.0%   4.5%   6.0%   6.0%   9.5%   14.0%    A4   43.0%   6.0%   4.5%   6.5%   7.5%   3.5%   29.0%    A5   38.5%   11.5%   7.0%   4.5%   5.0%   18.5%   15.0%    B1   38.5%   10.5%   20.0%   15.5%   11.5%   4.0%   0.0%    B2   23.5%   21.0%   22.0%   15.0%   16.5%   2.0%   0.0%    B3   27.0%   26.5%   10.5%   16.5%   13.5%   6.0%   0.0%    B4   18.0%   24.0%   25.5%   16.5%   10.5%   5.5%   0.0%    B5   10.0%   22.0%   28.0%   21.0%   11.5%   7.5%   0.0%   It is clear that the usage pattern for the A features is different than the usage pattern for the B features. A good percentage of the respondents never use any of the A features. But there is a sizable minority that use the five A features once a day or more often. The B features, on the other hand, tend to show a more symmetric usage pattern, although none of the respondents uses any of the B features every day. We will look at the results of the archetypal analysis first. The goal of archetypal analysis is to identify extreme patterns in the data so that all the observations can be reproduced as mixtures of those extremes. It is easiest to think about what the previous sentence means by thinking of physical measurements. Find the shortest person on basketball team. Find the tallest person on the basketball team. Everyone on the team falls between these two extremes. The taller players are more like the tallest player. The shorter players are more like the shortest player. Any particular player's height could be represented as a mixture of the tallest and shortest players. Thus, one might guess that a 50-50 player had a height about half way between the tallest and shortest. We could add a dimension for weight and talk about the extreme points in a two-dimensional height-weight space. Hopefully, you get the point. Manuel J. A. Eugster, who co-authored and maintains the archetypes package in R, has a paper on Archetypal Athletes that will walk you carefully through both the calculations and examples from basketball and soccer. This is a website well worth visiting. Look for his Journal of Statistical Software article and take the time to review the slides from his talks. Eugster has created a comprehensive and easy to use package with extensive documentation - http://www.statistik.lmu.de/~eugster/ . So what do we get when we analyze our feature usage data? Like a K-means clustering, one needs to specify the number of archetypes to fit, that is, you must rerun the analysis for differing numbers of archetypes and compare the results. Fortunately, the package includes a stepArchetypes function that allows one to run multiple number of archetypes solution as one time and produce a screeplot.   The residual sum-of-squares (RSS) indicates how well the original data can be reproduced as mixtures of the archetypes. That is, the screeplot helps us see how many archetypes are necessary to reproduce our data. Here we have an elbow at three archetypes. So this is where we start. In addition, we will run a K-means clustering with three segments and compare the results.     K1  K2  K3   AA1  AA2  AA3    49   52   99    42   50   108     25%   26%   50%    21%   25%   54%            A1   5.9   1.6   1.3    5.5   3.3   1.0   A2   5.1   3.4   1.5    6.1   3.6   1.0   A3   5.1   3.5   1.5    6.3   3.4   1.0   A4   6.3   5.2   1.3    6.8   5.0   1.0   A5   6.0   3.8   2.0    6.5   4.6   1.0   B1   3.4   2.6   2.3    1.5   5.7   1.0   B2   3.8   2.7   2.5    2.2   5.5   1.2   B3   3.7   2.7   2.5    2.0   5.8   1.1   B4   3.6   2.9   2.7    2.0   5.6   1.4   B5   3.8   3.2   3.0    1.9   6.0   1.9   The means for the three K-means clusters are given in the first three columns. Pick any feature, say B2, and compare the means for the three clusters: K1 > K2 >K3. Regardless of the feature, Cluster 1 has higher usage than Cluster 2, which in turn, has higher usage than Cluster 3. We can see this clearly by plotting the 200 observations using the first two dimensions from an unrotated principal component.  The arrows show the projection of the features onto the principal component space. The first principal component represents intensity, and the second principal component separates usage of the A features (numbered 1-5) from usage of the B features (numbered 6-10). The three clusters form \"bands\" that seem to be more perpendicular to the A features than to the B features. And we can see this in the cluster means with the greatest differences in the first five A features. The rank ordering is the same in the second five B features, but the means are not as different from each other. Now let's return to the table of means to look at the usage profiles for our three archetypes. The first archetype (AA1) defines feature A users, the second archetype (AA2) shows the highest levels of feature B usage along with moderate feature A usage, and the third archetype (AA3) portrays seldom or non-users. Our second archetype is interesting, and their usage pattern would make sense if the B features were more advanced, so that beginning users only used the A features and more experienced users concentrated on the B features but also used the more basic A features less often. Let's take a look at the scatterplot.   Here, as before with K-means, we find the same observations and the same arrows representing the 10 feature usages, but now the numbers indicated which of the three archetypes is the closest to each observation. Although we are not required to force respondents into mutually exclusive segments, there are times when this helps with our interpretation. This is an important point. Archetypal analysis yields graded segment membership. Every respondent is a mixture of different archetypes. The shape of the cloud of individuals in these scatterplots can be called a \"fan-spread\" because feature usage becomes more differentiated as usage level increases (as one moves from left to right along the first dimension). That is, light users are more homogeneous, while more intense users show greater diversity in their feature usage. One might guess that a 4-archetype solution might divide the 2's in the above scatterplot into feature-B-only users and all-feature users. And you would be right as shown below.   Let's finish this discussion by showing the relationship between the archetype and K-mean profiles. Archetypes are located on the edges of the cloud of individual observations. K-means are profiled using averages of segment members so they are pulled toward the center of their clusters. Thus, in the following plot the K-means segments can be found in the interior of the cloud. Moreover, the three K-means segments lie along a line dominated by the first principal component, usage intensity. In contrast, archetypal analysis is able to reveal the underlying fan-spread pattern. Each respondent is triangulated as a mixture of three archetypes representing no usage, feature A usage, and feature B usage.   Conclusions  The reader must be warned that I have deliberately created a data set where archetypal analysis ought to do better than K-means. In my defense, the data set reflects what I see all the time when analyzing usage data. It is a realistic example reflecting how respondents use features in the real world. K-means, which works well with compact and clearly separated clusters, fails to uncover the underlying structure when the data follow a fan-spread pattern. How prevalent is the fan-spread in marketing data? You see it in importance ratings with higher raters showing more differentiation in their ratings. You see it in attitude ratings with greater differentiation in the attitudes of individual with more intensely held beliefs. We could go on with additional examples, but we do not want to limit the usefulness of archetypal analysis to the fan-spread model. Archetypal analysis is an appropriate model for customer heterogeneity whenever the underlying structure is best defined by the extremes. Thus, all contrastive categorization would benefit from archetypal analysis. Contrastive categorization is what we use in decision making: Apple vs. PC, Democrat vs. Republican, low cost vs. premium, big bank vs. local bank, SUV vs. minivan, etc.  We exaggerate differences among products and their users in order to aid us in our marketplace decisions. In fact, the desire to use contrastive categories is so strong in marketing research that even when we use K-means to identify clusters and averages to profile those clusters, our segment names will be embellished to reflect a prototypical member that is more extreme than their segment means indicate. Consequently, a segment of users with moderate price sensitivity will be labeled as price sensitive. Soon we will forget that this segment displays only moderate levels of price sensitivity. The segment label is reified, and the segment members caricatured. Finally, archetypal analysis is worth pursuing whenever you believe that the market is evolving and new segments are forming. Early adopters may not show up in a K-means because they are few and at the periphery of the respondent cloud. However, even small concentrations of users at the edge will reduce the residual sum-of-squares in an archetypal analysis. We have no guarantee that new segments in the process of forming will be discovered, but at least now, we have a technology that provides the ability to explore that frontier.  Appendix Showing the R Code to Reproduce This Analysis #generates 200 observations with the following #category level percentages and correlation matrix library(orddata) prob <- list( c(60,5,5,5,5,5,15)/100, c(55,5,5,5,5,5,20)/100, c(55,5,5,5,5,10,15)/100, c(45,5,5,5,5,5,30)/100, c(40,15,5,5,5,15,15)/100, c(30,20,15,15,15,5)/100, c(25,20,20,15,15,5)/100, c(20,25,20,15,15,5)/100, c(15,25,25,15,15,5)/100, c(10,25,30,15,15,5)/100 ) prob #uses matrix multiplication of factor loadings to create #a correlation matrix with a given pattern loadings<-matrix(c( .4,.6, 0, .4,.6, 0, .4,.6, 0, .4,.6, 0, .4,.6, 0, .4,.0,.6, .4,.0,.6, .4,.0,.6, .4,.0,.6, .4,.0,.6), 10, 3, byrow=TRUE) loadings cor_matrix<-loadings %*% t(loadings) diag(cor_matrix)<-1 cor_matrix ord<-rmvord(n = 200, probs = prob, Cor = cor_matrix) apply(ord,2,table) # these are the commands needed to run the archetypal analysis library(archetypes) aa<-stepArchetypes(ord, k=1:10, nrep=5) screeplot(aa) rss(aa) aa_3<-bestModel(aa[[3]]) round(t(parameters(aa_3)),3) aa_3_profile<-coef(aa_3) aa_3_cluster<-max.col(aa_3_profile) table(aa_3_cluster) #code for K-means kcl_3<-kmeans(ord, 3, nstart=25, iter.max=100) table(kcl_3$cluster) t(kcl_3$centers) #profiles for K-means and archetypes added to original data # as supplementary points in order to map the results in a  # two-dimensional principal component space aa_profile<-parameters(aa_3) kcl_profile<-kcl_3$centers ord2<-rbind(ord,aa_profile,kcl_profile) row.names(ord2)<-NULL #ease package to use with supplementary points library(FactoMineR) pca<-PCA(ord2, ind.sup=201:206, graph=FALSE) #plots K-means plot(pca$ind$coord[,1:2], type=\"n\", xlim=c(-3.9,5.8), ylim=c(-3.9,5.8)) text(pca$ind$coord[,1:2], col=kcl_3_labels, labels=kcl_3_labels) arrows(0, 0, 7*pca$var$coord[,1], 7*pca$var$coord[,2], col = \"chocolate\", angle = 15, length = 0.1) text(7*axes$loadings[,1], 7*axes$loadings[,2], labels=1:10) #plots archetypes plot(pca$ind$coord[,1:2], type=\"n\", xlim=c(-3.9,5.8), ylim=c(-3.9,5.8)) text(pca$ind$coord[,1:2], col=aa_3_cluster, labels=aa_3_cluster) arrows(0, 0, 7*pca$var$coord[,1], 7*pca$var$coord[,2], col = \"chocolate\", angle = 15, length = 0.1) text(7*axes$loadings[,1], 7*axes$loadings[,2], labels=1:10) #plots K-means centroids and archetype profiles plot(pca$ind$coord[,1:2], pch=\".\", cex=4, xlim=c(-3.9,5.8), ylim=c(-3.9,5.8)) text(pca$ind.sup$coord[,1:2], labels=c(\"A1\",\"A2\",\"A3\",\"K1\",\"K2\",\"K3\"), col=c(rep(\"blue\",3),rep(\"red\",3)),cex=1.5)"], "link": "http://joelcadwell.blogspot.com/feeds/26014630312250367/comments/default", "bloglinks": {}, "links": {"http://2.blogspot.com/": 3, "http://3.blogspot.com/": 3, "http://www.lmu.de/": 1}, "blogtitle": "Engaging Market Research"}, {"content": ["Surveys become engaging when they become games, or at least, take on some of the characteristics of games. This is the argument made by those advocating the gamification of marketing research [ http://researchaccess.com/2011/12/market-research-trends-2012-part-one-gamification/ ]. Because it is a new and evolving approach to writing a survey, there is no one data collection technique that can be called gamification. But one method that is getting some traction is the replacement of long series of ratings with a sequence of elimination tournaments. As an example, we would stop asking respondents to rate the importance of 20 or 30 or 50 features using a 5- or 7- or 10-point scale. Instead, respondents would see a sequence of tasks or competitions that have the effect of sorting features into a set of ordered categories. Like all elimination tournaments, the early rounds would remove the least important features so that respondents could concentrate on making finer distinctions among the more important features. All the features losing in the first round would receive the same score = 1. Features removed in the second round would get the second lowest score = 2. And this would continue until the Kth round with the remaining features receiving a score = K. For example, in the first round all the features would be presented and the respondents would be asked which of the features pass a easy test. \"Which, if any, of the following features do you not care about at all? That is, when you are considering what to buy, you would not notice or think about this feature.\" Note that we are asking the respondent to imagine themselves in the marketplace making a product purchase. We want to access the information that customer would use when making an actual purchase. The second round would provide a somewhat more severe marketplace test of the surviving features. \"Which, if any, of the following features would you consider but would not have much of an impact on your purchase? That is, all things being equal, you might prefer a product with this feature but not enough that you would pay more for it or go out of your way to buy it.\" A next obvious question would ask if they would be willing to pay more for a product with this feature. Finally, we might end with the \"deal breaker\" question: \"I would not buy a product without this feature.\" This sequence of increasing difficult tests through which features must pass is not unlike the purchase funnel where potential customers must pass through the stages of awareness, interest, desire and action (AIDA) or awareness, consideration, preference and purchase. Thus, the purchase funnel asks potential customers if they are aware, would they consider, what is their preference, and would they buy. We could have done the same here, asking the complete set of four questions each of the 20 to 50 features. But we made it more of a game with less repetition over features and more competition among features. There are no limits to the creativity of the tournament design. We would expect that feature importance ratings would be adapted to fit the product category so that buying an airline ticket would not be handled the same as buying shampoo or subscribing to an online service. In each case we are attempting to mimic the purchase context as closely as possible and ask about behavioral intent in the marketplace. We do not ask the respondent to infer how important a feature is to them. We ask, instead, what they might do or think when making a purchase and infer feature importance from their behavioral intent. I have provided only one relatively straightforward example in order to illustrate the process. However, you should note that we are not trying to rank order the features. This is not a MaxDiff task where respondents must tell us which feature is most and least important. That is, suppose that a respondent finds none of the features to have any impact on their purchase behavior. We are not forcing respondents to tell us which of the features that they do not care about are more important to them. We must be able to distinguish between causal users who find few features to be important and more committed user who find many features to be important. We are only trying to sort the features into a sequence of ordered categories as if the respondents were rating each feature. But now it's more game-like and thus more engaging.  How do analyze the results of our tournaments?  After our sequence of tournaments, we have an ordinal scale along which features can be arrayed. Features with the highest score are better than features with the second highest score, but we do not know how much better. It might be the case that the last hurdle is much more difficult than any of the previous tournaments so that only the most desirable features make it over. Consequently, getting from a 3 to a 4 may be a much more impressive feat than moving from a 2 to a 3 (i.e., the scale is not equal interval). Nor can we know the properties of our ordinal scale until the data are collected. That is, our sequence of increasing more difficult tests that features need to pass will not be the same in every study. The obstacles to buying are not the same as the obstacles to subscribing. We will use the R statistical language to analyze our ordinal scale. We do not want to assume that we have an equal-interval scale, because we do not. We have a set of ordered categories into which features have been sorted by each respondent. But, neither do we wish to learn the complexities of odds ratios or the different cumulative links models needed to run ordinal regression or item response analysis. A compromise is optimal quantification, a one-step procedure that Jan de Leeuw and Patrick Mair use in their r package called aspect . Intuitively, quantification is a straightforward process. This means that the underlying mathematical requires an advanced degree but the output can be interpreted and used easily by any market research who can read a crosstabs. Let's look at an example, which is far easier than trying to explain in the abstract. The table below shows the results from our tournaments using 20 features and 200 respondents.       % Respondents   Scaled Category Scores    feature   mean   sd   1   2   3   4   1   2   3   4   1   2.15   0.84   29%   28%   44%   0%   -1.48   0.06   0.92    2   2.25   0.98   28%   32%   29%   12%   -1.40   0.00   0.64   1.73   3   2.27   1.06   31%   27%   27%   16%   -1.35   0.04   0.74   1.36   4   2.29   1.12   33%   25%   23%   20%   -1.28   -0.03   0.72   1.33   5   2.31   1.12   30%   33%   16%   23%   -1.27   -0.17   0.76   1.38   6   1.70   0.83   54%   22%   24%   0%   -0.85   0.37   1.56    7   1.57   0.76   59%   25%   17%   0%   -0.79   0.75   1.73    8   1.60   0.66   50%   41%   10%   0%   -0.97   0.77   1.71    9   1.61   0.71   52%   35%   13%   0%   -0.89   0.63   1.86    10   1.66   0.72   49%   37%   15%   0%   -0.88   0.41   1.96    11   2.84   1.09   16%   21%   27%   37%   -1.54   -0.94   0.18   1.08   12   2.96   1.14   16%   21%   17%   47%   -1.63   -0.82   -0.21   0.97   13   2.58   1.13   24%   24%   25%   28%   -1.43   -0.39   0.27   1.29   14   2.74   1.25   27%   15%   18%   42%   -1.30   -0.87   0.26   1.02   15   3.42   0.91   6%   13%   16%   66%   -2.20   -1.66   -0.74   0.68   16   1.70   0.66   42%   48%   11%   0%   -1.11   0.57   1.74    17   2.08   1.00   35%   32%   23%   11%   -1.20   0.11   0.92   1.69   18   1.45   0.50   55%   45%   0%   0%   -0.90   1.11     19   1.60   0.67   51%   39%   11%   0%   -0.94   0.76   1.77    20   1.96   1.08   49%   19%   21%   12%   -0.97   0.29   1.28   1.28   Looking at the means or the percentages, feature #15 is the winner. It is eliminated by only 6% of the respondents in round 1 and almost two-thirds consider it a must-have feature. Feature #18, on the other hand, does not do as well with more than half of the respondents telling us that they would not even notice or think about this feature and the remaining respondents saying it would not have much impact. Feature #18 never makes it to round 3. If all I was concern about was identifying the best feature, then my work is done - Feature 15 is the winner. And who likes Feature 15? Who should be my target market? All I have is the 66% top-box respondents. Is this my target - the top two-thirds?  Perhaps we could narrow our target audience by using the other features. If we looked at all the pairwise crosstabs (not shown here), we find that there is a strong relationship between Feature 12 and Feature 15 with respondents attracted to Feature 12 also attracted to Feature 15. Could we not use Feature 12 with a smaller 47% top-box to help us narrow down our target market? Better yet, could we not look for a factor structure underlying the 20 features and use all the features loading on the same factor as Feature 15?  Since our feature scores are ordinal, we would not want to calculate a Pearson correlation coefficient that assumes equal intervals. Instead, we would need to factor analyze a matrix of polychoric correlations. Or, we could use the R package aspect to quantify the ordinal scales and give us interval-level data that we can use in any analysis. This is what is shown in the last four columns of the above table. So what is a scaled category score? Well, let's see. We would not be surprised to find that all the features were positively related either because different subgroups of features share common attributes within the subgroup (e.g., cost savings) or because respondents who are more involved with the product category tend to want more features and respondents who are less involved tend not to know or want as much from the category (e.g., heavy versus light users). Scaled category scores use this interrelationship to place the ordinal categories along a continuum. Let's look at Feature 15. Everyone wants it. Knowing that a particular respondent is one of the 66% who want Feature 15 is not very informative. We see this in the scaled category scores for Feature 15. A \"4\" is assigned a scale score of 0.68, and every respondent with a \"4\" on Feature 15 has their \"4\" replaced with 0.68. Since these are z-scores, we know that a \"4\" on Feature 15 places any respondent who tells us that Feature 15 is a deal-breaker at 0.68 standard deviation units above the mean. A \"4\" on Feature 12 puts the respondent almost one standard deviation above the mean. In general, the scaled category scores for 4's are highest when there are fewer respondents giving 4's. And a similar pattern can be seen for the 1's. Let us assume that there is an underlying continuum called feature importance or demand. Each feature and each respondent can be placed along this continuum. Demanding customers who want lots of features are located toward the high end. Causal users who are looking for only the most basic features can be found near the low end of this continuum. Features wanted by everyone fall toward the low end because they are \"easier\" tests of demand. Everyone wants these features and to know that you want them too tells me only that you exceed a low demand threshold. Of course, if you are not interested in the most popular features, you must be at the very low end of the scale. On the other hand, features desired by only a few are placed at the upper end of the demand continuum (that is, only the most demanding customers want these features). Thus, quantification uses the network of relationships among the features and among the respondents to calculate both scaled category scores and scaled respondent scores.  Finally, we should remember that these types of scale transformations are not new to data analysis. For example, we use a log transformation to normalize skewed variables such as advertising expenditures. Nonlinear relationships are made linear using power transformations (e.g., squaring X). In this case, we are seeking transformations of the ordinal categories (originally coded as 1, 2, 3, and 4) so that the features have the highest possible correlations. We have optimizes the sum of all the correlations by replacing the original score (the number of the round in which the feature was eliminated) with the new scaled category scores.  Conclusions  We are done. We have transformed our ordinal categories obtained by entering the features into a sequence of increasingly more difficult tournaments. We now have a new data set with individual respondent data, which can be analyzed using all our usual statistical techniques. Feature importance or demand scores can be factor analyzed to determine if some features are more highly interrelated and form groups of features with common characteristics (e.g., save money). Individual respondent scores can be clustered to determine if there are groups of customers seeking the same features. Regression analyses can be run to predict any outcome measure of interest from the feature importance scores. Moreover, we have engaged respondents to think about the features, not in the abstract without any referent, but within the actual purchase context. We have kept the game realistic so that our findings can be generalized to the marketplace. Appendix: R code needed to reproduce the analysis # generates random ordinal data with the following marginal probabilities # and the correlations specified by the bifactor loadings library(orddata) prob <- list( c(30,30,40)/100, c(30,30,30,10)/100, c(30,25,30,15)/100, c(30,25,25,20)/100, c(30,30,20,20)/100, c(60,20,20)/100, c(60,25,15)/100, c(60,30,10)/100, c(55,35,10)/100, c(55,30,15)/100, c(15,20,25,40)/100, c(15,15,20,50)/100, c(25,20,25,30)/100, c(25,15,20,40)/100, c(5,10,15,70)/100, c(45,40,15)/100, c(35,35,20,10)/100, c(55,45)/100, c(55,35,10)/100, c(50,20,20,10)/100 ) prob loadings<-matrix(c( .5,.5, 0, 0, 0, .5,.5, 0, 0, 0, .5,.5, 0, 0, 0, .5,.5, 0, 0, 0, .5,.5, 0, 0, 0, .5,.0,.5, 0, 0, .5,.0,.5, 0, 0, .5,.0,.5, 0, 0, .5,.0,.5, 0, 0, .5,.0,.5, 0, 0, .5,.0, 0,.5, 0, .5,.0, 0,.5, 0, .5,.0, 0,.5, 0, .5,.0, 0,.5, 0, .5,.0, 0,.5, 0, .5,.0, 0, 0,.5, .5,.0, 0, 0,.5, .5,.0, 0, 0,.5, .5,.0, 0, 0,.5, .5,.0, 0, 0,.5), 20, 5, byrow=TRUE) loadings cor_matrix<-loadings %*% t(loadings) diag(cor_matrix)<-1 cor_matrix ord<-rmvord(n = 200, probs = prob, Cor = cor_matrix) # loads the aspect package and runs the optimal quantificiation library(aspect) rescaled<-corAspect(ord, level=\"ordinal\") summary(rescaled) scores<-rescaled$scoremat"], "link": "http://joelcadwell.blogspot.com/feeds/6015575507541307606/comments/default", "bloglinks": {}, "links": {"http://researchaccess.com/": 1}, "blogtitle": "Engaging Market Research"}, {"content": ["Whatever happened to those evaluations that your airline asked you to complete after taking a flight? They ask you for a number of ratings about buying your ticket, attributes of the plane, the service you received, and if you were satisfied, if you would recommend, and if you would fly again.   The airline is certainly concerned about tracking changes in these ratings over time. But they might also be interested in increasing customer loyalty (i.e., satisfaction, recommendation, and repeat purchase). For the later, the airline might request a \"key driver analysis.\" The term \"driver analysis\" is used because the airline is looking for a marketing strategy that will increase loyalty. The word \"key\" is used because the airline wants to find the drivers with the biggest impact. \"What is the one thing that we could do to increase customer loyalty?\"   Here is one answer -- a network visualization of the correlations among all the ratings. It can be produced using the R statistical programming language in one line of code. I claim that clients find the network engaging. That is, anyone can look at the figure below and quickly see the interrelationships among the ratings and what is driving the different manifestations of customer loyalty. It is an easy picture to understand and helps clients to think strategically. Let's see if I can support that claim.           What it is? It is a mapping of the correlations among 15 ratings with the colors added to show ratings with highest mutual intercorrelations. The nodes are the ratings. The lines are the correlations. Here we only show lines for correlations above a specified cutoff value.   The greater the correlation between two variables, the thicker the line will be. So, the aqua or light blue nodes are interconnected by thicker lines because they are all highly correlated with each other. You can think of this as a customer service component. The green circles refer to the aircraft seating and cleanliness. The ticketing process is represented by the red circle, although their lines are less thick, suggesting a less cohesive component than customer service. Finally, the outcomes associated with customer loyalty are shown in purple.   Pressure Points = Driver Analysis    Say the airline asks you how they could increase customer satisfaction. You find the Satisfaction node on the left hand side, and you look for thick lines leading to it. There are several pathways to increased customer satisfaction. For example, all four customer service ratings have sizeable paths. If we were able to improve customer perceptions of friendliness (apply pressure to the Friendliness node), the effect would spread along its path to Satisfaction. The improvement in perceived Friendliness would also spread to perceptions of Courtesy, Service, and Helpfulness since these are also connected. In fact, if the airline were to make changes that impacted all four service components at the same time (apply pressure simultaneously to four nodes), they could possibly see even greater improvement. Perhaps we should not be asking \u201cwhat is the one thing that will most increase customer loyalty,\u201d but what is the one area where we should concentrate our efforts.   Moreover, we can see that the \"drivers\" of repeat purchase (Fly Again) are different from the drivers of customer satisfaction. Otherwise, Fly Again would be positioned closer to Satisfaction. In fact, the network visualization makes it obvious that the key drivers change as one moves from Satisfaction to Recommendation to Fly Again.    Comparison to More Traditional Key Driver Analyses    Multiple regression is the most common form of key driver analysis. How did our network map perform relative to regression analysis? Here are the standardized regression coefficients from three separate regressions of Satisfaction, Recommend, and Fly Again on all 12 predictors.       Standardized Regression Weights     Sat   Recommend   Fly Again   (Intercept)   0.00   0.00   0.00   Easy Reservation   0.05   0.16   0.12    Preferred Seats   0.04   0.15   0.14    Flight Options   0.05   0.11   0.14    Ticket Prices   0.04   0.06   0.10    Seat Comfort   0.09   0.09   0.03   Seat Roominess   0.07   0.17   0.10    Overhead Storage   0.02   0.19   0.16    Clean Aircraft   0.10   0.15   0.09    Courtesy   0.06   0.00   -0.01   Friendliness   0.15   0.00   -0.01   Helpfulness   0.13   -0.02   0.10    Service   0.14   -0.09   0.05        R-Squared  0.59  0.61  0.63    These coefficients are consistent with what we learned from the network. The largest weights for satisfaction come from the customer service components, while Recommend and Fly Again are influenced more by ticketing and cabin characteristics.   Warning, it's caveat time.  The data are observational. We do not know the causal connections among these ratings. Does friendliness impact satisfaction? Or does satisfaction make it less likely that customers will give lower friendliness ratings?     Appendix      I have used an R package called qgraph to produce the network visualization. You call the package using the command library(qgraph) and create the network graph using the following code:   gr<-list(1:4,5:8,9:12,13:15)  qgraph(cor(ratings),layout=\"spring\", groups=gr, labels=names(ratings), label.scale=FALSE, minimum=0.50)   The 15 ratings are located in a data frame called ratings. The map uses the correlation matrix among the 15 ratings as the proximity matrix to create the network. We have asked for the \u201cspring\u201d layout, which has the effect of placing more highly correlated variables near each other and away from less or negatively correlated variables.  The author is Sacha Epskamp . He works with the PsychoSystems Project. Either of the following two links will take you to web sites that will explain qgraph and the network visualization approach in greater depth: http://sachaepskamp.com/ or http://www.psychosystems.org/ .   Libraries like qgraph are one of the great strengths of the R programming language. The PsychoSystems Project is a group of programmers and researchers attempting to introduce a new metaphor for understanding the basis of psychological measurement. You should use either link to learn about their work. However, for us what is important is that Sacha Epskamp has spent a considerable amount of time and work to create a single line of code that generates exactly the type of graph that one would want if they needed to show the interrelationships among a set of ratings.   Finally, the ratings data set was randomly generated using a specific factor model. It was not essential to our discussion for the reader to know this because the simulated data set mimics the structure that underlies most of the satisfaction data sets one finds in marketing research. I have seen this structure over and over again from customer satisfaction surveys across markets, including my research with the airlines. However, in order to reproduce the analysis shown in this posting, you will need to run the necessary R code. I have listed everything you will need below.   R Code to Generate the Simulated Data and Run All Analyses # The goal is to show all the R code that you would need # to reproduce everything that has been reported. # We will use the mvtnorm package in order to randomly # generate a data set with a given correlation pattern. # First, we create a matrix of factor loadings. # This pattern is called bifactor because it has a # general factor with loadings from all the items # and specific factors for separate components. # The outcome variables are also formed as # combinations of these general and specific factors. loadings <- matrix(c ( .33, .58, .00, .00, # Ease of Making Reservation .35, .55, .00, .00, # Availability of Preferred Seats .30, .52, .00, .00, # Variety of Flight Options .40, .50, .00, .00, # Ticket Prices .50, .00, .55, .00, # Seat Comfort .41, .00, .51, .00, # Roominess of Seat Area .45, .00, .57, .00, # Availability of Overhead Storage .32, .00, .54, .00, # Cleanliness of Aircraft .35, .00, .00, .50, # Courtesy .38, .00, .00, .57, # Friendliness .60, .00, .00, .50, # Helpfulness .52, .00, .00, .58, # Service .43, .10, .20, .30, # Overall Satisfaction .35, .50, .40, .20, # Purchase Intention .25, .50, .50, .00), # Willingness to Recommend nrow=15,ncol=4, byrow=TRUE) # Matrix multiplication produces the correlation matrix, # except for the diagonal. cor_matrix<-loadings %*% t(loadings) # Diagonal set to ones. diag(cor_matrix)<-1 library(mvtnorm) N=1000 set.seed(7654321) #needed in order to reproduce the same data each time std_ratings<-as.data.frame(rmvnorm(N, sigma=cor_matrix)) # Creates a mixture of two data sets: # first 50 observations assinged uniformly lower scores. ratings<-data.frame(matrix(rep(0,15000),nrow=1000)) ratings[1:50,]<-std_ratings[1:50,]*2 ratings[51:1000,]<-std_ratings[51:1000,]*2+7.0 # Ratings given different means ratings[1]<-ratings[1]+2.2 ratings[2]<-ratings[2]+0.6 ratings[3]<-ratings[3]+0.3 ratings[4]<-ratings[4]+0.0 ratings[5]<-ratings[5]+1.5 ratings[6]<-ratings[6]+1.0 ratings[7]<-ratings[7]+0.5 ratings[8]<-ratings[8]+1.5 ratings[9]<-ratings[9]+2.4 ratings[10]<-ratings[10]+2.2 ratings[11]<-ratings[11]+2.1 ratings[12]<-ratings[12]+2.0 ratings[13]<-ratings[13]+1.5 ratings[14]<-ratings[14]+1.0 ratings[15]<-ratings[15]+0.5 # Truncates Scale to be between 1 and 9 ratings[ratings>9]<-9 ratings[ratings<1]<-1 # Rounds to single digit. ratings<-round(ratings,0) # Assigns names to the variables in the data frame called ratings names(ratings)=c( \"Easy_Reservation\", \"Preferred_Seats\", \"Flight_Options\", \"Ticket_Prices\", \"Seat_Comfort\", \"Seat_Roominess\", \"Overhead_Storage\", \"Clean_Aircraft\", \"Courtesy\", \"Friendliness\", \"Helpfulness\", \"Service\", \"Satisfaction\", \"Fly_Again\", \"Recommend\") # Calls qgraph package to run Network Map library(qgraph) # creates grouping of variables to be assigned different colors. gr<-list(1:4,5:8,9:12,13:15) qgraph(cor(ratings),layout=\"spring\", groups=gr, labels=names(ratings), label.scale=FALSE, minimum=0.50) # Calculates z-scores so that regression analysis will yield # standardized regression weights scaled_ratings<-data.frame(scale(ratings)) ols.sat<-lm(Satisfaction~Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices + Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft + Courtesy + Friendliness + Helpfulness + Service, data=scaled_ratings) summary(ols.sat) ols.rec<-lm(Recommend ~ Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices + Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft + Courtesy + Friendliness + Helpfulness + Service, data=scaled_ratings) summary(ols.rec) ols.fly<-lm(Fly_Again ~ Easy_Reservation + Preferred_Seats + Flight_Options + Ticket_Prices + Seat_Comfort + Seat_Roominess + Overhead_Storage + Clean_Aircraft + Courtesy + Friendliness + Helpfulness + Service, data=scaled_ratings) summary(ols.fly)"], "link": "http://joelcadwell.blogspot.com/feeds/6141953043749241591/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://sachaepskamp.com/": 1, "http://www.psychosystems.org/": 1}, "blogtitle": "Engaging Market Research"}]