[{"blogurl": "http://www.sas-programming.com\n", "blogroll": [], "title": "SAS Programming for Data Mining"}, {"content": ["Loss Given Default (LGD) is a key business metric of risk in financial service. One unique feature of this metric is overdispersion and the other is multi-mode. \n \nFinite mixture model is an effective way to accommodate both. Multi-mode refers to the case where the distribution has more than one peak. Over-dispersion refers to the case where the dispersion of the data is more than what the assumed model implies. [3] provides a good introduction to finite mixture models. \n \nConsider again the loss given default case discussed in the paper of Matt Flynn [1]. In this section, a complete modeling process is presented using finite mixture model that leverage the computing capability of PROC FMM in SAS v9.3 to model the variance in data that is not captured in a unified single model. Notice that because the dependent variable is bounded between 0 and 1, there are several options for modeling purpose. One can use a beta model, as discussed above; or a logit transformation can be applied first to the dependent variable (lgd) and a normal distribution is usually assumed to model the data. In this exhibition, the second approach is used. \n \nFirst of all, a complete Exploratory Data Analysis (EDA) is conducted to study the distribution and relationship between and among dependent variable and covariates. Here, EDA consists of two exercises. At step one, the distribution and pairwise relationship of variables are studied. Then, non-parametric models are applied to each covariate to study the possible functional forms between dependent variable and independent variables. \n \nThe first figure shows the distribution of logit transformed Loss Given Default data. Apparently, it mimics normal distribution much closer than the raw distribution, but still it skews towards left rather than symmetric. Figure 2 below shows a matrix view of pairwise relationship between pairs of the four variables in the data \n \n \n \n \nFigure 1. Histogram of Logit Transformed Loss Given Default Variable \n \n \n \n \nFigure 2. Matrix Scatter Plot of All Variables. \n \n \n \nIn addition to a simple descriptive analysis using raw variables, it is often useful to conduct a non-parametric univariate regression so that the possible functional relationship between dependent variable and each covariate can be studied. Figure 3 shows the LOESS regression results of logit transformed LGD against the three covariates, namely, the annual average default rate, leverage coefficient by companies and industrial average default rate. It is noticeable that there is slightly quadratic relationship between logit(LGD) and average default rate by year and by industry. The linear relationship between logit(LGD) and leverage coefficient, however, is robust. On the other hand, we suspect that there is over-dispersion that can\u2019t be captured adequately by these variables. \n \n \n \n \n \n \n \n \n \nFigure 3. LOESS regression exhibition LGD against average default rate by year, leverage coefficient by firm and industrial average default rate. \n \nIn order to examine the sample and their relationship closer, an OLS model is built with both average default rates by year and by firm are modeled using quadratic relationship and leverage coefficient by firm is modeled as a linear term. \n \n \n \n \nFigure 4. Diagnostics from polynomial OLS \n \nMajor model fitting diagnostic results are shown in figure 4. Key observations are: \n \n1. The fit is reasonably well, with residuals approximately normally distributed. \n \n2. There is noticeable over-dispersion when the residuals are plotted against predicted values. The variance is smaller at higher predicted values but larger at lower range. \n \n3. Q-Q plot also indicates existence of over-dispersion. \n \nAs an alternative approach to a polynomial OLS, a finite mixture model is proposed. As a demonstration, we don\u2019t go through the complete modeling process, especially the testing process to determine appropriate number of latent components; instead, a 2-component finite mixture model is used while only linear terms of the three covariates will enter the model. The purpose of this design is to show that even with a simpler and hence easier to interpret individual model structure, a finite mixture model is able to come up a better model that captures major stochastic effects in the data. \n \nThe following code builds a 2-component mixture model and examines the distribution of residuals. \n \nods html;\nods graphics on;\nproc fmm data=lgddata2 plots(unpack);\n  model lgd2 = lev lgd_a i_def /k=2 ;\n  output out=fmm_out pred resid(component) resid(overall);\nrun;quit;\nods graphics off;\nproc sgplot data=fmm_out;\n  density resid_1 /type=kernel;\n  density resid_2 /type=kernel;\n  histogram resid ;\n  density resid /type=normal;\nrun;\n \n \n \n \n \n \nFigure 5. Distribution of residuals from a 2-component normal mixture model \n \n \nThere are two key features from PROC FMM that worth mention. First, the mixture of distributions can be from different families. For example, a normal distribution can be mixed with a T-distribution. Second, the probability each data point belongs to a latent class can be modeled with covariates, so to enhance the model interpretability. \n \nThe following code demonstrates a 2-component mixture model with one component is modeled as normally distributed while the other one is modeled as T-distributed. \n \n \n/* mixture of heterogeneous distributions */ \nods graphics on; \nproc fmm data=lgddata2; \nmodel lgd2 = lev lgd_a i_def /dist=normal;\n  model  + lev lgd_a i_def /dist=t ;\n  output out = fmm_out2 pred resid(component) resid(overall);\nrun; \nods graphics off; \n \nThe idea is simple. Outline a group of individual models of common distribution family in one MODEL statement, and the other groups of individual models of the same distribution family in other MODEL statement, without specifying the dependent variable but instead a \u2018+\u2019 symbol to indicate it is one layer add-up to existing individual models. Each group of individual models can be a mixture of multiple components. For example, in above code, if \u2018K=2\u2019 is specified in one of the MODEL statement, say in the first MODEL statement, then it tells SAS to model the data as a mixture of 2-component normal distribution and 1-component T distribution. This capability of modeling data with heterogeneous mixture distributions is a powerful tool in predictive modeling and advanced analytics. Of course, in this particular sample data, there is no immediate benefit by using a heterogeneous mixture distribution. \n \nThe second feature that gives PROC FMM extreme power in advanced analytics is the capability to model probability model of latent classes using covariates, so that analysts are able to study which factors will contribute to classifying data points into different latent classes. This feature gives analyst the power to interpret latent class with insightful factors and even provide better model fit. What an analyst needs to do is to add the following statement: \n \n \nPROBMODEL &covars; \n \n \nThe probability model can be one of the following four: LOGITISTIC, PROBIT, LOG-LOG, and COMPLEMENTARY LOG-LOG. Please refer to SAS/STAT manual [2] for details. \n \nReference: \n[1]. Matt Flynn, http://www.casact.org/education/spring/2011/handouts/C11-Flynn.pdf \n[2]. SAS Institute, Inc., SAS/STAT v9.3 User's Guide \n[3]. Geoffrey McLachlan, David Peel, Finite Mixture Models, John Wiley & Sons, 2000 \nComplete Code:\n \n \ndata lgddata;\n informat lgd lev 12.9 lgd_a 6.4 i_def 4.3;\n input lgd lev lgd_a i_def;\n label lgd = 'Real loss given default'\n  lev = 'Leverage coefficient by firm'\n  lgd_a = 'Mean default rate by year'\n  i_def = 'Mean default rate by industry';\n cards;\n0.747573451 0.413989786 0.6261 1.415\n0.99 0.413989786 0.6849 1.415\n0.06581075 0.230361142 0.4566 1.183\n0.351287992 0.541339309 0.6261 2.353\n0.25844921 0.541339309 0.4566 2.353\n0.01968009 0.812 0.6715 0.743\n0.931035513 0.546732229 0.6715 2.353\n0.341254925 0.71 0.6715 1.183\n0.35075456 0.855339361 0.6715 2.353\n0.045826764 0.313983237 0.6261 0.743\n0.025754193 0.190648237 0.4566 0\n0.759732568 0.490953756 0.6261 2.353\n0.757989999 0.910788759 0.6261 1.415\n0.6 0.336071518 0.6261 1.183\n0.374480256 0.414862374 0.4566 0.967\n0.168726407 0.612063995 0.6261 1.183\n0.283909643 0.693928717 0.6261 2.353\n0.747018382 0.937072431 0.6261 1.415\n0.686300059 0.801162532 0.6715 2.353\n0.050051313 0.365725066 0.4566 0\n/* More data is not shown here to save space */\n;\nrun;\n\n/* dep var is between (0, 1), using logit transformation */\ndata lgddata2;\n  set lgddata;\n lgd2=log(lgd/(1-lgd));\nrun;\n\n/* check raw dep var distribution */\nproc sgplot data=lgddata2;\n  histogram lgd2;\n density lgd2/type=normal;\nrun;\n\n/* check relationship between raw dep and covariates */\nproc sgscatter data=lgddata2;\n  matrix lgd2 lev lgd_a i_def;\nrun;\n\n\n/* study the relationship */\nods graphics on;\nproc loess data=lgddata2;\n  model lgd2 = lev;\nrun;\n\nproc loess data=lgddata2;\n  model lgd2 = lgd_a;\nrun;\n\nproc loess data=lgddata2;\n  model lgd2 = i_def;\nrun;\nods graphics off;\n\ndata lgddata2;\n  set lgddata2;\n lgd_a2=lgd_a**2;\n i_def2=i_def**2/10;\nrun;\n\n/* build baseline model */\nods graphics on;\nproc reg data=lgddata2;\n  model lgd2 = lev lgd_a i_def lgd_a2 i_def2;\n output out=reg_out pred resid;\nrun;quit;\nods graphics off;\n\n\nods html;\nods graphics on;\nproc fmm data=lgddata2 plots(unpack);\n  model lgd2 = lev lgd_a i_def /k=2 ;\n *probmodel lev lgd_a i_def;\n output out=fmm_out pred resid(component) resid(overall);\n *bayes;\nrun;quit;\nods graphics off;\n\nproc sgplot data=fmm_out;\n  density resid_1 /type=kernel;\n density resid_2 /type=kernel;\n *density resid_3 /type=kernel;\n histogram resid ;\n density  resid /type=normal;\nrun;\n\n\n/* mixture of heterogeneous distributions */\nods graphics on;\nproc fmm data=lgddata2;\n  model lgd2 = lev lgd_a i_def /dist=normal;\n model  + lev lgd_a i_def /dist=t ;\n output out=fmm_out2 pred resid(component) resid(overall);\n PROBMODEL &covars;\nrun;\nods graphics off;\n\n\nproc sgplot data=fmm_out2;\n  density resid_1 /type=kernel;\n density resid_2 /type=kernel;\n *density resid_3 /type=kernel;\n histogram resid ;\n density  resid /type=normal;\nrun;"], "link": "http://www.sas-programming.com/feeds/7983410932353094623/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 2, "http://1.blogspot.com/": 1, "http://www.casact.org/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Call center management is both Arts and Sciences. While driving moral and setting up strategies is more about Arts, staffing and servicing level configuration based on call load is in the domain of Sciences. \n \nThe science part of call center management is based on Queueing Theory, which studies \"the Phenomena of standing, waiting and serving\" [1], [2].The queueing model abstracts the complex phenomena of call center acticities into an idealized mathematical framework, so that it is possible to study the key characteristics and dynamics of the real world. There are many different models that could be applied to Call Center management, one simple yet popular model is called Erlang-C model. Some other highly related models are Erlang-B and Erlang-A models, but they won't be covered in this post. \n \nIn this article, Erlang-C model will be briefly described and the computation of key parameters of this model will be discussed, followed by a complete suite of SAS code pieces. It is interesting that Erlang-C model is rarely touched in the SAS community. Searching through SAS-L archives, there were only two discussions relevant, and they are set apart by 10 years [3], [4] ! The responders includes legendary David Cassell, SAS guru Daniel J. Nordlund, SAS computing expert Rick Wicklin ( his SAS blog ), and of course me. \n \nErlang-C model is a commonly used mathematical framework for many Call center staffing and management softwares. Based on Kendall notation, this model can be expressed as M/M/C/\\inf, where the first M indicates a Poisson distribution for Arrival process, while the second M indicates an Exponential distribution for servicing time. Both 'M' refers markovian the 'C' indicates number of servers which is a non-negative integer, while the last \\inf refers infinite number of places in the system. Erlang-C system has extra assumption that it is First In First Out (FIFO) with infinite calling population. Therefore, Erlang-C model describe a Call Center that serves customers in the order they arrive in, while customers arrives following a Poisson process and the servicing time is Exponentially distributed. When a customer is not immediately served, it is put on hold until next available server (CSR). The customers are assumed to be patient so that they won't drop out but instead wait how-long-it-takes until next CSR. The system can take infinite number of waiting customers. \n \nIn Erlang-C model, the traffic to a Call Center is measured in a so-called term of Offered Traffic Load, which is defined to be the ratio between the parameter \\lambda of Poisson process and the parameter \\mu of Exponential servicing time. Offered Traffic Load is therefore interpreted as the average number of services that are trying to happen simultaneously. It is free of time unit, but does has its own unit, called Erlangs. \n \nAt the core of Erlang-C model is the Erlang-C function. The Erlang C function computes the probability that an arriving customer in the Erlang C queueing model will find that all servers are busy. This is the same as the fraction of arriving customers that are delayed (i.e., must wait) before beginning service. \n \nErlang-C function is expressed as: \n \n  P_c(n, x) = dPoisson(n, x)/(dPoisson(n, x) - (1-x/n)*pPoisson(n, x)) \n \nwhere dPoisson(n, x) is the density function of Poisson distribution with support 0,1, .., n and shape parameter x. pPoisson(n, x) is the corresponding cumulation distribution. \n \nIn Erlang-C function, there are three key elements: \n \n 1. Probability of delay, P_c. \n 2. # of servers, n. \n 3. Offered Traffic Load, x. \n \nGiven any two, the last parameter can be calculated or approximated. For example, in the code pieces below: \n \n 1. Function ErlcFractionDelay(Nsrv, traffErlang) calculates Probability of delay given number of servers and offered traffic Load in Erlangs units. \n \n 2. Function ErlcNsrv(traffErlang, Erlc_max) estimates the minimum number servers to reach below the max probability of delay given by Erlc_max for a given offered traffic load of traffErlang. \n \n 3. Function ErlcTrafFromFractionDelay(Nsrv, fractionDelay) Compute the max traffic load that Nserv servers can handle with queuing probability less or equal to fractionDelay. \n \nIn addition to the three key numbers in Erlang-C model, there are some other paremeters of particular interests to Call Center management. For example, the probability an arriving call will be put in queue for no more than certain waiting time given the average handling time. Function ErlcGoS(Nsrv, traffErlang, AHT, AWT) will calcualte this quantity. \n \nThe results of this set of functions has been calibrated against the data from [5], specifically Table 1. of [5] is reproduced exactly. The DATA STEP at the end of the program below will reproduce the results, and here is the result my own run. \n \n \n \n \n \nFor a complete list of quantities that Erlang-C model will work with and compute for, the webpage of Abstract Micro Systems @ http://abstractmicro.com/erlang/helppages/mod-c.htm provides a very good introduction. \n \n Reference: \n[1]. Kleinrock, Leonard (January 1975). Queueing Systems: Volume I \u2013 Theory . New York: Wiley Interscience. ISBN 978-0471491101. \n \n[2]. Kleinrock, Leonard (April 1976). Queueing Systems: Volume II \u2013 Computer Applications . New York: Wiley Interscience. ISBN 978-0471491118. \n \n[3]. http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0210C&L=sas-l&P=R10763 \n \n[4]. http://www.listserv.uga.edu/cgi-bin/wa?A2=ind1206C&L=sas-l&P=R45472 \n \n[5]. Tibor Mi\u0161uth, Erik Chrom\u00fd, Ivan Baro\u0148\u00e1k. Method for Fast Estimation of Contact Centre Parameters Using Erlang-C Model , in Proceedings of 2010 Third International Conference on Communication Theory, Reliability, and Quality of Service, DOI 10.1109/CTRQ.2010.38 \n \nAll functions below have relatively detailed comments. The functions are provided as is without any implied and explicated warranties, and are for research purpose only. \n \n \n\nproc fcmp outlib=sasuser.funcs.Erlc; \n\n  function ErlcFractionDelay(Nsrv, traffErlang); \n /*\n  Entry  : ErlcFractionDelay\n  Purpose  : Compute the Probability of Delay when an arriving call is not\n     answered immediately and put in queue\n  Usage  : P=ErlcFractionDelay(Nsrv, traffErlang)\n  Inputs  : Nsrv=Number of Server (CSR)\n     traffErlang=Traffic in Erlang\n  Handle Error : Return -1 if traffic in Erlang is larger than the number of \n       servers;\n     Return -2 if either traffic in Erlang or Number of servers is \n       negative;\n     Return -3 if number of servers is not integer;       \n */\n   if traffErlang>Nsrv then do;\n   *put \"Erlang amount should be smaller than Number of Servers\";\n Ec=-1; return(Ec);\n   end; \n else if (traffErlang<0) or (Nsrv<0) then do;\n   *put \"Both Erlang amount and Number of Servers should be larger than 0\";\n Ec=-2; return(Ec);\n end;\n else if abs(int(Nsrv)-Nsrv)>1E-8 then do;\n   *put \"Number of Servers should be integer\";\n Ec=-3; return(Ec);\n end; \n   else do; \n   _p= PDF('POISSON', Nsrv, traffErlang);\n   denom = _p\n     + (1-traffErlang/Nsrv)*CDF('POISSON', Nsrv-1, traffErlang);\n   Ec = _p / denom;\n   return (Ec);\n end;\n  endsub;\n\n function ErlcGoS(Nsrv, traffErlang, AHT, AWT);\n /*\n  Entry  : ErlcGoS\n  Purpose  : Compute the Probability an arriving call will be put in\n     queue and wait for no more than AWT when the average\n     handling time is AHT. This term is also called Grade of \n     Service (GoS). Note that the probability that an\n     arriving call will NOT be put in queue is:\n      P=1-ErlcFractionDelay();\n  Usage  : GoS=ErlcGoS(Nsrv, traffErlang, AHT, AWT)\n  Inputs  : Nsrv=Number of Server (CSR)\n     traffErlang=Traffic in Erlang\n     AHT=Average Handling Time\n     AWT=Average Waiting Time\n  Handle Error : Return -1 if traffic in Erlang is larger than the number of \n       servers;\n     Return -2 if either traffic in Erlang or Number of servers is \n       negative;\n     Return -3 if number of servers is not integer;  \n      Return -4 if either of two time parameters is negative; \n */\n   if traffErlang>Nsrv then do;\n   *put \"Erlang amount should be smaller than Number of Servers\";\n  GoS=-1; return(GoS);\n   end; \n else if (traffErlang<0) or (Nsrv<0) then do;\n   *put \"Both Erlang amount and Number of Servers should be larger than 0\";\n GoS=-2; return(GoS);\n end;\n else if abs(int(Nsrv)-Nsrv)>1E-8 then do;\n   *put \"Number of Servers should be integer\";\n GoS=-3; return(GoS);\n end; \n   else if AHT<0 or AWT<0 then do;\n   put \"Both Average Holding Time and Average Waiting Time should be >0\"; \n GoS=-4; return(GoS);\n end;\n else do;\n  GoS = 1-ErlcFractionDelay(Nsrv, traffErlang)*exp(-1*(Nsrv-traffErlang)*AWT/AHT);\n  return(GoS);\n end;\n endsub;\n\n function ErlcNsrv(traffErlang, Erlc_max);\n /*\n  Entry  : ErlcNsrv\n  Purpose  : Compute the required number of servers (CSRs) to handle\n     given traffic load with queuing probability no more than\n     Erlc_max\n  Usage  : N=ErlcNsrv(traffErlang, Erlc_max)\n  Inputs  : traffErlang=Traffic load in Erlang\n     Erlc_max=max Queuing probability\n  Handle Error : Return -1 if max queuing probability is not within (0, 1) \n       range so to violate probability definition;\n     Return -2 if traffic in Erlang is negative;\n */\n\n /*REF:\n  Algorithm is based on that from paper \"Method for Fast \n    Estimation of Contact Centre Parameters Using \n    Erlang C Model\" by Tibor Mi\u0161uth, Erik Chrom\u00fd \n    and Ivan Baron\u00e1k from Slovak University of \n    Technology. This paper appeared at \"The Third \n    International Conference on Communication \n    Theory, Reliability, and Quality of Service\", \n    2010\n */\n  if Erlc_max>=1 or Erlc_max<=0 then do;\n  *put \"Max Erlang-C value should be between 0 and 1\";\n  N=-1; return(N);\n  end;\n  else if traffErlang<0 then do;\n    *put \"Traffic in Erlang should be >0\";\n  N=-2; return(N);\n  end;\n  else do;\n   _k=floor(traffErlang)+1; s=0;\n   do N=1 to _k; \n  s=(1+s)*N/traffErlang;\n    end;\n    _threshold=(1-Erlc_max)/((1-traffErlang/N)*Erlc_max);\n    do while (s<=_threshold);\n     N=N+1;\n  s=(1+s)*N/traffErlang;\n  _threshold=(1-Erlc_max)/((1-traffErlang/N)*Erlc_max);\n  end;\n    return(N);\n  end;\n endsub;\n\n function ErlcNsrvFromGoS( traffErlang, AHT, AWT, GoS);\n /*\n  Entry  : ErlcNsrvFromGoS\n  Purpose  : Compute the required number of servers (CSRs) to handle\n     given traffic load with given service level (GoS):\n       Average Handling Time /\n       the Probability an arriving call is put in queue\n       and wait no more than Average Waiting Time\n  Usage  : N=ErlcNsrvFromGoS( traffErlang, AHT, AWT, GoS)\n  Inputs  : traffErlang=Traffic load in Erlang\n     AHT=Average Handling Time\n     AWT=Average Waiting Time\n     GoS=probability an arriving Call in put in queue\n      but wait no more than AWT\n  Handle Error : Return -1 if either time parameter is negative;\n     Return -2 if queuing probability is not in (0, 1) range;\n     Return -3 if traffic in Erlang is negative\n */\n   if AHT<0 or AWT<0 then do;\n   *put \"Both Average Holding Time and Average Waiting Time should be >0\"; \n  N0=-1; return(N0);\n end; \n else if GoS<0 or GoS>1 then do;\n  *put \"GoS should be between 0 and 1\";\n N0=-2; return(N0);\n end;\n else if traffErlang<0 then do;\n   *put \"Traffic in Erlang must be positive real number\";\n N0=-3; return(N0);\n end;\n   else do; \n   N0=floor(traffErlang)+1;\n   _GoS=ErlcGoS(N0, traffErlang, AHT, AWT);\n   do while (_GoS<=GoS);\n   N0=N0+1;\n   _GoS=ErlcGoS(N0, traffErlang, AHT, AWT);\n   end;\n   return(N0);\n end;\n endsub;\n\n function ErlcNsrvFromWait( traffErlang, AHT, AWT);\n /*\n  Entry  : ErlcNsrvFromWait\n  Purpose  : Compute the required number of servers (CSRs) to handle\n     given traffic load with given Average Handling Time and\n     no more than given Average Waiting Time\n  Usage  : N=ErlcNsrvFromWait( traffErlang, AHT, AWT)\n  Inputs  : traffErlang=Traffic load in Erlang\n     AHT=Average Handling Time\n     AWT=Average Waiting Time     \n  Handle Error : Return -1 if either time parameter is negative;     \n     Return -2 if traffic in Erlang is negative\n */\n   if AHT<0 or AWT<0 then do;\n   *put \"Both Average Holding Time and Average Waiting Time should be >0\"; \n Nsrv=-1; return(Nsrv);\n end;\n else if traffErlang<0 then do;\n   *put \"Traffic in Erlang must be positive real number\";\n N0=-2; return(N0);\n end;\n else do;\n   Nsrv=floor(traffErlang)+1;\n   c=ErlcWait (Nsrv, traffErlang, AHT);\n   do while(c>AWT);\n     Nsrv=Nsrv+1;\n   c=ErlcWait (Nsrv, traffErlang, AHT);;\n   end;\n   return (Nsrv);\n end;\n endsub;\n\n function ErlcNwaiting(Nsrv, traffErlang);\n /*\n  Entry  : ErlcNwaiting\n  Purpose  : Compute the average number of calls waiting \n     in queue, given number of servers and traffic\n     in Erlang\n  Usage  : N=ErlcNwaiting(Nsrv, traffErlang)\n  Inputs  : Nsrv=Number of Server\n     traffErlang=Traffic load in Erlang  \n  Handle Error : Return -1 if traffic in Erlang is larger than number\n        of servers;     \n     Return -2 if number of servers is negative\n     Return -3 if traffic in Erlang is negative\n     Return -4 if number of servers is not integer\n */\n   if traffErlang>Nsrv then do;\n   *put \"Erlang amount should be smaller than Number of Servers\";\n Q=-1; return(Q);\n   end; \n else if (Nsrv<0) then do;\n   *put \"Both Erlang amount and Number of Servers should be larger than 0\";\n Q=-2; return(Q);\n end;\n else if traffErlang<0 then do;\n   *put \"Traffic in Erlang must be positive real number\";\n Q=-3; return(Q);\n end;\n else if abs(int(Nsrv)-Nsrv)>1E-8 then do;\n   *put \"Number of Servers should be integer\";\n Q=-4; return(Q);\n end; \n   else do; \n   Q=traffErlang/(Nsrv-traffErlang)*ErlcFractionDelay(Nsrv, traffErlang);\n  return(Q);\n end;\n endsub;\n\n function ErlcTrafFromFractionDelay(Nsrv, fractionDelay);\n /*\n  Entry  : ErlcTrafFromFractionDelay\n  Purpose  : Compute the max traffic load that N servers\n     can handle with queuing probability less or equal\n     to fractionDelay. \n  Usage  : A=ErlcTrafFromFractionDelay(Nsrv, fractionDelay)\n  Inputs  : Nsrv=Number of Server\n     fractionDelay=Probability of waiting in queue\n         when a call arrives     \n  Handle Error : Return -1 if probability of waiting in Queue is not\n        in range of (0, 1) \n     Return -2 if number of servers is negative     \n     Return -3 if number of servers is not integer\n */\n /* NOTE:\n   Using Bisection Method to solve for Erlang \n  given N server and Erlang C number. Bisection \n  is simple and robust. Speed is not of primiary\n  concern here.\n */\n   if fractionDelay>1 or fractionDelay<0 then do;\n    *put \"Number of servers must be positive integers\";\n c=-1; return(c);\n end;\n else if Nsrv<=0 then do; \n    *put \"Fraction of Delay must be real number between 0 and 1\";\n c=-2; return(c);\n end;\n else if abs(int(Nsrv)-Nsrv)>1E-8 then do;;\n  c=-3; return(c);\n end;\n else do;\n    min=1E-8; max=Nsrv-1E-8;\n    itermax=500; iter=1;\n    do while (iter<=itermax);\n     c=(min+max)/2;\n    _x=ErlcFractionDelay(Nsrv, c)-fractionDelay;\n    if abs(_x)<1E-8 or abs(max-min)/2<1E-8 then do;     \n    iter=itermax+1;\n    end;\n    else do;\n      iter=iter+1;\n  _tmp=ErlcFractionDelay(Nsrv, max)-fractionDelay;\n    if sign(_x)=sign(_tmp) then max=c;\n      else min=c;\n    end;\n   end;  \n  return (c);\n end;\n endsub;\n\n function ErlcWait (Nsrv, traffErlang, AHT);\n /*\n  Entry  : ErlcWait\n  Purpose  : Compute the Average Speed of Answer (ASA, also called\n     Average Waiting Time, AWT), given the number of servers, \n     traffic in Erlang and average handling time. \n  Usage  : N=ErlcWait(Nsrv, traffErlang, AHT)\n  Inputs  : Nsrv=Number of Server\n     traffErlang=Traffic load in Erlang  \n     AHT=Average Handling Time\n  Handle Error : Return -1 if Average Handling Time is negative   \n     Return -2 if number of servers is negative\n     Return -3 if traffic in Erlang is negative\n     Return -4 if number of servers is not integer\n     Return -5 if traffic in Erlang is larger than the\n       number of servers\n */\n  if AHT<0 then do;\n   *put \"Average Holding Time should be >0\"; \n  ASA=-1; return(ASA);\n end;\n else if (Nsrv<0) then do;\n   *put \"Both Erlang amount and Number of Servers should be larger than 0\";\n ASA=-2; return(ASA);\n end;\n else if traffErlang<0 then do;\n   *put \"Traffic in Erlang must be positive real number\";\n ASA=-3; return(ASA);\n end;\n else if abs(int(Nsrv)-Nsrv)>1E-8 then do;\n   *put \"Number of Servers should be integer\";\n ASA=-4; return(ASA);\n end; \n else if traffErlang>Nsrv then do;\n  ASA=-5; return(ASA);\n end; \n else do;\n   ASA=ErlcFractionDelay(Nsrv, traffErlang)*AHT/(Nsrv-traffErlang);\n  return(ASA);\n end;\n endsub;\n\n function ErlcK (Nsrv, traffErlang);\n /*\n  Entry  : ErlcK\n  Purpose  : Compute the average number of calls in queue, given \n     number of servers and traffic in Erlang. It is the \n     sum of average number of calls being served (traffErlang) \n     and average number of calls waiting in queue.\n  Usage  : N=ErlcK(Nsrv, traffErlang)\n  Inputs  : Nsrv=Number of Server\n     traffErlang=Traffic load in Erlang  \n  Handle Error : Return -1 if traffic in Erlang is larger than number\n        of servers;     \n     Return -2 if number of servers is negative\n     Return -3 if traffic in Erlang is negative\n     Return -4 if number of servers is not integer\n */\n  if traffErlang>Nsrv then do;\n   *put \"Erlang amount should be smaller than Number of Servers\";\n K=-1; return(K);\n   end; \n  else if (Nsrv<0) then do;\n   *put \"Both Erlang amount and Number of Servers should be larger than 0\";\n K=-2; return(K);\n end;\n else if traffErlang<0 then do;\n   *put \"Traffic in Erlang must be positive real number\";\n K=-3; return(K);\n end;\n else if abs(int(Nsrv)-Nsrv)>1E-8 then do;\n   *put \"Number of Servers should be integer\";\n K=-4; return(K);\n end; \n else do;\n   K = traffErlang*(1+ ErlcFractionDelay(Nsrv, traffErlang)/(Nsrv-traffErlang));\n  return (K);\n end;\n endsub;\n\n function ErlcT( Nsrv, traffErlang, AHT);\n /*\n  Entry  : ErlcT\n  Purpose  : Compute the average time a call will spend in the system,\n     which is the sum of average handling time and average \n      waiting time (ASA). Three relavent parameters are\n     number of servers, traffic in Erlang and average handling\n     time. \n  Usage  : N=ErlcT(Nsrv, traffErlang, AHT)\n  Inputs  : Nsrv=Number of Server\n     traffErlang=Traffic load in Erlang \n      AHT=Average Handling Time \n  Handle Error : Return -1 if average handling time is negative;     \n     Return -2 if number of servers is negative\n     Return -3 if traffic in Erlang is negative\n     Return -4 if number of servers is not integer\n     Return -5 if traffic in Erlang is larger than number \n       of servers\n */\n  if AHT<0 then do;\n   *put \"Average Holding Time should be >0\"; \n  T=-1; return(T);\n end;\n else if (Nsrv<0) then do;\n   *put \"Both Erlang amount and Number of Servers should be larger than 0\";\n T=-2; return(T);\n end;\n else if traffErlang<0 then do;\n   *put \"Traffic in Erlang must be positive real number\";\n T=-3; return(T);\n end;\n else if abs(int(Nsrv)-Nsrv)>1E-8 then do;\n   *put \"Number of Servers should be integer\";\n T=-4; return(T);\n end; \n else if traffErlang>Nsrv then do;\n  T=-5; return(T);\n end;\n   else do; \n   T=AHT + AHT*ErlcFractionDelay(Nsrv, traffErlang)/((Nsrv-traffErlang));\n  return(T);\n end;\n endsub;\n\nrun;\noptions cmplib=sasuser.funcs;\n\n\ndata _null_;\n  lambda=667; AHT=150; AWT=20;\n A=lambda*150/3600;\n \n do j=1 to 10;\n  N=floor(A)+j;\n  Pc=ErlcFractionDelay(N, A);\n GoS=ErlcGoS(N, A, AHT, AWT); \n  Q=ErlcNwaiting(N, A);\n  T=ErlcT(N, A, AHT); \n K=ErlcK(N, A);\n rho=A/N; \n put N= @8 Pc= percent.4 @16 K= bestd8.1 @26 T= bestd8.1 \n    @38 Q= bestd8.1 @50 Gos= bestd8.1 @62 rho= percent8.4;\n end;\nrun;"], "link": "http://www.sas-programming.com/feeds/1806703496617507180/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://abstractmicro.com/": 1, "http://www.uga.edu/": 2, "http://2.blogspot.com/": 1, "http://blogs.sas.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Test the Stochastic Gradient Decending Logistic Regression in SAS. The logic and code follows the code piece of Ravi Varadhan, Ph.D from this discussion of R Help. The blog SAS Die Hard also has a post about SGD Logistic Regression in SAS. \n \n \n \n \n\nfilename foo url \"http://www.biostat.jhsph.edu/~ririzarr/Teaching/754/lbw.dat\" ;\n\ndata temp;\n infile foo length=len;\n input low age lwt race smoke ptl ht ui ftv bwt;\n put low age lwt race smoke ptl ht ui ftv bwt;\n if _n_>1;\nrun;\nproc standard data=temp out=temp mean=0 std=1;\n  var age lwt smoke ht ui;\nrun;\n\nproc contents data=temp out=vars(keep=varnum name type) noprint; run;\n\nproc sql noprint;\n  select name into :covars separated by \" \"\n from vars\n where lowcase(name) in (\"age\", \"lwt\", \"smoke\", \"ht\", \"ui\")\n ;\n select cats(\"b_\", name) into :covars2 separated by \" \"\n from vars\n where lowcase(name) in (\"age\", \"lwt\", \"smoke\", \"ht\", \"ui\")\n ; \n  select count(*)+1 into :nparms\n  from vars\n where lowcase(name) in (\"age\", \"lwt\", \"smoke\", \"ht\", \"ui\")\n ;\nquit;\n%put &covars2;\n\n%lr_sgd(temp, beta, low, &covars, \n  alpha=0.01, decay=0.98, \n  criterion=0.00001, maxiter=1000);\n\n\noptions fullstimer;\nproc logistic data=temp outest=_beta desc noprint;\n  model low = age lwt smoke ht ui;\nrun;\n\n \n \nExecution log shows: \n****************************************************************************************** \n606 \n \n \n607 %lr_sgd(temp, beta, low, &covars, alpha=0.01, decay=0.98, criterion=0.00001, maxiter=1000); \n \nMLOGIC(LR_SGD): Beginning execution. \n \nMLOGIC(LR_SGD): Parameter DSN has value temp \n \nMLOGIC(LR_SGD): Parameter OUTEST has value beta \n \nMLOGIC(LR_SGD): Parameter RESPONSE has value low \n \nMLOGIC(LR_SGD): Parameter COVARS has value age ht lwt smoke ui \n \nMLOGIC(LR_SGD): Parameter ALPHA has value 0.01 \n \nMLOGIC(LR_SGD): Parameter DECAY has value 0.98 \n \nMLOGIC(LR_SGD): Parameter CRITERION has value 0.00001 \n \nMLOGIC(LR_SGD): Parameter MAXITER has value 1000 \n \nMPRINT(LR_SGD): options nosource nonotes; \n \nMPRINT(LR_SGD): options nomlogic nomprint; \n \nIteration 1, time used 0.046, converge criteria is 0.355 \n \nIteration 2, time used 0.032, converge criteria is 0.1886732527 \n \nIteration 3, time used 0.14, converge criteria is 0.110486903 \n \nIteration 4, time used 0.031, converge criteria is 0.0694367942 \n \nIteration 5, time used 0.031, converge criteria is 0.0458381768 \n \nIteration 6, time used 0.032, converge criteria is 0.0313858459 \n \nIteration 7, time used 0.031, converge criteria is 0.0221141453 \n \nIteration 8, time used 0.047, converge criteria is 0.0159488055 \n \nIteration 9, time used 0.031, converge criteria is 0.0125402184 \n \nIteration 10, time used 0.031, converge criteria is 0.0102548103 \n \nIteration 11, time used 0.031, converge criteria is 0.008416673 \n \nIteration 12, time used 0.047, converge criteria is 0.0069325467 \n \nIteration 13, time used 0.032, converge criteria is 0.0057299824 \n \nIteration 14, time used 0.031, converge criteria is 0.0047522323 \n \nIteration 15, time used 0.031, converge criteria is 0.0039546116 \n \nIteration 16, time used 0.031, converge criteria is 0.0033017935 \n \nIteration 17, time used 0.047, converge criteria is 0.0027657536 \n \nIteration 18, time used 0.031, converge criteria is 0.0023241894 \n \nIteration 19, time used 0.032, converge criteria is 0.0019592976 \n \nIteration 20, time used 0.046, converge criteria is 0.0016568231 \n \nIteration 21, time used 0.032, converge criteria is 0.0014053171 \n \nIteration 22, time used 0.031, converge criteria is 0.0011955575 \n \nIteration 23, time used 0.031, converge criteria is 0.0010200936 \n \nIteration 24, time used 0.047, converge criteria is 0.0008728877 \n \nIteration 25, time used 0.031, converge criteria is 0.0007490333 \n \nIteration 26, time used 0.032, converge criteria is 0.0006445313 \n \nIteration 27, time used 0.031, converge criteria is 0.0005561129 \n \nIteration 28, time used 0.047, converge criteria is 0.0004810987 \n \nIteration 29, time used 0.031, converge criteria is 0.0004172864 \n \nIteration 30, time used 0.031, converge criteria is 0.0003628606 \n \nIteration 31, time used 0.031, converge criteria is 0.0003163211 \n \nIteration 32, time used 0.032, converge criteria is 0.0002764247 \n \nIteration 33, time used 0.047, converge criteria is 0.0002421384 \n \nIteration 34, time used 0.031, converge criteria is 0.0002126018 \n \nIteration 35, time used 0.047, converge criteria is 0.0001870963 \n \nIteration 36, time used 0.046, converge criteria is 0.0001650204 \n \nIteration 37, time used 0.032, converge criteria is 0.0001458692 \n \nIteration 38, time used 0.031, converge criteria is 0.000129218 \n \nIteration 39, time used 0.031, converge criteria is 0.0001147086 \n \nIteration 40, time used 0.047, converge criteria is 0.0001020383 \n \nIteration 41, time used 0.031, converge criteria is 0.0000909506 \n \nIteration 42, time used 0.032, converge criteria is 0.0000812278 \n \nIteration 43, time used 0.031, converge criteria is 0.0000726846 \n \nIteration 44, time used 0.047, converge criteria is 0.0000651629 \n \nIteration 45, time used 0.031, converge criteria is 0.0000585277 \n \nIteration 46, time used 0.031, converge criteria is 0.0000526634 \n \nIteration 47, time used 0.031, converge criteria is 0.0000474707 \n \nIteration 48, time used 0.047, converge criteria is 0.0000428642 \n \nIteration 49, time used 0.063, converge criteria is 0.0000387705 \n \nIteration 50, time used 0.031, converge criteria is 0.0000351261 \n \nIteration 51, time used 0.031, converge criteria is 0.000031876 \n \nIteration 52, time used 0.032, converge criteria is 0.0000289726 \n \nIteration 53, time used 0.031, converge criteria is 0.0000263748 \n \nIteration 54, time used 0.031, converge criteria is 0.0000240465 \n \nIteration 55, time used 0.031, converge criteria is 0.0000219565 \n \nIteration 56, time used 0.031, converge criteria is 0.0000200774 \n \nIteration 57, time used 0.047, converge criteria is 0.0000183854 \n \nIteration 58, time used 0.172, converge criteria is 0.0000168595 \n \nIteration 59, time used 0.031, converge criteria is 0.0000154815 \n \nIteration 60, time used 0.032, converge criteria is 0.0000142352 \n \nIteration 61, time used 0.031, converge criteria is 0.0000131064 \n \nIteration 62, time used 0.031, converge criteria is 0.0000120826 \n \nIteration 63, time used 0.031, converge criteria is 0.0000111528 \n \nIteration 64, time used 0.032, converge criteria is 0.0000103072 \n \nIteration 65, time used 0.031, converge criteria is 9.5372542E-6 \n \nTotal Time is 2.56 sec. \n \nTotal Iteration is 65, convergence status is Converged. \n \nAt Final Iteration, max difference is 9.5372542E-6 \n \nMPRINT(LR_SGD): source; \n \nMLOGIC(LR_SGD): Ending execution. \n \n \n****************************************************************************************** \n \nThe macro %LR_SGD. \n \n/*\n SAS macro:\n  Logistic Regression using Stochastic Gradient Descent. \n Name: \n  %ls_sgd();\n Copyright (c) 2009, Liang Xie (Contact me @ xie1978 at gmail dot com)\n \n \n The SAS macro is a demonstration of an implementation of logistic \n regression modelstrained by Stochastic Gradient Decent (SGD).This \n program reads a training set specified as &dsn_in, trains a logistic \n regression model, and outputs the estimated coefficients to &outest. \n Example usage:\n\n %let inputdata=train_data;\n %let beta=coefficient;\n %let response=Event;\n %lr_sgd(&inputdata, &beta, &response, &covars, \n   alpha=0.008, decay=0.8, \n   criterion=0.00001, maxiter=1000);\n\n\n The following topics are not covered for simplicity: \n  - bias term \n  - regularization \n  - multiclass logistic regression (maximum entropy model)   \n  - calibration of learning rate\n\n Distributed under GNU Affero General Public License version 3. This \n program is free software: you can redistribute it and/or modify\n it under the terms of the GNU Affero General Public License as\n published by the Free Software Foundation, only version 3 of the\n License.\n\n This program is distributed in the hope that it will be useful,\n but WITHOUT ANY WARRANTY; without even the implied warranty of\n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n GNU Affero General Public License for more details. \n\n */\n\n%macro logistic(dsn_in, outest, response, alpha=0.0005);\nproc score data=&dsn_in score=&outest type=parms out=score(keep=score);\n  var intercept &covars;\nrun;\n\ndata _xtemp/view=_xtemp;\n  merge &dsn score ;\n  _w=&response - 1/(1+exp(-score));\n /*\n array x{*} intercept &covars;\n _w=&response - 1/(1+exp(-score));\n do i=1 to dim(x); x[i]=x[i]*_w; end; \n */\nrun;\n\ndata _x&outest;\n array x{*} intercept &covars;\n array _x{*} b_intercept &covars2;\n retain b_intercept &covars2;\n retain logneg logpos 0;\n modify _x&outest;\n do i=1 to dim(x); x[i]=_x[i]; end;\n \n do until (eof);\n  set _xtemp end=eof;\n  do i=1 to dim(x);\n   _x[i]=_x[i]+&alpha*x[i]*_w;\n  end;\n end;\n  replace;\nrun;\n\n%mend;\n\n%macro compare(dsn1, dsn2);\ndata _null_;\n  merge &dsn1 &dsn2;\n array _x1{*} intercept &covars;\n array _x2{*} b_intercept &covars2;\n retain maxdiff 0;\n do i=1 to dim(_x1);  \n  maxdiff=max(maxdiff, abs(_x1[i]-_x2[i])); \n *put _x1[*]=;\n *put _x2[*]=;\n end;\n call symput('maxdiff', maxdiff);\nrun;\n%mend;\n\n\n\n\n%macro lr_sgd(dsn, outest, response, covars, \n    alpha=0.0005, decay=0.9, \n    criterion=0.00001, maxiter=1000);\noptions nosource nonotes;\noptions nomlogic nomprint;\n%local i t0 t1 dt maxdiff status stopiter;\n\n%let t00=%sysfunc(datetime());\n\ndata &dsn;\n  set &dsn;\n intercept=1; _w=1;\nrun;\n\ndata &outest;\n  retain _TYPE_ \"PARMS\" _NAME_ \"SCORE\";\n  array x{*} intercept &covars;\n do i=1 to dim(x); \n  x[i]=0;\n end;\n drop i;\n output;\nrun;\n\ndata _x&outest;\n  retain _TYPE_ \"PARMS\" _NAME_ \"SCORE\";\n  array bx{*} b_intercept &covars2;\n array x{*} intercept &covars;\n set &outest;\n do j=1 to dim(x); bx[j]=x[j]; end;\n keep b_intercept &covars2 _TYPE_ _NAME_;\n drop j;\nrun;\n\nsasfile _x&outest load;\n%let stopiter=&maxiter;\n%let status=Not Converged.;\n%do i=1 %to &maxiter;\n %let t0=%sysfunc(datetime());\n\n %logistic(&dsn, &outest, &response, alpha=&alpha);\n %compare(&outest, _x&outest); \n data &outest;\n   retain _TYPE_ \"PARMS\" _NAME_ \"SCORE\";\n   array bx{*} b_intercept &covars2;\n  array x{*} intercept &covars;\n  set _x&outest;\n  do j=1 to dim(x); x[j]=bx[j]; end;\n  keep intercept &covars _TYPE_ _NAME_;\n  drop j;\n run;\n %let alpha=%sysevalf(&alpha * &decay);\n %let alpha=%sysfunc(max(0.00005, &alpha));\n %let t1=%sysfunc(datetime());\n %let dt=%sysfunc(round(&t1-&t0, 0.001));\n %put Iteration &i, time used &dt, converge criteria is &maxdiff; \n %if %sysevalf(&maxdiff<&criterion) %then %do;\n  %let stopiter=&i;\n  %let i=%eval(&maxiter+1);\n %let status=Converged.;\n %end;\n%end;\nsasfile _x&outest close;\n%let t11=%sysfunc(datetime());\n%let dt=%sysfunc(round(&t11-&t00, 0.01));\n%put Total Time is &dt sec.;\n%put Total Iteration is &stopiter, convergence status is &status;\n%put At Final Iteration, max difference is &maxdiff;\noptions mlogic mprint notes source;\n%mend;"], "link": "http://www.sas-programming.com/feeds/2505374346425236390/comments/default", "bloglinks": {}, "links": {"http://sasdiehard.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://r.nabble.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["SAS used to not support multithreading in PCA, then I figured out that its server version supports this functionality, see here . Today, I found this mutlithreading capability is finally available in PC SAS v9.22. \n \nThe figure above indicates that all 4 threads in my PC are utilized. FYI, My PC uses an Intel 2core 4threads CPU. This multi-threading capability directly help any work relying on SVD due to the direct relationshipbetween SVD and PCA, see here . \n \nNotice that in order to observe the effect of mutli-threading by comparing Real User Time and CPU Time, I/O should not be a bottleneck, that is why in the code, all outputs, either to screen or to data sets, are suppressed. \n \nPS: It turns out that the multi-threading capability is only available when SAS is building up SSCP /USSCP matrix in PROC PRINCOMP. \n \n \n\noptions fullstimer;\ndata _junky;\n  length id x: 8;\n array x{800};\n do id=1 to 5E3;\n  do j=1 to dim(x);\n  x[j]=ranuni(0);\n end;\n drop j; output;\n end;\nrun;\n\nproc princomp data=_junky noprint;\n  var x:;\nrun;"], "link": "http://www.sas-programming.com/feeds/3028162046104849377/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://listserv.uga.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1, "http://www.sas-programming.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Today, Rick (blog @ here ) wrote an article about random number seed in SAS to be used in random number functions in DATA Step. Rick noticed when multiple random number functions are called using different seeds, only the first one matters. \n \nThis is so true. In fact, SAS Manual also has a comprehensive writting on this issue, namely, how to control seeds for each iteration of the random number generation process and how to generatie multiple statistically independent streams of random numbers, see here . In fact, sasCommunity.org also has an article about this issue, see here . \n \nTo echo Rick's post, there is a way to control the seed so that NOT only the first one matters: use CALL routines which by theory will generate computationally independent random number sequence. But if the two seeds are too close, the generated sequences may not be statistically independent. Again, refer to the SAS manual for details. \n \n \n \n\ndata normal;\n seed1 = 11111;\n seed2 = 22222;\n seed3 = 383333;\n do i = 1 to 1000;\n  call rannor(seed1, x1);\n  call rannor(seed2, x2); \n  call rannor(seed3, x3);\n x4=rannor(seed2);\n x5=rannor(seed3);\n  output;\n end;\nrun;\n\nproc sgscatter data=normal;\n matrix x1-x5/ markerattrs = (size = 1);\nrun;"], "link": "http://www.sas-programming.com/feeds/5189434209973827446/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://support.sas.com/": 1, "http://2.blogspot.com/": 1, "http://www.sascommunity.org/": 1, "http://blogs.sas.com/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Partial Least Square (PLS) is a powerful tool for discriminant analysis with large number of predictors [1]. \n \nPLS extracts latent factors that maximize the covariance between independent variables and dependent variables. This process is equivalent to Generalized Eigenvalue Decomposition of the following formula [2]: \n$$X'HXw =\\phi X'Xw $$. For PLS $$H=Y'Y$$ Note that Canonical Correlation Analysis (CCA) follows the same generalized eigenvalue decomposition problem, specifically, for CCA, $$H=Y'(YY')^{-1}Y$$. \n \nIn SAS, PROC PLS implements 2 forms of PLS, namely the original NIPALS [2] and SIMPLS [3]. When there is only one dependent variable, the two algorithms generate the same output.PLS is a computationally very demanding algorithm. While powerful, when the dimension of the problem at hand becomes very large, PROC PLS will encounter issues such as insufficient memory and very long computing time. \n\nThere is a rescue when only one dependent variable Y presents. In this case, CCA and PLS differ only up to a fixed scale parameter. Therefore we can use PROC CANCORR, which is very scalable and multithreaded, to solve the PLS problem. The obtained weights and loadings will not be the same but the difference is only up to a fixed scale parameter. \n \nIn the follow log, we demonstrate the behavior of PROC PLS and PROC CANCORR on a server with 4GB accessible memory when the number of independent variable is 5000 and sample size is 100K. PROC PLS reported insufficient memory and stopped computing in 45seconds after exhausting all accessible memory, while PROC CANCORR continued and finished computation in slightly more than 7 minutes. Both procedures used up 3.92GB memory available to SAS from the system. Also note that PROC CANCORR used more than 33minutes of CPU time, indicating its very good scale up capability in a multi-core environment. \n \nReferece: \n[1] Barker, M and Rayens, W (2003), \"Partial Least Squares for Discrimination\", Journal of Chemometrics , 17, 166-173 \n \n [2] Sun, L; Ji, S; Yu, S; and Ye, J (2009), \n \" On the Equivalence Between Canonical Correlation Analysis and Orthonormalized Partial Least Squares \", In Proceedings of the 21st \n International Joint Conference on Artificial Intelligence ( IJCAI 2009) . \n \n[3] Wold, H. (1966), \u201cEstimation of Principal Components and Related Models by Iterative Least Squares,\u201d in P. R. Krishnaiah, ed., Multivariate Analysis , New York: Academic Press. \n \n[4] de Jong, S. (1993), \u201cSIMPLS: An Alternative Approach to Partial Least Squares Regression,\u201d Chemometrics and Intelligent Laboratory Systems , 18, 251\u2013263. \n \n \n \nNOTE: PROCEDURE PRINTTO used (Total process time):\n  real time   0.00 seconds\n  user cpu time  0.00 seconds\n  system cpu time  0.00 seconds\n  Memory       3920274k\n  OS Memory       3926280k\n  Timestamp   11/16/2011 2:11:22 PM\n  Page Faults      0\n  Page Reclaims      9\n  Page Swaps      0\n  Voluntary Context Switches  1\n  Involuntary Context Switches  1\n  Block Input Operations   0\n  Block Output Operations   0\n  \n\n16   options fullstimer;\n17   data x;\n18    length id y x: 8;\n19   array x{5000};\n20    do id=1 to 1E5;\n21    y=rannor(0);\n22    do j=1 to dim(x);\n23    x[j]=rannor(0);\n24   end;\n25   output;\n26   drop j;\n27   end;\n28   run;\n\nNOTE: The data set WORK.X has 100000 observations and 5002 variables.\nNOTE: Compressing data set WORK.X increased size by 0.06 percent. \n  Compressed is 100066 pages; un-compressed would require 100009 pages.\nNOTE: DATA statement used (Total process time):\n  real time   1:07.58\n  user cpu time  1:02.41\n  system cpu time  5.16 seconds\n  Memory       3920274k\n  OS Memory       3926280k\n  Timestamp   11/16/2011 2:12:29 PM\n  Page Faults      0\n  Page Reclaims      510\n  Page Swaps      0\n  Voluntary Context Switches  63\n  Involuntary Context Switches  107\n  Block Input Operations   0\n  Block Output Operations   0\n  \n\n29   \n30   \n31   proc pls data=x method=simpls noprint;\n32    model y =x1-x5000;\n33   run;\n\nERROR: The SAS System stopped processing this step because of insufficient memory.\nNOTE: There were 100000 observations read from the data set WORK.X.\nNOTE: PROCEDURE PLS used (Total process time):\n  real time   45.89 seconds\n  user cpu time  40.11 seconds\n  system cpu time  5.77 seconds\n  Memory       3920284k\n  OS Memory       3926280k\n  Timestamp   11/16/2011 2:13:15 PM\n  Page Faults      0\n  Page Reclaims      978075\n  Page Swaps      0\n  Voluntary Context Switches  91\n  Involuntary Context Switches  162\n  Block Input Operations   0\n  Block Output Operations   0\n  \n35   proc cancorr data=x noprint;\n36    var y;\n37   with x1-x5000;\n38   run;\n\nNOTE: PROCEDURE CANCORR used (Total process time):\n  real time   7:02.85\n  user cpu time  33:13.85\n  system cpu time  4.40 seconds\n  Memory       3920284k\n  OS Memory       3926280k\n  Timestamp   11/16/2011 2:20:18 PM\n  Page Faults      0\n  Page Reclaims      126339\n  Page Swaps      0\n  Voluntary Context Switches  2096\n  Involuntary Context Switches  83359\n  Block Input Operations   0\n  Block Output Operations   0\n  \n\n39 \n40   proc printto; run;"], "link": "http://www.sas-programming.com/feeds/8395996439289391529/comments/default", "bloglinks": {}, "links": {"http://www.asu.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["In Chapter 3 of \"Bayesian Computation with R\", Jim Albert talked about how to conduct 2 fundamental tasks of Statistics, namely Estimation and Hypothesis Testing in a single parameter framework. \n \nThe structure of this chapter is organized as the following: \n \n     +--------Non-infomative Prior: estimate variance parameter in a normal model \n     | \n     | \n+------Estimation +--------Informative Prior : estimate a gamma model \n|     | \n|     | \n|     +--------Sensitivity to the choice of Prior : Robustness \n| \n| \n+----- Hypothesis Testing : A binomial test \n \n \n/*** 3.2 ***/\nproc import datafile=\"c:\\footballscores.txt\" out=football\n   dbms=tab replace;\nrun;\n\ndata football;\n  set football end=eof;\n  d=favorite - underdog -spread;\n  d2=d**2;\n  if eof then call symput('n', _n_);\nrun;\n\nproc means data=football noprint;\n  var d2;\n  output out=sum sum(d2)=v;\nrun;\ndata _null_;\n  set sum;\n  call symput('v', v);\nrun;\n\n\n%put &n;\n%put &v;\n\n\ndata P;\n  call streaminit(100000);\n  do i=1 to 1000;\n  p=rand('CHISQUARE', &n)/&v;\n  s=sqrt(1/p);\n  output;\n  drop i;\n  end;\nrun;\n\n\nods select none;\nproc univariate data=p ;\n  var s;\n  histogram /midpoints=12.5 to 15.5 by 0.25      \n    cbarline=blue cfill=white \n    outhistogram=hist;\n  title \"Histogram of ps\";\nrun;\ntitle;\nods select all;\n\naxis1 label=(angle=90 \"Frequency\");\nproc gbarline data=hist;\n  bar _midpt_/discrete sumvar=_count_ space=0 axis=axis1 ;\n  title \" \";\nrun;\ntitle;\n\n/* 3.3 */\ndata y;   \n  retain alpha 16\n   beta 15174\n   yobs 1\n   ex 66\n  ;\n  lam=alpha/beta; \n  scale=1/beta;\n  do y=0 to 10;\n  py1=pdf('POIS', y, lam * ex);\n  py2=pdf('GAMMA', lam, alpha, scale);\n  py3=pdf('GAMMA', lam, alpha + y, 1/(beta+ex));\n  py = round(py1*py2/py3, 0.001);\n  output;\n  keep y py;\n  end;\nrun;\n\n\ndata lambdaA;\n  retain alpha 16 \n   yobs 1\n   beta 15174\n   ex  66\n   seed 89878765\n  ;\n  shape=alpha+yobs;\n  scale=1/(beta +ex);\n  do i=1 to 1000;\n  rannum=rangam(seed, shape)*scale;\n  output;\n  keep rannum;\n  end;\nrun;  \n\n\n%let n=20;\n%let y=5;\n%let a=10;\n%let p=0.5;\n\ndata _null_;\n  m1 = pdf('binomial', &y, &p, &n)\n  * pdf('beta', &p, &a, &a)\n  / pdf('beta', &p, &a+&y, &a+&n-&y);\n put m1=;\n  lambda = pdf('binomial', &y, &p, &n)\n   / (pdf('binomial', &y, &p, &n)+m1);\n  put lambda=;\nrun;\n\n\ndata lambda;\n  interval=(5-(-4))/100;\n  do loga=-4 to 5 by interval;\n  a=exp(loga);\n  m1 = pdf('binomial', &y, &p, &n)\n   * pdf('beta', &p, a, a)    \n   / pdf('beta', &p, a+&y, a+&n-&y);\n  lambda = pdf('binomial', &y, &p, &n)\n    / (pdf('binomial', &y, &p, &n)+m1); \n  output; \n  end;\nrun;\n\nsymbol interpol=j;\naxis1 label=(angle=90 \"Prob(coin is fair)\");\naxis2 label=(\"log(a)\");\nproc gplot data=lambda;\n  plot lambda*loga /vaxis=axis1 haxis=axis2;\nrun;quit;\ngoptions reset=all;\n\n\n\n/* Robustness of Bayesian Method \n In this example, Jim tries to estimate the IQ of Joe based on two \n different prior distributions, Normal and T. The posterior distribution \n of the estimate will be compared to demonstrate the robustness of \n Bayesian method.\n*/\n\ndata summ1;\n  retain mu 100 tau 12.16 sigma 15 n 4;\n input ybar;\n  se=sigma/sqrt(n);\n tau1=1/sqrt(1/se**2 + 1/tau**2);\n mu1= (ybar/se**2 + mu/tau**2)*(tau1**2);\n  keep ybar mu1 tau1;\ncards;\n110\n125\n140\n;\nrun; \n\ndata theta;\n  interval=(140-60)/200;\n tscale=20/quantile('T', 0.95, 2);\n do theta=60 to 140 by interval;\n   tdensity=1/tscale*pdf('T', (theta-mu)/tscale, 2);\n ndensity=1/10*pdf('NORMAL', (theta-mu)/tau);\n output;\n keep theta tdensity ndensity;\n end;\nrun;\n\ndata summ2;\n  tscale=20/quantile('T', 0.95, 2);\n  input ybar;\n do theta=60 to 180 by (180-60)/500;\n   like=pdf('NORMAL', (theta-ybar)/7.5);\n prior=pdf('T', (theta-mu)/tscale, 2);\n post=prior*like; \n   output; \n end;\ncards;\n110\n125\n140\n;\nrun;\n\nproc stdize data=summ2 method=sum out=summ2;\n  by ybar;\n  var post;\nrun;\n\ndata summ2v;\n  set summ2;\n  m=theta*post;\n s=theta**2*post;\nrun;\nproc means data=summ2v noprint;\n  by ybar;\n var m s;\n output out=summ2(keep=ybar m s) sum(m)=m sum(s)=s;\nrun;\ndata summ2;\n  set summ2;\n s=sqrt(s-m**2);\nrun;\n\ndata summ;\n  merge summ1 summ2;\nrun;\n\n\n/* Hypothesis Testing \n In this example\n*/\ndata _null_;\n  pvalue=2*cdf('BINOMIAL', 5, 20, 0.5);\n put pvalue= ;\nrun;\n\ndata _null_;\n  retain n 20\n   y 5\n a 10\n p 0.5\n ;\n m1=pdf('BINOMIAL', y, n, p) * pdf('BETA', p, a, a) / pdf('BETA', p, a+y, a+n-y);\n lambda = pdf('BINOMIAL', y, n, p)/(pdf('BINOMIAL', y, n, p) + m1);\n put lambda= best.;\nrun;\n\n\ndata pbetat_out;\n  length _stat_ $ 12;\n  input p0 prob a b s n;\n f=n-s;\n lbf=s*log(p0) + f*log(1-p0) + logbeta(a, b) - logbeta(a+s, b+f);\n bf=exp(lbf);\n post=prob*bf/(prob*bf + 1 - prob);\n _stat_='BayesFactor'; nvalue=bf;\n output;\n _stat_='Posterior'; nvalue=post;\n output;\n keep _stat_ nvalue;\ncards;\n0.5 0.5 10 10 5 20\n;\nrun;\n \n  \ndata lambda;\n  retain n 20\n    y 5 \n    p 0.5\n a 10\n  ;\n  do loga=-4 to 5 by (5+4)/100;\n   a=exp(loga);\n _x=pdf('BINOMIAL', y, p, n);\n   m2=_x*pdf('BETA', p, a, a)/pdf('BETA', p, a+y, a+n-y);\n lambda=_x/(_x + m2);\n keep loga m2 lambda;\n output;\n end;\nrun;\n\nproc sgplot data=lambda;\n  series x=loga y=lambda;\n label lambda='Prob(coin is fair)'\n   loga='log(a)'\n ;\n format lambda 7.1 loga 7.0;\nrun;"], "link": "http://www.sas-programming.com/feeds/6346867125800656043/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Recently, I am working on coding in SAS for a set of regularized regressions and need to compute trace of the projection matrix: \n$$ S=X(X'X + \\lambda I)^{-1}X' $$. \n \nWikipedia has a well written introduction to Trace @ here . \n \nTo obtain the inverse of matrix (X'X + \\lambda I) in SAS/STAT, there are multiple ways: \n1. Build the SSCP matrix first, then inverse it using the following methods: \n 1.1. Using SVD based method, shown @ here but not demonstrated in this post; \n 1.2. Using PROC REG, shown @ here , and also demonstrated in the code below; \n 1.3. Using SWEEP operator, shown @ here under Method 0 and Method 1; \n2. Since (X'X + \\lambda I) is the SSCP for a Ridge Regression with ridge parameter=\\lambda, it is handy to directly use the ODS OUTPUT InvXPX= statement to obtain the inversed matrix, when X is appended by a diagonal matrix of \\lambda*I_{p, p}. SAS code is demonstrated below; \n \nTrace of the project matrix S is a key concept in modern regression analysis. For example, the effective degree of freedom of the model in a regularized linear regression is trace(S)/N, see [1] for details. For another example, in approximating leave-one-out-cross-validation using GCV, trace(S)/N is a key component (formula 7.52 of ~[1]). \n \nCheck the reference and the cited works therein for more information. \n \n \n \nRef: \n \n1. Hastie et al., The Elements of Statistical Learning, 2nd Ed. \n \n \n \n/* Use the SocEcon example data created in\n Example A.1: A TYPE=CORR Data Set Produced by PROC CORR\n on page 8153 of SAS/STAT 9.22 User Doc\n*/\ndata SocEcon;\ninput Pop School Employ Services House;\ndatalines;\n5700 12.8 2500 270 25000\n1000 10.9 600 10 10000\n3400 8.8 1000 10 9000\n3800 13.6 1700 140 25000\n4000 12.8 1600 140 25000\n8200 8.3 2600 60 12000\n1200 11.4 400 10 16000\n9100 11.5 3300 60 14000\n9900 12.5 3400 180 18000\n9600 13.7 3600 390 25000\n9600 9.6 3300 80 12000\n9400 11.4 4000 100 13000\n;\nrun;\n\n%let depvar=HOUSE;\n%let covars=pop school employ services;\n%let lambda=1;\n/* Below is the way to obtain trace(S), where S is the project matrix in a (regularized) linar regression. \n For further information, check pp.68, pp.153 of Elements of Statistical Learning,2nd Ed.\n*/\n\n/* For details about TYPE=SSCP special SAS data, consult:\n Appendix A: Special SAS Data Sets, pp.8159 of SAS/STAT 9.22 User's Guide\n*/\nproc corr data=SocEcon sscp out=xtx(where=(_TYPE_='SSCP')) noprint;\n  var &covars;\nrun;\n\n\ndata xtx2;\n  set xtx;\n array _n{*} _numeric_;\n array _i{*} i1-i5 (5*0);\n do j=1 to 5;\n  if j=_n_ then _i[_n_]=\u03bb\n else _i[j]=0;\n end;\n _n[_n_]=_n[_n_]+1;\n drop j _TYPE_ _NAME_;\nrun;\n\n/* Obtain the inverse of (XTX+\\lambda*I)\n Note that we explicitly specified Intercept term in the \n covariate list and fit a model without implicit intercept \n term in the model.\n*/\nproc reg data=xtx2 \n   outest=S0(type=SSCP\n     drop=i1-i5 _MODEL_ _DEPVAR_ _RMSE_)\n   singular=1E-17;\n  model i1-i5 = Intercept &covars / noint noprint;\nrun;quit;\n\ndata S0;\n  set S0; \n length _NAME_ $8;\n _NAME_=cats('X_', _n_);\nrun;\n\nproc score data=SocEcon score=S0 out=XS0(keep=X_:) type=parms;\n  var &covars;\nrun;  \n\ndata XS0X;\n  merge XS0 X;\n array _x1{*} X_:;\n array _x0{*} intercept pop school employ services;\n do i=1 to dim(_X1);\n  _x1[i]=_x1[i]*_x0[i];\n end;\n rowSum=sum(of _x1[*]);\n keep rowSum;\nrun;\nproc means data=XS0X noprint;\n  var rowSum;\n output out=trace sum(rowSum)=Trace;\nrun;\n \n \n \nVerify the result using R: \n \n \n> socecon<-read.csv('c:/socecon.csv', header=T)\n> x<-as.matrix(cbind(1, socecon[,-5]))\n> xtx<-t(x)%*%x\n> phi<-xtx+diag(rep(1, 5))\n> \n> # method 1. \n> S<-x%*%solve(phi)*x\n> sum(S)\n[1] 4.077865\n> # method 2. \n> S<-(x%*%solve(phi))%*%t(x)\n> sum(diag(S))\n[1] 4.077865\n>\n \n \nOf course, except for method 1.2 shown above, we can also use Method 2 mentioned above, and obtain the same inverse matrix: \n \n\ndata SocEcon2 /view=SocEcon2;\n  set x end=eof;\n array x{5} Intercept &covars (5*0);\n Intercept=1;\n output;\n if eof then do;\n  do j=1 to dim(x);\n  x[j]=\u03bb\n  output;\n  drop j;\n  x[j]=0;\n end;\n end;\nrun;\n\nods select none;\nods output InvXPX=S1;\nproc reg data=SocEcon2 singular=1E-17;\n  model y = Intercept &covars /noint i;\nrun; quit;\nods select all;\n\n \n \nAs a side point, it is also of interests to compare the computational performance of both illustrated methods. SVD-based approach and SWEEP operator based approach are omitted. \n \nUsing a SAS data set of 1001 covariates (including Intercept term) and 1E6 observations, total 7.6GB on a windows PC, the test was done on two dramatically different machines: one WindowsXP laptop with mediocre HDD and a 2-core T7300 CPU; the other one is a high-end Server running Linux64 with Disk Array and fast CPUs totaled 16 cores. \n \nOn the PC,: \n-> Using method 1.2, the time used decomposition is listed below: \nPROC CORR: real time: 25:47.72; CPU time: 15:26.15 \nDATA step on XTX: real time: 2.28 sec; CPU time: 0.07 sec \nPROC REG : real time: 15.45 sec; CPU time: 27.31 sec; \nTotal real time: 26:05.45 \n \n-> Using method 2, the time used is: \nPROC REG: real time: 48:28.44; CPU time: 1:16:46.41 \n \nOn the server: \n -> Using method 1.2, the time used decomposition is listed below: \nPROC CORR: real time: 5:40.61; CPU time: 5:40.58 \nDATA step on XTX: real time: 0.05 sec; CPU time: 0.05 sec \nPROC REG: real time 1.71 sec; CPU time: 5.27 sec \nTotal real time: 5:42.37 \n \n-> Using method 2, the time used is: \nPROC REG: real time: 6:01.46; CPU time: 19.13.49 \n \nThe performance is summarized below:"], "link": "http://www.sas-programming.com/feeds/2415426124549242306/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2, "http://en.wikipedia.org/": 1, "http://www.sas-programming.com/": 3}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Rick Wicklin discussed in his blog the performance in solving a linear system using SOLVE() function and INV() function from IML. \n \nSince regression analysis is an integral part of SAS applications and there are many SAS procedures in SAS/STAT that are capable to conduct various regression analysis, it would be interesting to benchmark their relative performance using OLS regression, the fundamental regression analysis of all. \n \nThe analysis will compare REG, GLMSELECT, GENMOD, MIXED, GLIMMIX, GLM, ORTHOREG, HPMIXED and TRANSREG on 10 OLS regressions with 100 to 1000 variables, incremental at 100, and with the number of observations twice the number of variables to avoid possible numerical issues. HPMIXED uses sparse matrix techniques and will be put into great disadvantage in this comparison using large dense matrices. A macro wraps them together: \n \n \n\n%macro wrap;\nproc printto log='c:\\testlog.txt';run;\n\n%let t0=%sysfunc(datetime(), datetime.);\n%let procnames=GLM REG GLMSELECT ORTHOREG MIXED GLIMMIX GENMOD ;\n%let nproc=%sysfunc(countW(&procnames));\n%put Test &nproc PROCEDURES;\n%do i=1 %to 10;\n %let nobs=%sysevalf(&i*100);\n options nonotes;\n data _temp;\n   array x{&nobs};\n\t do i=1 to 2*&nobs;\n\t do j=1 to &nobs;\n\t  x[j]=rannor(0);\n   end;\n\t y=rannor(0);\n\t drop i j;\n\t output;\n\t end;\t\t \n  run;\n  options notes;\n  sasfile _temp load;\n  ods select none;\n\n  %do j=1 %to &nproc;\n   %let proc=%scan(&procnames, &j);\n\t %put &proc;\n\t proc &proc data=_temp;\n\t  model y = x1-x&nobs;\n\t run;\n %end;\n %put TRANSREG;\n proc transreg data=_temp;\n   model identity(y) = identity(x1-x&nobs);\n run;\n sasfile _temp close;\n ods select all;\n%end;\nproc printto; run;\n%mend;\n%wrap;\n \nAfter running all iterations, the SAS log is parsed to obtain procedure names and corresponding real time and CPU time. The following SAS code does this job: \n \n \n\ndata proc_compare;\n  infile \"c:\\testlog.txt\";\n\t input;\n\t retain procedure ;\n\t retain realtime cputime realtime2 ; \n\t length procedure $12.;\n\t length realtime cputime $24.;\n\t if _n_=1 then id=0;\n\t x=_infile_;\n\t if index(x, 'PROCEDURE')>0 then do;\n\t procedure=scan(_infile_, 3);\t\t\n\t\tif procedure=\"REG\" then id+1;\t\t\n\t end;\n\t\n\t if index(x, 'real time')>0 then do;\n\t _t1=index(_infile_, 'real time');\n\t _t2=index(_infile_, 'seconds');\n\t if _t2=0 then _t2=length(_infile_);\n   realtime=substr(_infile_, _t1+9, _t2-_t1-9);\n\t if index(realtime, ':')>0 then do;\n \t  realtime2=scan(realtime, 1, ':')*60;\n\t  sec=input(substr(realtime, index(realtime, ':')+1), best.);\n\t  realtime2=realtime2+sec;\t\t \n\t end;\n\t else realtime2=input(compress(realtime), best.);\n\t end;\n\t if index(x, 'cpu time')>0 then do;\n\t _t1=index(_infile_, 'cpu time');\n\t _t2=index(_infile_, 'seconds');\n\t if _t2=0 then _t2=length(_infile_);\n\t cputime=substr(_infile_, _t1+8, _t2-_t1-8);\n\t if index(cputime, ':')>0 then do;\n \t  cputime2=scan(cputime, 1, ':')*60;\n\t  sec=input(substr(cputime, index(cputime, ':')+1), best.);\n\t  cputime2=cputime2+sec;\n\t end;\n\t else cputime2=input(compress(cputime), best.);\n\t keep id size procedure cputime2 realtime2 ;\n\t size=id*100;\n\t if compress(procedure)^=\"PRINTTO\" then output;\n\tend;\nrun;\n \nWe then visualize the results using the following code: \n \n\ntitle \"Benchmark Regression PROCs using OLS\";\nproc sgpanel data=proc_compare;\n  panelby procedure /rows=2;\n  series y=cputime2 x=size/ lineattrs=(thickness=2);\n\t label cputime2=\"CPU Time (sec)\"\n\t  size=\"Problem Size\"\n\t\t ;;\t\n\t colaxis grid;\n\t rowaxis grid;\nrun;\ntitle;\n\ntitle \"Closer Look on REG vs. GLM vs. GLMSELECT\";\nproc sgplot data=proc_compare uniform=group;\n  where procedure in (\"GLMSELECT\", \"REG\", \"GLM\");\n  series x=size y=cputime2/group=procedure curvelabel lineattrs=(thickness=2);\n\t label cputime2=\"CPU Time (sec)\"\n\t  size=\"# of Variables\"\n\t\t ;;\n  yaxis grid ;\n  xaxis grid ;\nrun;\ntitle;\n\n \n  \n  \n  \n \nIt is found that PROC GLM and GLMSELECT beat all other procedures with large margin while HPMIXED is the slowest followed by GLIMMIX. Surprisingly, REG is slower than both GLM and GLMSELECT even though it utilized multi-threading technique while GLMSELECT does not: \n \n************ Partial LOG of the last iteration ******** \nNOTE: PROCEDURE REG used (Total process time): \nreal time 6.79 seconds \ncpu time 9.36 seconds \n \n \nNOTE: There were 2000 observations read from the data set WORK._TEMP. \nNOTE: PROCEDURE GLMSELECT used (Total process time): \nreal time 3.06 seconds \ncpu time 2.96 seconds \n******************************************************** \n \nThe performance gap between REG and GLM/GLMSELECT is getting larger when the number of variables increases to be more than 700. \n \nBoth REG and GLMSELECT are developed by the same group of developers in SAS, as far as I know. \n \n********************* PS : **************************** \nRick and Charlie pointed out that real time is a more fair measure, which I agree. \n \nThe reading of real computing time has large variance from run to run because the testing enviornment is not very clean and there are many background window programs running. Below is part of the log file of another run with 2000 variables and 4000 records: \n \nNOTE: PROCEDURE REG used (Total process time): \n  real time   2.26 seconds \n  cpu time   7.76 seconds \n  \n \n \nNOTE: PROCEDURE GLM used (Total process time): \n  real time   3.57 seconds \n  cpu time   4.58 seconds \n \n \nNOTE: There were 2000 observations read from the data set WORK._TEMP. \nNOTE: PROCEDURE GLMSELECT used (Total process time): \n  real time   3.50 seconds \n  cpu time   3.44 seconds \n  \nWe see that REG has lower real time comparing to GLM/GLMSELECT, even though cpu time is about twice the average of GLM/GLMSELECT. In a case where BY-processing is used, GLMSELECT will use multi-threading as specified in PERFORMANCE statement, and the gap in real time between REG and GLMSELECT will be eliminated. In a collaborating environment, more CPU time also means competing for more resources. Below we show the real time of the same run as in above CPU Time figure. \n \n  \nNote that CPU Time difference and pattern is pretty consistent.Below is the mean CPU Time and its 90% C.I. of 100 runs using REG /GLM /GLMSELECT on different size of problems."], "link": "http://www.sas-programming.com/feeds/2224946647232293217/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1, "http://2.blogspot.com/": 3, "http://1.blogspot.com/": 1, "http://blogs.sas.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["We demonstrate a comparison among various implementations of Rolling Regression in SAS and show that the fastest implementation is over 3200X faster than traditional BY-processing approach. \n \nMore often than not, we encounter a problem where an OLS over a rolling time window is required, see [1], [2], [3], [4], [5], [6], [7], for a few examples. \n \nOne solution is to resort to SAS MACRO, but it is extremely inefficient and can't handle large dataset in reality, [8]. This method is shown below as Method[4]. It couldn't finish the test in allowable time using the sample data below. \n \nThe other common solution is to use the BY-processing capability of PROC REG after re-shaping the data into appropriate format, see [9], [10]. This method is demonstrated as Method[3] below. While certainly much better than above one, it is still not the fastest and requires more memory. \n \nThe third solution comes to play by recognizing that in OLS, what you need is the SSCP and you can easily build up the SSCP over rolling time window by resorting to PROC EXPAND. This is demonstrated as Method[2] below. This approach will further improve the speed but still requires large amount of memory if the data is big and many rolling windows are generated. \n \nSince what we need to do is to build the SSCP matrix and obtain the coefficient estimates based on the informaiton in SSCP, we can certainly code this in a DATA Step using ADJUST operator, which provides a solution that is both fast and low memory occupancy. See [11] for an introduction to ADJUST operator. To make this even faster, a modification of ADJUST operator, the SWEEP operator, can be used. For an introduction to SWEEP operator, see [11], [12]. In the code below, Method[0] implements the ADJUST operator, while Method[1] implements the SWEEP operator. \n \nThe experiment literally runs 499980 regressions each with 20 observations and 2 predictors , and the results are shown below: \n \n        Real Time |  CPU Time   | Memory       \n===================================================== \n \n Method 0 | 1.01 (seconds) | 1.01 (seconds)  | 611K \n \n Method 1 | 0.25 (seconds) | 0.24 (seconds)  | 432K \n \n Method 2 | 1.61 (seconds) | 0.94 (seconds)  | 50381K \n \n Method 3 | 80.54 (seconds) | 79.61 (seconds)  | 2322K \n \n Method 4 |   Failed   |   Failed    | Failed \n===================================================== \n \n Reference: \n[1]MYSAS.NET, http://www.mysas.net/forum/viewtopic.php?f=4&t=8070 \n[2]MYSAS.NET, http://www.mysas.net/forum/viewtopic.php?f=4&t=7898 \n[3]SAS-L, http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0604D&L=sas-l&P=R32485 \n[4]SAS-L, http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0704C&L=sas-l&P=R3305 \n[5]SAS-L, http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0802C&L=sas-l&P=R9746 \n[6]SAS-L, http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0801C&L=sas-l&P=R14671 \n[7]SAS-L, http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0810A&L=sas-l&P=R19135 \n[8]SAS-L, http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0802C&L=sas-l&P=R13489 \n[9]Michael D Boldin, \"Programming Rolling Regressions in SAS\", Proceedings of NESUG, 2007 \n[10]SAS-L, http://www.listserv.uga.edu/cgi-bin/wa?A2=ind0604D&L=sas-l&D=0&P=56926 \n[11]J. H. Goodnight, \"The Sweep Operator: Its Importance in Statistical Computing\", SAS Tech Report R-106, 1978 \n[12]Kenneth Lange, \"Numerical Analysis for Statisticians\", Springer, 1998 \n \n \nproc datasets library=work kill; run;\n\n\noptions fullstimer;\ndata test;\n  do seq=1 to 500000;\n   x1=rannor(9347957);\n   *x2=rannor(876769)+0.1*x1;\n   epsilon=rannor(938647)*0.5;\n   y = 1.5 + 0.5*x1 +epsilon;\n   output;\n  end;\nrun;\n\n/* Method 0.*/\nsasfile test load;\ndata res0;\n  set test;\n array _x{3,3} _temporary_ ;\n array _a{3,3} _temporary_ ;\n array _tempval{5, 20} _temporary_ ;\n m=mod(_n_-1, 20)+1;\n _tempval[1, m]=x1; \n _tempval[2, m]=y;\n _tempval[3, m]=x1**2;\n _tempval[4, m]=x1*y;\n _tempval[5, m]=y**2;\n\n link filler;\n if _n_>=20 then do;\n  if _n_>20 then do; \n     m2=mod(_n_-20, 20)+1;\n  _x[1,2]+(-_tempval[1, m2]);\n  _x[1,3]+(-_tempval[2, m2]);\n  _x[2,2]+(-_tempval[3, m2]);\n  _x[2,3]+(-_tempval[4, m2]);\n  _x[3,3]+(-_tempval[5, m2]);\n  end;\n  do i=1 to dim(_a, 1);\n   do j=1 to dim(_a, 2);\n   _a[i, j]=_x[i, j];\n  end;\n  end;\n   \n    do k=1 to dim(_a, 1)-1;\n   link adjust;\n    end;\n  Intercept=_a[1,3]; beta=_a[2,3];\n  keep seq intercept beta;\n  output;\n end;\n\n return;\nfiller:\n _x[1,1]=20; _x[1,2]+x1; _x[1,3]+y;\n _x[2,2]+_tempval[3,m]; _x[2,3]+_tempval[4,m]; _x[3,3]+_tempval[5,m];\n _x[2,1]=_x[1,2]; _x[3,1]=_x[1,3]; _x[3,2]=_x[2,3]; \nreturn;\n\nadjust:\n B=_a[k, k];\n do j=1 to dim(_a, 2);\n  _a[k, j]=_a[k, j]/B;\n end;\n do i=1 to dim(_a, 1);\n  if i ^=k then do;\n   B=_a[i, k];\n do j=1 to dim(_a, 2);\n  _a[i, j]=_a[i, j]-B*_a[k, j];\n end;\n end;\n end;\nreturn;\n\nrun;\nsasfile test close;\n\n\n\n/* Method 1.*/\n\nsasfile test load;\ndata rest0;\n  set test;\n array _x{4} _temporary_;\n array _a{2,20} _temporary_;\n m=mod(_n_-1, 20)+1;\n _a[1, m]=x1; _a[2,m]=y;\n link filler;\n\n m2=mod(_n_-20, 20)+1;\n if _n_>=20 then do;\n if _n_>20 then do;\n    link deduct;\n end;\n beta=(_x[2]-_x[1]*_x[4]/20)/(_x[3]-_x[1]**2/20);\n intercept=_x[4]/20 - beta*_x[1]/20;\n keep seq intercept beta ;\n output;\n end;\n return;  \nfiller:\n  _x[1]+x1;\n _x[2]+x1*y;\n _x[3]+x1**2;\n _x[4]+y;\nreturn;\ndeduct:\n  _x[1]=_x[1]-_a[1,m2]; \n _x[2]=_x[2]-_a[1,m2]*_a[2,m2];\n _x[3]=_x[3]-_a[1,m2]**2;\n _x[4]=_x[4]-_a[2,m2];\nreturn;\nrun;\nsasfile test close;\n\n\n\n/* Method 2.*/\n\n%macro wrap;\n%let window=20;\n%let diff=%eval(&window-0);\ndata testv/view=testv;\n  set test;\n  xy=x1*y; \nrun;\n\nproc expand data=testv method=none out=summary(keep=seq sumxy sumx1 sumy ussx1 may max);\n  convert x1=sumx1/transformout=(movsum &diff);\n  convert xy=sumxy/transformout=(movsum &diff);\n  convert x1=ussx1/transformout=(movuss &diff);\n  convert y =sumy /transformout=(movsum &diff);\n  convert y =may / transformout=(movave &diff);\n  convert x1 =max / transformout=(movave &diff); \nrun;\n\ndata result1;\n  set summary(firstobs=&window);\n  beta = (sumxy - sumx1*sumy/&window)/(ussx1 - sumx1/&window.*sumx1); \n  alpha= may - beta*max;\n  keep seq beta alpha; \nrun;\n%mend;\n\n%let t0=%sysfunc(datetime(), datetime24.);\n*options nosource nonotes;\n%wrap;\noptions source notes;\n%let t1=%sysfunc(datetime(), datetime24.);\n%put Start @ &t0;\n%put End @ &t1;\n\n \n\n/* Method 3.*/\n%let t0=%sysfunc(datetime(), datetime.);\n \ndata test2v/view=test2v;\n  set test;\n  array _x{2, 20} _temporary_ (20*0 20*0);\n  k=mod(_n_-1, 20)+1;\n  _x[1, k]=x1; _x[2, k]=y;\n  if _n_>=20 then do;\n   do j=1 to dim(_x, 2);\n    x=_x[1, j]; y=_x[2, j];\n    output;\n    keep seq x y;\n   end;\n  end;\nrun;\n\nods select none;\nproc reg data=test2v outest=res2(keep=seq x intercept);\n   by seq;\n   model y = x;\nrun;quit;\nods select all;\n\n%let t1=%sysfunc(datetime(), datetime.);\n%put Start @ &t0;\n%put End @ &t1;\n\n\n/* Method 4. */\n%macro wrap;\noptions nonotes;\nods select none;\n%do i=20 %to 500000;\n  %let fo=%eval(&i-19);\n  proc reg data=test(firstobs=&fo obs=&i) outest=_xres(keep=x1 intercept);\n   model y =x1;\n  run;quit;\n  %if %eval(&i=20) %then %do;\n   data res3; set _xres; run;\n  %end;\n  %else %do;\n  proc append base=res3 data=_xres; run;\n  %end;\n%end;\n\nods select all;\ndata res3;\n  set res3;\n  time=19+_n_;\nrun;\noptions notes;\n%mend;\n\n%let t0=%sysfunc(datetime(), datetime.);\n%wrap;\n%let t1=%sysfunc(datetime(), datetime.);\n%put Start @ &t0;\n%put End @ &t1;"], "link": "http://www.sas-programming.com/feeds/2743524723880802672/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://www.uga.edu/": 7, "http://www.mysas.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Profiling numerical variables is an integral part of data analytics, which generally consists of obtaining standard descriptive statistics such as quantiles, first central moments as well as missing ratio. \n \nIt is easily obtainable by using PROC MEANS (or PROC SUMMARY). But when we face very large data with many varibles, we will hit memory wall and very long processing time using default options in PROC MEANS. The most time consuming and memory intensive descriptive statistics is quantile calculation. The default method uses ordered statistics, which is the most accurate but also the most memory intensive and time consuming one. \n \nThere are two methods available to handle this scenario, both uses the one-pass method of Jain R. and Chlamtac I [1]. This method is able to obtain fairly accurate estimate of quantiles between P25 and P75, but for quantiles outside this range, the estimates will be more rough but given very large sample, maybe acceptable. Another problem is that this estimator is sensitive to the distribution of underlying data [2]. There are newer methods available that are insensitive to the distribution, such as [3]. \n \nIn PROC MEANS, we can specify QMETHOD=P2 or QMETHOD=HIST to call this one pass method. Or we can use PROC STDIZE, specifying PCTLMTD=ONEPASS to invoke P2 method to calculate quantiles. Each has its pro and con. \n \nPROC MEANS: \nPRO: Multithreaded / rich set of descriptive statistics; \nCON: Only pre-defined quantiles, output data set is not user friendly, in need of further manipulation; \n \nPROC STDIZE: \nPRO: quantiles of 0 to 100 are available, statistics output data is in a user friendly format; \nCON: No multithreads, lack of higher central moments statistics, need to surpress data output explicitly; \n \nOf couse, due to the way the output statistics are organized from PROC STDIZE OUTSTAT=, it can be parallelized very easily, and by examining the actual CPU time, PROC STDIZE is more efficient than PROC MEANS. Besides, higher central moments can be calculated from obtained first 2 central moments and this is done in the small output data set. \n \nBelow the difference between default method and the two new approaches is illustrated. \n \n \noptions fullstimer;\ndata test;\n  length id 8;\n array x{100} ;\n do id=1 to 1e5;\n  do j=1 to dim(x); x[j]=rannor(0)*1.2; end;\n output;\n drop j;\n end;\nrun;\n\n\nproc means data=test noprint q1 qmethod=os;\n  var x1-x100;\n output out=_mean\n    mean=mean1-mean100\n     std=std_x1-std_x100\n     q1=q1_x1-q1_x100\n     p95=p95_x1-p95_x100;\nrun;\n\nproc means data=test noprint q1 qmethod=p2 qmarkers=211;\n  var x1-x100;\n output out=_mean\n    mean=mean1-mean100\n     std=std_x1-std_x100\n     q1=q1_x1-q1_x100\n     p95=p95_x1-p95_x100;\nrun;\n\nods select none;\nproc stdize data=test \n     out=_null_ outstat=_stat pctlmtd=onepass \n  nmarkers=211\n     pctlpts=1 5 10 25 50 75 90 95 99;\n   var x1-x100;\nrun;\nods select all;\n \nTake a look at the log: \n \n  \nDue to smaller sample size, the most gain is on the memory side. Default approach of PROC MEANS used 643MB memory, which specifying QMETHOD=P2, the memory usage reduced to only 7.3MB. The most memory efficient is PROC STDIZE with PCTLMTD=ONEPASS, only 0.68MB memory was used. \n \nWe also examine the difference on quantile estimates using the Ordered Statistics (OS) method and P2 method in PROC MEANS. There are observable differences but not that significant: \n \n  \n \n \nReference: \n[1]. Jain R. and Chlamtac I. (1985), \"The Algorithm for Dynamic Calculation of Quantiles and Histograms Without Storing Observations,\" Communications of the ACM, 28(10), 1076\u20131085. \n[2]. SAS/STAT(R) 9.2 User's Guide, Second Edition \n[3]. Alsabti, Khaled; Ranka, Sanjay; and Singh, Vineet (1997), \"A One-Pass Algorithm for Accurately Estimating Quantiles for Disk-Resident Data\". L.C. Smith College of Engineering and Computer Science - Former Departments, Centers, Institutes and Projects. Paper 4. http://surface.syr.edu/lcsmith_other/4"], "link": "http://www.sas-programming.com/feeds/660326104865544458/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://surface.syr.edu/": 1, "http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["PROC GLIMMIX is good tool for generalized linear mixed model (GLMM), when the scale is small to medium. When facing a large scale GLMM, such as modeling all ZIPs nested in Counties nested in all 51 States in US, a 64-bit machine with extremely large memory is required and the computing may last for months! In fact, more often than not, the modeler will encounter situation where PROC GLIMMIX reports in log: \n \nERROR: Integer overflow on computing amount of memory required. \n \nNOTE: The SAS System stopped processing this step because of insufficient memory. \n \n \nIn a strictly nested hierarchical model, however, the variance covariance matrix is very sparse, and taking advantage of this property can accelerate computing by many folds. \n \nThe %HPGLIMMIX SAS macro is made for large scale Hierarchical Mixed Models, and stands for High Performance GLIMMIX. As an example, a sample data using Gamma Regression is shown below, with all ZIPs in AK, AL, AR, AZ with 2-level hierarchies: State and ZIP within State, total 4 blocks with max 693 columns per block. The reason not all ZIPs and all states are used is simply because PROC GLIMMI blows up on the machine. \n \nCopmaring the estimates and std errors from both runs, they are the same, but drastically different running time of 71sec using %HPGLIMMIX v.s. 35min39sec using GLIMMIX. \n \n \n \n\n1715\n1716 options nomprint nomlogic;\n1717 %hpglimmix(data=temp2,\n1718    stmts=%str(\n1719     class zip zip_state;\n1720     model y = x ;\n1721     random int zip/subject=zip_state;\n1722      ),\n1723    error=gamma,\n1724    link=LOG,\n1725    options=NOTEST);\nNOTEST\n\n  The HPGLIMMIX Macro\n\nData Set   : WORK.TEMP2\nError Distribution : GAMMA\nLink Function  : LOG\nResponse Variable : Y\n\n\nJob Starts at : 06JUN2011:15:51:19\n HPGLIMMIX Iteration History\n\nIteration Convergence criterion\n 1   0.0081058432 13 sec\n 2   0.0004213646 13 sec\n 3   2.7137935E-7 13 sec\n 4   3.1854799E-9 12 sec\n\nOutput from final Proc HPMixed run:\nJob Ends at : 06JUN2011:15:52:30\n1726 options nomprint nomlogic;\n1727\n1728 proc glimmix data=temp2;\n1729  class zip zip_state;\n1730  model y = x /s dist=gamma;\n1731  random int zip /subject=zip_state;\n1732 run;\n\n\n\nNOTE: Convergence criterion (PCONV=1.11022E-8) satisfied.\nNOTE: PROCEDURE GLIMMIX used (Total process time):\n  real time   35:38.93\n  cpu time   34:30.90\n\n \n \nGLIMMIX output: \n \n \n \n \n    Covariance Parameter Estimates             Standard    Cov Parm  Subject  Estimate  Error    Intercept zip_state 0.000152 0.000125    zip   zip_state 0.000066 3.105E-6    Residual     0.000405 2.281E-6    Solutions for Fixed Effects         Standard  Effect  Estimate  Error  DF t Value Pr > |t|  Intercept  6.5873 0.006180  3 1065.95  <.0001  x    0.003436 0.000218 62634  15.79  <.0001 \n \n \n  %HPGLIMMIX output: \n \n \n     _cov data from HPGLIMMIX     Obs CovParm  Subject  Estimate     1  Intercept zip_state 0.000152     2  zip   zip_state 0.000066     3  Residual     0.000405     _soln data from HPGLIMMIX     Obs Effect  Estimate  StdErr     1  Intercept  6.5873  0.006180     2  x    0.003436 0.000218"], "link": "http://www.sas-programming.com/feeds/8354118659515520074/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Demo SAS implementation of Regularized (Linear) Discriminate Analysis of J. Friedman (1989)[1]. Simpler introduction can be found at [2]. Regularized QDA follows similarly. \n \nTo save coding, I called R within SAS to finish the computation. For details to see how to call R within SAS, check here . \n \n   \n \n \n \n\n\n \nReference: \n1. Friedman, J. (1989). Regularized discriminant analysis, Journal of the American Statistical Association 84: 165-175. \n2. Friedman, J; Hastie, T; Tibshirani, R (2008). The Elements of Statistical Learning, section 4.3.1"], "link": "http://www.sas-programming.com/feeds/7500982122418019594/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 2, "http://www.amazon.com/": 1, "http://feedads.doubleclick.net/": 2, "http://www.sas-programming.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Try out Elastic Net [1] in normal linear regression, using Naive algorithm. Exploring possibilities for GLM Elastic Net in SAS. \n \n  \n\n\n \n1. Zou, H and Hastie, T (2005). Regularization and variable Selection via the Elastic Net, Journal Of The Royal Statistical Society Series B."], "link": "http://www.sas-programming.com/feeds/194600123265301116/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://3.blogspot.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["In Chapter 2 of the book \" Bayesian Computation with R \" by Jim Albert , the philosoph of Bayesian statistics is introduced using an example where a parameter regarding proportion of heavy sleeping college students, i.e. those have more than 8 hours sleep a day, is studied. According to Bayes'Rule, the posterior is proportional to the product of prior probability and likelihood given prior, where the proportion factor is the likelihood of data over the whole parameter space. As we can see, the key point here is the specification of prior probability. \n \nIn order to illustrate this idea, Jim introduced three types of prior probability: \n1. Prior probability over a discrete grid of choice, given a weight which serves as hyper-parameter; \n2. Prior probability by a chosen probability. In most cases, the chosen probability is a so called conjugate prior becuase the posterior is the same family as the prior. In the book, a beta probability is chosen and it is conjugate in the sense that the posterior is also a beta distribution, only with different parameter; \n3. A dense grid of prior probability choices which is called histogram prior; \n \n \nUsing a discrete grid of prior choices, we first generate a list of prior probabilities, in data set PRIOR, that we believe the true proportion of heavy sleepers should be, and assign a weight, i.e. the magnitude of credibility, for each choice in a data set P, which serves as hyper-parameter. We then consolidate the two data sets into one and plot the prior against its corresponding weight. \n \n \n/* Chapter 2 of Bayesian Computation with SAS */\n/* In this chapter, Jim talked about applying \n Bayesian Rule to learn posterior probability\n using an example where the proportion of \n students sleeping over 8 hours a day is studies.\n Bayes Rule:\n  Posterior(p|data) \\prop Prior(p)* likelihood(data|p)\n*/\noptions fullstimer formchar='|----|+|---+=|-/\\<>*' error=3;\n/***************************************************/\ndata prior;\n  input prior @@;\ncards;\n2 4 8 8 4 2 1 1 1 1\n;\nrun;\n\ndata p;\n  do p=0.05 to 0.95 by 0.1;\n  output;\n end;\nrun;\n\ndata p;\n  retain sum_prior 0;\n  do until (eof1);\n  set prior end=eof1;\n  sum_prior+prior;\n  end;\n  do until (eof2);\n  merge p prior end=eof2;\n  prior=prior/sum_prior;\n  drop sum_prior;\n  output;\n  end;\nrun;\n\ngoptions reset=all;\nsymbol interpol=Needle value=none line=1 color=black;\naxis1 label=(angle=90 'Prior Probability') order=(0 to 0.3 by 0.1) minor=none;\naxis2 label=('p') order=(0 to 1 by 0.2) minor=none;\nproc gplot data=p;\n  plot prior*p /vaxis=axis1 haxis=axis2;\nrun;quit;\n \n  \n \nNext, we calculate the posterior probability given prior and likelihood conditional on prior. In the code below, we observe 11 heavy sleepers out of a total sample of 27. The first DATA STEP calculates the likelihood conditional on each prior choice, the posterior is output in the data set P2, where the normalization factor is the sum of conditional likelihood, scaled by max likelihood for numerical stability. You can observe the shape of posterior against hyper-parameter using GPLOT. The INTERPOL=NEEDLE option in SYMBOL statement minic the histogram style line of PLOT function in R. \n \n \n%let n0=16;\n%let n1=11;\n\ndata p;  \n  set p end=eof1 nobs=ntotal;\n  if p>0 & p<1 then do;\n  log_like=log(p)*&n1 + log(1-p)*&n0; \n  end;\n  else \n  log_like=-999*(p=0)*(&n1>0) + (p=1)*(&n0>0); \n  log_posterior=log(prior) + log_like; \n  if log_posterior>max_like then max_like=log_posterior;  \nrun;\n\nproc means data=p noprint;\n  var log_posterior;\n  output out=_max max(log_posterior)=max;\nrun;\n\ndata p2;\n  set _max;\n  sum_post=0;\n  do until (eof);\n  set p end=eof;\n  sum_post+exp(log_posterior-max);   \n  end;\n  do until (eof2);\n  set p end=eof2;\n  posterior=exp(log_posterior-max)/sum_post;\n  output;\n  keep p prior posterior;\n  end;\nrun;\n\ngoptions reset=all;\nsymbol value=NONE interpol=Needle;\naxis1 label=(angle=90 'Posterior Probability');\naxis2 label=('p');\nproc gplot data=p2;\n  plot posterior*p /vaxis=axis1 haxis=axis2;\nrun;quit; \n\n \n  \n \nAfter tried discrete grid of prior, beta conjugate prior is introduced. Jim mentioned that by matching the believed 50% and 90% percentiles, the parameter of prior beta distribution can be obtained by try-and-error approach. We note that the believed percentiles data indicates that the beta distribution skewed to 0, so that parameter alpha is less than parameter beta and we can use a binary search to nail down the values, which I came down to approximately 3.4 and 7.5. But I will use the {3.4, 7.4} vector as in the book. Since beta distribution is a conjugate prior for proportion, the computation is very easy. The curves for prior, likelihood and posterior are visualized. \n \n \n/* beta prior */\n\n%let a=3.4;\n%let b=7.4;\n%let s=11;\n%let f=16;\n\ndata p;\n  do i=1 to 500;\n  p=i/500;\n prior=pdf('beta', p, &a, &b);\n like=pdf('beta', p, &s, &f);\n post=pdf('beta', p, &a+&s, &b+&f);\n drop i;\n output;\n end;\nrun;\n\ngoptions reset=all;\nsymbol1 interpol=j color=red width=1 value=none;\nsymbol2 interpol=j color=blue width=2 value=none;\nsymbol3 interpol=j color=gree width=3 value=none;\naxis1 label=(angle=90 'Density') order=(0 to 5 by 1) minor=none;\naxis2 label=('p')  order=(0 to 1 by 0.2) minor=none;;\nlegend label=none position=(top right inside) mode=share;\nproc gplot data=p;\n  plot post*p prior*p like*p /overlay \n          vaxis=axis1 \n          haxis=axis2 \n          legend=legend;\nrun;quit;\n\n \n  \nSince we obtain closed form for posterior distribution, we can conduct inference based on exact calculation; alternatively, we can use simulated random sample. \n \n/* generate 1000 random obs from posterior distribution */\ndata rs;\n  call streaminit(123456);\n  do i=1 to 1000;\n   x=rand('beta', &a+&s, &b+&f);\n output;\n end;\nrun;\n\nproc univariate data=rs noprint;\n  var x;\n  histogram /midpoints=0 to 0.8 by 0.05 \n    cbarline=blue cfill=white \n    outhistogram=hist;\n  title \"Histogram of ps\";\nrun;\ntitle;\n\naxis1 label=(angle=90 \"Frequency\");\nproc gbarline data=hist;\n  bar _midpt_/discrete sumvar=_count_ space=0 axis=axis1 ;\n title \"Histogram of ps\";\nrun;\ntitle;\n\n/* hist prior */\ndata midpt;\n  do pt=0.05 to 0.95 by 0.1;\n  output;\n  end;\nrun;\n \n  \nIn the book, Jim also mentioned using so called Histogram prior. The Histogram prior is a densed-version of discrete prior, with the distance between discrete choices filled in with dense choices of the same value, separated only by a small distance. It provides a more refined description of prior believe than the discrete prior. We plot the prior and posterior as well as random sample from posterior to give a visual impression. Note that we use PROC STDIZE method=SUM to normalize prior variable. \n \n \ndata prior;\n  input prior @@;\ndatalines;\n2 4 8 8 4 2 1 1 1 1\n;\nrun;\nproc stdize data=prior method=sum out=prior; var prior; run;\n\ndata _null_;\n  set midpt nobs=ntotal1;\n  call symput('nmid', ntotal1);\n  set prior nobs=ntotal2;\n  call symput('nprior', ntotal2);\n  stop;\nrun;\n\ndata histprior;\n  array _midpt{&nmid} _temporary_;\n  array _prior{&nprior} _temporary_;\n  if _n_=1 then do;\n   k=1;\n   do until (eof1); \n   set midpt end=eof1;\n   _midpt[k]=pt; if k=2 then lo=_midpt[k]-_midpt[k-1];\n   k+1;\n   end;\n   do k=1 to &nmid;\n   _midpt[k]=round(_midpt[k]-lo/2, 0.00001);\n   put _midpt[k]=;\n   end; \n   k=1;\n   do until (eof2);\n   set prior end=eof2;\n   _prior[k]=prior;\n   put k= _prior[k]=;\n   if k<&nprior then k=k+1;  \n   end;\n   drop k lo; \n  end;\n \n  do p=1/500 to 1 by 1/500;\n   sk=0;\n   do k=1 to &nmid;\n   if p>=_midpt[k] then sk=min(&nmid, sk+1); \n   end;\n   histprior=_prior[sk];\n   output;\n  end;\n  stop;\nrun;\n\n\ngoptions reset=all;\nsymbol interpol=hiloj;\naxis1 label=(angle=90 \"Prior Density\") order=(0 to 0.3 by 0.05);\naxis2 label=(\"p\")      order=(0 to 1 by 0.2);\nproc gplot data=histprior;\n  plot histprior*p /vaxis=axis1 haxis=axis2;\nrun;quit;\n\ndata histposterior;\n  set histprior;\n  like=pdf('BETA',p, &s+1, &f+1);\n  post=like*histprior;\nrun;\n\n\n%put &s;\ngoptions reset=all;\nsymbol interpol=hiloj;\naxis1 label=(angle=90 \"Posterior Density\") order=(0 to 1 by 0.2);\nproc gplot data=histposterior;\n  plot post*p/vaxis=axis1;\nrun;quit;\n\nproc means data=histposterior sum;\n  var post;\nrun;\n\nproc stdize data=histposterior method=sum out=data1;\n  var post;\nrun;\nproc means data=data1 sum;\n  var post;\nrun;\n \n  \n  \n In R, the SAMPLE function is able to do a URS sampling with given probability. SAS's PROC SURVEYSELECT seems can't do this with method=URS. But URS sampling with given probability is very easy to code in SAS DATA STEP with the help of FORMAT. \n \n \ndata fmt;\n  merge histposterior(rename=(post=start)) \n  histposterior(firstobs=2 in=_2 rename=(post=end));\n  if _2;\nrun;\n\ndata fmt;\n  set data1 end=eof;\n  retain start;\n  retain fmtname 'ursp' type 'n';\n  if _n_=1 then start=post;\n  else do;\n  end=start+post; label=p;\n  keep fmtname start end label;\n  output;\n  start=end;\n  end;\n  if eof then do;\n  hlo='O';\n  label=.;\n  output;   \n  end;\nrun;\n\nproc sort data=fmt nodupkey out=fmt; \n  by start end;\nrun;\n\nproc format cntlin=fmt; run;\n\ndata samp;\n  do i=1 to 1000;\n   p=input(put(ranuni(78686), ursp.), best.);\n   output;\n  end;\nrun;\n\nproc univariate data=samp noprint;\n  title \"Histogram of ps\";\n  histogram p/midpoints=0.15 to 0.65 by 0.05 \n     outhist=samp_hist \n     vaxislabel=\"Frequency\" vscale=count;  \nrun;\ntitle;\n  \n \n  \n \nOnce we obtain posterior distribution of a variable, we can easily predict the value of some statistics given new samples. In the book, depending on the choice of weighting density, two types of predictions are discussed: 1. prior prediction; 2. posterior prediction. The calculation is straight-forward given formula. \n \n \n/* prediction */\ndata prior;\n  input prior @@;\ncards;\n2 4 8 8 4 2 1 1 1 1\n;\nrun;\n\ndata p;\n  do p=0.05 to 0.95 by 0.1;\n  output;\n end;\nrun;\n\ndata p;\n  retain sum_prior 0;\n do until (eof1);\n  set prior end=eof1;\n sum_prior+prior;\n end;\n do until (eof2);\n  merge p prior end=eof2;\n prior=prior/sum_prior;\n drop sum_prior;\n output;\n end;\nrun;\n\n/* prediction using discrete prior */\n%let m=20;\ndata pred;\n  do ys=0 to &m;   \n  pred=0;\n   do i=1 to 10;\n    set p point=i; \n pred = pred + prior * pdf('BINOMIAL', ys, p, &m);\n put p= prior= pred= ;\n end;\n output; \n end;\n stop;\nrun;\n\n/* prediction using beta(a, b) prior */\n%let m=20;\n%let a=3.4;\n%let b=7.4;\ndata pred;\n  do ys=0 to &m;\n  pred=comb(&m, ys) *exp(logbeta(&a + ys, &b+&m -ys) - logbeta(&a, &b));\n  output;\n  end;\nrun;\n\n/* prediction by simulation */\n%let n=1000;\ndata pred;\n  do i=1 to &n;\n  p=rand('BETA', &a, &b);\n ys=rand('BINOMIAL', p, &m);\n drop i;\n output;\n end;\nrun;\n\nproc freq data=pred noprint;\n  table ys /out=y_freq;\nrun;\n/*\nproc gbarline data=y_freq;\n  bar y /discrete sumvar=percent;\nrun;quit;\n*/\ndata y_freq;\n  set y_freq;\n percent=percent/100;\n label ys='y';\nrun;\n\ngoptions reset=all border;\naxis1 label=(angle=90 \"Predictive Probability\") order=(0 to 0.14 by 0.02);\naxis2 label=(\"y\") order=(0 to 16) offset=(15 points, 15 points) minor=none;\nsymbol interpol=needle;\nproc gplot data=y_freq;\n  plot percent * ys/vaxis=axis1 haxis=axis2 ;\nrun;quit;\n\n/* summarize discrete outcome with given coverage probability */\n/* input dsn is y_freq from PROC FREQ. The following can be used as gauge.\ndata y_freq;\n  input y percent;\ndatalines;\n0 0.013\n1 0.032\n2 0.065\n3 0.103\n4 0.102\n5 0.115\n6 0.114\n7 0.115\n8 0.095\n9 0.083\n10 0.058\n11 0.036\n12 0.029\n13 0.014\n14 0.015\n15 0.006\n16 0.005\n;\nrun;\n*/\n%let covprob=0.9;\nproc sort data=y_freq out=y_freq2;\n  by percent;\nrun;\n\ndata y_freq2;\n  set y_freq2;\n retain cumprob 0;\n cumprob+percent;\n if cumprob<(1-&covprob) then select=0;\n else select=1;\nrun;\n\nproc means data=y_freq2 sum;\n  where select=1;\n var percent;\nrun;\n \n  \n \ufeffIn Summary: \n 0. There different choices for specifying Prior believe that can be incorporated into Bayesian computation:  a.) A discrete grid of possible values, each value with some probability  b.) An appropriate distribution, where the parameters is determined from existing information  c.) A dense grid of possible values that span the whole range \n 1. We demonstrated calculation under these three scenarios and their impact on posterior distribution of statistics of interests; \n 2. We can conduct Sampling With Replacement with given sampling probability by using FORMAT and uniform random variable generator \n 3. We demonstrated that prediction in a Bayesian framework is straighforward, but depending on whether we use prior distribution or posterior distribution"], "link": "http://www.sas-programming.com/feeds/4705248398300447375/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2, "http://feedads.doubleclick.net/": 2, "http://4.blogspot.com/": 3, "http://www.amazon.com/": 1, "http://2.blogspot.com/": 6, "http://bayes.bgsu.edu/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["The book \" Bayesian Computation with R \" by Jim Albert is an easy to read entry level book on applied Bayesian Statistics. While the book was written for R users, it is not difficult to translate the languages between R and SAS and I believe it is a good way to show Bayesian capability of SAS. In the next several months, I am going to translate all R code in the book into SAS. Readers are encouraged to buy this book to understand what's behind the code. The posts here will only spend minimum effects to explain the statistics underlying while most resource will still be denoted to SAS coding. \n \n This post will cover Chapter 1. \n \n We first follow section 1.2.2, using PROC IMPORT to read in TAB deliminated file, which corresponds to read.table(file, sep='\\t', header=T) in R. We also designated PDF as Output Destination. \n \n/* Chapter 1 of Bayesian Computation with R */\n\n%let folder=C:\\Documents\\SAS for Bayesian Compu with R;\n%let datafolder=&folder\\data;\n\nlibname data \"&datafolder\";\noptions missing='.' formchar = \"|----|+|---+=|-/\\<>*\" errors=2 fullstimer;\n\n/*@@@@@@ Section 1.2.2 @@@@@*/\n/*--> Read Tab Deliminated Data into SAS */\nproc import datafile=\"&datafolder/studentdata.txt\" out=student dbms=dlm replace;\n  datarow=2;  /* redundent with GETNAMES= statement */\n guessingrows=2; /* Only use first 2 rows of data to guess the type of data */\n  delimiter='09'x; /* this is redudent if we specify DBMS=TAB in PROC IMPORT option */\n getnames=YES; /* This is the same as 'header=True' in R's table.read function */\nrun;\n\nods pdf file=\"&folder/Chapter1.pdf\";\n/*--> To see Variable Names and print first row of this data \n Use SPLIT=\"*\" to specify the split character being \"*\", which controls \n line breaks in column headings\n*/\nproc print data=student(obs=1) split=\"*\"; run;\n \n SAS output looks like: \n   Section 1.2.3, summarize frequency tables and use Barplot to visualize the counts. \n \n PROC FREQ directly corresponds to table() function in R, but provides much richer functionality. PROC GBARLINE is used to answer what barplot() does in R. The LABEL= statement in SAS is a very handy tool for annotation purpose for PRINT or GRAPH. \n \n PROC MEANS somehow corresponds to summary() function to obtain summary statistics for quantitative variables. You can sepecify MAXDEC= to keep desired number of decimals in print out. \nHistogram can be obtained in SAS via either PROC UNIVARIATE or newly introduced PROC SGPLOT. PROC UNIVARIATE provides more control though. As shown below, you can specify desired mid-points while you can't do so in SGPLOT. On the other hand, SGPLOT is very convenient if you want to overlay density curve, either a normal fit or kernel one. \n \n\n/*@@@@ section 1.2.3 @@@*/\n/*--> To summarize and graph a single Batch */\nproc freq data=student;\n  table Drink;\nrun;\n\n/*--> Barplot on single variable */\ntitle \"Original idea, summarize and plot\";\nproc freq data=student noprint;\n  table Drink /out=t;\nrun;\ngoptions reset=all noborder;\nproc gbarline data=t;\n  bar Drink /discrete space=4 sumvar=count;\nrun;quit;\ntitle;\n/*--> Actually, in SAS, it is better to directly use \n PROC GBARLINE */\ngoptions reset=all noborder;\ntitle \"In SAS, PROC GBARLINE directly summarize\";\nproc gbarline data=student;\n  bar Drink /discrete space=3 ;\nrun;quit;\ntitle;\n\ndata student;\n  set student;\n hours_of_sleep=WakeUp-ToSleep;\n /* Label can be useful for annotation purpose */\n label hours_of_sleep=\"Hours Of Sleep\"; \nrun;\n\nproc means data=student \n   /* Request same statistics as in the book */\n   min q1 median q3 max nmiss    \n   /* MAXDEC= specifies keeping 2 decimals */\n   maxdec=2;       \n var hours_of_sleep;\nrun;\n\n/*--> Histogram using PROC UNIVARIATE */\nproc univariate data=student noprint;\n  var hours_of_sleep;\n histogram /midpoints=2.5 to 12.5 by 1;\nrun;\n\n/* new SGPLOT is a handy way to draw histogram and density */\nproc sgplot data=student;\n  histogram hours_of_sleep/scale=count;  \n  density hours_of_sleep/scale=count type=normal;\nrun;\n \n Compare SAS outputs: \n \n   \n \n   \n \n    Boxplot to visualize distribution of quantitative variables by some classification variable. Here, formula such as var1*var2 style corresponds to var1~var2 style in R. \n \n\n/*@@@ section 1.2.4 @@@@*/\n/*---> Compare Batches using Boxplot */\nproc sort data=student out=studentsorted; by Gender; run;\nproc boxplot data=studentsorted;\n  plot hours_of_sleep*Gender;\nrun; \n\nproc means data=student \n   min q1 median q3 max nmiss \n   nway maxdec=2;\n  class Gender;\n var Haircut;\nrun;\n \n SAS output looks like: \n \n  \n  \n \n R has built-in jitter function, however, we have to make our own in SAS. This verision doesn't apply Fuzzy yet, only for demo purpose. \n \n\n%macro jitter(var, newname, data=last, factor=1, amount=);\n/* follow the JITTER function in R, no fuzzy applied yet . \n if amount is given, then use this value for disturbance\n else if amount is 0, then use the range of &var for disturbance\n else if amount is NULL, then use the smallest difference among\n  distinct values of &var for disturbance. if all values are \n  the same, then use the value obtained from range\n\n if range of &var is 0, then use the lower range for disturbance\n otherwise \n*/\n%let blank=%str( );\n%if &data=' ' %then %let data=last;\n%local fid;\n\ndata _null_;\n  fid=round(ranuni(0)*10000, 1);\n call symput('fid', compress(fid));\nrun;\nproc means data=&data noprint;\n  var &var;\n output out=_util&fid \n   range(&var)=range \n   min(&var)=min \n   max(&var)=max;\nrun;\ndata _util&fid;\n  set _util&fid;\n if range^=0 then z=range; else z=min;\n if z=0 then z=1;\nrun;\n\n%if %eval(&amount=&blank) %then %do;\n proc sort data=&data.(keep=&var where=(&var^=.)) \n    out=_xuti&fid nodupkey;\n  by &var;\n run;\n data _duti&fid;\n  set _xuti&fid nobs=ntotal end=eof;  \n array _x{2} _temporary_;\n if ntotal=1 then do;\n  amount=&factor/50*abs(&var);\n keep amount;\n output;\n stop;\n end;\n else do;\n  if _n_=1 then do; \n   _x[1]=&var; _x[2]=constant('BIG'); \n  end;\n else do;\n    _x[2]=min(_x[2], &var - _x[1]);\n  _x[1]=&var;\n  if eof then do;\n   amount=&factor/5*abs(_x[2]);\n  keep amount;\n  output;\n  end;\n end;\n end;\n run;\n \n%end;\n%else %if %eval(&amount=0) %then %do;\n data _duti&fid;\n  set _util&fid;\n amount=&factor*z/50;\n keep amount;\n output;\n run;  \n%end;\n%else %do;\n data _duti&fid;\n  amount=&amount;\n keep amount;\n output;\n run;\n%end;\n\nproc sql noprint;\n  select name into :keepvars separated by ' '\n from sashelp.vcolumn\n where libname='WORK' \n and memname=%upcase(\"&data\") \n and memtype=\"DATA\"\n ;\nquit;\ndata &data;  \n array _x{1} _temporary_;\n  if _n_=1 then do;\n  set _duti&fid;\n _x[1]=amount;\n end;\n  set &data;\n &newname=&var + ranuni(0)*(2*_x[1])-_x[1];\n label &newname=\"jitter(&var)\";\n keep &keepvars &newname;\nrun;\nproc datasets library=work nolist;\n  delete _duti&fid _xuti&fid _util&fid;\nrun;quit;\n%mend; \n \n \n Now we can apply jitter function to the two variables studied in the book and check them out. For graphics, we use axis statement to define how we want the two axes to appear in print out, and use symbol statement to define the appearance of symbols for data. Here we ask for black circles, not connected (interpol=none). \n \n\n%jitter(hours_of_sleep, xhos, data=student, factor=1, amount=); \n%jitter(ToSleep, xts, data=student, factor=1, amount=);\n\ngoptions border reset=all;\naxis1 label=(\"jitter(ToSleep)\") \n  major=(height=2 number=5) minor=none;\naxis2 label=(angle=90 \"jitter(Hours of Sleep\")  \n  major=(height=2 number=5) minor=none;\nsymbol1 value=circle interpol=none color=black;\nproc gplot data=student;\n  plot xhos*xts/haxis=axis1 vaxis=axis2;\nrun;quit;  \n \n  \n In order to overlay a fitted regression line, we have multiple ways to do it in SAS, and the Statistical Plot procedures are particularly handy to generate this type of figures. Compare the code and corresponding figures (in the same order of LINEPRINTER, ODS GRAPHICS, Manually Drawn, SGPLOT). For manually drawn figures, since we ask for connected dots in order to obtain a line overlay on circles, we have to sort the data by X-axis variable to make the line as straight as possible. The manually drawn figure is also the one closest to the R figure in the book. \n \n\n\n/* old school LINEPRINTER option*/\nproc reg data=student lineprinter;\n  title \"Line Printer Style\";\n  model Hours_of_Sleep=ToSleep /noprint;\n var  xhos xts; \n plot pred.*xts='X' xhos*xts/overlay symbol='o';\nrun;quit;\ntitle;\n\n/* Using ODS GRAPHICS */\nods select none; /* to suppress PROC REG output */\ntitle \"ODS Graphics\";\nods graphics on;\nproc reg data=student ;\n  model Hours_of_Sleep=ToSleep;\n var  xhos xts;\n output out=student pred=p; \n  ods select FitPlot; \nrun;quit;\nods graphics off;\ntitle;\nods select all;\n\n/* Following standard R approach, tedious in SAS */\nproc sort data=student out=studentsorted;\n  by ToSleep;\nrun;\ngoptions border reset=all;\naxis1 label=(\"jitter(ToSleep)\")  order=(-2 to 6 by 2) major=(height=2 number=5) minor=none;\naxis2 label=(angle=90 \"jitter(Hours of Sleep\") offset=(, 2 pct) major=(height=2 number=5) minor=none;\nsymbol1 value=circle interpol=none color=black;\nsymbol2 value=none interpol=line color=black;\nproc gplot data=studentsorted;\n  plot xhos*xts p*xts/overlay haxis=axis1 vaxis=axis2; \nrun;quit;  \n\n/* Using build-in REGRESSION capability of GPLOT by specifying\n INTERPOL=R<><0><> option in SYMBOL statement. \n  In the first <>, you identify the type of regression: \n   L=linear, Q=quadratic, C=cubic\n  In the second <>, you specify if intercept is omitted;\n  In the third <>, you specify type of CI:\n   CLM=CI of mean predicted value;\n   CLI=CI of individual points;\n */\ngoptions border reset=all;\naxis1 label=(\"jitter(ToSleep)\")  order=(-2 to 6 by 2) major=(height=2 number=5) minor=none;\naxis2 label=(angle=90 \"jitter(Hours of Sleep\") offset=(, 2 pct) major=(height=2 number=5) minor=none;\nsymbol1 value=circle interpol=rlclm95 color=black ci=blue co=blue;\nproc gplot data=student;\n  plot xhos*xts/regeqn /* haxis=axis1 vaxis=axis2*/;\nrun;quit;\n\n/* New SGPLOT has build in regression line capability: REG, SPLINE, LOESS */\nproc sgplot data=student;\n  scatter x=xts y=xhos;\n reg x=ToSleep y=Hours_of_Sleep;\nrun;\n \n  \n  \n     \n  Now, to study the robustness of T-Statistics which relies on several assumptions, such as normality, homoskedasticity, etc. First, we need to define a \"function\" to calculate T-statistics like in the book. \n \n\n/* section 1.3 Robustness of T-Statistics */\n/* section 1.3.2 Write a function to compute T-Statistics */\n%macro tstatistics(vars, dsn=last, outdsn=, class=);\n%if &class='' %then %let classstmt=;\n%else %let classstmt=class &class;\n\n%let nvars=%sysfunc(count(&vars, ' '));\n\n%let var1=%scan(&vars, 1, ' ');\n%let var2=%scan(&vars, 2, ' ');\nproc means data=&dsn noprint nway;\n  &classstmt;\n var &vars;\n output out=tdata(where=(_STAT_ in ('STD', 'MEAN', 'N')));\nrun;\ndata &outdsn;\n  set tdata; by &class _TYPE_;\n retain m n mean1 mean2 std:;\n select(compress(_STAT_));\n   when('N') do;\n  m=&var1;\n  n=&var2;\n end;\n when('MEAN') do;\n    mean1=&var1;\n  mean2=&var2;\n end;\n   when('STD') do;\n  std1=&var1;\n  std2=&var2;\n end;\n otherwise;\n end;\n  if last._TYPE_ then do;\n  sp=sqrt(((m-1)*std1**2 + (n-1)*std2**2)/(m+n-2));\n  t= (mean1-mean2)/(sp * sqrt(1/m + 1/n));\n keep &class _TYPE_ m n std: mean: sp t;\n output;\n end;\nrun;\n%mend;\n \n Just like the \"source(file)\" command in R, in SAS, we have a counterpart called %include (or in abv. %inc ). Suppose you have a SAS file called \"tstatistics.sas\" in some folder &folder , the following statement will read in the source code from this file. Since this is a SAS file, suffix \".sas\" is not necessary. \n \n \nfilename BCR \"&folder\";\n%inc BCR(tstatistics);\n \n The following code pieces conduct the four simulate scenarios illustrated in the book and draw the last figure. \n \ndata test;\n  input x y;\ncards;\n1 5\n4 4\n3 7\n6 6\n5 10\n;\nrun;\n\n%tstatistics(x y, dsn=test, outdsn=tdata2, class=);\n\n/* section 1.3.3 Monte Carlo study on the Robustness of T-Statistics */\n%let alpha=0.1;\n%let m=10;\n%let n=10;\n%let Niter=10000;\ndata test;\n  retain seed1 99787 seed2 76568;\n do iter=1 to &Niter;\n  do j=1 to max(&n, &m);\n   if j<=&m then call rannor(seed1, x); else x=.;\n if j<=&n then call rannor(seed2, y); else y=.;\n keep iter x y;\n output;\n end;\n end;\nrun;\n\n%tstatistics(x y, dsn=test, outdsn=tdata2, class=iter);\n\ndata _null_;\n  set tdata2 end=eof;\n retain n_reject 0;\n if abs(t)>quantile('T', 1-&alpha/2, n+m-2) then n_reject+1;\n if eof then do;\n  call symput('n_reject', n_reject/_n_);\n end;\nrun;\n%put &n_reject;\n\n\n/* section 1.3.4 Study the robustness of T-Statistics */\n\n%let alpha=0.1;\n%let m=10;\n%let n=10;\n%let Niter=10000;\ndata test;\n  retain seed1 987687 seed2 76568;\n array x[4];\n array y[4];\n  do iter=1 to &Niter;\n   do j=1 to max(&n, &m);\n  do k=1 to 4;\n  select (k);\n   when(1) do;\n      if j<=&m then call rannor(seed1, x[k]); else x[k]=.;  \n      if j<=&n then call rannor(seed2, y[k]); else y[k]=.;\n  end;\n  when(2) do;\n      if j<=&m then call rannor(seed1, x[k]); else x[k]=.;\n      if j<=&n then call rannor(seed2, y[k]); else y[k]=.;\n      y[k]=y[k]*10;\n  end;\n  when(3) do;\n      if j<=&m then x[k]=rand('T', 4); else x[k]=.;\n      if j<=&n then y[k]=rand('T', 4); else y[k]=.; \n  end;\n  otherwise do;\n      if j<=&m then call rannor(seed1, x[k]); else x[k]=.;\n      if j<=&n then call ranexp(seed2, y[k]); else y[k]=.;\n      x[k]=10+x[k]*2; y[k]=y[k]/0.1;\n  end;\n  end;   \n end;\n   keep iter x1-x4 y1-y4;\n   output;\n   end;\n  end;\nrun;\n\n%tstatistics(x1 y1, dsn=test, outdsn=tdata1, class=iter);\n\n%tstatistics(x2 y2, dsn=test, outdsn=tdata2, class=iter);\n\n%tstatistics(x3 y3, dsn=test, outdsn=tdata3, class=iter);\n\n%tstatistics(x4 y4, dsn=test, outdsn=tdata4, class=iter);\n\ndata _null_;\n  n_reject=0;\n  do until (eof1);\n   set tdata1 end=eof1 nobs=n1;\n   retain n_reject 0;\n   if abs(t)>quantile('T', 1-&alpha/2, n+m-2) then n_reject+1;\n   if eof1 then do;\n    *call symput('n_reject', n_reject/_n_);\n  p_reject=n_reject/n1;\n  put \"NOTE>> Prob(Rejection)=\" p_reject 8.4;\n   end;\n end;\n n_reject=0;\n do until (eof2);\n   set tdata2 end=eof2 nobs=n2;\n   retain n_reject 0;\n   if abs(t)>quantile('T', 1-&alpha/2, n+m-2) then n_reject+1;\n   if eof2 then do;\n    *call symput('n_reject', n_reject/_n_);\n  p_reject=n_reject/n2;\n  put \"NOTE>> Prob(Rejection)=\" p_reject 8.4;\n   end;\n end;\n\n do until (eof3);\n   set tdata3 end=eof3 nobs=n3;\n   retain n_reject 0;\n   if abs(t)>quantile('T', 1-&alpha/2, n+m-2) then n_reject+1;\n   if eof3 then do;\n    *call symput('n_reject', n_reject/_n_);\n  p_reject=n_reject/n3;\n  put \"NOTE>> Prob(Rejection)=\" p_reject 8.4;\n   end;\n end;\n\n do until (eof4);\n   set tdata4 end=eof4 nobs=n4;\n   retain n_reject 0;\n   if abs(t)>quantile('T', 1-&alpha/2, n+m-2) then n_reject+1;\n   if eof4 then do;\n    *call symput('n_reject', n_reject/_n_);\n  p_reject=n_reject/n4;\n  put \"NOTE>> Prob(Rejection)=\" p_reject 8.4;\n   end;\n end;\nrun;\n\n/* draw the overlay density curves of empirical T-statistics and T-distribution */\nproc sort data=tdata2;\n  by t;\nrun;\ndata tdata2;\n  set tdata2;\n dt=pdf('T', t, 18);\nrun;\n\nods select none;\nods output Controls=Controls;\nods output UnivariateStatistics=UniStat;\nproc kde data=tdata2 ;\n  univar t /out=density method=snr unistats;\nrun;\nods select all;\n\ndata density;\n  set density;\n dt=pdf('T', value, 18);\nrun;\n\ndata _null_;\n  set UniStat;\n if Descr='Bandwidth' then call symput ('bw', compress(round(t, 0.0001)));\nrun;\n\ngoptions reset=all border;\n\ntitle \"Comparing Empirical and Theoretical Densities\";\nlegend label=none position=(top right inside) mode=share;\naxis1 label=(\"N=&Niter Bandwidth=&bw\") \n  order=(-4 to 8 by 2) offset=(1,1) \n  major=(height=2) minor=none;\naxis2 label=(angle=90 \"Density\")  \n  order=(0 to 0.4 by 0.1) offset=(0, 0.1) \n  major=(height=2) minor=none;\nsymbol1 interpol=join color=black value=none width=3;\nsymbol2 interpol=join color=grey value=none width=1;\nproc gplot data=density;\n  plot density*value dt*value/overlay legend=legend\n         haxis=axis1 vaxis=axis2 ;\n  label dt='t(18)' density='Exact'; \nrun;quit;\n\nods pdf close;\n  \n Key Points Summary: \n \n0. We can use FORMCHAR option to eliminate weird characters \n \n1. We can use PROC IMPORT to import deliminated data and it provided flexible options to control the output; \n \n 2. We can use PROC FREQ to summarize categorical data \n \n 3. We can use PROC MEANS and PROC UNIVARIATE to generate descriptive statistics for numerical data \n \n 4. We can study the dispersion of numerical variable by categorical variable using BOXPLOT \n \n 5. We provided a JITTER function for better graphical presentation of heavily overlapped data \n \n 6. We showed that there are multiple ways to visualize scatter plots together with fitted regression lines: \n \n 6.1 LINEPRINTER in PROC REG \n 6.2 Using PROC REG to obtain regression line and overlay it on the scatter plot \n 6.3 Using ODS Graphics in PROC REG \n 6.4 Using INTERPOL=R<><><> option in SYMBOL statement \n 6.5 Using PROC SGPLOT \n 7. We demonstrated how to conduct robust T-test in SAS following the code in the book."], "link": "http://www.sas-programming.com/feeds/6025913615236309735/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 6, "http://feedads.doubleclick.net/": 2, "http://1.blogspot.com/": 5, "http://4.blogspot.com/": 3, "http://www.amazon.com/": 1, "http://2.blogspot.com/": 1, "http://bayes.bgsu.edu/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Obtain summary statistics over a rolling window for a given data, usually on a time dimension, is not quit easy in SAS, especially the rolling window may contain different number of records and the maximum number is unknown without pass the data once. For example, given a transaction data over several days, a business analyst wants to summarize the data for each 24 hour period. This is actually a recent question on SAS-L. \n \nThere are several approaches. Typically people use an array that is large enough to handle a reasonable guess of maximum number of records within the given interval. Or use a hash table so to manage the data cells dynamically. The coder basically needs to build a stack which is first in first out. Both of these methods are not easy to code and error prone for a beginner. \n \nThis rolling window problem can be efficiently solved by using MultiLabel Format, a feature tend to be ignored. Here are two example. We want to summarize for a rolling window of size 5. Pay attention to the second example. \n \ndata fmt;\n  retain fmtname 'rollwindow' type 'n' hlo 'M';\n  do start=1 to 10;\n   end=start+5; \n  label=cats('time', start);\n  output;\n  end; \n  hlo='O'; label='Out-Of-Bound';\n  output;\nrun;\n\ndata dsn;\n  do time=1 to 20;\n   x=rannor(0);\n   y=ranuni(0);\n   output;\n  end;\nrun;\n\nproc format cntlin=fmt; run;\n\nproc means data=dsn noprint;\n  class time /preloadfmt mlf; \n  format time rollwindow.;\n  var x y;\n  output out=summary_roll mean(x y)= std(x y)= /autoname;\nrun;\n \n\ndata dsn2;\n  do time=1 to 20;\n   k=ranpoi(10, 10);\n   do j=1 to k;\n   time=time+j/(k+1);\n   x=rannor(0); y=ranuni(0);\n   output;\n   end;\n  end;\n  drop k j;\nrun;\nproc means data=dsn2 noprint;\n  class time/preloadfmt mlf exclusive;\n  format time rollwindow.;\n  var x y;\n  output out=summary_roll2 mean()= std()=/autoname;\nrun;\n\n\n/**************************************************** \n   non-rolling but shrinking time window, \n   similar for growing time window \n*****************************************************/\ndata fmt2;\n  retain fmtname 'winx' type 'n' hlo 'M';\n  do start=1 to 10;\n   end=18; \n  label=cats('time', start);\n  output;\n  end; \n  hlo='O'; label='Out-Of-Bound';\n  output;\nrun;  \n\nproc format cntlin=fmt2 cntlout=fmt_all;\nrun;\n\nproc means data=dsn2 noprint;\n  class time/preloadfmt mlf exclusive;\n  format time winx.;\n  var x y;\n  output out=summary_roll3 mean()= std()=/autoname;\nrun;\n\n**************** An example **************;\ndata TradeDate;\ninput TradeDate yymmdd10.;\nformat TradeDate yymmdd10.;\ncards;\n2007-01-04\n2007-01-05\n2007-01-08\n2007-01-09\n2007-01-10\n2007-01-11\n2007-01-12\n2007-01-15\n2007-01-16\n2007-01-17\n2007-01-18\n2007-01-19\n2007-01-22\n2007-01-23\n2007-01-24\n2007-01-25\n2007-01-26\n2007-01-29\n2007-01-30\n2007-01-31\n ;\nrun;\ndata raw;\ninput id $ Date_S yymmdd10. +1 Date_e yymmdd10. Buy;\nformat Date_S Date_E yymmdd10.;\ncards;\nA001 2007-01-09 2007-01-24 24.5\nA001 2007-01-12 2007-01-16 56.6\n ;\nrun;\n\n/*------------- Desired Output ------------*\nid  Date_S  Date_E  Buy Hold_Days\nA001 2007-01-09 2007-01-24 24.5 12\nA001 2007-01-12 2007-01-30 56.6  3\n-------------------------------------------*/\ndata fmt;\n  set raw;\n  retain fmtname 'tdate' type 'n' hlo 'M';\n  start=Date_S; end=Date_e;\n  label=cats(ID, _n_);\nrun;\nproc format cntlin=fmt out=fmt_ref;\nrun;\n\nproc means data=tradeDate noprint nway;\n  class TradeDate/mlf exclusive preloadfmt ;\n  format TradeDate tdate.;\n  var TradeDate;\n  output out=_test n()=_freq_;\nrun;\n\n \nWhen the data is getting large, there will be some computing difficulty and insufficient resource. To solve this problem, simply sort and divide the original data into smaller pieces with an overlap equals to the size of rolling window. When combine the summarized pieces, you should discard the overlap part from the rest pieces. \n \nThe multilabel approach has an overhead that is smaller overall to the other methods and can be easily changed to accommandate other sizes of rolling windows."], "link": "http://www.sas-programming.com/feeds/8224701217907225470/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Low Rank Radial Smoothing using GLIMMIX [1], a semiparametric approach to smooth curves [2]. Specifying TYPE=RSMOOTH option in RANDOM statement, we can implement this spline smooth approach. The best thing is that for future scoring, data preparation is extremely easy by using the OUTDESIGN= & NOFIT options in v9.2 PROC GLIMMIX, then use PROC SCORE twice on this design matrix to score the fixed effects design matrix X and the random effects design matrix Z, respective, add up together is the score from this radial smoothing method. \n \n[Coming soon] \n \n \n \n \nproc glimmix data=train_data absconv=0.005;\n  model y = &covars /s;\n  random &z /s type=rsmooth knotmethod=equal(20);\nrun;\n\nproc glimmix data=test nofit outdesign=test2;\n  model y=&covars /s;\n  random &z /s type=rsmooth knotmethod=equal(20);\nrun;\n\n\nproc score data=test2 score=beta_fix type=parms out=score_fix;\n  var &covars;\nrun;\n\nproc score data=test2 score=beta_random type=parms out=score_random;\n  var _z:;\nrun;\n \n \n \n \nReference: \n \n1. SAS Institute, Statistical Analysis with the GLIMMIX procedure Course Notes, SAS Press, SAS Institute \n2. D Rupper, M.P. Wand, R.J. Carroll, Semiparametric Regression, Cambridge University Press, Cambridge, 2003"], "link": "http://www.sas-programming.com/feeds/869207075818705085/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Using SAS to implement Flexible Discriminant Analysis [Chapter 12, ESL]. \n \nSince a discriminant analysis is equivalent to a 2-step process, i.e. regress first then conduct discriminant analysis, it is easy to implement the so called generalized discriminant analysis shown in Ch.12.4--12.6 of Elements of Statistical Learning. The basic idea here is to use some regression analysis procedures, such as using PROC REG and its RIDGE= option in MODEL statement for the ridge regression, and then use the prediction from L2 regularized regression in next step's discriminant analysis. Using PROC GLMSELECT, we can replace L2 regularization with a L1 regularization. \n \nA piece of prototype code looks like this: \n \nPROC GLMMOD data=&yourdata OUTDESIGN=&design\n  CLASS &dep_var;\n  model X = &dep_var /noint;\nRUN;\ndata &yourdata\n  merge &yourdata &design\n  rename Col1-Col&k = Y1 -Y&k\nrun;\n%let deps= Y1-Y&k /* for the case of 5-class problem */\nPROC REG DATA=&yourdata RIDGE=&minridge to &maxridge by 0.1 OUTEST=beta;\nMODEL &deps = &covars ;\nOUTPUT OUT=predicted PRED=&dep._HAT;\nRUN;\n\nPROC DISCRIM DATA=predicted &options\nCLASS &dep\nVAR &dep._HAT;\nRUN;"], "link": "http://www.sas-programming.com/feeds/4101905517772952711/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Partial Least Square is one of several supervised dimension reduction techniques and attracts attention in recent years. In the one hand, PLS is able to generate a series of scores that maximize linear correlation between dependent variables and independent variables, on the other hand, the loading of PLS can be regarded as similar counterpart from factor analysis, hence we can rotate the loadings from PLS therefore eliminate some of the non-significant variable in terms of prediction. \n \n \n%macro PLSRotate(Loading, TransMat, PatternOut, PatternShort, \n     method=VARIMAX, threshold=0.25);\n/* VARIMAX rotation of PLS loadings. Only variables having \n large loadings after rotation will enter the final model. \n\n Loading dataset contains XLoadings output from PROC PLS \n and should have variable called NumberOfFactors\n TransMat is the generated Transformation matrix;\n PatternOut is the output Pattern after rotation;\n PatternShort is the output Pattern with selected variables\n*/\n\n%local covars;\nproc sql noprint;\n  select name into :covars separated by ' '\n from sashelp.vcolumn\n where libname=\"WORK\" & memname=upcase(\"&Loading\") \n  & upcase(name) NE \"NUMBEROFFACTORS\"\n & type=\"num\"\n ;\nquit;\n%put &covars;\n\ndata &Loading.(type=factor);\n   set &Loading;\n   _TYPE_='PATTERN';\n   _NAME_=compress('factor'||_n_);\nrun;\nods select none;\nods output OrthRotFactPat=&PatternOut;\nods output OrthTrans=&TransMat; \nproc factor data=&Loading method=pattern rotate=&method simple; \n   var &covars;\nrun;\nods select all;\n\ndata &PatternShort;\n  set &PatternOut;\n array _f{*} factor:;\n _cntfac=0;\n do _j=1 to dim(_f); \n  _f[_j]=_f[_j]*(abs(_f[_j])>&threshold); _cntfac+(_f[_j]>0); \n  end;\n if _cntfac>0 then output;\n drop _cntfac _j;\nrun;\n%mend;\n \nHere I try to replicate the case study in [1] which elaborated how to do and properties of VARIMAX rotation to PLS loadings. The PROC PLS output, after various tweaks on convergence criteria and singularity conditions, is still a little different from the result reported in [1] for factors other than the leading one, therefore, I will directly use the U=PS matrix in pp.215. \n \n \n\ndata loading;\ninput factor1-factor3;\ncards;\n-0.9280 -0.0481 0.2750\n0.0563 -0.8833 0.5306\n-0.9296 -0.0450 0.2720\n-0.7534 0.1705 -0.5945\n0.5917 -0.0251 -0.6450\n0.9082 0.3345 0.1118\n-0.8086 0.4551 -0.3800\n;\nrun;\n\n\nproc transpose data=loading out=loading2;\nrun;\n\ndata loading2(type=factor);\n  retain _TYPE_ \"PATTERN\";\n set loading2;\nrun;\n\n\nods select none;\nods output OrthRotFactPat=OrthRotationOut;\nods output OrthTrans=OrthTrans; \nproc factor data=Loading2 method=pattern rotate=varimax simple; \n   var col1-col7;\nrun;\nods select all;\n \n \nReference: \n[1] Huiwen Wang; Qiang Liu , Yongping Tu , \" Interpretation of PLS Regression Models with VARIMAX Rotation\", Computational Statistics and Data Analysis, Vol.48 (2005) pp207 \u2013 219"], "link": "http://www.sas-programming.com/feeds/6855499064591918912/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["One guy asked in a SAS forum about a typical table look up problem: \nHe has a data with two IDs: \nid1 id2 \na b \na e \nb c \nb e \nc e \nd e \n \nand he wants to generate a new data set with the following structure according to above information : \nid a b c d e \na 0 1 0 0 1 \nb 1 0 1 0 1 \nc 0 1 0 0 1 \nd 0 0 0 0 1 \ne 1 1 1 1 0 \n \nThe real data is potentially big. \n***************************; \nAt first look, this is a typical table look up problem SAS programmers facing almost everyday, that is duplicate keyed lookup table. It is a simple one because there is no inherent relationship among records. \n \n \ndata original;\n input id1 $ id2 $;\ndatalines;\na b\na e\nb c\nb e\nc e\nd e\n;\nrun;\n\nproc datasets library=work nolist;\n  modify original;\n index create id1 id2;\nquit;\n\nproc sql;\n  create table all_cases as\n select a.*, monotonic() as seq\n from (\n select distinct id1 as id\n from original\n union\n select distinct id2 as id\n from original\n ) as a\n order by a.id\n ;\nquit;\n\nproc sql noprint;\n  select id into :idnames separated by ' '\n from all_cases\n ;\nquit;\n\ndata new;\n if _n_=1 then do;\n  declare hash _h(dataset:'all_cases');\n  _h.defineKey('id');\n  _h.defineData('seq');\n  _h.defineDone();\n  end; \n  set all_cases;\n\n array _a{*} &idnames \n\n id1=id; \n set original key=id1;  \n _mx_=%sysrc(_sok);\n \n do while (_iorc_=%sysrc(_sok)); \n  rc=_h.find(key:id2); if rc=0 then _a[seq]=1;\n id1=id;\n  set original key=id1; \n \n end;\n _ERROR_=0;\n \n id2=id; \n set original key=id2;  \n do while (_iorc_=%sysrc(_sok)); \n  rc=_h.find(key:id1); if rc=0 then _a[seq]=1;\n id2=id;\n  set original key=id2; \n end;\n _ERROR_=0;\n do j=1 to dim(_a); _a[j]=max(0, _a[j]); end;\n keep id &idnames\nrun;\n \nOn the other hand, this problem can be solved in a more SASsy way like this: \n \n \ndata original;\n input id1 $ id2 $;\ndatalines;\na b\na e\nb c\nb e\nc e\nd e\n;\nrun;\n\nproc sql;\n  create table newx as\n  select a.id1, a.id2, (sum(a.id1=c.id1 & a.id2=c.id2)>0) as count\n  from \n  (select a.id as id1, b.id as id2 \n  from all_cases as a, all_cases as b) as a\nleft join original as c\n  on a.id1=c.id1 or a.id2=c.id1\n group by a.id1, a.id2\n ;\nquit;\n\nproc transpose data=newx out=_freq_t name=id2;\n  by id1;\n  var count;\n  id id2;\nrun;\n\ndata _freq_t;\n  set _freq_t;\n  array _n{*} _numeric_;\n  do i=1 to dim(_n);\n  _n[i]=(_n[i]>0);\n  end;\n  drop i;\nrun;\n\nproc transpose data=_freq_t(drop=id2) out=_freq_t2 name=id1;\n  id id1;\nrun;\n\nproc sql noprint;\n  select id1, count(distinct id1) into :covars separated by ' ', :count\n  from _freq_t; \nquit;\n\ndata new2;\n  set _freq_t;\n  array _x{*} &covars\n  array _x2{&count} _temporary_;\n\n  do j=1 to &count _x2[j]=_x[j]; end;\n  set _freq_t2;\n  do j=1 to &count _x[j]=(_x[j]+_x2[j]>0); end;\n  drop j id2;\nrun;"], "link": "http://www.sas-programming.com/feeds/5475932701046935991/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["Just back from KDD2010. In the conference, there are several papers that interested me. \n \nOn the computation side, Liang Sun et al.'s paper [1], \"A Scalable Two-Stage Approach for a Class of Dimensionality Reduction Techniques\" caught my eyes. Liang proves that a class of dimension reduction techniques, such as CCA, OPLS, LDA, etc, that relies on general eigenvalue decomposition, can be computed in a much cheaper way by decomposing the original computation into a least square problem and a much smaller scale eigenvalue decomposition problem. The equivalence of their two stage approach and direct eigenvalue decomposition is rigourously proved. \n \nThis technique is of particular interest to ppl like me that only have limited computing resources and I believe it would be good to implement their algorithm in SAS. For example, a Canonical Discriminant Analysis with above idea is demonstrated below. Note also that by specifing RIDGE= option in PROC REG, the regularized version can be implemented as well, besides, PROC REG is multi-threaded in SAS. Of course, the computing advantage is only appreciatable when the number of features is very large. \n \nThe canonical analysis result from reduced version PROC CANDISC is the same as the full version. \n \nIn fact, this exercise is the answer for Exercise 4.3 of The Elements of Statistical Learning [2] \n \n[1]. Liang Sun, Betul Ceran, Jieping Ye, \" A Scalable Two-Stage Approach for a Class of Dimensionality Reduction Techniques \", KDD2010, Washington DC. \n \n[2]. Trevor Hastie, Robert Tibshirani, Jerome Friedman, \"The Elements of Statistical Learning\", 2nd Edition. \n \n  \n \n \n\n\n proc format; \n  value specname \n   1='Setosa ' \n   2='Versicolor' \n   3='Virginica '; \n run; \n \n data iris; \n  title 'Fisher (1936) Iris Data'; \n  input SepalLength SepalWidth PetalLength PetalWidth \n   Species @@; \n  format Species specname.; \n  label SepalLength='Sepal Length in mm.' \n   SepalWidth ='Sepal Width in mm.' \n   PetalLength='Petal Length in mm.' \n   PetalWidth ='Petal Width in mm.'; \n  symbol = put(Species, specname10.); \n  datalines; \n 50 33 14 02 1 64 28 56 22 3 65 28 46 15 2 67 31 56 24 3 \n 63 28 51 15 3 46 34 14 03 1 69 31 51 23 3 62 22 45 15 2 \n 59 32 48 18 2 46 36 10 02 1 61 30 46 14 2 60 27 51 16 2 \n 65 30 52 20 3 56 25 39 11 2 65 30 55 18 3 58 27 51 19 3 \n 68 32 59 23 3 51 33 17 05 1 57 28 45 13 2 62 34 54 23 3 \n 77 38 67 22 3 63 33 47 16 2 67 33 57 25 3 76 30 66 21 3 \n 49 25 45 17 3 55 35 13 02 1 67 30 52 23 3 70 32 47 14 2 \n 64 32 45 15 2 61 28 40 13 2 48 31 16 02 1 59 30 51 18 3 \n 55 24 38 11 2 63 25 50 19 3 64 32 53 23 3 52 34 14 02 1 \n 49 36 14 01 1 54 30 45 15 2 79 38 64 20 3 44 32 13 02 1 \n 67 33 57 21 3 50 35 16 06 1 58 26 40 12 2 44 30 13 02 1 \n 77 28 67 20 3 63 27 49 18 3 47 32 16 02 1 55 26 44 12 2 \n 50 23 33 10 2 72 32 60 18 3 48 30 14 03 1 51 38 16 02 1 \n 61 30 49 18 3 48 34 19 02 1 50 30 16 02 1 50 32 12 02 1 \n 61 26 56 14 3 64 28 56 21 3 43 30 11 01 1 58 40 12 02 1 \n 51 38 19 04 1 67 31 44 14 2 62 28 48 18 3 49 30 14 02 1 \n 51 35 14 02 1 56 30 45 15 2 58 27 41 10 2 50 34 16 04 1 \n 46 32 14 02 1 60 29 45 15 2 57 26 35 10 2 57 44 15 04 1 \n 50 36 14 02 1 77 30 61 23 3 63 34 56 24 3 58 27 51 19 3 \n 57 29 42 13 2 72 30 58 16 3 54 34 15 04 1 52 41 15 01 1 \n 71 30 59 21 3 64 31 55 18 3 60 30 48 18 3 63 29 56 18 3 \n 49 24 33 10 2 56 27 42 13 2 57 30 42 12 2 55 42 14 02 1 \n 49 31 15 02 1 77 26 69 23 3 60 22 50 15 3 54 39 17 04 1 \n 66 29 46 13 2 52 27 39 14 2 60 34 45 16 2 50 34 15 02 1 \n 44 29 14 02 1 50 20 35 10 2 55 24 37 10 2 58 27 39 12 2 \n 47 32 13 02 1 46 31 15 02 1 69 32 57 23 3 62 29 43 13 2 \n 74 28 61 19 3 59 30 42 15 2 51 34 15 02 1 50 35 13 03 1 \n 56 28 49 20 3 60 22 40 10 2 73 29 63 18 3 67 25 58 18 3 \n 49 31 15 01 1 67 31 47 15 2 63 23 44 13 2 54 37 15 02 1 \n 56 30 41 13 2 63 25 49 15 2 61 28 47 12 2 64 29 43 13 2 \n 51 25 30 11 2 57 28 41 13 2 65 30 58 22 3 69 31 54 21 3 \n 54 39 13 04 1 51 35 14 03 1 72 36 61 25 3 65 32 51 20 3 \n 61 29 47 14 2 56 29 36 13 2 69 31 49 15 2 64 27 53 19 3 \n 68 30 55 21 3 55 25 40 13 2 48 34 16 02 1 48 30 14 01 1 \n 45 23 13 03 1 57 25 50 20 3 57 38 17 03 1 51 38 15 03 1 \n 55 23 40 13 2 66 30 44 14 2 68 28 48 14 2 54 34 17 02 1 \n 51 37 15 04 1 52 35 15 02 1 58 28 51 24 3 67 30 50 17 2 \n 63 33 60 25 3 53 37 15 02 1 \n ; \n proc candisc data=iris out=outcan distance anova; \n  class Species; \n  var SepalLength SepalWidth PetalLength PetalWidth; \n run;\n \n ods select none;\n proc glmmod data=iris outdesign=H(keep=COL:);\n   class Species;\n  model SepalLength=Species/noint;\n run; \n\n data H;\n   merge H iris;\n run;\n\n/**************************\nfor efficiency consideration, a view can also be used:\ndata H/view=H;\n  set iris;\n  array _S{*} Col1-Col3 (3*0);  \n  do j=1 to dim(_S); _S[j]=0; end;\n  _S[Species]=1;\n  drop j;\nrun;\n****************************/\n proc reg data=H outest=beta;\n   model Col1-Col3 = SepalLength SepalWidth PetalLength PetalWidth;\n output out=P p=yhat1-yhat3;\n run;quit;\n ods select all;\n\n\n proc candisc data=P;\n   class Species;\n var yhat1-yhat3;\n run;"], "link": "http://www.sas-programming.com/feeds/1890102299009695989/comments/default", "bloglinks": {}, "links": {"http://www.asu.edu/": 1, "http://www.amazon.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["In the 2010 SASware Ballot\u00ae, a dedicated PROC for Randomized SVD was among the options. While an official SAS PROC will not be available in the immediate future as well as in older SAS releases, it is fairly simple to implement this algorithm using existing SAS/STAT procedures. \n \nRandomized SVD will be useful for large scale, high dimension data mining problems, for instance Text Mining. In SAS/Base and SAS/STAT, lack of sparse matrix operation capability puts any serious Text Mining task at the edge of infeasibility, such as using LSI or NMF algorithms. Randomized SVD provides an economic alternate solution by sacrificing a little accuracy which is bounded under the three sampling schema proposed by the authors [1], while the code below demos sampling schema 1. \n \n \n\n/* Randomized SVD with sampling schema 1. */\n%let dim=2048;\n%let nobs=1e4;\n%let s=256;\ndata matrix;\n  array _x{*} x1-x&dim;\n  do id=1 to &nobs;\n  do _j=1 to dim(_x); _x[_j]=sin(mod(id, _j))+rannor(id); end;\n output;\n drop _j;\n end;\nrun;\n\n%let datetime_start = %sysfunc(TIME()) ;\n%let time=%sysfunc(datetime(), datetime.); %put &time;\ndata seed;\n  array _x{*} x1-x&dim;\n  do _j=1 to dim(_x); _x[_j]=0; end;\n output;\n stop;\nrun;\n\nproc fastclus data=matrix seed=seed  out=norm(keep=ID DISTANCE)\n    maxiter=0 maxclusters=1 noprint replace=none;\n  var x1-x&dim;\nrun;\ndata normv/ view=normv;\n  set norm(keep=DISTANCE);\n  DISTANCE2=DISTANCE**2;\n  drop DISTANCE;\nrun;\nproc means data=normv noprint;\n  var DISTANCE2;\n  output out=matrixnorm sum(DISTANCE2)=Frobenius_sqr;\nrun;\ndata prob;\n  set matrixnorm ;\n retain Frobenius_sqr;\n do until (eof);\n  set norm end=eof;\n _rate_=DISTANCE**2/Frobenius_sqr;\n keep ID _rate_;\n output;\n end;\nrun;\n\ndata matrixv/view=matrixv;\n  merge matrix prob(keep=_rate_);\nrun;\n\nods select none;\nproc surveyselect data=matrixv out=matrixsamp(drop=SamplingWeight ExpectedHits NumberHits _rate_) \n     sampsize=&s method=pps_wr outhits ;\n  size _rate_;\nrun;\nods select all;\n\nproc transpose data=matrixsamp out=matrixsamp;\n  var x1-x&dim;\nrun;\n\nproc princomp data=matrixsamp outstat=testv(where=(_type_ in (\"USCORE\"))) \n    noint cov noprint;\n var col1-col&s;\nrun;\ndata testV_t/view=testV_t;\n  retain _TYPE_ 'PARMS';\n set testv(drop=_TYPE_);\nrun;  \n\nproc score data=matrixsamp score=testV_t type=parms \n   out=SW(keep=ID Prin:);\n var col1-col&s;\nrun;\n\ndata seed;\n  array _s{*} prin1-prin&s;\n do _j=1 to dim(_s); _s[_j]=0; end;\n drop _j; output; stop;\nrun;\n\nproc fastclus data=SW seed=seed maxiter=0 maxc=1 replace=NONE out=SW2(drop=CLUSTER) noprint;\n  var prin1-prin&s;\nrun;\n\ndata HHT;\n  set SW2;\n array _x{*} prin1-prin&s;\n do _j=1 to dim(_x); _x[_j]=(_x[_j]/distance)**2; end;\n drop _j distance;\nrun;\n\nproc transpose data=HHT out=HHT2(drop=_LABEL_);  \nrun;\ndata HHT2; \n  _TYPE_='PARMS';\n  set HHT2; \n rename COL1-COL2048=x1-X2048;\nrun;\n\nproc score data=matrix score=HHT2 type=parms out=P(drop=x:);\n  var x:;\nrun;  \n\n%let time=%sysfunc(datetime(), datetime.); %put &time;\n%put PROCESSING TIME: %sysfunc(putn(%sysevalf(%sysfunc(TIME())-&datetime_start.),mmss.)) (mm:ss) ;\noptions notes source;\n\n \nReference: \n[1], P. Drineas and M. W. Mahoney , \"Randomized Algorithms for Matrices and Massive Data Sets\", Proc. of the 32nd Annual Conference on Very Large Data Bases (VLDB), p. 1269, 2006."], "link": "http://www.sas-programming.com/feeds/7542050131525605054/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["In some data mining applications, matrix norm has to be calculated, for instance [1]. You can find a detailed explanation of Matrix Norm on Wiki @ Here \n \nInstead of user written routine in DATA STEP, we can obtain \"Entrywise\" norm via PROC FASTCLUS efficiently and accurately. \n \n \ndata matrix;\n  input X1-X5;\ndatalines;\n1 2 4 5 6\n7 8 9 0 1\n2 3 4 5 6\n3 4 5 6 7\n7 8 9 0 2\n2 4 6 8 0\n;\nrun;\n\ndata seed;\n  input X1-X5;\ndatalines;\n0 0 0 0 0\n;\nrun;\n\noptions nosource;\nproc export data=matrix outfile='c:\\matrix.csv' dbms=csv replace; run;\noptions source;\n\nproc fastclus data=matrix seed=seed  out=norm(keep=DISTANCE)\n    maxiter=0 maxclusters=1 noprint ;\n  var x1-x5;\nrun;\n\n/* \nIn output file NORM, variable DISTANCE is the square root of Frobenius norm. If LEAST=P option is specified, then p-norm is calculated. In PROC FASTCLUS, you can specify p in the range of [1, \\inf].\n\nNow what you got is vector norm for each row, taking the sum of squares of DISTANCE, you obtain the Frobenius norm of the data matrix, which can be easily obtained through PROC MEANS on a data view: \n*/\ndata normv/ view=normv;\n  set norm(keep=DISTANCE);\n  DISTANCE2=DISTANCE**2;\n  drop DISTANCE;\nrun;\nproc means data=normv noprint;\n  var DISTANCE2;\n  output out=matrixnorm sum(DISTANCE2)=Frobenius_sqr;\nrun;\n \nYou can use the following R code to verify the results; \n \nmat <- read.csv('c:/matrix.csv', header=T)\n#verify vector norm\nvnorm <- apply(mat, 1, function(x){sqrt(sum(x^2))});\n#verify norm of the matrix\nx<-as.matrix(mat)\nsqrt(sum(diag(t(x)%*%x)))\n \nPS: \n1. Of course, above process is designed for implementing the randomized SVD in [1]. If only the matrix Frobenius norm is of interests, you can also use the following code snippet: \n \n \ndata matrixv/view=matrixv;\n  set matrix;\n  array _x{*} x1-x5;\n  array _y{*} y1-y5;\n  do j=1 to dim(_x); _y[j]=_x[j]**2; end;\n  keep y1-y5;\nrun;\n\nproc means data=matrixv noprint;\n  var y1-y5;\n  output out=_var(drop=_TYPE_ _FREQ_) sum()=/autoname;\nrun;\n\ndata _null_;\n  set _var; \n  norm=sqrt(sum(of _numeric_));\n  put norm=;\nrun;\n/* --LOG WRITES:\nnorm=28.635642127\nNOTE: There were 1 observations read from the data set WORK._VAR.\n*/\n\n \n2. Using its built-in computing engine for Eucleadian Distance, PROC FASTCLUS is also a powerful tool to search for the data point in main table that is CLOEST to the a record in lookup table. This technique is shown Here and [2]. \n \n \n Reference: \n[1], P. Drineas and M. W. Mahoney , \" Randomized Algorithms for Matrices and Massive Data Sets \", Proc. of the 32nd Annual Conference on Very Large Data Bases (VLDB), p. 1269, 2006. \n \n[2], Dorfman, Paul M.; Vyverman, Koen; Dorfman, Victor P., \" Black Belt Hashigana \", Proc. of the 2010 SAS Global Forum, Seattle, WA, 2010"], "link": "http://www.sas-programming.com/feeds/26676881494384467/comments/default", "bloglinks": {}, "links": {"http://feedads.doubleclick.net/": 2, "http://en.wikipedia.org/": 1, "http://www.sas-programming.com/": 1}, "blogtitle": "SAS Programming for Data Mining"}, {"content": ["data nonlinear;\n  do x=1 to 627;\n  p=(sin(x/100)+1)*0.45;\n  do j=1 to 100;\n  x1=x+(j-1)/100;\n  if ranuni(8655645)<=p then y=1; else y=0;\n  output; drop p j;\n end;\n end;\nrun;\n\nproc rank data=nonlinear out=nonlinearrank groups=20;\n  var x1;\n ranks rank1;\nrun;\n\nproc means data=nonlinearrank noprint;\n  class rank1;\n var y x1;\n output out=_mean(where=(_type_=1)) mean(y)=y mean(x1)=x1;\nrun;\n\n%inc \"C:\\Documents and Settings\\lxie\\Desktop\\SAS Prog and Docs\\Boost macro2 ver3.0.sas\";\n%inc \"C:\\Documents and Settings\\lxie\\Desktop\\SAS Prog and Docs\\predict macro.sas\";\n\n%boost2(nonlinear, 1, outputds, outwts, 100, 3);\n\n%macro pred(niter);\n%do i=1 %to &niter;\n %predict(nonlinear, 1, outputds, outlogds, out_pred, sumpred&i, 3, &i);\n%end;\ndata sumpred_all;\n  merge %do i=1 %to &niter;\n   sumpred&i.(rename=(sum_pred=sum_pred&i))\n  %end;;\nrun;\n%mend;\n\n%pred(100);\n\nproc datasets library=work nolist;\n  delete sumpred1-sumpred100;\nquit;\n\noptions nosource;\nproc export data=sumpred_all outfile=\"c:\\sumpred.csv\" dbms=csv replace; run;\noptions source;\n \nIn R, use this code piece to recover the animation: \n \nlibrary(caTools);\ntest<-read.csv('c:/sumpred.csv', header=T)\nminmax<-quantile(as.matrix(test), c(0,1))\nsz=314; p=100;\ny<-sort(sample(1:62700, size=sz))\nimage=array(0, c(sz, p, 100))\nfor (i in (1:100)){\n pic=matrix(0, ncol=p, nrow=sz)\n trace=round((test[y,i]-minmax[1])/(minmax[2]-minmax[1])*p);\n for(j in (1:sz)){\n  image[j, trace[j], i]=1\n }\n}\nwrite.gif(image, \"c:/boost.gif\", col=gray(1:2/2))\nim = read.gif(\"c:/boost.gif\")\nfor(i in 1:100){\n image(im$image[,,i], col=(im$col), \n   main=paste('Iter', i), y=1:100, x=1:314, \n   xlab=\"Index\", ylab=\"Percentage of 1\")\n}\n\ntest2<-test[,-1]-test[-100]\ntest2<-cbind(test[,1], test2)\n\npar(mfrow=c(2,1))\nfor (i in (1:100)){\n plot(test2[,i], type='l', ylim=c(-0.5, 1), \n   main=paste('Iteration', i, 'Delta'),\n   ylab='Delta')\n plot(test[,i], type='l', ylim=minmax, \n   main=paste('Iteration', i, 'Score'),\n   ylab='Score')\n Sys.sleep(0.1)\n}"], "link": "http://www.sas-programming.com/feeds/2897517425666845174/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://feedads.doubleclick.net/": 2}, "blogtitle": "SAS Programming for Data Mining"}]