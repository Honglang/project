[{"blogurl": "http://r4stats.com\n", "blogroll": [], "title": "r4stats.com"}, {"content": ["My workshop R for SAS, SPSS and Stata Users has been popular over the years, but it\u2019s time for an overhaul. A common request has been to simplify it, so I have moved data management to a separate 4-hour workshop, Managing Data with R . This makes it much easier to absorb the basics in the remaining two 4-hour sessions. When you\u2019re ready for more, you can take the other workshop which I\u2019ll be offering several time per year. Detailed course outlines are available at the workshop links above and at the Revolution Analytics web site."], "link": "http://r4stats.com/2012/10/02/workshop-redesigned/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://bit.ly/": 3}, "blogtitle": "r4stats.com"}, {"content": ["R has several ways to specify which variables to use in an analysis. Some of the most frustrating errors can result from not understanding the order in which R searches for variables. This post demonstrates that order, hopefully smoothing your future use of R. \n If all your variables are vectors in your workspace, using them in an analysis is easy: simply name them. For example, you could build a linear model (regression) using the lm function like this: \n lm(y ~ x) \n However, data frames exist for a good reason. They help organize variables and keep the values of each observation (the rows) locked together. For example, when you sort a data frame, all the rows of a data frame are moved, not just the single variable you\u2019re sorting on. Once variables are stored in a data frame however, referring to them gets more complicated. R can include variables from multiple places (e.g. two data frames or a data frame and the workspace) so it becomes important to know your options and how R views them. \n You can specify the names of both a data frame and a variable using the compound forms mydata$myvar or mydata[\"myvar\"]. However, that often means that you have to type the name of the data frame quite a lot. \n If you use the form \u201cwith(mydata,\u2026\u201d then R will look in that data frame for the \u201cshort\u201d variable names before it looks elsewhere, like in your workspace. That allows you to type the data frame name only once per function call, but in a long program you would still end up typing it a lot. \n Modeling functions in R often let you specify \u201cdata = mydata\u201d allowing you to use short variable names in formulas like \u201cy ~ x\u201d. The result is like the \u201cwith\u201d function, you must type the data frame name once per function call. (SAS users take note: variables used outside of formulas will not be found with this approach!) \n Finally, you can attach the data frame with \u201cattach(mydata)\u201d. This copies the variables into a temporary space that lets you then refer to them by their short names. This has the big advantage of allowing all the following function calls to use short variable names. Unfortunately, it has the big disadvantage of being confusing. Confusion #1 is that people feel that variables they create will go into the data frame automatically; they will not. Unless you specify a data frame using either mydata$newvar or mydata[\"newvar\"], new variables are created in your workspace. Confusion #2 is that R will look in your workspace before it looks at the attached versions of variables. So if variables with the same names exist there, those will be used instead. Confusion #3 is that even though detach(mydata) will reverse the process, if you run your program multiple times, you may have attached the data multiple times and detaching once does not fully undo the attached state. As confusing at that is, I use attach frequently and rarely get burned by it. \n For example, with variables x and y stored in mydata (and nowhere else) you could do a linear regression model using any one of these approaches: \n lm(mydata$y ~ mydata$x) \n lm(mydata[\"y\"] ~ mydata[\"x\"]) \n with(mydata, lm(y ~ x)) \n lm(y ~ x, data = mydata) \n attach(mydata) \nlm(y ~ x) \n As if that weren\u2019t complicated enough, both x and y do not have to both be in the same data frame! The x variable could be in mydata and the y variable could be in the workspace or in an attached version of mydata or some other data frame. That would be dangerous, of course, since it would be up to you to ensure that the values of each observation match or the resulting model would be nonsense. However, this kind of flexibility can also be very useful. \n With all this flexibility, it\u2019s important to know the order in which R chooses variables. A simple example can show us the order R uses. Here I am creating four data frames whose x and y variables will have a slope that is indicated by the data frame name. For example, the variables in df10 have a slope of 10. This will make it easy for us to see which version of the variables R is using. \n > y <- c(1,2,3,4,5,6,7,8,9,10)\n> x <- c(1,2,5,5,5,5,5,8,9,10)\n> df1 <- data.frame(x, y)  \n> df10 <- data.frame(x, y = y*10 )\n> df100 <- data.frame(x, y = y*100 )\n> df1000 <- data.frame(x, y = y*1000)\n> rm(y, x)\n> ls()\n[1] \"df1\" \"df10\" \"df100\" \"df1000\" \n Notice that I have deleted the original x and y variables so at the moment, varibles x and y exist only within the data frames. Running a regression with lm(y ~ x) will not work since R does not look into data frames unless you tell it to. Even if it did, it would have no way to know which set of x\u2019s and y\u2019s to use. Next I will take two different approaches to \u201cselecting\u201d a data frame. I attach df1 and copy the variables from df10 into the workspace. \n > attach(df1)\n> y <- df10$y\n> x <- df10$x \n Next, I do something rarely useful, calling a linear model using both \u201cwith\u201d and \u201cdata=\u201d. Which will dominate? \n > with(df100, lm(y ~ x, data = df1000))\n\nCall:\nlm(formula = y ~ x, data = df1000)\n\nCoefficients:\n(Intercept)   x \n   0   1000 \n Since the slope is 1000, it\u2019s clear that the \u201cdata=\u201d argument was dominant. So R would look there first. If it found both x and y, it would stop looking. But if it only found one variable, it would continue to look elsewhere for the other. If the other variable where in the \u201cwith\u201d data frame, it would then use it. \n Next I\u2019ll remove the \u201cdata\u201d argument and see what happens. \n > with(df100, lm(y ~ x))\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)   x \n   0   100 \n This time the \u201cwith\u201d data frame was used for both variables. If variable either had not been in that data frame, R would have continued to look in the workspace and in the attached copy. But which would it use first? Next, I\u2019m not specifying a data frame at all. \n > lm(y ~ x)\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)   x \n   0   10 \n The slope of 10 tells us that it found the copies of x and y that I copied from df10 into the workspace. Let\u2019s delete those variables and list the objects in our workspace to ensure that they\u2019re gone. \n > rm(y, x)\n> ls()\n[1] \"df1\" \"df10\" \"df100\" \"df1000\" \n Both x and y are clearly gone. So lets see if we can still use them. \n > lm(y ~ x)\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)   x \n   0   1 \n We deleted x and y but we can still use them! However, we see from the slope of 1 that R has used a different pair of x and y variables. They\u2019re the ones that were copied to my search path when I used \u201cattach(myDf1)\u201d. I had to remember that I had attached them. It\u2019s this kind of confusion that makes many R users avoid using attach. Finally, I\u2019ll detach df1 and see what happens. \n > detach(df1)\n> lm(y ~ x)\nError in eval(expr, envir, enclos) : object 'y' not found \n Now, even though all the data frames in our workspace contain an x and y variable, R does not look inside to find any of them. Even if it did, it would have no way of know which to choose. \n We have seen that R looks in various places for variables. In order, they are: what you specify in \u201cdata=\u201d, using \u201cwith(mydata,\u2026\u201d, your workspace and finally attached copies of your data frame. The most recently attached copies are the ones it will use first. I hope this will help you use R with both less typing and less confusion."], "link": "http://r4stats.com/2012/09/25/specifying-variables-in-r/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "r4stats.com"}, {"content": ["The\u00a0 TIOBE Community Programming Index \u00a0ranks the popularity of programming languages,\u00a0but from a programming language perspective rather than as analytical software\u00a0( http://www.tiobe.com ). It extracts measurements from blogs, entries in Wikipedia, books on Amazon, search engine results, etc. and combines them into a single\u00a0 index . The July 2012 rankings place\u00a0SAS in 24th place and R in 28th. This is a reversal from the January rankings, which had\u00a0R in 24th place and SAS at 31st. \n The\u00a0 Transparent Language Popularity Index \u00a0is very similar to the TIOBE Index except that, as you might guess, its ranking software, algorithm and data are published for all to see. I didn\u2019t find this index until July of 2012 at which time it ranks R in 12th place and SAS in 25th. \n I have updated this information in my ongoing article, The Popularity of Data Analysis Software ."], "link": "http://r4stats.com/2012/07/10/sas-beats-r/", "bloglinks": {}, "links": {"http://www.tiobe.com": 1, "http://r4stats.com/": 1, "http://feeds.wordpress.com/": 1, "http://www.tiobe.com/": 2, "http://lang-index.sourceforge.net/": 1}, "blogtitle": "r4stats.com"}, {"content": ["The open source R software for analytics has a reputation for being hard to learn. It certainly can be, especially for people who are already familiar with similar packages such as SAS, SPSS or Stata. Training and documentation that leverages their existing knowledge and points out where\u00a0their previous knowledge is\u00a0likely to mislead them can save much of frustration. This is the approach used in my books, R for SAS and SPSS Users and R for Stata Users as well as the workshops that are based on them. My next Internet-based workshop starts June 26 . \n Here is a list of complaints about R that I commonly hear from people learning it. In the comments section below, I\u2019d like to hear about things that drive you crazy about R. \n Misleading Function or Parameter Names (data=, sort, if) \n The most difficult time people have learning R is when functions don\u2019t do the \u201cobvious\u201d thing. For example when sorting data, SAS, SPSS and Stata users all use commands appropriately named \u201csort.\u201d Turning to R they look for such a command and, sure enough, there\u2019s one named exactly that. However, it does not sort data sets! Instead it sorts individual variables, which is often a very dangerous thing to do. In R, the \u201corder\u201d function sorts data sets and it does so in a somewhat convoluted way. However there are add-on packages that have sorting functions that work just as SAS/SPSS/Stata users would expect. \n Perhaps the biggest shock comes when the new R user discovers that sorting is often not even needed by R. When other packages require sorting before they can do three common tasks: \n \n Summarizing / aggregating data \n Repeating an analysis for each group (\u201cby\u201d or \u201csplit file\u201d processing) \n Merging files by key variables \n \n R does not need to sort files before any of these tasks! So while sorting is a very helpful thing to be able to do for other reasons, R does not require it for these common situations.\u00a0\u00a0 \n Nonstandard Output \n R\u2019s output is often quite sparse. For example, when doing crosstabulation, other packages routinely provide counts, cell percents, row/column percents and even marginal counts and percents. R\u2019s built-in table function (e.g. table(a,b)) provides only counts. The reason for this is that such sparse output can be readily used as input to further analysis. Getting a bar plot of a crosstabulation is as simple as barplot( table(a,b) ). This piecemeal approach is what allows R to dispense with separate output management systems such as SAS\u2019 ODS or SPSS\u2019 OMS. However there are add-on packages that provide more comprehensive output that is essentially identical to that provided by other packages. \n Too Many Commands \n Other statistics packages have relatively few analysis commands but each of them have many options to control their output. R\u2019s approach is quite the opposite which takes some getting used to. For example, when doing a linear regression in SAS or SPSS you usually specify everything in advance and then see all the output at once: equation coefficients, ANOVA table, and so on. However, when you create a model in R, one command (summary) will provide the parameter estimates while another (anova) provides the ANOVA table. There is even a command \u201ccoefficients\u201d that gets only that part of the model. So there are more commands to learn but fewer options are needed for each. \n R\u2019s\u00a0commands\u00a0are also consistent, working across all the modeling types that they might apply to. For example the \u201cpredict\u201d function works the same way for all types of models that might make predictions. \n Sloppy Control of Variables \n When I learned R, it came as quite a shock that in a single analysis you can include variables from multiple data sets. That usually requires that the observations be in identical order in each data set. Over the years I have had countless clients come in to merge data sets that they\u00a0thought had observations in the same order, but were not! It\u2019s always safer to merge by key variables (like ID) if possible. So by enabling such analyses R seems to be asking for disaster. I still recommend merging files when possible by key variables before doing an analysis. \n So why does R allow this \u201csloppiness\u201d? It does so because it provides very useful flexibility. For example,\u00a0might plot regression lines of variable X against variable Y for each of three groups on the same plot. Then you can add group labels directly onto the graph. This lets you avoid a legend that makes your readers look back and forth between the legend and lines. The label data would contain only three variables: the group labels and the coordinates at which you wish them to appear. That\u2019s a data set of only 3 observations so merging that with the main data set makes little sense. \n Loop-a-phobia \n R has loops to control program flow, but people (especially beginners) are told to avoid them. Since loops are so critical to applying the same function to multiple variables, this seems strange. R instead uses the \u201capply\u201d family of functions. You tell R to apply the function to either rows or columns. It\u2019s a mental adjustment to make, but the result is the same. \n Functions That Act Like Procedures \n Many other packages, including SAS, SPSS and Stata have procedures or commands that do typical data analyses which go \u201cdown\u201d through all the observations. They also have functions that usually do a single calculation across rows, such as taking the mean of some scores for each observation in the data set. But R has only functions and those functions can do both. How does it get away with that? Functions may have a preference to go down rows or across columns but for many functions you can use the \u201capply\u201d family of functions to force then to go in either direction. So it\u2019s true that in R, functions act like procedures and functions. Coming from other software, that\u2019s a wild new idea. \n Naming and Renaming Variables is Way Too Complicated \n Often when people learn how R names and renames its variables they, well, freak out. There are many ways to name and rename variables because R stores the names as a character variable. Think of all the ways you know how to fiddle with character variables and you\u2019ll realize that if you could use them all to name or rename variables, you have way more flexibility than\u00a0the other\u00a0data analysis\u00a0packages. However, how long did it take you to learn all those tricks? Probably quite a while! So until someone needs that much flexibility, I recommend simply using R to read variable names\u00a0from\u00a0the same source as you read\u00a0the data. When you need to rename them, use\u00a0an add-on package that will let you\u00a0do so in a style that is similar to\u00a0SAS, SPSS or Stata. An example is here . You can\u00a0convert to\u00a0R\u2019s built-in approach when you need more flexibility.\u00a0 \n Inability to Analyze Multiple Variables \n One of the first functions beginners typically learn is mean(X). As you might guess, it\u00a0gets the mean of the\u00a0X variable\u2019s values. That\u2019s simple enough. It also seems likely\u00a0that to get the mean of two variables,\u00a0you\u00a0would just enter mean(X, Y). However that\u2019s wrong because functions in R typically accept only\u00a0single objects. The solution is to put those two variables into a single object such as a data frame: mean( data.frame(x,y) ). So the generalization you need to make isn\u2019t from one variable to multiple variables, but rather from one object (a variable) to another (a data set). Since other software packages are not object oriented, this is\u00a0a mental adjustment people have to make when coming to R from other packages.\u00a0(Note to R gurus: I could have used colMeans but it\u00a0does not make this example as clear.) \n Poor Ability to Select Variable Sets \n Most data analysis packages allow you to select variables that are next to one another in the data set (e.g. A\u2013Z or A TO Z). R generally lacks this useful ability. It does have a \u201csubset\u201d function that allows the form A:Z, but that form works only in that function. There are many various work-arounds for this problem but most do seem rather convoluted compared to other software. Nothing\u2019s perfect! \n Too Much Complexity \n People complain that R has too much complexity overall compared to other software. This comes from the fact that you can start learning software like SAS and SPSS with relatively few commands: the basic ones\u00a0to read and analyze data. However when you start to become more productive you then have to learn whole new languages! To help reduce repitition in your programs you\u2019ll need to learn the macro language. To use the output from one procedure in another, you\u2019ll need to learn an output management system like SAS ODS or SPSS OMS. To add new capabilities you need to learn a matrix language like SAS IML, SPSS Matrix or Stata Mata. Each of these languages has its own commands and rules. There are also steps for tranferring data or parameters\u00a0from one language to another. R has no need for that added complexity because it integrates all these capabilities into R itself. So it\u2019s true that beginners have to see more complexity in R. Howevever,\u00a0as they\u00a0learn more about R,\u00a0they begin to realize that there is actually less complexity and more power in R! \n Lack of Graphical User Interface (GUI) \n Like most other packages R\u2019s full power is only accessible through programming. However unlike the others, it does not offer a standard GUI to help non-programmers do analyses. The two which are most like SAS, SPSS and Stata are R Commander and Deducer . While they offer enough analytic methods to make it through an undergraduate degree in statistics, they lack control when compared to a powerful GUI such as those used by SPSS or JMP. Worse, beginners must initially see a programming environment and then figure out how to find, install,\u00a0and activate either GUI. Given that GUIs are aimed at people with fewer computer skills, this is a problem. \n Conclusion \n Most of the issues described above are misunderstandings caused by expecting R to work like other software that the person already knows. What examples like this have you come across? \n Acknowledgements \n Thanks to Patrick Burns and Tal Galili for their suggestions that improved this post."], "link": "http://r4stats.com/2012/06/13/why-r-is-hard-to-learn/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://r4stats.com/": 5, "http://www.deducer.org": 1, "http://socserv.mcmaster.ca/": 1}, "blogtitle": "r4stats.com"}, {"content": ["The 2012 results of the annual KDnuggets poll are in. It shows R in first place with 30.7% of users reporting having used it for a real project. Excel is almost as popular. It seems out of place among so many more capable packages, but Excel is a tool that almost everyone has and knows how to use. \n It\u2019s interesting to note that four of the top five packages used were open source. While open source packages are clearly playing a major role in analytics, people still reported using more commercial software (1086) than open source (927). \n For many other ways to measure analytic software popularity, see The Popularity of Data Analysis Software . I\u2019ve just added this graph to that article."], "link": "http://r4stats.com/2012/05/31/open-source-almost-even/", "bloglinks": {}, "links": {"http://r4stats.com/": 1, "http://feeds.wordpress.com/": 1, "http://r4stats.wordpress.com/": 1, "http://www.kdnuggets.com/": 1}, "blogtitle": "r4stats.com"}, {"content": ["Learning to use a data analysis tool well takes significant effort, so people tend to continue using the tool they learned in college for much of their careers. As a result, the software used by professors and their students is likely to predict what the next generation of analysts will use for years to come. I track this trend, and many others, in my article The Popularity of Data Analysis Software . In the latest update (4/13/2012) I forecast that, if current trends continued, the use of the R software would exceed that of SAS for scholarly applications in 2015. That was based on the data shown in Figure 7a, which I repeat here: \n Let\u2019s take a more detailed look at what the future may hold for R, SAS and SPSS Statistics. \n Here is the data from Google Scholar: \n   R SAS SPSS\n1995  8 8620 6450\n1996  2 8670 7600\n1997  6 10100 9930\n1998 13 10900 14300\n1999 26 12500 24300\n2000 51 16800 42300\n2001 133 22700 68400\n2002 286 28100 88400\n2003 627 40300 78600\n2004 1180 51400 137000\n2005 2180 58500 147000\n2006 3430 64400 142000\n2007 5060 62700 131000\n2008 6960 59800 116000\n2009 9220 52800 61400\n2010 11300 43000 44500\n2011 14600 32100 32000 \n ARIMA Forecasting \n We can forecast the use of R using Rob Hyndman\u2019s handy auto.arima function to forecast five years into the future: \n \n> library(\"forecast\")\n\n> R_fit <- auto.arima(R)\n\n> R_forecast <- forecast(R_fit, h=5)\n\n> R_forecast\n\n \u00a0Point Forecast Lo 80 Hi 80 Lo 95 Hi 95\n18   18258 17840 18676 17618 18898\n19   22259 21245 23273 20709 23809\n20   26589 24768 28409 23805 29373\n21   31233 28393 34074 26889 35578\n22   36180 32102 40258 29943 42417\n \n We see that even if the use of SAS and SPSS were to remain at their current levels, R use would surpass their use in 2016 ( Point Forecast column where 18-22 represent years 2012 -2016). \n If we follow the same steps for SAS we get: \n \n> SAS_fit <- auto.arima(SAS)\n\n> SAS_forecast <- forecast(SAS_fit, h=5)\n\n> SAS_forecast\n\n Point Forecast  Lo 80 Hi 80 Lo 95 Hi 95\n18   21200 16975.53 25424.5 14739.2 27661\n19   10300 853.79 19746.2 -4146.7 24747\n20   -600 -16406.54 15206.5 -24774.0 23574\n21   -11500 -34638.40 11638.4 -46887.1 23887\n22   -22400 -53729.54 8929.5 -70314.4 25514\n \n It appears that if the use of SAS continues to decline at its precipitous rate, all scholarly use of it will stop in 2014 (the number of articles published can\u2019t be less than zero, so view the negatives as zero). I would bet Mitt Romney $10,000 that that is not going to happen! \n I find the SPSS prediction the most interesting: \n \n> SPSS_fit <- auto.arima(SPSS)\n\n> SPSS_forecast <- forecast(SPSS_fit, h=5)\n\n> SPSS_forecast\n\n Point Forecast Lo 80 Hi 80 Lo 95 Hi 95\n18  13653.2 -16301 43607 -32157 59463\n19  -4693.6 -57399 48011 -85299 75912\n20  -23040.4 -100510 54429 -141520 95439\n21  -41387.2 -145925 63151 -201264 118490\n22  -59734.0 -193590 74122 -264449 144981\n \n The forecast has taken a logical approach of focusing on the steeper decline from 2005 through 2010 and predicting that this year (2012) is the last time SPSS will see use in scholarly publications. However the part of the graph that I find most interesting is the shift from 2010 to 2011, which shows SPSS use still declining but at a much slower rate. \n Any forecasting book will warn you of the dangers of looking too far beyond the data and I think these forecasts do just that. The 2015 figure in the Popularity paper and in the title of this blog post came from an exponential smoothing approach that did not match the rate of acceleration as well as the ARIMA approach does. \n Colbert Forecasting \n While ARIMA forecasting has an impressive mathematical foundation it\u2019s always fun to follow Stephen Colbert\u2019s approach: go from the gut. So now I\u2019ll present the future of analytics software that must be true, because it feels so right to me personally. This analysis has Colbert\u2019s most important attribute: truthiness . \n The growth in R\u2019s use in scholarly work will continue for two more years at which point it will level off at around 25,000 articles in 2014.This growth will be driven by: \n \n The continued rapid growth in add-on packages ( Figure 10 ) \n The attraction of R\u2019s powerful language \n The near monopoly R has on the latest analytic methods \n Its free price \n The freedom to teach with real-world examples from outside organizations, which is forbidden to academics by SAS and SPSS licenses (it benefits those organizations, so the vendors say they should have their own software license). \n \n What will slow R\u2019s growth is its lack of a graphical user interface that: \n \n Is powerful \n Is easy to use \n Provides journal style output in word processor format \n Is standard, i.e. widely accepted as The One to Use \n Is open source \n \n While programming has important advantages over GUI use, many people will not take the time needed to learn to program. Therefore they rarely come to fully understand those advantages. Conversely, programmers seldom take the time to fully master a GUI and so often underestimate its capabilities. Regardless of which is best, GUI users far outnumber programmers and, until resolved, this will limit R\u2019s long term growth. There are GUIs for R, but so many to choose from that none becomes the clear leader (Deducer, R Commander, Rattle, Red-R, at least two from commercial companies and still more here .) If from this \u201cGUI chaos\u201d a clear leader were to emerge, then R could continue its rapid growth and end up as the most used package. \n The use of SAS for scholarly work will continue to decline until it matches R at the 25,000 level. This is caused by competition from R and other packages (notably Stata) but also by SAS Instute\u2019s self-inflicted GUI chaos.\u00a0 For years they have offered too many GUIs such as SAS/Assist, SAS/Insight, IML/Studio, the Analyst application, Enterprise Guide, Enterprise Miner and\u00a0 even JMP (which runs SAS nicely in recent versions). Professors looking to meet student demand for greater ease of use could not decide what to teach so they continued teaching SAS as a programming language. Even now that Enterprise Guide has evolved into a good GUI, many SAS users do not know what it is. If SAS Institute were to completely replace their default Display Manager System with Enterprise Guide, they could bend the curve and end up at a higher level of perhaps 27,000. \n The use of SPSS for scholarly work will decline only slightly this year and will level off in 2013 because: \n \n The people who needed advanced methods and were not happy calling R functions from within SPSS have already switched to R or Stata \n The people who like to program and want a more flexible language than SPSS offers have already switched to R or Stata \n The people who needed a more advanced GUI have already switched to JMP \n \n The GUI users will stick with SPSS until a GUI as good (or close to as good) comes to R and becomes widely accepted. At The University of Tennessee where I work, that\u2019s the great majority of SPSS users. \n Stata\u2019s growth will level off in 2013 at level that will leave it in fourth place. The other packages shown in Figure 7b will also level off around the same time, roughly maintaining their current place in the rankings. A possible exception is JMP, whose interface is radically superior to the the others for exploratory analysis. Its use could continue to grow, perhaps even replacing Stata for fourth place. \n The future of Enterprise Miner and SPSS Modeler are tied to the success of each company\u2019s more mainstream products, SAS and SPSS Statistics respectively. Use of those products is generally limited to one university class in data mining, while the other software discussed here is widely used in many classes. \n So there you have it: the future of analytics revealed. No doubt each reader has found a wide range of things to disagree with, so I encourage you to follow the detailed blog at Librestats to collect your own data from Google Scholar and do your own set of forecasts. Or simply go from the gut!"], "link": "http://r4stats.com/2012/05/09/beginning-of-the-end/", "bloglinks": {}, "links": {"http://www.colbertnation.com/": 1, "http://feeds.wordpress.com/": 1, "http://librestats.com/": 1, "http://robjhyndman.com/": 1, "http://en.wikipedia.org/": 2, "http://r4stats.com/": 4, "http://r4stats.wordpress.com/": 1, "http://www.washingtonpost.com/blog": 1}, "blogtitle": "r4stats.com"}, {"content": ["I\u2019m almost done moving this site from Google Sites to WordPress. This post describes some of the some things I\u2019ve learned about WordPress.com. \n By default, WordPress.com makes your site look like a blog. I preferred it look like a web site that contains a blog.You can change that in Site Admin under Settings> Reading. The Front Page Displays box determines what people will see when the arrive. By default, that\u2019s your latest blog entry. You can change that to any page you like. \n WordPress allows a vary limited set of files to download. My book support files are in R, SAS, SPSS, Stata, sas7bdat, etc. so I zip them up into a single file. Since WordPress.com does not allow you to distribute zip files, I had to put them in my DropBox public folder and link to them from WordPress.com. \n You can organize your menus by either parent/child relationships among pages, or by using custom menus. Custom menus have the advantage of allowing all pages to be at the root of your site, keeping nice short URLs like \u201chttp://r4stats.com/popularity\u201d. However, many site templates do not support custom menus so you are very constrained in your choice of templates. Using the \u201cpopularity\u201d article as an example, I created a page called \u201cArticles\u201d and let it be the parent. So now the URL is \u201chttp://r4stats.com/articles/popularity\u201d. That\u2019s too bad since there are old links out there that\u00a0 use the short version. I\u2019ll put notes on them to redirect people. I could use \u201credirects\u201d but I would prefer people see the links and note the changes. That will address the many links that used \u201chttps://sites.google.com/site/r4statistics/\u201d rather than the shorter equivalent, \u201chttp:r4stats.com\u201d. \n One of the most frustrating problems I saw resulted from WordPress.com not letting you change the URL to the one you wanted. For example, I wanted my Miscellaneous page to have the URL http://r4stats.wordpress.com/misc, but it was insisting on adding a \u201c-2\u2033 to the end of it as in http://r4stats.wordpress.com/ misc-2 . I looked all over for a page that already used the \u201cmisc\u201d link but found none. Then it occurred to me that it might be in the trash. It was. When I deleted it, then it would allow me to reuse the simpler link. \n I spent a crazy amount of time trying to figure out how to get my program code examples to display in Courier or any similar monospaced font. I paid the $30 fee to edit my cascading style sheet (CSS) only to find that allowed me to change the whole page to Courier. I finally found that you click the upper-right-most button on the toolbar labeled Show/Hide Kitchen Sink. That made a style menu appear. It contains a style named \u201cPreformatted\u201d, which is monospaced. Tal Galili helpfully pointed me to a wonderful article he had written on how to make R code look nice on WordPress. \n So that\u2019s the sum of my lessons so far. On the whole I preferred the web site tool at Google Sites, but I do like having WordPress blogs built right into the site. That makes it easy to hook the blogs into R-Bloggers.com and PROC-X.com ."], "link": "http://r4stats.com/2012/05/05/wordpress-tips/", "bloglinks": {}, "links": {"http://www.r-statistics.com/": 1, "http://proc-x.com": 1, "http://feeds.wordpress.com/": 1, "http://R-Bloggers.com": 1}, "blogtitle": "r4stats.com"}, {"content": ["I\u2019ve had my site http://r4stats.com on Google Sites for a few years now and it\u2019s time to try something new. Most of the articles there are not very blog-like. For example, The Popularity of Data Analysis Software is an article that I update many times a year. If you\u2019ve never read it before, it would probably interest you. But after one thorough reading, only the major changes will be of interest. So I\u2019m contemplating writing short blog articles of the major changes while maintaining the whole article for its completeness. \n So I\u2019ll be fiddling around here until I decide if I can really\u00a0 blend these two ideas in a blog format."], "link": "http://r4stats.com/2012/04/25/hello-world/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "r4stats.com"}]