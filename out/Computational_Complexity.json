[{"blogurl": "http://blog.computationalcomplexity.org\n", "blogroll": [], "title": "Computational Complexity"}, {"content": ["The deadline for submissions to STOC has been extended to Monday, Nov 5 2012 5:00 p.m. EST."], "link": "http://blog.computationalcomplexity.org/feeds/4660271570201054371/comments/default", "bloglinks": {}, "links": {"http://stoc.yale.edu/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["Time again for the annual fall jobs post. As always the best places to look for academic CS positions are the job sites at the CRA and the ACM . Also check out the postdoc and other opportunities on the Theory Announcements site. It never hurts to check out the webpages of departments you might want to be at or to contact people to see if positions are available. \n \nI encourage everyone who has a job to offer in theoretical computer science at any level to post links in the comments. \n \nWith computer science enrollments expanding and the economy slowing recovering, I'm expecting quite an increase in the number of tenure-track jobs in computer science this year. On the other hand I'm expecting a decrease in the number of new postdoc positions though maybe more overseas. \n \nGood luck to everyone in the market."], "link": "http://blog.computationalcomplexity.org/feeds/8464571705443980020/comments/default", "bloglinks": {}, "links": {"http://www.cra.org/": 1, "http://jobs.acm.org/": 1, "http://dmatheorynet.blogspot.com/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["At Dagstuhl I was delighted when I saw the title of a talk to be given Planarizing Gadgets for Perfect Matching do not Exist because I had asked the question about a gadget for planar Ham Cycle here . I was hoping to ask the authors if there techniques could be used to show that there was not Planarizing gadget for Ham Cycle (NOTE- I had either forgot or never knew that this was already known and was posted as an answer to my query to cstheory stackexchange, here .) \n \nThe paper Planarizing Gadgets for Perfect Matching do not Exist (or if you can get to it the MFCS 2012 version here ) is by Rohit Gurjar, Arpita Korwar, Jochen Messner, Simon Straub, and Thomas Thierauf. The talk was given by Jochen and was excellent. \n \nPerfect matching is in P (Edmonds 1965) but is it in NC? Not known--- however it is in RNC (Mulmuley, Vazirani, Vazirani 1987). What about Planar graphs? They are different--- counting the number of perfect matching in a graph is Sharp-P complete (Valiant 1979) but counting the number of perfect matchings in a planar graph is in NC (Vazirani 1989). So of course Planar Graph Matching is in NC. Can we use this to get Graph Matching in NC? perhaps be a reduction? This would be neat since we would be using a reduction to prove a problem EASY rather than to prove a problem HARD. (I think this has been done before but is rare-- readers, if you know a case comment on it.) Perhaps there is some planarizing gadget: given a graph G use some gadgets to get rid of crossings and produce a planar graph G' such that G has a perfect matching iff G' has a perfect matching. That would be AWESOME! However, from the very title of the paper, we can guess this is not true. This paper shows that something AWESOME is not possible! A downer but worth knowing. \n \nJochen proved this and then went on to say that they had done the same thing for HAM CYCLE! That is, there is no planarization gadget for Ham cycle! (He also acknowledged that this was already known independently.) SO I didn't get to ask my question since they already had answered it. Great! \n \n Their interest in planarization was related to an OPEN problem--- is Graph Matching in NC? By contrast my interest in Planarization gadgets for Ham Cycle was pedagogical--- I was in search of a better proof that Planar Ham Cycle is NPC- though there is no new theorem here. \n \n I am delighted to know the result! \n \n Their results says that a certain type of reduction won't work. Might some other reduction work? My sense is this is unlikely. \n \n So--- is Graph Matching in NC? Since I believe NC=RNC I think yes. Will it be proven by showing NC=RNC or will it be proven directly (leaving NC=RNC open)? Or will the ideas that lead to Graph Matching in NC help to show NC=RNC? This is one of those questions that might be solved within a decade, as opposed to P vs NP which won't be resolved for quite some time."], "link": "http://blog.computationalcomplexity.org/feeds/4189779124597986601/comments/default", "bloglinks": {}, "links": {"http://eccc.hpi-web.de/": 1, "http://cstheory.stackexchange.com/": 1, "http://dx.doi.org/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["I tweeted the audio of this song last week and here is the video . Recorded at Dagstuhl on October 18th. Written by Fred Green who also plays piano. Performed by David Barrington with Steve Fenner on chorus. \n \nFred gives apologies to Gilbert and Sullivan, the Complexity Zoo , and Tom Lehrer \n \n \n \n Lyrics by Fred Green, copyright 2012 \n \n To the tune of \"I Am the Very Model of a Modern Major General\" \n \n There's P and NP, BPP and ZPP and coNP, \n And TC0 and AC0 and NC1 and ACC, \n There's PSPACE, LOGSPACE, PPSPACE and ESPACE, EXPSPACE, IPP, \n And LIN and L and Q and R, and E, EE and E-E-E. \n There's SPARSE and TALLY, PL, P/Poly, NP/poly, \n There's PromiseP and PromiseBPP and PromiseBQP, \n There's FewP, UP, QP, UE, N-E-E, N-E-E-E, \n And EXP and NEXP, FewEXP, and NE-EXP, and also Max-N-P. \n And EXP and NEXP, FewEXP, and NE-EXP, and also Max-N-P \n And EXP and NEXP, FewEXP, and NE-EXP, and also Max-N-P \n And EXP and NEXP, FewEXP, and NE-EXP, and also Max-N, Max-N-P. \n There's Sigma_nP, Delta_nP, Theta_nP, Pi_nP, \n We know BPP's in Sigma_2P intersection Pi_2P. \n And NP to the NP to the NP to the NP \n To the NP to the NP, that's the pol-y-nom-yal hierarchy! \n \n There's #P, gapP, PP, coC=P and MidBitP, \n And ModP, Mod_kP, Mod_kL, ParityP, MPC, \n There's FNP, NPSV, NPMV, and SAC, \n SAC0, SAC1, SZKn and SPP. \n There's BQP and DQP and EQP and NQP, \n And RQP and VQP and YQP and ZQP, \n And BPQP, FBQP, ZBQP, QRG, \n QAC0, QNC0, QNC1, Q-A-C-C. \n QAC0, QNC0, QNC1, Q-A-C-C \n QAC0, QNC0, QNC1, Q-A-C-C \n QAC0, QNC0, QNC1, Q-A-C, A-C-C \n There's QSZK, QMA and QAM and QIP, \n And IP, MIP, QMIP and also PCP, \n And PPPad and PPcc, PSK and PQUERY, \n And PP to the PP, PExp, PPA and PPP. \n \n These complexity classes are \n the ones that come to mind, \n And there may be many others but they \n haven't been defined."], "link": "http://blog.computationalcomplexity.org/feeds/6479499879720000876/comments/default", "bloglinks": {}, "links": {"http://youtu.be/": 1, "http://qwiki.stanford.edu/": 1, "http://t.co/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["With a shout out to the friendly folks attending FOCS this week, some short announcements. \n \nRead the STOC CFP before you submit the paper. There are significant changes to the submission format and procedure. Deadline is November 2. \n \n Complexity will be co-located with STOC in 2013. Submission deadline is November 30. \n \nThe new Simons Institute for the Theory of Computing has a call for workshop proposals and research fellowships. \n \nThere will be a symposium to celebrate a new professorship named after SIGACT and STOC founder Patrick Fischer at Michigan on November 5 and a celebration of the 80th birthday of Joe Traub on November 9th at Columbia."], "link": "http://blog.computationalcomplexity.org/feeds/8420955995789168635/comments/default", "bloglinks": {}, "links": {"http://eecs.umich.edu/": 1, "http://theory.stanford.edu/": 1, "http://www.columbia.edu/": 1, "http://simons.berkeley.edu/": 1, "http://dimacs.rutgers.edu/": 1, "http://computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["Nerd Shot from Dagstuhl Seminar 12421 \n \n   \n  \n Lance: Welcome to another Typecast from beautiful Schloss Dagstuhl. I\u2019m here with Bill for the Workshop on Algebraic and Combinatorial Methods in Computational Complexity .  Bill: Beautiful? I thought this place was designed to be ugly so that we actually get work done.  Lance: So what work did you get done today, BIll?  Bill: I watched the debate. And you?  Lance: Steve Fenner and I came up with the easiest to describe PSPACE-complete problem ever!  Bill: Was it one of those poset things that you and Steve\u2019s students work on.  Lance: A generalization of poset games but easier to describe. But we are getting off topic...  Bill: as did Obama and Mitt.  Lance: Bill my two minutes aren\u2019t up yet. Anyway you\u2019ll have to read about this new PSPACE-complete problem in a future post.  Bill: Since you didn\u2019t ask, let me tell you about my favorite talk, Rank bounds for design matrices and applications by new Rutgers professor Shubhangi Saraf ( Powerpoint ). Despite the awful title  Lance: which is why I skipped that talk  Bill: it used complexity theory techniques to prove new things in math, a generalization of the Sylvester-Gallai theorem. You have n points on the plane...  Lance: Wait Bill, It will take longer to tell the S-G theorem than it would have to explain the new PSPACE-complete problem!  [Steve Fenner shows up with beer in hand. He goes off to get Lance one too.]  Bill: OK, I\u2019ll leave this for a later post. What was your favorite talk?  Lance: Believe it or not it was an algorithms talk. Atri Rudra gave a very simple algorithm to do a join operation motivated by reconstructing 3-d collections of points from projections. [ Powerpoint ]  Bill: Yes, and it may have applications to complexity as most real world algorithms do.  [Steve arrives with Lance\u2019s Beer. There is much happiness.] Steve: My favorite talk so far was Rahul Santhanam\u2019s [ abstract ] Reminded me of the good old days of complexity.  Lance: Let the guy give me beer and he thinks he can weasel his way into our typecast.  Bill: Lance, that\u2019s how I got started in this business.  Lance: Rahul had some clever co-author, didn\u2019t he?  Steve: No one important. Lance something?  Harry Buhrman: I like the GCT talk by Josh Grochow. [ abstract ]  Bill: In the future we\u2019ll all have to learn GCT to get started in this field. I\u2019m glad I\u2019m living in the past. Lance, you paid me the highest compliment in my talk. You didn\u2019t fall asleep and you even picked a fight with me.  Lance: Only because I had to stay awake to help the audience understand your confusing presentation .  Bill: It was only one slide.  Lance: It was only one fight.  Bill: I still feel as complimented as a bit that\u2019s just been toggled .  Lance: I\u2019m happy for you. Actually, it was not that bad a result. Now that\u2019s my highest compliment.  Harry: Hey, this isn\u2019t fair, we\u2019ve haven\u2019t heard all the talks yet.  [Both Harry and Steve are talking later tonight]  Lance: Life isn\u2019t fair, get over it.  Bill: Let\u2019s call it a day.  Lance: Watch my twitter feed later this week for a special musical complexity tribute.  Bill: I can\u2019t wait.  Lance: So until next time, remember that in a complex world, best to keep it simple."], "link": "http://blog.computationalcomplexity.org/feeds/125691929571160759/comments/default", "bloglinks": {}, "links": {"http://www.dagstuhl.de/": 7, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["This week Bill and I have traveled to Germany for the Dagstuhl Seminar on Algebraic and Combinatorial Methods in Computational Complexity . Plenty of newly minted Nobel laureates here, winners of the Peace Prize last Friday. But this post celebrates today's winners of the Economics Prize , Al Roth and Lloyd Shapley for their work in matching theory that has made a difference in the real world. \n \nIn 1962, Shapley and David Gale created the first algorithm that finds stable marriages . David Gale would surely have shared this award had he not passed away in 2008. Nicole Immorlica's guest obit of Gale nicely describes this work and its applications including matching medical students with residencies. \n \nAl Roth uses matching algorithms for a variety of projects, most notably creating large scale kidney exchanges , saving lives with algorithmic mechanism design. Doesn't get cooler than that."], "link": "http://blog.computationalcomplexity.org/feeds/1256956277313466370/comments/default", "bloglinks": {}, "links": {"http://www.nytimes.com/": 2, "http://en.wikipedia.org/": 1, "http://www.dagstuhl.de/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["When John Hennessy gave his talk on MOOCs at the CRA Snowbird meeting he recommended the book Why Does College Cost So Much? by Robert Archibald and David Feldman, both economics professors at William and Mary. I've never seen a good answer to the title question so I read through the book. To overly simplify their main thesis: It's not that college has gotten more expensive, it's that most everything else has gotten cheaper. Technological advances in manufacturing and shipping have made greatly lessened the cost of goods, and the rate of inflation is calculated based on a basket of goods. So service industries, particularly those that require highly educated people and don't benefit directly from technology, look expensive in comparison. College costs closely map to medical and dental expenses, and closely followed broker expenses until technology made brokerages cheaper. \n \nArchibald and Feldman even argue that there isn't a college affordability crisis for the majority of Americans: They are still better off than 30 years ago even if we take out college expenses. Hardly the doom and gloom scenario that Hennessey was portraying. \n \nTheir main point is that one cannot increase the number of students to faculty without decreasing the quality of education. That's where MOOCs come in, supposedly the solution to allow faculty to be far more efficient in the number of students they can teach without reducing quality. Might help control college costs but could harm research at top tier universities and many other universities might cease to exist. \n \nAlas perception is reality and the public sees college expenses growing dramatically compared to the general cost of living and blames wasteful spending at universities. Curing this \"disease\" might kill the patient."], "link": "http://blog.computationalcomplexity.org/feeds/4885185874640815209/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://cra.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["Aravind asked me to post on this again (NOTE- registration-for-free deadline \nis TOMMOROW!!!!!) \n \nThe University of Maryland at College park is having a Theory Day on Wed Oct 24! Come hear \n Distinguished talks by Julia Chuzhoy and Venkataesan Guruswami! \n Short talks (is that code for NOT distinguished?) by Bill Gasarch, MohammadTaghi Hajiaghayi, Jonathan Katz, Samir Khuller, David Mount, Elaine (Runting) Shi, and Aravind Srinivsans. (I never realized I was first alphabetically until now.) \n Discussions in hallways for those that learn more that way! \n For more info see the link above, but note one thing: Its FREE! NOTE: This is purposely after the NJ FOCS conference and is an easy Amtrak ride from NJ. I like theory days in general and often go to the NY theory days. They are free and only one day. I recommend going to any theory day that is an Amtrak Ride away. (Might depend on how long the trip is- There is a 13-hour Amtrak from Atlanta Georgia to Maryland, though I doubt I'll see Lance there.) I get a lot out of theory day as noted in this post about NY theory day. What are good ways to get the word out about events. The major conferences and also the NY Theory Days have a long enough tradition that they don't need much advertising. \n Email is not as useful as it used to be since we all get too much of it. \n There IS a website for theory announcements here , and also one of our links, but more people need to post there and read there. A chicken and egg problem. \n Twitter. No central authority. If Aravind had a twitter account (I doubt he does) then he could tweet to his followers, but that would not be that many people. \n Any ideas?"], "link": "http://blog.computationalcomplexity.org/feeds/3363777659533458526/comments/default", "bloglinks": {}, "links": {"http://dmatheorynet.blogspot.com/": 1, "http://www.umd.edu/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["(This post was done with the help of Lane Hemaspaandra and John Purtilo.) \n \nThe 8th amendment of the US Constitution states \n \n Excessive bail shall not be required, nor excessive fines imposed, nor cruel and unusual punishments inflicted. \n There is an ambiguity here. Let C be cruel and U be unusual. They are saying NOT(C AND U) = NOT(C) OR NOT(U). Common sense would dictate that they meant NOT(C) AND NOT(U). \n \n(This article was emailed to me by Lane H. along with the idea for this post.) This article (see also this Wikipedia article ) is an example where the CRUEL but NOT UNUSUAL argument seems to have been explicit. The case was about a MANDATORY life sentence in prison for possessing over 650 grams of cocaine, in Michigan. Is that a lot? (I never could figure out that Metric System.) In terms of numbers or getting high I really don't know if 650 grams is a lot, but legally its NOT A LOT--- the only other state that comes close to this kind of penalty is Alabama with a life-sentence for 6500 grams---that is not a typo. (See the Wikipedia articles section on White's criticism of Kennedy's argument.) I quote the syllabus of the decision which is not written by the members of the Supreme Court and is not part of the decision, but is rather prepared by the Office of the Clerk (of the Supreme Court)---who, one assumes, is pretty darned good at extracting the key points of the ruling, and so the syllabi are very useful. \n Severe, mandatory penalties may be cruel, but they are not unusual in the \nconstitutional sense, having been employed in various forms throughout \nthe Nation's history. \n Some past rulings HAVE indicated that a sentences that is out-of-proportion with the crime MAY be considered Cruel and Unusual . But, alas, unlike mathematics, definitions can change over time. (Well- in math that happens sometimes, but not often and usually not with dire consequences.) \n One could argue that Capital Punishment is C but NOT(U). And indeed, the courts have often upheld it. Did they they use the argument that Capital punishment is C but NOT(U), hence it does not violate the 8th amendment ? This article (emailed to be my Lane) makes that line of reasoning explicit and is against it. \n If someone commits anti-Semitic vandalism and the courts decide that he or she is forced to read Anne Frank's Diary, that would be U but NOT(C). Not sure how they would enforce this- give a quiz? Are Cliff notes okay? What if the vandal saw the movie instead? Would this really work? (I honestly don't know.) Is this Hypothetical? In America YES. John found a case in Italy and I found a case in Germany ). If this gets to be a common punishment for anti-Semitic crimes then it may no longer be unusual. I could find no other real cases where people convicted of crimes had, as part of their sentence, that they had to read something (though IANAL so there could be some I don't know about). \n If an Occupy Wall Street guy vandalizes a Financial Institution's offices and is forced to read Atlas Shrugged that would be unusual. But is it cruel? (My opinion: YES) How about the Cliff notes? (My opinion: NO) Is this hypothetical? (My opinion: YES.) \n What if a teenage girl was in Juvenile court for cutting off the hair of a 3-year old (against the 3-year old's will) and the Judge agreed to reduce the sentence if the teen's mother cut off the teen's pony tail in court. This would be considered unusual. But is it cruel? Is it hypothetical? No \n It is most likely that the phrase Cruel and Unusual was not meant to be \nbroken down into its component parts. \n \nSo what Logic did the founders use? \nThomas Jefferson knew more math than any of the founding fathers. But alas, \nhe was off in France when the constitution was written."], "link": "http://blog.computationalcomplexity.org/feeds/771340091062553073/comments/default", "bloglinks": {}, "links": {"http://germanherald.com/": 1, "http://www.cornell.edu/": 1, "http://www.jta.org/": 1, "http://en.wikipedia.org/": 2, "http://www.bravepages.com/": 1, "http://www.cbsnews.com/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["The MacArthur Foundation announced their 2012 Fellows, also know as the genius awards. Among the list two names of interest to my readers, Maria Chudnovsky and Daniel Spielman . \n \nMy long time readers first heard of Maria back in 2003 when I posted about a great talk she gave as a graduate student giving a polynomial-time algorithm to test for perfect graphs. That was just a start in her incredible career as a graph theorist. \n \nDan is a regular in the blog for the various awards he's won, most notably (before the MacArthur) for his Nevanlinna prize . I believe Dan is my first genius co-author , alas not one of the papers that causing him to win awards. \n \nI've seen many cases where researchers get fantastic results early in their career and can never live up to the hype. Dan and Maria exceeded it. Congrats to both of them."], "link": "http://blog.computationalcomplexity.org/feeds/4283599091698107038/comments/default", "bloglinks": {}, "links": {"http://dx.doi.org/": 1, "http://www.macfound.org/": 3, "http://blog.computationalcomplexity.org/": 2}, "blogtitle": "Computational Complexity"}, {"content": ["I went to the QIS workshop on quantum computing which was on the College Park Campus. I went Thursday (reception- free food!) and Friday (free lunch!) but had to miss the Friday free dinner and the Saturday session. \n \n Going to a conference that is ON your campus usually makes it FURTHER away for you. If I was from out of town I would have gotten a Hotel Room in the same hotel as the conference. As it was I walked from my office- a 45 minute walk It would have been shorter but it was a quantum random walk. \n Scott Aaronson was there. We were talking about teaching class while being taped. He said that being taped changes what he does. I cleverly pointed out that the act of measuring Scott, changes Scott. He cleverly replied that the search for a NEW and FUNNY quantum joke has not ended yet. \n Frank Gaitan gave a talk on using quantum annealing to find Ramsey Numbers . FINALLY a real application for Quantum Computing! (The downside- I was going to use Quantum Computers Find Ramsey Numbers! for an April Fools Day post.) \n Umesh Vazarni's talk on CLASSICAL results proven using QUANTUM techniques was great. This notion seems to be for real. Its looking more and more like even if you don't like quantum you will have to learn it. A particular example of this is this paper \n Linear vs Semidefinite Extended Formulations: Exponential Separation and Strong Lower Bounds by Fiorini, Massar, Pokutta, Tiwaray, de Wolf. \n Yi-Kai Liu gave a talk on Quantum Information in Machine Learning and Cryptography. We discuss a small part of his talk, a result by Oded Regev. (Daniel Apon gave a full talk on this small part at the UMCP Complexity Seminar, his slides are here .) GAPSVP(\u03b3) is the following problem: Given an n-dim lattice and a number d output YES if the shortest vector in L is \u2264 d, and output NO if the shortest vector in L is > \u03b3 d (if it's neither we don't care what you output). This is NP-hard to solve exactly or within O(1) approx (though to be hard for evern poly approx) and it's a good problem for crypto to use. LWE is the Learning with Errors Problem. There is a quantum-reduction that shows that GAPSVP \u2264 LWE, so if GAPSVP is hard then LWE is hard. So there are now three scenarios: \n Quantum computers are not built. Factoring is still hard classically. Crypto goes on as it is now (maybe not- there is a classical reduction from GapSVP to LWE, but for weaker parameters- so maybe you can base crypto on LWE). \n Quantum computers are not built. Factoring is easy classically. GAPSVP is hard. Do Crypto based on GAPSVP. \n Quantum computers are built. Factoring is now easy. GAPSVP is hard. Do Crypto based on LWE. THIS is what the result allows us to do! \n Quantum computers are built. Factoring is now easy. GAPSVP is easy. Now you are in trouble. \n New word: Stoquastic. Not sure what it means. \n Issac Chuang spoke about the difficulty of teaching quantum computing since the students have different backgrounds. He has devised (or helped devise) Online Tutoring systems for it that seem to be working very well. I didn't know that quantum computing was at the level to worry about how-to-teach-it. Then again, any course has these concerns, so it's good to see that he did something about it. (Even so, I doubt I'll invest a lot of time and effort into an online tutoring system for my Ramsey Theory course next spring.) \n There were some talks on or touching on Quantum-Prog Languages, Quantum-CAD, Quantum-architecture. I suspect that if quantum computers are ever built we will find that some of the assumptions of this work were wrong; however, I also suspect that having people who have thought about these issues will be valuable."], "link": "http://blog.computationalcomplexity.org/feeds/7228447394720113608/comments/default", "bloglinks": {}, "links": {"http://arxiv.org/": 2, "http://www.nyu.edu/": 1, "http://www.umd.edu/": 2}, "blogtitle": "Computational Complexity"}, {"content": ["A few weeks ago, Suresh wrote a post Things a TCSer should have done at least once with the caveat \n \nThis list is necessarily algorithms-biased. I doubt you'll need many of these if you're doing (say) structural complexity. \nBasically begging a response. So here is what every structural complexity theorists should have done. \n \n Define a new complexity class that has some reason for being. \n To keep balance in the world, you should also collapse two other complexity classes. \n While you are at it, separate two complexity classes that weren't separable before. \n Create a new relativized world. Extra points if in this work you collapse two complexity classes while separating two others. \n Use Kolmogorov complexity, information theory or the probabilistic method as a proof technique. They are really all the same technique in disguise. \n Use the sunflower lemma, or the Local Lovasz lemma, or some other weird probabilistic or combinatorial lemma just for the fun of it. \n Invoke VC dimension to solve a problem. I should have at least one in common with Suresh and Sauer's lemma is sometimes useful in complexity. \n Have a theorem that starts \"Assuming the extended Riemann hypothesis...\" \n Give a \"simpler\" proof of a nasty theorem. \n [Advice given by Noam Nisan many moons ago] Try to settle P v NP. In both ways. Only by really trying and failing can you understand why the easy stuff doesn't work."], "link": "http://blog.computationalcomplexity.org/feeds/4755074676378891782/comments/default", "bloglinks": {}, "links": {"http://geomblog.blogspot.com/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["(In this post I quote attempted posts to the blog. \nI transcribe them as they are, so if you see a missing period \nor awkward language, its not me (this time) its them.) \n \nWe moderate comments but do so very lightly. \nThere is no hard and fast rules but roughly speaking \nwe block comments that are BOTH offensive AND off-topic. \nThere may be exceptions- like if its on topic but REALLY REALLY offensive \nand adds nothing to the discussion. \nThere may more benign exceptions- like if I post a question and will block the \nanswers so that when I reveal the answer the next day its more dramatic. \nOf if someone posts information that is not public yet. \nIn these case we hope they try to post non-anonymously so we can email them \nand tell them why they were blocked. There are other isolated cases as well. \nAll of these are very rare. \n \nRecent attempted comments do not fall into these rules and we had to \ndecide on them. The following was an attempted comment on my post \nabout \n STOC 2012-Workshops and honored talks \n \n Hi there STOC 2012- workshop and honors talks Loved every second! Great views on that! \nThat actually breaks the mold! Great thinking! \n \nThis comment is NEITHER offensive NOR off-topic.Its a bit odd- I can't tell if \nthe Great view is of my post or of the talks I was writing about. \nIt does sound awkward. Why is that? IT WAS GENERATED BY A SPAMBOT!!! \nHow do I know this? Because if you click on the author you are directed to a site thatsells you paints for your living room.Hence we block such posts. \n \nSo, they think the readers of our blog are into interior decorating. \nI am sure that some are, I don't think our readers are a particularly good market for this. Technology is good enough to find our blogs and try to use spambots on them,but not good enough (or there is no incentive) to figure out which blogs are worthtargeting.This is part of a bigger problem I blogged about \n here where I noted that technology is good enough to know that I am a book review editor for SIGACT NEWS but notgood enough (or there is no incentive) to figure out that I only review comp sci and math books, and not \nbooks on (say) politics. \n \nA borderline case: an attempted comment on the blog \n A natural function with very odd properties was \n \n Awesome logic. You truly have some expert skills and enhanced my knowledge on Cantor Set Construction Agreements. \n \n \nThis one did not link to any product so it might be legit, except thatit is awkward sounding and the same person tried to submit,as a comment to Six Questions a about natural and unnatural mathematical objects \n \n This truly enhanced my skills. very helpful Job Proposal. \n \nClearly spam, though I'm not sure why since there is no link to a product. \n \nThese posts are trying to pass a Turing Test- but so far they are not succeeding. \n \nSometimes they only positive comments I get are from spambots. Oh well."], "link": "http://blog.computationalcomplexity.org/feeds/2046724359191513688/comments/default", "bloglinks": {}, "links": {"http://blog.computationalcomplexity.org/": 4}, "blogtitle": "Computational Complexity"}, {"content": ["Consider the following game on a poset , each player takes turns picking an element x of a finite poset and removes all y \u2265 x. First one to empty the poset wins. I posted last March about a high school student, Adam Kalinich, who showed how to flip the winner of a poset game . \n \nFinding the winner of a poset game is in PSPACE by searching the game tree. A corollary of Adam's work showed that poset games were hard for Boolean formula leaving a huge gap in the complexity of finding the winner. \n \nDaniel Grier, an undergrad at the University of South Carolina, has settled the problem and shows that determining the winner of a poset game is PSPACE-complete. His reduction is ridiculously simple (though not obvious) and the proof is not that complicated either. \n \nGrier starts from Node Kayles which is a game on an undirected graph where each player takes turns removing a vertex and all its neighbors. Whomever empties the graph first wins. Thomas Schaefer showed the PSPACE-completeness of Node Kayles back in 1978. \n \nGrier's reduction from Node Kayles to posets is very simple: Let G be the graph. Have one element in the poset for each vertex of G, all incomparable. For each edge e=(u,v) we add two more elements, one above the vertex elements corresponding to u and v, and one below every vertex element other than u and v. That's the whole construction. \n \nGrier shows that if G has an odd number of edges and no immediate win for the first player then the first player wins the Node Kayles game if and only if the first player wins the corresponding poset game. \n \nYou can read more details in Grier's short paper . It's really neat seeing high school students and undergrads solving interesting open problems. We need more problems like poset games."], "link": "http://blog.computationalcomplexity.org/feeds/8196532692182063373/comments/default", "bloglinks": {}, "links": {"http://arxiv.org/": 2, "http://dx.doi.org/": 2, "http://en.wikipedia.org/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["Early registration for the FOCS conference in New Jersey is September 27th. There is some travel support available for students and postdocs, deadline is this Friday the 21st. \n \nSTOC and Complexity will be co-located in Palo Alto in early June. STOC CFP (deadline November 2), Complexity CFP (deadline November 30). \n \n SODA comes back to the US and New Orleans January 6-8. Accepted Papers ."], "link": "http://blog.computationalcomplexity.org/feeds/8479351675838844789/comments/default", "bloglinks": {}, "links": {"http://dimacs.rutgers.edu/": 2, "http://www.computationalcomplexity.org/": 1, "http://www.siam.org/": 2, "http://theory.stanford.edu/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["In the year 4000BC my great-great-...-great grandmother tried to solve (in today's terms) the equation \n x 2 + 2x + 2 = 0 \n She discovered that if it had a solution then there would be a number a such that a 2 =-1. Since there clearly was no such number, the equation had not solution. She missed her chance to (depending on your viewpoint) discover or invent complex numbers. \n \nFast Forward 6012 years. \n \nIn the year 2012 I wondered: is there a probability p such that if you flip a coin that has prob(H)=p twice the prob that you get HT is 1/2? This leads to \n p(1-p)=1/2 \n If you solve this you get p=(1+i)/2. Hence there is no such coin. WAIT A MINUTE! I don't want to miss the chance that my great...great grandmother missed! In the real world you can't have a coin with prob(H) = (1+i)/2. But is there some meaning to this? \n \nMore generally, for any 0 \u2264 d \u2264 1 there is a p &in C (the complex numbers) such that ``prob(HT)=d.'' The oddest case (IMHO) was to take d=1. You then get that if a coin has prob(H)=(1+\\sqrt(-3))/2 then prob(HT)=1. Does that mean it always happens? No since prob(TH)=1. Do the probs of HH, HT, TH, TT add up to 1? Yes they do since some are negative. \n \nIs there an interpretation or use for this? I know that quantum mechanics uses stuff like this. Could examples like this be good for education? Are there non-quantum examples of the uses of this thatcould be taught in a discrete math course?"], "link": "http://blog.computationalcomplexity.org/feeds/3168893514408307020/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Computational Complexity"}, {"content": ["A couple of weeks ago Suresh tweeted the following result of James Orlin \n \nMax flows in O(nm) time or better. jorlin.scripts.mit.edu/Max_flows_in_O\u2026 \n\u2014 Suresh Venkat (@geomblog) August 31, 2012 \nI'm thinking, wow, max flow is one of the major standard algorithms problems, and O(nm) time (n = number of vertices, m = number of edges) seems like a great clean bound. But there hasn't been much chatter about this result beyond Suresh's tweet. \n \nReading Orlin's paper gives some clues. The previous best bound due to King, Rao and Tarjan has a running time of O(nm log m/(n log n) n) = O(nm log n) just a logarithm off from O(nm). Orlin doesn't directly give an O(nm) algorithm, his takes time O(nm+m 31/16 log 2 n). It's the minimum of the running times of King-Rao-Tarjan and Orlin's algorithms that yields O(nm). Nor is O(nm) tight, Orlin also gives an algorithm with a running time of O(n 2 /log n) when m=O(n). \n \nI don't mean to knock Orlin's work, he makes real progress on a classical algorithmic problem. But somehow I think of O(nm) as a magical bound when it is really just another bound. I'm just fooled by simplicity."], "link": "http://blog.computationalcomplexity.org/feeds/5232165705138952124/comments/default", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://en.wikipedia.org/": 1, "http://t.co/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["Two quantum announcements (emailed to me by Umesh Vazirani, and producedhere almost exactly) and then some thoughts of mine quantum computing. \n \n Announcement one: The NSF has a new initiative to try to address the lack of tenured faculty (particularly in computer science departments) involved in quantum computation research. CISE-MPS Interdisciplinary Faculty Program in Quantum Information \n \nThe initiative provides a paid sabbatical year to interested tenured faculty to visit a strong quantum computing group, so that can reposition their research interests. The rationale behind the solicitation is to increase the number of tenured researchers in quantum computation, but also to break through the \"quantum skepticism\" in faculty hiring decisions in those departments where there are no faculty actively involved in quantum computing research. \n \n Announcement two: In support of this program, Carl Williams (a physicist working in Quantum Information who put together the US Vision for Quantum Information Science for the Office of the President) and Umesh have put together a workshop where interested individuals can learn about the initiative, the field and make contacts with people from the major quantum computing centers: see here . \n \nThe initiative comes at a particularly opportune moment for researchers in complexity theory, given the increasing relevance of quantum techniques in complexity theory --- the 2-4 norm paper of Barak, et al (SDPs, Lasserre), exponential lower bounds for TSP polytope via quantum communication complexity arguments (See Drucker and de Wolf paper Quantum proofs for classical theorems for several apps of Q to Complexity, and see \n here for the TSP polytope result) \nquantum Hamiltonian complexity as a generalization of CSPs, lattice-based cryptography whose security is based on quantum arguments, etc. \n \n MY COMMENTS: Umesh gives as a reason quantum is important its uses in other parts of complexity theory. While that is certainly good there are other intellectual reasons why Quantum is worth studying. \n Factoring is in Quantum P! There are MANY problems (maybe 10) where Quantum seemsto be faster than classical. I wouldn't really want to push this point sincequantum computer aren't build yet. More generally, if one claims a field is validfor real practical value, those arguments may become less believable over time. \n \n Quantum computing can be used to simulate quantum systems- I think this was one of the original motivations. \n \n Quantum computing is valuable for a better understanding of Physics. \nThis was first told to be my Fred Green (A Physics PhD who went into computer science)and I made it the subject of this blog entry. \nI like his quote so much that I will quote it here \n \nLearning quantum computing helped me understand quantum mechanicsbetter. As a physicist I never thought about measurement theoryor entanglement, which were foundational issues , irrelevantto what was doing. In quantum computing, we reason about thesethings all the time. \nOver the years others have told me similar things. \n \n \n \n \n \n \n \n \n \n \n \n \n Side note: The word Quantum is mostly misused in popular culture. Quantum Leap meant a big leapwhen actually quantum means small. The James Bond movie Quantum of Solace used it correctlybut was an awful movie. Oh well."], "link": "http://blog.computationalcomplexity.org/feeds/1251551422632754353/comments/default", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://homepages.cwi.nl/": 1, "http://www.nsf.gov/": 1, "http://www.umd.edu/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["Among the many conference/journal discussions, one systems person said the reason they don't publish in journals is that their work is very dependent on current technology. A few years in the future the technology will change and this research is no long relevant. The best systems research develop ideas that transcends technology, but much research in systems have a limited lifespan of relevancy. \n \nIn theory, if you prove a theorem that theorem will be true forever. In fact that theorem was always true, we just didn't know it. So it is important to have refereed long-term archival write ups of these results. A theorem could be supplanted by another result or get less interesting over time but it always remains true. Theory results transcend technology, though most theory result have a zero lifespan of relevancy."], "link": "http://blog.computationalcomplexity.org/feeds/4676111875248523694/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Computational Complexity"}, {"content": ["The University of Maryland at College park is having a Theory Day on Wed Oct 24! Come hear \n \n Distinguished talks by Julia Chuzhoy and Venkataesan Guruswami! \n \n Short talks (is that code for NOT distinguished?) by Bill Gasarch, MohammadTaghi Hajiaghayi, Jonathan Katz, Samir Khuller, David Mount, Elaine (Runting) Shi, and Aravind Srinivsan. (I never realized I was first alphabetically until now.) \n \n Discussions in hallways for those that learn more that way! \n \n \nFor more info see the link above, but note one thing: Its FREE! \n \nI like theory days in general and often go to the NY theory days. They are free and only one day. I recommend going to any theory day that is an Amtrak Ride away. (Might depend on how long the trip is- There is a 13-hour Amtrak from Atlanta Georgia to Maryland, though I doubt I'll see Lance there.) I get a lot out of theory day as noted in this post about NY theory day \n \n(ADDED LATER AT THE REQUEST OF THE ORGANIZER: \nTheory day at UMCP is intentionally after the NJ Focs and is an easy amtrak \naway from NJ. The stop is New Carolton)"], "link": "http://blog.computationalcomplexity.org/feeds/4340153588131699020/comments/default", "bloglinks": {}, "links": {"http://www.umd.edu/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}, {"content": ["The following is a paraphrase of a comment at the end of the Suggested Readings section of Spivak's calculus book: \n Abel remarked that he attributed his profound knowledge of mathematics to the fact that he read the masters, rather than the pupils. \n Are you better off reading the Masters or the pupils? This of course depends on the masters and the pupil and other factors. \n I have heard that Godel's original papers (even when translated) are well written and show a profound understanding of the subject and why its important. \n However, we now have a better understanding of what Godel did and better ways to express it. \n The Masters may include the motivation which may be lost in later papers. \n Often the first proof of anything is ugly or odd and later proofs really clean it up. \n Often the first proof of anything uses only basic concept- later abstractions may hide the heart of the proof. \n As a practical matter sometimes the early papers are not available (thanks to paywalls or obscurity) or in a language you do not read. \n If Lance and I ever do a book-of-blog-posts I will clean up some of the spelling, make some of the arguments more clear (perhaps indicate where I am being sarcastic in cases where it was not understood), improve the writing. This will make it better than the blog but less authentic. \n \n \n \n \n \n \n \n Here are examples where the Masters papers may not be worth reading: \n Recursion theory in the early 1960's had several infinite injury arguments. I have heard that they were known to work only because the lemmas and proofs worked out. Only after Bob Soare's excellent article on the topic were they really understood. For 0''' priority arguments it is also true that the early papers are not the ones to read. \n Example (and the real motivation for this post). I have tried to read Ramsey's original article. I knew that his goal was a problem in logic, and I wanted to know what that problem was. I had a hard time reading the paper. (I did my own writeup .) Why was his version so hard to read? (1) He never uses the words coloring or graph or hypergraph . He doesn't mention that if you have six people at a party either three of them know each other or three of them don't know each other. Perhaps he didn't go to many parties. (2) He uses odd terms at time. (3) His paper is rather abstract. If he had just proven a simple case then it would be obvious how to proceed to his abstract case. This is true for both his combinatorial theorem--- he only proves (what we would call) the hypergraph version, and also the Logic theorem. \n The Cliff notes for Atlas Shrugged are far better than the book. Shorter too. They are online for free here which makes sense since Ayn Rand was known for her altruism. \n \n \n \n SO- what do you think? Examples of cases where the Master is better to read? Examples of cases where the Pupil (or more generally later summaries, surveys, expositions) is better to read?"], "link": "http://blog.computationalcomplexity.org/feeds/2131195086239732300/comments/default", "bloglinks": {}, "links": {"http://www.cliffsnotes.com/": 1, "http://www.umd.edu/": 3}, "blogtitle": "Computational Complexity"}, {"content": ["I spent the first half of my life in the jet age but not in the Internet age. I could fly anywhere in the world but the fastest way to get a research paper to another scientist was to bring the paper on the plane with me. \n \nOne could imagine technological innovation going the other way around where we had a functioning Internet but no air travel. Maybe we would have done a much better job creating virtual meetings and conferences. \n \nSuppose you had to choose a world to live in: \n \n Jets but no 'net. We know what this world looked like. \n Net but no jets. We can only imagine. \n \n \nWhich one would you choose? \n \n \n \n[This question came from a discussion with Paul Royal, a Georgia Tech research scientist in Information Security. We chose different answers.]"], "link": "http://blog.computationalcomplexity.org/feeds/8585886315629668523/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Computational Complexity"}, {"content": ["Neil Armstrong died on August 25, 2012. He was the first man to walk on the moon. (Since they always say this I wonder if Women walked on the moon earlier.) Ray Bradbury died on June 5, 2012, though on this (and perhaps other) blogs with was overshadowed by death of Mihai Patrascu on the same day. \n \nWhen man landed on the moon I thought that this would be a common thing- that we'd goto the moon about once a year. In the end only 12 people walked on the moon and we stopped going in 1972. See here for details. \n \nWhy didn't we go more often? Maybe there wasn't much to see- you see one moonrock you've seen them all. Are unmanned flights much better in terms of science-for-the-money? I suspect yes. Also, one reason America put men on the moon was to beat the Russians to it. Once we already did that, the point was made, so no reason to go again. \n \nIf we went now it would cost much less. So perhaps we should have waited for the technology to catch up and go later for much cheaper. That's not quite right- one reason we have some of the technology is that we went then. But in some areas- computers in particular- certainly we would have still made progress without going to the moon. On the other hand going to the moon when we did was quite inspiring to some people. (Are you one of those people?) \n \nRay Bradbury's classic Farenheit 451 was about censorship- the government had firemen who burned books. Books made of paper. I suspect that within 10 years e-books will be the standard (some exceptions- Art books, maybe some Math books). Will that make censorship easier or harder? The Arab Spring was caused partially because the government could not control social media. However, in China the government is pretty good at blocking access to the Web. But still, some gets through. So to rephrase the question- does current technology make censorship easier or harder? I don't have an answer to this question- but I invite your intelligent commentary. \n \nRay Bradbury himself has said that the book was also about people choosing a shallow culture (e.g., TV over books). Modern technology has been a mixed bag for this. Some TV shows will one day (or even now) be seen as classics (e.g., The Simpsons) while others are of course going to be seen as vapid, shallow, and not worth much (e.g., Madmen)."], "link": "http://blog.computationalcomplexity.org/feeds/8178902093590152256/comments/default", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 4}, "blogtitle": "Computational Complexity"}, {"content": ["Leonid Levin will receive the Knuth Prize, and give the corresponding lecture, at FOCS this year. The Knuth Prize is jointly given by ACM SIGACT and the IEEE TC-MFCS for outstanding contributions to theoretical computer science. \n \nLevin easily deserves the award alone for his amazing two-page 1971 paper, actually two major research lines \n \n \n An independent discovery of NP-completeness and \n Universal search \n \nToday we call the seminal NP-completeness result for Satisfiability the Cook-Levin theorem. \n \nLevin did so much more, from the \"right\" definition for average-case hardness to (with Hastad, Impagliazzo and Luby) producing pseudorandom generators from any one-way function. \n \nCongrats to Leonid!"], "link": "http://blog.computationalcomplexity.org/feeds/4690448069462758587/comments/default", "bloglinks": {}, "links": {"http://www.sigact.org/": 1, "http://www.acm.org/": 1, "http://dimacs.rutgers.edu/": 1, "http://blog.computationalcomplexity.org/": 1}, "blogtitle": "Computational Complexity"}]