[{"blogurl": "http://rjlipton.wordpress.com\n", "blogroll": [], "title": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["Real implications from a fake-paper prank? \n \n \n \n \n \n \n \n \n \n Composite of src1 , src2 \n \n \n \n \n \nMarcie Rathke was not among the speakers whose talks I heard at the Algorithms Workshop at St. John\u2019s College, Oxford University, two weeks ago. She is \u201cfamous\u201d for her recent paper , \u201cIndependent, Negative, Canonically Turing Arrows of Equations and Problems in Applied Formal PDE,\u201d which was just accepted by the journal Advances in Pure Mathematics . Her paper could have been presented at the Oxford workshop since solving PDE\u2019s is a vital algorithmic task, Oxford is known for formal approaches, and this was part of Oxford\u2019s observance of the Turing Centennial. But it was not. \n \nToday, October 30th, the eve of Halloween, is often called \u201cMischief Night.\u201d It is a night when many children play tricks before they receive their treats on the next night\u2014Halloween. Thus today is a good time to think about tricks and possible treats in the world of mathematics and theory. \n \n Mischief Night And Turing Arrows \n \n \nKen has strong ties to Oxford, and was excited to discover that Mischief Night\u2014which his New Jersey neighborhood called \u201cCabbage Night\u201d\u2014connects to St. John\u2019s College. The earliest reference given by Wikipedia traces it to the college\u2014in 1790. This is a story related by the college fellows about a headmaster supporting a school play that included \n \n\u201can Ode to Fun which praises children\u2019s tricks on Mischief Night in most approving terms.\n \n \nWhether the college today would have approved staging a talk by \u201cMarcie Rathke\u201d is dubious, because her paper is entirely fake. It is a \u201ctrick\u201d not a \u201ctreat.\u201d The paper is a fake, a fraud, and complete nonsense\u2014is there incomplete nonsense? \n \nThe paper was generated by Mathgen , a web applet that creates papers in a \u201cstructuredly random\u201d fashion. \n \nKen and I are disappointed, as the topic of \u201cTuring Arrows\u201d sounds cool and important. Surely there is room for a creative genius\u2014a human genius\u2014to conceptualize and develop it. Arrows suggest relational order theories of the kind proposed by the cosmologist Lee Smolin, which are a system of \n \n\u201cknots and networks [such that] the geometry of space arises out of [a] fundamental quantum level which is made up of an interwoven network [of] processes.\u201d\n \n \nPer theories by fellow cosmologist Max Tegmark, those processes would be Turing-computable. This would align the Turing Arrows programme with Alan Sokal\u2019s pioneering work on transformative approaches to quantum gravity . \n \n SCIgen and Real Gen? \n \n \nMathgen is an offshoot of SCIgen , which calls itself \u201cAn Automatic CS Paper Generator.\u201d The program was written and is maintained by MIT graduate students Jeremy Stribling, Max Krohn, and Dan Aguayo. Their program aims \n \n\u201cto maximize amusement, rather than coherence.\u201d\n \n \nThe applet is easy to use: just enter some author names\u2014yours or others\u2019\u2014and click go. Ken did this with our names, and to quote Queen Victoria, \n \nWe are not amused.\n \n \nOr rather, we are be mused, because what we got looks suspiciously good. Here are the title and abstract of the auto-generated paper: \n \n \n \nBoth of us have done real research related to constructive number theory, and we have talked about Ryan and Virginia Williams\u2019 approach to matrix product using triangles. Notably, Ken and I just recently wrote a post on polytope approaches to factoring which of course relates to the Sieve of Eratosthenes. \n \nSo we wonder, does Mathgen read G\u00f6del\u2019s Lost Letter ? More to the point, could we soon build a program that devours mathematical texts the way Google Translate does with massive data? We have blogged before about how far the methodology of IBM\u2019s \u201cWatson\u201d might be adapted to do mathematics. We see some easy improvements the SCIgen people can make, such as assuring that quantities are defined or at least introduced before they are used. With the same level of research input as Watson, how scary-plausible could Mathgen become? \n \n Skolem and Golem \n \n \nThe logician Thoralf Skolem lent his name to \u201cSkolemization\u201d , which remains an essential process in automated theorem proving today. He also developed primitive recursive arithmetic , and his Wikipedia bio goes so far as to say that if his work is interpreted with a certain hindsight, \n \n\u201cSkolem can be seen as an unwitting pioneer of theoretical computer science.\u201d\n \n \nAll of this raises the spectre of the \u201cgolemization\u201d of mathematics. The Golem is an \u201can animated anthropomorphic being, created entirely from inanimate matter.\u201d Kurt G\u00f6del supposedly killed off David Hilbert\u2019s programme to automate mathematics, but in later life he was not so sure. A project at Oxford titled ALEPH for \u201cA Learning Engine for Proposing Hypotheses\u201d draws from the Golem story. \n \nI have been thinking more about Skolem\u2019s Problem from the wonderful talk by Jo\u00ebl Ouaknine, and have shared some further thoughts with him about solutions in special cases. Now I wonder, can we automate the creation of an exhaustive list of special cases that can be researched by computer, at least in the manner of PolyMath ? \n \n Skolem Cases \n \n \nWe are working on the general Skolem problem and have made some progress. For example, we believe we can handle some special cases\u2014more in the near future. \n \nDefine the class of numbers to be those positive numbers of the form where is a prime and both and are less than . Possibly we can allow to grow slowly with , not just be fixed. \n \n Theorem 1 Let be algebraic numbers, let , and let be polynomials in with integer coefficients. Then we can decide whether or not there is an so that \t \n \n \nfor in . \n \n \nWe will discuss this and more soon. The proofs use properties in algebraic number theory that are computational. We hope it will be a real treat and not a trick. \n \n Open Problems \n \n \nCan we solve more cases of Skolem\u2019s Problem? Can we automate them? Are we any closer to a mathematical \u201cGolem,\u201d at least for Skolem? \n \nIncidentally, talks at the workshop are now online, and ditto talks from the Princeton Turing Centennial Celebration, which we covered last May. \n \nFinally, happy Halloween and enjoy your treats. All the best to those affected by Hurricane Sandy as well\u2014it has postponed Halloween in many communities."], "link": "http://rjlipton.wordpress.com/2012/10/30/mathematical-mischief-night/", "bloglinks": {}, "links": {"http://polymathprojects.org/": 1, "http://feeds.wordpress.com/": 1, "http://algo.ac.uk/": 1, "http://blog.richmond.edu/": 1, "http://www.ac.uk/": 1, "http://blog.computationalcomplexity.org/": 1, "http://pdos.mit.edu/": 1, "http://rjlipton.wordpress.com/": 12, "http://thatsmathematics.com/": 1, "http://en.wikipedia.org/": 9, "http://www.princeton.edu/": 1, "http://www.co.uk/": 1, "http://thatsmathematics.com/blog": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["Proofs that are about the length of a tweet. \n \n  \n \nJack Dorsey is the creator of Twitter. We thank him for the creation of of tweets , which are of course messages of up to 140 characters in length. \n \nToday Ken and I wish to talk about very short proofs\u2014proofs that could almost fit into a single tweet. \n \nIt\u2019s a fun topic, but also touches on a serious one: what is a proof really supposed to do? \n \nThese proofs must be hard to find but quick to verify, which is the essential idea of . \n \n Euler\u2019s Example \n \nLet\u2019s look at problems whose solutions fit into a single tweet, or almost. \n \n \nTrying to generalize Fermat\u2019s Last Theorem, the great Leonhard Euler conjectured that an th power cannot be written as the sum of fewer than nontrivial powers for . \n \nLeon Lander and Thomas Parkin used a computer to solve the problem for and found a counterexample. Here is their whole paper: \n \n \n \nNote, if they had just sent the answer it would have been: \n \n \nWriting this as a LaTeX formula would have easily fit into a single Tweet\u2014too bad they did their work 46 years ago. \n \nTwo decades later, Noam Elkies found a method to construct an infinite series of counterexamples for the case. He showed that \n \n \nwhere\n \n \nIn 1988, Roger Frye found the smallest counterexample based on Elkies\u2019 ideas:\n \n \n Gems \n \nNote that always has the integer roots , which are distinct when . And \n \n \nis solvable in integers by , , , among many other possibilities. \n \nThat is, \n \n \nhas four distinct integer roots in pairs and . \n \nGoing from degree to , we can solve \n \n \nwith , , and . These induce , , , and , again giving -many distinct integer roots. \n \nCan we do this with for \n \n \nThere had been a conjecture\u2014indeed, a claimed proof\u2014of \u201cno,\u201d but Dominic Symes found an example\u2014or counterexample: \n \n \nThis gives distinct roots , as you can verify. Andrew Bremner found two infinite families of solutions. \n \nCan we take it a step further to degree with and ? Nobody knows. This is related to Stephen Smale\u2019s Fourth Problem which we just blogged about. \n \nNamely, the left-hand sides are straight-line programs of length . If there are distinct integer zeroes, this violates the conjecture . \n \nA degree- polynomial with distinct integer roots and a short straight-line program is called a gem by Bernd Borchert, Pierre McKenzie, and Klaus Reinhardt. \n \nThey give -gems for up to , but skipping . See Borchert\u2019s project page for more, including relevance to factoring. \n Short Proofs and Interaction \n \nKen and I believe this issue of very short proofs highlights the real reason we prove theorems in mathematics. \n \nA proof is not just a thing that we write out and then feel happy with the knowledge that something is proved. No. A proof must be something that can be understood by others. \n \nTo be understood it must be checkable. The above are extreme examples\u2014you can just do the arithmetic. The existence of the proof is what was hard. \n \nIn some areas of computer science proofs are viewed in a different way. One area where I hear statements like \u201cthe proof is long and boring\u201d is cryptographic protocols. \n \nThat some proofs of protocols are tedious is exactly what I do not like about them. I do not trust a long and boring proof, exactly because it is unlikely to be checked by others. \n \nOf course protocols themselves are often proofs, of knowledge or identity or privilege or authority. The individual steps required of the user can be short. \n \nThe idea of interaction takes proofs beyond . The interaction can be short, or have short rounds. Still, the proof that it is a proof can be long. \n \nEven for problems like factoring, interaction can shorten the proof that one has a proof. Suppose I have a long proof of an efficient algorithm. What can I do? \n \nI can say, \u201cTweet me a number.\u201d Using ASCII for base 128, a 1,024-bit RSA modulus can fit into 140 characters. Then I tweet back and you verify . \n \nWith interaction you can\u2019t accuse me of pre-computing results or exploiting holes like Arjen Lenstra\u2019s project . This is a short proof of my proof. \n \nMost protocols cannot self-prove this way because you have to prove security against possible attacks. But there as in math, it\u2019s good to ask how far short proofs can go. \n \n Open Problems \n \nCould there be a very short proof of your favorite open problem? Can we make more proofs like tweets? \n \nIs a post easier to read with Tweet-length paragraphs, sometimes with equations in-between, each giving one idea? Or our usual longer ones? \n \n End Note We add our congratulations to Leonid Levin, himself known for very short papers, on his 2012 Knuth Prize . Ken heard his lecture at FOCS this past Monday\u2014here is a nice post by Thomas Vidick on it. \n \n[21519-->217,519]"], "link": "http://rjlipton.wordpress.com/2012/10/26/short-and-tweet/", "bloglinks": {}, "links": {"http://www.sigact.org/": 1, "http://feeds.wordpress.com/": 1, "http://blog.computationalcomplexity.org/": 1, "http://rjlipton.wordpress.com/": 4, "http://mycqstate.wordpress.com/": 1, "http://www-ti.uni-tuebingen.de/": 3, "http://en.wikipedia.org/": 1, "http://projecteuclid.org/": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["A report on a recent workshop at Oxford University \n \n  \n Jo\u00ebl Ouaknine, Georg Gottlob, and Andreas Pieris are faculty at Oxford University in St. John\u2019s College. They recently held a workshop on algorithms to celebrate the creation of an algorithms group within the computer science department. \n \nToday I wish to give a report on the talks that were given at the workshop. \n \n \nThe workshop was a joy to attend. It was well organized and well attended, and the talks were terrific. The talks covered algorithms in the broadest sense. I was honored to be included among the speakers, but will not say anything about my talk. Unfortunately I could only attend about two-thirds of the talks, and therefore will only comment on those I had the pleasure of seeing in-person. I heard that all the talks were great, and wish I could have been there for all of them. The ones I missed were given by Leslie Goldberg, Monika Henzinger, and Elias Koutsoupias. \n \nJo\u00ebl told me they plan to put the talks on the web\u2014I will update you if and when this happens. \n \n The Talks \n \nHere are those talks that I did get to hear. \n \n \nPaul Goldberg: Towards Zillions of PSPACE-completeness Results . Paul spoke on non-zero-sum games and the complexity of solving them. He focused mostly on various versions of Brouwer\u2019s Fixed Point Theorem in a discrete setting. This leads to quite interesting questions about how to encode creation path following problems, and he had exquisitely beautiful two- and three-dimensional diagrams of colored blocks arranged in complex patterns. The key insight is: any algorithm for these problems that solves them by a \u201cpath following method\u201d must be PSPACE-complete. \n \nWhat does it mean for an algorithm to be complete? The algorithm is working on a problem for which there are multiple correct answers\u2014in this case, equilibria. Reproducing the particular outputs favored by the algorithm is the PSPACE-complete task. \n \nJoan Feigenbaum: DISSENT: Accountable, Anonymous Communication on the Internet . Joan spoke on a current large DARPA sponsored project that she is involved with on how to communicate on the Internet and really stay anonymous. There are current ad-hoc methods that are in wide use, but she points out that they are well-known to be attackable. Often simple traffic analysis methods can be used to get around the claims that users remain hidden. Her group is working on a system, which they call DISSENT, that has provable properties. This was a great example of using basic tools from modern crypto and putting them together in ways that solve an important problem. The project goal is to make the system practical, and to work with huge numbers of users. \n Peter Jeavons: Discrete Optimisation\u2014The Big Picture . Peter took the big picture, as he likes to say. He also focused on the last 100 days of research in this classic area. In his view almost all hard optimization problem can be viewed as constraint satisfaction problems (CSP). He then showed how the structure of CSP\u2019s are related in a beautiful way to classic universal algebra. While there has been great progress in the recent past\u2014100 days\u2014some of the key ideas date back to work of Emil Post from 1945. A great combination of a grand vision and hard concrete results.\n Rapha\u00ebl Clifford: Lower Bounds for Streaming Problems . Rapha\u00ebl spoke on lower bounds for streaming problems\u2014big surprise\u2014but the interesting part was that he could use methods that were developed already. These are lower bound methods for data structure problems based on the so called \u201ccell-probe\u201d model. This model assumes that there is a random-access memory that can be read and written one cell at a time. There already are a multitude of results for data structures based on this model, but somewhat surprisingly Rapha\u00ebl is able to adapt them to solve his streaming problems.\n Phil Blunsom: A Bayesian Approach to Learning the Structure of Human Languages . Phil spoke on the acquisition of languages. His group is one of the best in the world at learning languages without any prior models or assumptions about the language. His group gets scores on known test sets that are better than anyone else\u2019s.\n Georg Gottlob: Hypergraph Transversals and Monotone Boolean Formula Dualization . Georg gave a wonderful talk on a very old problem about hypergraphs. Recall a hypergraph is nothing more than a finite collection of subsets of a set. You can think of graphs, no \u201chyper,\u201d as the special case where the sets are all pairs of vertices. Many problems can be expressed as properties of hypergraphs. One of the big open questions in the area is whether or not a hypergraph is the transversal of another one. The surprising result is that this can be done in time\n \n where is the number of vertices. Georg is able to generalize this result and show that it can be done in small space: \t \n \n \nThis is an unusual space bound, and the result seems to be quite neat. \n Jo\u00ebl Ouaknine: Decision Problems for Linear Recurrence Sequences . Jo\u00ebl spoke on a problem that has been open for over 80 years: the Skolem Conjecture. The Skolem question is just this: consider a linear recurrence over the rationals. Does there exist an so that ? In more details:\n \n where the coefficients are reals, and the initial values are also reals. \n \nHe quoted Terence Tao\u2014as we did \u2014saying: \n \nIt is faintly outrageous that this problem is still open; it is saying that we do not know how to decide the Halting Problem even for \u2018linear\u2019 automata!\n \n \nI agree. I have worked on some related theorems that are now years old\u2014see here \u2014with Ravi Kannan, and are well aware of the difficulty of the area. Jo\u00ebl gave a masterful explanation of the area, stated what is known about it, and presented some new types of barriers to further advances. I will say more about the type of barrier in a the next section. \n Robert Tarjan: Algorithm Design: Theory and Practice . Bob spoke on some new results on data structures and algorithms that are driven by practical questions. One example was on balanced binary search trees\u2014something Bob is the world expert on. He re-visited some of his classic work on splay trees, and showed how they can be improved in two ways. The data structures can be made to handle very uneven frequencies of access and still yield better performance. Also they can be made more concurrent. Today such data structures are used in many-core applications, in which the old method had a hot spot at the root. That is, all updates would touch the root. Bob\u2019s new result yields a data structure in which almost all updates are near the leaves, and this yields very good concurrent performance.\n \n \n A Type Of Proof Barrier \n \nIn computer science theory we have a variety of barriers that explain why it is hard to prove something. Prominent examples that come to mind are the oracle limitations, the natural proof barriers, and the algebrization barrier. There is another type of barrier: If you solve X, then you solve Y; but Y is really hard. \n \nKen and I have pointed out one example recently in a post on the famous Hartmanis-Stearns conjecture. We noted that a proof of the conjecture would entail a super-linear Turing Machine lower bound on integer multiplication, which on current knowledge is held to be extremely unlikely to be proved. \n \nJo\u00ebl Ouaknine\u2019s work with James Worrell, and recently including Matt Daws, does the same for the Skolem conjecture. He shows that either resolution of the Skolem problem would solve deep open problems in number theory. If Skolem\u2019s conjecture is true, then there are new results that seem beyond the reach of current methods, and the same if the conjecture is false. The pivotal concept is the Lagrange values of real numbers : \n \n \nFor \u201cmost\u201d this is zero, but for most (not just many) important irrational numbers precious little is known, even whether is computable. What their work shows is that for the real angular parts of certain classes of complex algebraic numbers on the unit circle, decidability of Skolem\u2019s problem (for order 6 or higher) implies computability for one class, while undecidability implies positivity of a related quantity for another class. This is a very neat state of affairs. \n \nOne of his solved special cases is even neater, for those who are fans of the counting hierarchy : \n Theorem 1 Whether a linear recurrence of order at most 5 has no negative terms is decidable, and belongs to the complexity class\n \n \n \n Open Problems \n \nAt dinner the Oxford people told the following \u201cjoke\u201d: \u201cHow many Oxford dons does it take to change a light bulb?\u201d The answer is: \n \n Change? \n \n \nThe context of this is the difficulty of creating the algorithms group. Ken remembers the discussions around creating a computer science department in the 1980\u2032s. They seem to be off to a great start, and look to be one of the top groups in the world in the near future. Cheers to them. \n \nDoes Skolem\u2019s Problem have more direct implications for issues in complexity lower bounds?"], "link": "http://rjlipton.wordpress.com/2012/10/21/algorithms-at-oxford/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://algo.ac.uk/": 1, "http://qwiki.stanford.edu/": 1, "http://www.ac.uk/": 1, "http://rjlipton.wordpress.com/": 4, "http://en.wikipedia.org/": 3}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["Factoring again, always factoring\u2026 \n \n  \n \nPeter Borwein is among those who carry forward the title mission of Alan Turing\u2019s famous paper, \u201cOn Computable Numbers\u2026\u201d With David Bailey and Simon Plouffe, he found a new kind of fast formula for computing individual digits of without having to calculate all preceding ones, as we covered here and here . His brother Jonathan Borwein is likewise among the \u201cfathers\u201d of experimental mathematics as a discipline. Peter is currently the Founding Director of the IRMACS Centre at Simon Fraser University. \n \nToday we discuss a recent survey paper with his co-author Joe Hobart on the power of division. \n \n \nThere are other talented pairs of brothers in mathematics and computer science theory. The Lenstras\u2014Jan, Hendrik, Arjen, and Andries\u2014account for six pairs all by themselves. I (Dick) have been a colleague of Michael Fischer, whose late brother Patrick is being honored with a symposium this coming November 5th at the University of Michigan. The Chudnovsky brothers , Gregory and David, also do experimental mathematics. Gregory Chudnovsky won a MacArthur \u201cgenius grant\u201d in 1981, and this reminds us to congratulate graph theorist Maria Chudnovsky (alongside Daniel Spielman) on her just-announced MacArthur award, but she is not related. My colleague Dana Randall is the sister of physicist Lisa Randall, but I don\u2019t know any comparable sister combinations within theory. \n \nThe survey paper by Borwein and Hobart is in the August/September issue of the American Mathematical Monthly, and is titled \u201cThe Extraordinary Power Of Division in Straight Line Programs.\u201d \n \nThe point of this note is to advertise [a] lovely circle of ideas..that if it is possible to divide quickly, then it is possible to factor quickly.\n \n \nReleasing the power of division, however, requires teaming it with the humble floor function. \n \n The Floor Function \n \nKen and I both remember courses in high school and/or college in which the integer floor of a real number was written . This notation goes back, of course, to Carl Friedrich Gauss. But it wasn\u2019t named for Gauss, and perhaps that made it liable to replacement by someone with a better idea. Kenneth Iverson split Gauss\u2019 brackets by introducing separate floor and ceiling functions: \n \n \nHe introduced those symbols into his programming language APL , and the worth of good notation proved out in their now-universal adoption. To quote the singer-songwriter Paul Simon: \n \nThere\u2019s been some hard feelings here \nAbout some words that were said \nBeen some hard feelings here \nAnd what is more \nThere\u2019s been a bloody purple nose \nAnd some bloody purple clothes \nThat were messing up the lobby floor \nIt\u2019s just apartment house rules \nSo all you \u2018partment house fools \nRemember: one man\u2019s ceiling \nIs another man\u2019s floor \nOne man\u2019s ceiling \nIs another man\u2019s floor. \n \n \n A Smale Problem \n \nIn 1998 Steven Smale published his list of great problems for the next century\u2014this century. After leading off with the Riemann Hypothesis, Poincar\u00e9 Conjecture, and P vs. NP, he stated: \n \n Problem 4: Integer zeros of a polynomial of one variable.\n \n \nGiven a polynomial with integer coefficients, define to be the number of steps needed to create a formula for starting with and , where each step is addition, subtraction, or multiplication of two previously-created formulas. A sequence of such steps is called a straight-line program for . Let be the number of zeroes of that are integers\u2014could be none, could be many. The problem is simple: \n \nIs there a fixed such that for all , ?\n \n \nIf has no integer zeroes this says nothing, but if has many integer zeroes, then this says is hard to compute. An example of such an is the factorial polynomial, \t \n \n \nIn terms of the number of bits of , this has exponentially many integer zeroes, so an affirmative answer to the problem implies an exponential lower bound on the size of straight-line programs for this . This might not be surprising, but then Smale goes on to state a simpler, concrete problem. \n \nIs there a constant such that for all -bit numbers , ?\n \n \nA strong negative answer would be such that , giving an exponential lower bound in terms of the number of bits. \n \nHere for an integer means the number of steps required by a straight-line program that starts with just , no variables. With Michael Shub, Smale showed that if there is no sequence of integers that \u201chelp\u201d in the sense of , then the problem analogous to in their model with Lenore Blum of computation over is intractable, giving a -style consequence for that model. \n \n Information Overload and Extraction \n \nIn one sense the algebraic models are weak, and in another they are super powerful. They do not allow you to treat numbers as strings of bits, so as to encode arbitrary Boolean functions in them. They do not give you arbitrary coefficients on the polynomials , but only ones you can build up from by , which is why even computing integers is a challenge. \n \nHowever, they allow storing values with unlimited precision, and repeated squaring can create some huge values. If we square a -digit number , and do it times, then we have the number which has digits. There is no way a \u201creal\u201d -time program can enumerate all those digits, although a straight-line program is allowed to have them. The question is, how may we access them? \n \nWe have often on this blog considered exponential-size structures that are succinct , meaning there is a polynomial-size object that allows extracting any specified entry of the structure. For instance, the structure can be a long string or number and the object can be a small circuit such that for all , returns the th bit. We could modify this idea to create a circuit returning the bits between and , provided . (We index the least significant bit by .) \n \nWe do not know whether short straight-line programs preserve succinctness. Besides the connection to factoring, this could even be related to issues about the complexity of integer multiplication that we raised here . But division and the floor function do the same job nicely. Namely, to get the bits of a big number in the place through the place, which we denote by , compute: \n \n \nThe mod trick here is general: to compute mod do . \n \n Factoring Via Factorials \n \nThe next trick noted by Borwein and Hobart is to compute a binomial coefficient by powering and bit-extraction. \n Theorem 1 In standard binary notation, \n \n \n \nWhat happens is that the products of align the \u2018s in columns so that their sums give each binomial coefficient, and the offset of from the right locates the sum for . Even if is a big number like , this can be done in steps by repeated squaring. \n \nIn fact, raising to the -th power when is not a power of involves the problem of finding an efficient straight-line program to compute itself. But in any event, when is exponential in the complexity parameter , the process is no worse than polynomial in , even though the theorem creates a number with about digits. The range of bits extracted\u2014usually with some leading s\u2014still has exponential size, however. \n \nIf we can manipulate those bits, however, then we can factor. The trick is the following: \n Theorem 2 If is even, , then . \n \nBy recursing from to , or when is odd, doing and recursing from to , we can write a standard straight-line program that computes in time , where and is the time to compute . If , as above when division and floor are allowed to extract bits, then has a -sized straight-line program. \n \nFinally, to factor an -bit Blum integer of the form with primes , we can use Newton\u2019s method to compute close enough to to assure , so that . In the absence of a direct comparison for -bit numbers like , Euclid\u2019s algorithm can still be implemented using division and floor, as shown in the paper. The result is a -sized straight-line program that does factoring. \n \n Wiz or Swiz? \n \nI am stopping in England on my way to Doha, and Ken gave me some prep about the natives. The British have a word \u201cswiz\u201d which means something that doesn\u2019t turn out as advertised. Division and floor are easy functions, so saying that allowing them in programs means we can factor means we can factor, no? The \u201cswindle\u201d is that and are exponential-sized numbers, which the straight-line programs are not being fairly charged for. \n \nStill, we may not need to compute all parts of these numbers. Right off the bat, we only need to compute , and then still yields . While computing , we can try to keep things reduced mod . We have blogged recently about cases where a \u201clazy\u201d approach can be more efficient. Perhaps a breakthrough in the \u201ceasy\u201d problem of integer multiplication can find new regularities in powering that help. If the straight-line model used by Shub and Smale can simulate integer division, even just for restricted cases, all these factors come into play. \n \n Open Problems \n \nBorwein and Hobart themselves ask, \n \nGiven a straight line program for (\u2026), is it possible to find a nontrivial factorization of just given the information in the program?\n \n \nThe paper gives other open problems, of which the last is whether the known polynomial-time algorithms for factoring polynomials can be implemented by comparably efficient straight-line programs. \n \n[clarified that paper is a survey]"], "link": "http://rjlipton.wordpress.com/2012/10/16/one-mans-floor-is-another-mans-ceiling/", "bloglinks": {}, "links": {"http://eecs.umich.edu/": 1, "http://www.cloud9.net/": 1, "http://www.sfu.ca/": 1, "http://rjlipton.wordpress.com/": 7, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 5, "http://reference.edu.sa/": 1, "http://www.jstor.org/": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["A geometric puzzle based on the Thing movies \n \n  \n \nJames Arness was not a scientist, but was an actor who is best known for having played Marshall Dillon in the long-running TV series \u201cGunsmoke.\u201d This series had a 20-year run, unheard of nowadays for scripted shows with actors that are animate, as opposed to animated . \n \nToday I wish to raise a puzzle about an earlier movie that he starred in called The Thing . \n \n \nThe full title of the movie is: The Thing from Another World . Arness played a creature who was discovered by scientists at the North Pole embedded in the ice. It is one of the great science-fiction movies of all times, in my opinion. It is filled with wonderful dialogue and scary scenes. The leader of the scientific group is Dr. Arthur Carrington, who at one point says: \n \nThere are no enemies in science, only phenomena to be studied.\n \n Not sure I agree with this statement, especially when the phenomenon is an eight foot creature who is really frosted about being defrosted. Okay maybe this is too much, but anyway Arness\u2019s creature is pretty upset. \n \nLater there were two further Thing movies, both directed by John Carpenter and both named \u201cThe Thing.\u201d As part of Hollywood\u2019s approach to originality these movies take place at the south pole. One stars Kurt Russell, and the other stars Mary Elizabeth Winstead, and the latter is the prequel to Russell\u2019s movie. Let\u2019s use the terminology Thing I for the original, Thing II for Kurt Russell\u2019s movie, and Thing III for the most recent movie. \n  \n \n Goofs and Setups \n \nAs with many movies there are errors, goofs, and continuity mistakes in these movies. See this for a list of them for the movie Thing I. Here is one: \n \n At the end, the co-pilot throws his tool to force the thing up on the walkway, but immediately after electrocuting the monster the co-pilot is seen holding the same instrument.\n \n \nMy problem is not a simple error or goof. What I am puzzled about in the Thing movies, all three of them, is more fundamental. It is part of the setup of the whole story. \n \nIt comes down to a geometric problem, and I am terrible at geometry. So my hope is that you will be able to explain how do they get the creature out of the ice? Let\u2019s get started explaining the situation. \n \n The Ice Puzzle \n \nLong before there were refrigerators there were ice boxes. People collected ice from rivers or frozen lakes during the winter\u2014harvested it\u2014and then stored it for the summer months. Cutting out the blocks of ice is straightforward: drill a hole through the ice, which was usually a foot or two thick, then use a saw to cut out a block. Since ice floats the block does not sink and it can be pulled out of the river or harvested. There was a whole industry based on harvesting ice in this way. Here is a quote to further explain how the procedure worked: \n \n Ice harvesting generally involved waiting until approximately a foot of ice had built up on the water surface in the winter. The ice would then be cut with either a handsaw or a powered saw blade into long continuous strips and then cut into large individual blocks for transport by wagon back to the icehouse.\n \n \nMy puzzle is based on cutting out blocks of ice in very different situation. In Thing I and Thing III the scientists discover a creature that is just below the surface of the ice. They can see through the ice a vague outline of the creature, and see that it is frozen in solid ice. The size of the creature is large, so it is about 4 by 4 by 8 feet in dimensions. In both movies they decide to remove the creature encased in the ice. In a later scene we see the huge block of ice\u2014an almost perfect rectangular block\u2014placed on a table somewhere back at their base. Of course there is the small issue that the block would weigh about six thousand pounds, but that is not what I am puzzled about. \n \nWhat I am puzzled about is, how did they cut out the block? The problem is the ice where the creature is has essentially infinite depth. It is not frozen over a riverr\u2014no, the ice extends down at least hundreds of feet. So removing the block is quite different than removing a block of ice from a river. The issue to me is a geometric one: I see how they can cut down the sides of the block with saws or some other tools. What I do not see is how can they cut the bottom out? In the river case there is no need to cut the bottom, since the bottom is bounded by the unfrozen river water. There is no such interface in this case. So how in the world can they cut out the bottom of the block? \n \nI am puzzled. How can they do this? \n \n The Ice Puzzle: More Details \n \nLet me try and make the puzzle into a more precise math problem. Here is the block of ice that you want to remove: \n \n \n \nYou can cut from the surface down with a saw. So you can make this into: \n \n \n \nwhere the black arrows are cuts your saw can make. The problem is how do you get a cut that goes in the horizontal direction? I see in real life that one could dig up a huge trench around the whole area and then proceed to cut out the block. That would be extremely time consuming and certainly there seems to be no way that could be done in the Thing I and probably not in Thing III. \n \nIs there a trick I am missing? The best I can see is to do the following: \n \n \n \nDig down the extra on each side and remove the two triangular pieces L and R, then one could get down to cut across. Is this how they did it? It is, however, awfully difficult to cut at an angle. \n \n A Way Out, and Some Problems? \n \nKen suggests that perhaps the monster was encased in a glacier flowing down at an angle. Then a cut at an angle to the glacier can be made by a Pendulum saw , also called a \u201cswing saw.\u201d Pendulum saws use gravity to guide the blade, and are used to cut ice from rivers even today. This helps if we also suppose that the other methods can be used to make cuts that are normal to the glacier\u2019s surface. \n \nAnother possibility is that there were fault planes beneath the ice, say parallel to the surface. Then we only need four normal cuts that intersect with the hidden plane, whose presence we have detected perhaps by ultrasound. If this were the idea, would it be worth five seconds of movie time to show it? \n \nThis last idea suggests some computational problems. You are given a known arrangement of bounded fault planes beneath the surface, at various depths or maybe even all at the same depth. How many blocks of ice, or how much volume of ice, can you obtain with cuts that are normal to the surface? Can such a problem be NP-hard? \n \n Open Problems \n \nSo how did they get the creature out of the frozen ice? Or is it just movie magic that should not be looked at too carefully?"], "link": "http://rjlipton.wordpress.com/2012/10/12/my-thing-about-the-thing-movies/", "bloglinks": {}, "links": {"http://rjlipton.wordpress.com/": 5, "http://www.imdb.com/": 5, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 3}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["Election and debate special from GLL\u2014note update \n \n  \n \nAram Harrow is both a computer scientist and a physicist\u2014something that makes him unique and special. Few people can explain what a probabilistic Turing Machine is and also what complementarity is. He has done some beautiful work in pure and applied mathematics, work that uses his dual abilities. \n \nToday Dick and I wish to describe and interpret an intricate recent theorem of his in a joint paper with Alexandra Kolla and Leonard Schulman. \n \nAram is no stranger to these pages. Of course he has also been our honored participant in our year-long debate on the feasibility of building quantum computers, all during his current appointment in the Computer Science and Engineering department of the University of Washington in Seattle. He is also one of three mainstays on the prominent quantum computing blog created by Dave Bacon, and during our debate still managed to get a little bit of other work done. In January he will join the MIT Department of Physics as an assistant professor, and we congratulate him on that. \n \nAs related at the bottom of his post , the paper grew out of work on the unique games conjecture , seeking to apply quantum methods though the end result was \u201cclassical.\u201d The impetus included models of physical noise, such as featured in Aram\u2019s debate with Gil Kalai. To bring this full circle, Gil himself is the first person credited for discussions that spurred the paper, but these were in 2010 long before the debate idea arose, and not with Aram. \n \n The Proof \n \nUsually we discuss the paper first, or at least state its main results, but exceptional times demand exceptions. Their \u201cProof overview\u201d subsection begins by defining a \u201c senate operator ,\u201d based on the idea that in the U.S. Senate, states have equal weight regardless of population. Relaxing equal-weighting will lead to our main open problem. The Section~3 title summarizes their pivotal Proposition~2 as \u201c senates dominate dictatorships. \u201d A main ingredient of their proof are the Krawtchouk Polynomials, whose Internet page talks about \u201cthe destiny of the scientist during the Great Terror.\u201d \n \nOur interdisciplinary push seems to have entered Politics, but the real point is: the proof is hard . So hard that the authors tell me they have no idea of the value of the constant whose existence constitutes their main theorem. We will still try to give some idea of its significance, and why the result is hard, and how you can approach the issues without having to delve into the whole party manifesto. But you will not be able to make like President Gerald Ford in his first debate ( parody ) with Jimmy Carter in 1976: \n \n\u201cIt was my understanding that there would be no math in this debate.\u201d\n \n \n The Paper \n \nThe paper\u2019s title, \u201c Dimension-free L2 maximal inequality for spherical means in the hypercube ,\u201d may not generate any 15-second sound bites. Nor may it help predict the outcome of the U.S. Presidential election, but it does contain an important new theorem. \n \nLet\u2019s first say why this theorem is significant and hard. The term \u201chypercube\u201d is just the set of binary strings of length \u2014one of the favorite objects for computer scientists. In this world, the sphere of radius around a point , written , is the set of strings that are Hamming distance from . Thus, if and the points in the sphere are: \t \n \n \nNow imagine we select a small fraction of the points from the hypercube, say . Can one find a single point that is special, in that every sphere around of any radius has less than half of its points in the selected set? Clearly, this seems quite reasonable, if is small enough. \n \nThis is a case where the intuition is correct, but proving it is hard. A standard method of proving that special points exist is to pick at random, essentially using the probabilistic method . Alas this works for spheres of one radius, or a few radii, but does not seem to be powerful enough to show that the point works for all radii. \n \nThe probabilistic method is extremely powerful, but when it fails , often the requirements for a proof become very difficult. This is the case with their new result. Actually in our opinion this is one of the most important aspects of the new paper. Since the probabilistic method does not always work, any new ideas that can solve problems where it fails are automatically of great importance. Here one idea that Aram tells me was important is a different kind of \u201ccomplementarity,\u201d namely that where is the vertex opposite in the hypercube. \n \nLet\u2019s turn now to state the theorem in a mathematically precise manner and probe its reach. \n \n The Result \n \nThe new inequality involves the standard norm of a function , which is defined by \n \n \nThe other functions involved maximize the average of over the spheres: \n \n \nThe key clause in the paper\u2019s title is that the bound given by their inequality does not depend on . Here is their new result: \n Theorem 1 There exists such that for all and , \n \n \n \n \nPretty neat, no? Well it\u2019s neat , but it can use some interpretation. \n \nSpeaking mathematically, call \u201clight\u201d if is below the average of the squares, i.e., . We might hope to \u201cbulk up\u201d by finding an average over one of the spheres enclosing that is greater. For many \u2018s there will be some \u2018s for which this makes a big difference. However, the theorem shows that under norms the cumulative effect of this extra allowance will never be more than a fixed constant factor\u2014independent of the dimension. That\u2019s fine, but can we give a more helpfully vivid interpretation? \n \n An Election Interpretation \n \nIt is election season in the United States again. One hears often about how demographic and personal characteristics channel one\u2019s political preferences, and that there are \u201ctypical Democratic voters\u201d and \u201ctypical Republican voters.\u201d The categories are often binary or close to it: male/female, married/single, urban/suburban-or-rural, over/under age 50, and so on. \n \nWe can define them by positions on single issues: pro-life/pro-choice, gun-rights/gun-control, for/against gay marriage, immigration amnesty, anything like that. We can also add categories that seem irrelevant: righthanded/lefthanded, dark/light eyes or hair, tall/short, nearsighted/not, Coke/Pepsi. We can even include divisions that are not roughly balanced in the population, such as vegetarian/carnivore. Hence also we can break multi-way categories into binary pairs, such as X/not-X for every ethnic group X. Then the possible combinations of values of characteristics correspond to the vertices of the -dimensional hypercube. \n \nWhat justifies saying that a combination of characteristics is \u201ctypical\u201d for a party is that if you vary some of them\u2014any number of them\u2014you still find mostly people who vote for that party. Importantly this should be true for changing any set of characteristics, not just specific ones. Given the strength of this requirement, do typical voters exist? The fact of single-issue voting may even make this seem unlikely. However, the following broad statement holds in enough cases to give a way to approach the issues in their paper: \n \n Theorem 2 \n If a party wins by a large enough landslide, then it has typical voters. \n \n \nThat is, there are voters such that not only did vote for the winner, but for any , over all voters who differ from in characteristics, the winner got a majority of votes from those as well. Well this comes with some caveats , so we need to look closer. \n \n The One-Vertex, One-Vote Case \n \nLet us first suppose that every combination has exactly one voter. Let be the set of nodes that vote for the loser . Then we define an indicator function by: \n \n \nNow if the loser won an portion of the vote, then since , we have: \n \n \nBy the theorem applied to , squaring both sides, \n \n \nDividing by makes the left side an average such that . There must exist some giving at most the average value, i.e. such that , which implies that for all , \n \n \nTaking small enough that makes for all . Thus the loser achieves no more than a minority over all of the spheres centered on . This carries over to all of the Hamming balls , which include their interiors, as well. \n \nThis makes a typical voter. By extending the averaging argument, one can show there are many typical voters. Thus when the election map is close to all-red or all-blue, there are many places where the same color prevails in all concentric spheres. However, things become trickier when we vary the requirements on the space. \n \n Weighted Measures \n \nNow suppose we have a weighted measure on the hypercube. With regard to this measure, \n \n \nIt also now makes sense to define weighted versions of the averages over the Hamming spheres, leading to the operator \n \n Does their theorem now hold in the form that there exists such that for all , ? Expanded, this now means: \n \n Theorem? We ask whether for all , all counting measures on , and all , there exists (possibly depending on or on ) such that \n \n \n Update 10/9/12: With no dependence on , or restriction on values , this has been refuted\u2014see Addendum below. \n \nWe can also define the modified max-averaging operator with regard to the balls rather than the spheres , and ask the same question. The paper does not address such matters directly, and we understand from private communication with the authors that they have not arrived there yet, because seemingly simpler cases are unresolved. If it were true, then we could derive the existence of \u201ctypical voters\u201d in the most general setting. \n \nTo see how, assume that the -many voters who share the same characteristics all vote the same way. (If this is not true, we can create some new characteristics that separate them, and are free to add them as a co-ordinates to every vector because the count is allowed to be anything including zero.) Let again be the set of that vote for the loser. Then is the total number of votes for , out of a total electorate of . Define to be the 0-1 valued indicator function of as before. Then \n \n \nwhile \n \n \nThe \u201cTheorem?\u201d and dividing both sides by yield \n \n \nAgain an averaging argument with respect to yields the existence of such that for all , \n \n \nHere the right-hand side has where is the loser\u2019s percentage of the total vote, and a large enough victory makes . This implies that the loser still has a minority in every Hamming sphere centered on the voter . \n \n Can We Use the Inequality They Proved? \n \nWe can try to work instead with the function \n \n \nThen we obtain \n \n \nwhile (using the original maximizing operator again) \n \n \nIn general this does not look very tractable, nor does the proved theorem seem either to imply or follow from the \u201cTheorem?\u201d above. \n \nIn our specific election case, we might try \n \n \nThen , and the theorem they proved gives \n \n \nThe averaging argument, however, is still in terms of not as just above, so we get the existence of an such that for all , \n \n \nNeither side of this inequality yields the desired interpretation. The Cauchy-Schwarz inequality can relate the numerator to , but the inequality goes in the wrong direction for our purpose. This hints how tricky the considerations in the paper are. \n \nThe problem appears to be mainly the \u201cwhite space\u201d caused on the map when . We can argue that this still should see good behavior as a limiting case of always being positive but fractional (where we can also stipulate that ) and letting it tend to zero for some . \n \nResolving this, however, may entail solving some easier problems that Aram and the others have attempted, such as for spaces formed by Cartesian products of other graphs (besides the -fold product of 2-cliques giving the hypercube). Moreover their type of inequality is open even for , and for other norms including the -norm. An -norm inequality ostensibly would help our desired election conclusions, but one of their key propositions does not extend to it, at least not conveniently as they remark. As always we refer interested readers to the paper for full details and further questions. \n Addendum \n Aram contributes a counterexample to the unrestricted \u201cTheorem?\u201d, to some restricted versions and also with balls in place of spheres, and to the existence of \u201ctypical voters\u201d in the weighted case with regard to spheres. Namely, let there be only voters, with the origin voting for the loser , i.e. , and her neighbors voting for the winner ( everywhere else). Then \n \n \nbecause the neighbors see only the origin at distance 1. Thus there is no inequality with a constant independent of . Moreover, none of the winning voters is typical, again because its sphere at distance 1 has only the origin, while even the ball with radius 1 leaves a tied 1-1 result. \n \n However , the weight-2 nodes such as still have the loser in a minority of all properly enclosing balls, so they are \u201ctypical combinations\u201d even with there. Perhaps the most relevant cases are where for all and , and for balls not spheres\u2014though Hamming balls lack the \u201ccomplementarity\u201d property mentioned near the top. \n Open Problems \n \nFor what other norms and spaces does their inequality hold? \n \nDoes their inequality hold under weighted counting measures on , with reasonable restrictions on and perhaps using balls not spheres? Can we obtain the special case of \u201ctypical voters\u201d from the inequality they have without the one-vertex, one-vote restriction, or under milder added assumptions? Can we prove it if and ? \n \nWill math matter to the upcoming Presidential debates? \n \n[Changed to distinguish \"ball\" from \"sphere\" consistently, added note on partial refutation of weighted case.]"], "link": "http://rjlipton.wordpress.com/2012/10/08/whats-blue-and-white-and-red-all-over/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://arxiv.org/": 2, "http://rjlipton.wordpress.com/": 4, "http://en.wikipedia.org/": 2, "http://orthpol.narod.ru/": 1, "http://www.nbc.com/": 1, "http://dabacon.org/": 3, "http://pjmedia.com/": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["Final summations of the Kalai-Harrow debate \n \n \n \n \n \n \n \n \n \n source\u2014our congratulations \n \n \n \n \n \nWilliam Unruh is a professor in the Theoretical Physics group of the University of British Columbia. He is known for the Unruh effect , which predicts that an accelerating observer will feel radiation that an observer in constant motion will not. This does not contradict relativity, but experimental tests have so far produced debatable results. He also wrote a paper soon after Peter Shor\u2019s famous algorithm appeared in 1994, showing how quantum noise would swamp a direct implementation of it. This was an early spur for quantum error correction, and Unruh is said to have withdrawn his skepticism after the quantum fault-tolerance theorem was established. \n \nToday we conclude our debate between Gil Kalai and Aram Harrow on the scalability of quantum computers. What is our computational world\u2014our mundus computationalis? \u2014one with quantum supremacy or classical control? \n \n \nUnruh\u2019s Science Canada profile (see also this Q&A ) includes this quotation in his own voice: \n \n\u201cWilliam Blake said \u2018If a fool would persist in his folly, he would become wise.\u2019 I\u2019m grateful that society allows me and other scientists to persist.\u201d\n \n \nOn behalf of Aram we note that Unruh has not persisted in his critical stance\u2014at least the quantum computing notes on Unruh\u2019s group webpage are skepticism-free. On behalf of Gil, we note the value of persisting to make us wiser, besides the possibility of ultimately being right. We have persisted since January, and on behalf of Dick, I (Ken) thank both principals. \n \nOur debate ends with a summation by Gil and then by Aram. The first part of the summation turned into a renewed discussion of Conjecture 1. In this post Gil begins by responding to Aram\u2019s two other detailed defenses in our first round of the debate, posts II and III . He then reviews his original conjectures in light of these responses, our debate\u2019s second round in which one conjecture was partially refuted, and copious back-and-forth discussion in many comments to these posts. Then Aram gives his summation, updating the considerations that answered Unruh\u2019s questions long ago. We conclude with thanks and some \u201cwhat could be next?\u201d considerations. \n \n Gil Kalai: An overall look at Aram\u2019s posts \n \nI will continue to discuss the arguments that Aram made in his interesting posts. Aram raised important technical and conceptual issues, and some of the conceptual differences between our points of view will certainly stay unsettled for quite a while. However, with the exception of Conjecture C as we discussed earlier, all the technical arguments against my conjectures raised by Aram\u2019s are either weak to start with or incorrect. \n \nAram\u2019s classical fault-tolerance test indeed poses a major challenge to quantum information theory, but I regard my explanation that is based on Conjecture 1 not only as adequate but also as advantageous compared to other explanations. Aram\u2019s recent study of Conjecture 1 is useful, but his formulation is incomplete and his interpretation of the conjecture is incorrect. We discussed these matters in a previous post . \n \nThe concern that my conjectures are in conflict with the linearity of quantum mechanics is wrong\u2014as exemplified by (among other things) Aram\u2019s own shadow qubit example. The technical claim from the second post that pairwise positive correlation is insufficient to imply error-synchronization is incorrect. Aram\u2019s thought experiments from the third post are interesting but do not support Aram\u2019s claim. The specific claims that the high correlation of the kind predicted by my conjectures are not physical are not correct, and this general issue represents an important difference on appropriate modeling of noise. We will discuss some of these matters now. \n \n Correlated Errors and Entanglement \n A quick review \n \nCentral to my argument is the assertion that noise can be highly correlated and the assertion that errors will be highly correlated when we try to create highly entangled states. Before discussing Aram\u2019s objections to my conjectures I would like to emphasize two important points. \n \nThe first is that the ultimate obstacle failing universal quantum computers is simply that the rate of noise will be too high. Correlation is an important part of the picture, but the first-order obstacle is the rate of noise in terms of qubit errors. High correlation and many qubit errors go hand in hand because it is reasonable to assume that the rate of noise in terms of trace-distance is stable over time for quantum evolutions since trace-distance is invariant under unitary operations. But, when the noise is very correlated, the rate in terms of qubit errors scales up linearly compared to the rate in terms of trace distance. \n \nThe second point is the distinction between special-purpose devices and general-purpose devices which was discussed in the previous post . I regard my conjectures as describing noisy special-purpose quantum devices, and this description will explain why general-purpose quantum devices cannot be built. \n \nMany of the arguments and intuitions against my conjectural picture of noisy quantum devices are based on taking for granted a scenario of a universal quantum device. For example, Peter Shor commented : \n \n\u201c\u2026the universe needs to know what code you are using in order to foil you. This attributes both more intelligence and more malice to the universe than I am willing to believe that it has.\u201d\n \n \nThis makes sense if the code is created on a universal machine. But if every code (that can be created at all) requires a specific machine, then neither malice nor intelligence on the part of the universe to tell them apart is required. \n \n The main objections to my conjectures \n \nConjectures 3 and 4 proposed an explicit relation between the intended state and the noise. Conjecture 3 asserts that a pair of entangled qubits are subject to error (information leak) with positive correlation. Conjecture 4 asserts that qubits whose state is very entangled are subject to error-synchronization, namely there is a substantial probability for errors effecting a large fraction of all qubits. In my papers I present a formal mathematical description of Conjectures 3 and 4 and point out that a certain strong form of Conjecture 3 implies Conjecture 4. One idea that was raised in the discussion ( especially with commenter matt ) is that error-synchronization follows from Conjecture 1, for every quantum error-correction code that genuinely depends on qubits. \n \nThe main objections to my conjectures were: \n \n The conjectured properties of noise violate the linearity of quantum mechanics. \n \nTo the best of my judgement this is not true. Let me mention three points: \n \n \n \nThe class of Hamiltonian quantum evolutions (namely smoothed Lindblad evolutions) that describe the \u201cbad noise\u201d forms a subclass of all Hamiltonian evolutions of system and environment. \n \n \nAram described a quantum-theoretic scenario (shadow qubits) that may support the phenomenon I conjecture. The same applies to variations on John Preskill\u2019s models . \n \n \nThere are strong reasons to believe that the conjectures do apply in many cases, e.g., for small quantum computers, say those with at most 20 qubits. Why isn\u2019t this a violation of quantum mechanics linearity? \n \n \nMy conjectures can be regarded as nonlinear behavior of decoherence that are fully supported by quantum mechanics. The claim regarding QM\u2019s linearity also manifests a confusion between the behavior of special-purpose machines and of general-purpose machines. \n \n The conjectured properties of noise violate \u201cknown physics.\u201d \n \nMy response to this important point raises the central issue of choosing between two approaches for modeling: \n \n \nThe first approach is that in modeling open quantum systems we should take \u201cknown physics\u201d as part of the noise model.\n \n \nIt turns out that if you impose the \u201cknown physics\u201d on the noise it leads to surprising \u201cunknown physics\u201d that we will be able to create by controlled quantum evolutions via quantum fault-tolerance. Joe Fitzsimons\u2019 2-locality suggestions as well as John Preskill\u2019s model represent this approach. \n \n \nThe second approach is to let the \u201cknown physics\u201d (both for the noise and for the signal) emerge from the model itself.\n \n \nMy modeling follows the second approach. I expect that my models of noise will confirm these \u201cknown physics\u201d insights both for the noise and for the signal. I find the approach appealing since there is no reason to believe that when we have a natural quantum system the laws that describe the interaction of the system with the environment will be different from those that describe the interactions within the system. \n \n \n \n \n \n \n \n \n src \n \n \n \n \n \nJoe Fitzsimons\u2019s 2-locality argument was a specific attempt to explain why my conjectures violate known physics. It turned out, however, that a strict version of it is too strong to be realistic (see this comment by Preskill ), while a weak form of 2-locality is too weak to be in conflict with my conjectures. But again, my main objection to Joe\u2019s attempt was different: insights of this kind should emerge from the model rather than being imposed on the model. \n \nWhile I disagree with these two out-of-hand reasons to reject my conjectures, it is quite possible that my conjectures are simply false. They should be carefully examined on a case-by-case basis. Be that as it may, my conjectures are proposed also as a formal description for non-FT quantum evolutions even in a reality that allows or even demonstrates quantum fault-tolerance. \n \n Towards a theory of non-FT quantum evolution \n \n Switching the Quantum FT option OFF \n \nSuppose that we have universal quantum computers, on every desk and every lap. One way to think about my conjectures is as follows. Let the quantum computers be equipped with a quantum-FT on/off switch, such as noise-cancelling earphones have. My conjectures describe the behavior of noise when the quantum-FT switch is off. Moreover, many of the attempts to build quantum computers or to create experimental quantum evolutions do not apply fault-tolerance schemes to start with, so quantum computers with the quantum-FT turned off should suffice to describe them. \n \n A basic noise example \n \nThe most basic example of noise when the quantum FT is switched off is the following. \n \nYou have an algorithm described by a unitary operator . Let be a standard simple noise operation. Consider the noise operation . In words, this means that the noise at the end of the evolution is like applying at the beginning and then running the unitary operator noiselessly. My conjectures say that some part of the overall noise behaves just like this example. \n \n Smoothed Lindblad evolutions \n \nStart with a general (time-dependent) Hamiltonian evolution defined between times and on the system and the environment. Let denote the Hilbert space describing the system. The evolution can be described infinitesimally via the sum of two superoperators and , where describes a unitary evolution on and describes the \u201cnoise.\u201d (I assume that is Lindbladian and correspond to POVM measurement. But we can consider more general or more restricted scenarios.) Let be the unitary operator describing the evolution between times and . Note that unitary operators on induce an action on super-operators. \n \nLet be a positive kernel defined on . (In order to include standard noise we can allow an atom at 0.) The smoothing operator is obtained by replacing by a weighted average of a 1-parameter family of super-operators of the form with weight . (Here varies over all times between 0 and 1; it seems important to take times in the future as well as times in the past.) The discrete-time model is especially simple. I propose smoothed-in-time noise of this kind as a formal description of noise \u201cwithout fault-tolerance\u201d. Namely, this type of approximations to pure evolutions expresses formally the idea that quantum fault-tolerance was not activated and at the same time will not allow it. \n \n Other Topics in Brief \n \nThe debate touched on many technical and conceptual issues, and I tried to round up many of them in this comment , plus some follow-ups. Let me relate some here: \n \n Hamiltonian models and concentration of the number of errors \n \nIn various stochastic/Hamiltonian models for noise we witness Gaussian concentration of the number of errors. I regard this feature as unrealistic, and believe such models abstract away a crucial aspect of noise that may or may not allow fault tolerance. See this comment and the previous post overall. \n \n Bose Einstein-condensation; superconductivity; teleportation \n \nI proposed to examine Conjecture 1 (adapted) on Bose-Einstein condensation, and discussed it mainly in this thread . \n \nDavid Deutsch proposed (privately) superconductivity as a counterexample for my strong principle of noise. Peter Shor proposed Kitaev\u2019s teleportation-based quantum computing as a counterexample to my conjectures. (I have some reasons to think that smoothed Lindblad evolutions exclude this type of quantum computing.) \n \n Perturbation methods; renormalization group \n \nUnderstanding the feasible approximations for pure quantum states and evolutions is of interest not only in the context of controlled quantum evolutions. Non FT-evolutions are relevant for describing a quantum evolution on a small Hilbert space that neglect some of the degrees of freedom of a quantum system. \n \nJohn Sidles, Aram, Hal Swyers, and other commentators mentioned some connection with perturbation theory, and indeed this is a direction that I find very interesting. \n \nIn a private discussion Aram mentioned the renormalization group as an example that there can be an effective theory at some scale (i.e. low energy, or large distances) that results from \u201cintegrating out\u201d dynamics at higher energies/shorter distances/higher frequencies/etc, Again, this can be related to questions about modeling noise coming from neglecting internal structures of our qubits (or qudits). \n \n Censorship \n \nMy Conjecture C from the first post was refuted by Aram in conjunction with Steve Flammia. We discussed this matter in this post , where we also discussed a related 1980 paper by Anthony Leggett. The post on censorship discusses bounded-depth quantum circuits as \u201cShor/Sure separators,\u201d an idea that goes back to Unruh. Other alternatives for Conjecture C were offered by John Sidles in this comment based on tensor-rank, and also here , and by me in this comment (based on Pauli-expansion). \n \nIf small-depth quantum computation is indeed the limit of approximated pure evolutions this suggests that quantum computation not only offers no computational superiority, but rather that interesting computation in our world is classically controlled. \n \n Can the computation hide the geometry and physics? \n \nOne insight of classical simulation that people tend to take for granted in the quantum case is that computation abstracts away the physics and geometry of the simulated device. I challenge this insight and our third-round post was devoted to this issue. \n \n Anyons \n \nThere are fascinating suggestions for robust-to-noise states that (in principle) can be created experimentally, see this review paper . \n \nThe high-level concern is that since the proposed experimental process does not involve FT, the noisy states will satisfy Conjecture 1 and will not exhibit the expected robustness to noise. However, the description of how the stability of certain states increases, such as when two anyons are taken apart, is quite convincing\u2014and it will worth the effort to try giving a specific physical mechanism that supports my conjectures in this case. \n \n Simulating quantum physics on classical computers \n \nThe connections with classical simulations of quantum physics was mentioned quite a few times. A basic question is, what can be said about computations in physics that clearly require an exponential number of steps on a digital computer? Aren\u2019t such computations (which apparently nature can carry on routinely) a clear evidence for \u201cquantum supremacy?\u201d My guess is that in cases where the computation is intractable, the answer is irrelevant. \n \n The rate of noise \n \nNeither the ordinary models of noise nor my conjectures for non-FT noise give a lower bound on the rate of noise. One proposal in my paper , drawn after some work by Sergio Boixo, Lorenza Viola, and Gerardo Ortiz, is: \n \n Conjecture: The rate of noise in a time interval is bounded below by a measure of non-commutativity of the unitary evolutions in this interval.\n \n \n Non-flat substitutes for Hilbert spaces \n \nJohn Sidles offered, over many comments , a certain geometric theory of quantum evolutions on non-flat spaces, and connected it with various aspects of our debate. This is certainly very interesting. \n \n Non FT classical evolutions \n \nIt can be of interest (and perhaps harder compared to the quantum case) to try to describe classical evolutions that do not enable/hide fault tolerance and (long) computation. \n \n \n \n \n \n \n \n \n src \n \n \n \n \n Aram Harrow: Rebuttal and Summation \n \nQuantum mechanics has been a counter-intuitive theory from its birth, and the theory of quantum information in a sense aims to distill the strangest aspects of quantum mechanics. So it is not surprising that they both have faced a chorus of skepticism from the start, including echoes of today\u2019s debate. The problem with the skeptic\u2019s task\u2014from the original EPR paper to the Gil\u2019s contemporary objections\u2014is that their arguments entail an alteration of this supremely successful theory. Not only does the evidence unequivocally support quantum mechanics, there is not even any candidate theory that could replace it, waiting in the wings for experimental confirmation. \n \nGil\u2019s conjectures, then, are really pre-conjectures, meaning that they are neither purely mathematical conjectures nor specific claims related to observable phenomena, but rather organizing principles around which we might look for such claims. He conjectures that a consistent theory can be found that will encompass both the impossibility of quantum computers and the last century of experiment supporting quantum mechanics. This is a worthy goal and a fascinating intellectual challenge. But it is at least very hard, and is probably impossible, as evidenced in part by the difficulty that Gil faces in making precise statements about his conjectures. \n \nMore concretely, he can make precise statements about his conclusions, such as \u201creal-world quantum computers experience noise that makes them classically simulable,\u201d but not about any microscopic claim about physics, even in any concrete model, that could plausibly lead to these conclusions. Similarly, his conjectures have difficulty avoiding reference to the \u201cintended\u201d operation of a quantum device, although any physical principle we discover is unlikely to depend on our intentions. \n \nOf course, Gil does suggest a number of physical mechanisms that are candidates for de-railing quantum computing, or at least refuting the assumptions of the FT threshold theorem. He discusses self-synchronizing metronomes, smoothed Lindblad evolutions, constant-size fluctuations in temperature, special-purpose quantum computers introducing correlations in errors, and other ideas. Without at this point engaging with the details, I believe that each of these mechanisms suffers from some combination of failing to rule out quantum computing, inconsistency with existing observations (e.g., working classical computers) or imprecise predictions. \n \n Why I believe quantum computing should be possible \n \nI have tried to explain why I don\u2019t believe Gil\u2019s conjectures. But it is further important to ask why I, or anyone else, should believe that quantum computing is a plausible technological possibility. \n \nPart of it, as I\u2019ve said, is believing in quantum mechanics itself, with its exponentially large Hilbert spaces, its entanglement, its interference, and all its other counter-intuitive features. Quantum mechanics did not succeed by being a more intellectually appealing theory than its competitors. On the contrary, the route from Schr\u00f6dinger\u2019s equation to EPR to Bell\u2019s theorem to secure quantum communication took over 50 years, even though today it can be covered in a few hours of lectures. This delay is basically because many of the principals\u2014most notably Albert Einstein\u2014didn\u2019t want quantum mechanics to be true. Instead, many preferred to think about interpretations such as Bohmian mechanics that hold close to older intuitions, but are dogs\u2019 breakfasts to work with. \n \nQuantum mechanics succeeded by simply giving the right answer, over and over again , while no other theory could come close. And now, with the modern quantum-information perspective, many of the conceptual difficulties of the theory are being tamed. This textbook is a great example of the new pedagogical approach. \n \nBut couldn\u2019t we have quantum mechanics without quantum computing? Isn\u2019t that what this debate is all about? After all, as John Sidles has mentioned several times, many perturbation expansions are hard to compute, or not even convergent, and these can give rise to complicated many-body terms even when the original Hamiltonian is simple. Nothing about Schr\u00f6dinger\u2019s equation guarantees that we\u2019ll ever see anything resembling textbook quantum mechanics in the real world. \n \nLet me give a simple example of how things could have been much worse for us. Electrons are thought to be pointlike particles, but suppose that one day we discover they are, like protons, composites of much smaller particles. These particles could have their own dynamics that could ruin a two-slit experiment by effectively decohering the \u201cwhich path\u201d information. However, this turns out not to happen, and we do observe electron interference. Moreover, such alternate theories would work via the Pauli exclusion principle to alter the periodic table as we know it, so we can rule them out without any fancy experiments. \n \nIn practice, we see many areas of quantum coherence even in current areas of technology. We have multiple-quantum coherence in NMR, Bose-Einstein condensates, superconductivity (with topological effects and protection ), lasers, and simply the fact that transistors work the way we expect. Higher-order perturbation theory, or some new physics, could have derailed any of these technologies, but did not\u2014a fact explained in part by renormalization . \n \n \n \n \n \n \n \n \n src \n \n \n \n \n Thanks \n \n (Gil:) This debate gave me a great opportunity, for which I am thankful, to discuss my work on quantum computers. First and foremost let me thank Aram Harrow for initiating a public discussion of my paper and for being my partner in this debate. Aram did a great job in presenting the case for why quantum computers are feasible. He presented serious and interesting objections to my conjectures, and made many enlightening comments on technical and conceptual levels. I thank also the many commentators, and allow me to mention John Sidles for the breadth and spirit of his involvement. There were many excellent comments and questions by many people; I tried to respond to questions raised to me, and I enjoyed reading some of the further discussion and interesting ideas not related to my work. \n \nOf course, I am thankful to Ken Regan and Dick Lipton for hosting and editing the debate, for summarizing the earlier threads, for adding exciting introductions, for illuminating private discussions with Aram and me on the matter, and for very interesting related and unrelated blog posts during this time. \n \n (Aram:) I also want to thank Gil for his discussions both here, and in many private emails. It\u2019s been useful for me to think through these things, and has caused me to think about skepticism in a new way. (For example, am I like one of those establishment scientists who rejected continental drift out of hand? How certain can I be about my position on other controversial things, like the probability of global warming, or relativity , or my own atheism?) It has led to my work with Steve Flammia on Conjecture C, and also a new project on adversarial noise joint with Matt Hastings and Anup Rao, which I\u2019ll talk about soon on the Pontiff blog. \n \nAlso, I wish to thank the many commenters, who have gone far beyond the original posts into many interesting realms. I hope that my participation here will turn out well for the field of quantum information, which has faced a large amount of skepticism and misunderstanding from the field of computer science. Many arguments in favor of quantum computing inevitably stem from physics; however, I hope that this will help encourage dialogue and mutual understanding between physics and computer science more generally. Finally, I am grateful to Ken and Dick for hosting, editing and framing this debate, and for the contributions of their blog more generally. \n \n Conclusions \n \n (Gil:) The construction of universal quantum computers is a serious scientific possibility and an excellent working assumption. Building universal quantum computers will be a once-in-a-lifetime scientific and technological achievement. Operating quantum computers will lead to further terrific scientific and technological fruits, for decades, just like the discovery of X-rays. Quantum error-correction is an important concept for building fault-tolerant quantum computers and is a fundamental scientific concept on its own. Building quantum computers is an endeavor which should vigorously be pursued, and indeed it is pursued theoretically and experimentally by top researchers who already established impressive achievements. My university, The Hebrew University of Jerusalem, just established a new Quantum Information Science Center . \n \nYet, it may well be the case that universal quantum computers are not possible, and that \u201cno quantum fault-tolerance\u201d should be taken as the guiding principle in modeling quantum decoherence. Developing a theory of non-fault-tolerant quantum evolutions, and studying how quantum computers can fail, is an interesting and important endeavor. Non-FT quantum (and classic) evolutions may shed new light on existing models, offer new relevant models for quantum evolutions, and lead to substantial new insights on related physical questions, models, theories, and computational methods. \n \nFinally, it can be useful to draw two extreme scenarios as possible conclusions for this debate. One commonly believed scenario coined as \u201cquantum supremacy\u201d by Preskill asserts that the quantum world offers and enables superior forms of computational complexity. The other scenario, which we can call \u201cclassical control,\u201d asserts that computation in our quantum world can only be based on classical information\u2014via a repetition code or a similar mechanism\u2014and that realistic nearly-pure quantum evolutions are (approximately) of bounded depth. \n \n (Aram:) The quest to build quantum computers has been fraught with difficulties, but to my knowledge none of these are rooted in error correlations increasing with system sizes. Rather, they involve the sort of problems that elephants would have in dissecting fleas. Sometimes we don\u2019t know how to decouple noise sources, sometimes we have a hard time addressing qubits individually instead of collectively, sometimes our fabrication techniques are unreliable, and sometimes we can initialize or coherently manipulate or measure, but not all three. All of these are important challenges, and some could lead us to abandon a technology (e.g. if we need an optical table the size of a football field), but none suggests a fundamental showstopper. \n \nAt a certain point, we gain confidence that there is no monster hiding under the bed, not only because there hasn\u2019t been one before, but because nothing about our knowledge of the world would suggest such a monster. \n \n Open Problems \n \nChoose a platform that is currently being considered for an experimental implementation of quantum computing (e.g. ion traps, linear optics, superconducting qubits, etc.) and come up with a concrete noise model (i.e. derived from Schr\u00f6dinger\u2019s equation) that (a) is consistent with all existing experiments, and (b) provably rules out scalable quantum computing. Can one do this? or is it quixotic? \n \nDoes Conjecture 1 suffice to reduce the computational power of noisy quantum computers to BPP? Show that basing decoherence on smoothed Lindblad evolutions supports Conjectures 1-4 and leaves us with BPP. \n \nCan smoothed Lindblad operations be used in a non-trivial way to show how classical memory and computation emerges under noise? \n \nWhat is the correct picture of our mundus computationalis (Latin for \u201ccomputational world\u201d): quantum supremacy, classical control, or something in the middle?"], "link": "http://rjlipton.wordpress.com/2012/10/03/quantum-supremacy-or-classical-control/", "bloglinks": {}, "links": {"http://arxiv.org/": 5, "http://front.ucdavis.edu/": 2, "http://feeds.wordpress.com/": 1, "http://qcent.ac.il/": 1, "http://quantumfrontiers.com/": 1, "http://conservapedia.com/": 1, "http://plato.stanford.edu/": 1, "http://books.google.com/": 1, "https://groups.google.com/": 1, "http://www.quantumhorizons.org/": 1, "http://rjlipton.wordpress.com/": 30, "http://www.science.ca/": 2, "http://en.wikipedia.org/": 8, "http://gilkalai.wordpress.com/": 1, "http://dabacon.org/": 1, "http://www.ubc.ca/": 2, "http://farm7.flickr.com/": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["Is the end of the world uncomfortably close? \n \n \n \n \n \n \n \n \n \n src \n \n \n \n \n Derek de Solla Price created scientometrics , which is \u201cthe science of measuring and analyzing science.\u201d You can tell you\u2019re a natural theoretician if you instantly think, what happens when scientometrics is applied to itself? But there is no paradox of self-reference here, because scientometrics per se is applied to the science of the past, to papers that have already been published. Yet we who are doing science have to look ahead to impact on the future, and that involves unpublished possibilities. \n \nToday Ken and I want to discuss what keeps us worried sometimes. \n \n \nDavid Hilbert once said: \n \nOne can measure the importance of a scientific work by the number of earlier publications rendered superfluous by it.\n \n \nOur worry looking ahead is based exactly on this quotation. Some conjectures are what we will call \u201cnormal\u201d and others are \u201cabnormal.\u201d A normal conjecture is one whose truth does not wipe out previous developments. For example, in our opinion Andrew Wiles\u2019 two papers proving Fermat\u2019s Last Theorem, one joint with Richard Taylor, did not diminish most previous work\u2014indeed they built on it. A grand conjecture like the Langlands Program would validate our understanding of algebra and lash previous work together into a new foundation. But abnormal results do more than overshadow past work. \n \n P and NP: Equal or Not Equal? \n \nRight now the P versus NP page lists one dozen claimed proofs of since January 2011, and one dozen claimed proofs of over the same time. Many of the 70 previously-claimed proofs are still circulating. Some are easily seen to be incorrect, while others are less easily dismissed. \n \nMany of our colleagues believe that a proof of is unlikely for two reasons. First, they believe that ; second, they believe a proof that they are equal, if correct, would be very difficult. So difficult that no one will find it for years, decades, or perhaps forever. \n \nI (Dick) believe that is a possible outcome. I have explained many times here that this need not mean that there is a practical algorithm for , but there could be a polynomial-time algorithm. \n \nJin-Yi Cai, who has been a faculty colleague of both Ken and myself, is visiting Buffalo this weekend, and gives permission to relay his main reason for believing , though it doesn\u2019t quite extend to itself. Counting problems seem to yield especially notable examples where the general problem is -complete but a natural special case has a polynomial-time algorithm. Consider, for example, the FKT algorithm for counting perfect matchings in graphs that are planar , which is a foundation for incredibly deep and sweeping dichotomies (see Jin-Yi\u2019s own site for papers) showing every problem in the class is either in or -hard with nothing in-between. The reductions are usually based on algebraic methods such as eigenvalues and eigenvectors, which have no direct relation with the polynomial time algorithms; but they always manage to miraculously avoid hitting the domains of these algorithms, while sweeping everything else. If were equal to , the reductions would have no extrinsic reason to avoid hitting planar graphs and others that fall into the deep tractable cases. The fact of seems required to explain such serendipitous avoidance of deep cases. \n \n Normal Conjectures \n \nRight now in mathematics there is the potential proof solving one of the great open problems of number theory. This is the ABC Conjecture which we discussed here . Besides the length and complexity of this proof whether correct or not it appears to have substantial new mathematics. The biggest surprise with this new proof is that while many have believed all along that the ABC Conjecture is true, no one seemed to have any approach at all. Of course the proof could be wrong, could have a bug, yet it may also be correct. We will see. \n \nThe Riemann Hypothesis is a long-standing conjecture with deep impact. Although we may expect that a proof would carry foundational new insights, the fact of its truth would ratify beliefs we are already building on. The falseness of Riemann [in any of various forms] would certainly wipe out many papers with theorems that begin \u201cAssuming the [extended] Riemann Hypothesis\u2026\u201d\u2014as Lance Fortnow just now listed among \u201cThings a Complexity Theorist Should Do At Least Once.\u201d \n \nHowever, Riemann can be viewed as part of a spectrum of assertions about the distribution of primes and the behavior of the M\u00f6bius function. Cram\u00e9r\u2019s conjecture , as we discussed here , makes a much stronger assertion about gaps between primes than what follows directly from Riemann, versus for various . We can compare the consequences of Riemann being false with those of Cram\u00e9r\u2019s and/or some other stronger statements being false while Riemann is true. That we would feel no apocalypse in the latter case hints that number theory could move on without Riemann with much intact. Some desired properties need only a weaker hypothesis, and the Prime Number Theorem, which has many current applications including Borsuk\u2019s problem which we just discussed , does not need it at all. \n \nThere is a difference between \u201cnormal\u201d conjectures like the ABC Conjecture and our own . If yes, if it is true, then the world of complexity theory is altered forever. A huge part of computational complexity theory disappears. \n \n Abnormal Conjectures \n \nTo see why is special, let us assume for a moment that someone proves that expected result that . This would be great, would win awards, a million dollars, and have terrific impact. However, it would not change the world as we know it. It would confirm what most of the community believes is true. \n \nImagine, if you can, that someone proves the unexpected: they prove . Perhaps it is a real algorithm that can be run, or perhaps it is a galactic algorithm that cannot. In either case it would cause a rip\u2014not just a ripple\u2014through theory that would be stunning. \n \nMuch of CS theory would disappear. In my own research some of Ken\u2019s and my \u201cbest\u201d results would survive, but many would be destroyed. The Karp-Lipton Theorem is gone in this world. Ditto all \u201cdichotomy\u201d results between and -complete, and for , Jin-Yi\u2019s similar work. Many barrier results, such as oracle theorems and natural proofs, lose their main motivation, while much fine structure in hardness-versus-randomness tradeoffs would be blown up. The PCP Theorem and all the related work is gone. Modern cryptography could survive if the the algorithm were galactic, but otherwise would be in trouble. \n \nI am currently teaching Complexity Theory at Tech using the textbook by Sanjeev Arora and Boaz Barak, while Ken is using the new edition of the text by Steve Homer and Alan Selman, supplemented by his CRC Handbook chapters with Eric Allender and Michael Loui. Most of the 573 pages of Arora-Barak would be gone: \n \n \nDelete all of chapter 3 on . \n \nDelete all of chapter 5 on the polynomial hierarchy.\n \nDelete most of chapter 6 on circuits. \n \nDelete all of chapter 7 on probabilistic computation.\n \nMark as dangerous chapter 9 on cryptography. \n \nDelete most of chapter 10 on quantum computation\u2014who would care about Shor\u2019s algorithm then? \n \nDelete all of chapter 11 on the PCP theorem.\n \n \nI will stop here. Most of the initial part of the book is gone. The same for much of Homer-Selman, and basically all of the \u201cReducibility and Completensss\u201d CRC chapter. \n \n Open Problems \n \nIs the wanton conceptual destruction wrought by , or even more by or , a material reason to believe their opposites? How does it compare with Jin-Yi Cai\u2019s reason? \n \nWould it be a true apocalypse, or would we move to talking about \u201cnearly-linear\u201d versus \u201cquadratic-plus\u201d time, starting (say) with Merkle\u2019s Puzzles ? \n \n[updated paragraph on Jin-Yi's work]"], "link": "http://rjlipton.wordpress.com/2012/09/29/why-we-lose-sleep-some-nights/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.springer.com/": 1, "http://pages.wisc.edu/": 1, "http://www.buffalo.edu/": 1, "http://blog.computationalcomplexity.org/": 1, "http://www.issi-society.info/": 1, "http://rjlipton.wordpress.com/": 5, "http://www.tue.nl/": 1, "http://en.wikipedia.org/": 5, "http://www.amazon.com/": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["A possible barrier to proofs that factoring is in polynomial time \n \n  \n Mihalis Yannakakis is a Knuth Prize-winning complexity theorist and database expert. With Christos Papadimitriou in 1988, he framed the systematic study of approximation algorithms for -hard optimization problems around the classes and . The same two showed that the case of Traveling Salesman where all distances are 1 or 2 has polynomial-time approximation within of the optimum, but it is -hard to come within a factor of given . \n Today I want to talk about a \u201cbarrier\u201d to many attempts to prove that integer factoring is in polynomial time, including a recent one that drew on ideas that were first applied to Traveling Salesman and related problems. \n \nThe very first of 94 current entries on Gerhard Woeginger\u2019s indispensable page of claims to have resolved vs. is Ted Swart\u2019s attempt to create a linear program to solve Traveling Salesman by relaxing specially-designed integer programs. Alan Frieze and Yannakakis and others kept finding holes in Swart\u2019s polytopes where the desired integer-extrema properties could fail or unwanted integer solutions pop in, and Swart kept trying to patch them. Finally Yannakakis found a \u201cmeta-refutation\u201d\u2014a way of showing that no patch could work. To quote the page: \n \nIn 1988, Mihalis Yannakakis closed the discussion with his paper \u201cExpressing combinatorial optimization problems by linear programs\u201d (Proceedings of STOC 1988, pp. 223-228). Yannakakis proved that expressing the traveling salesman problem by a symmetric linear program (as in Swart\u2019s approach) requires exponential size. The journal version of this paper has been published in Journal of Computer and System Sciences 43, 1991, pp. 441-466.\n \n This is what we call an unconditional barrier . There are also conditional barriers, which are demonstrations that particular ways of solving one problem (such as proving a lower bound) entail solving an ostensibly harder problem (such as getting an unexpected upper bound or achieving a stronger-seeming lower bound). \n Barriers Have Domains \n The Maginot Line was a barrier, but it covered only one part of the border. Yannakakis\u2019 barrier has two qualifiers: \u201csymmetric\u201d and \u201clinear\u201d before programs. By the former, it left open the chance of advances that evaded the symmetry condition, which is basically that the effect of the constraints is invariant under permutations of the variables. Such was the feeling that a recent announcement, of the extension of the barrier by Samuel Fiorini, Serge Massar, Sebastian Pokutta, Hans Raj Tiwary, and Ronald de Wolf, was titled: \n \nDecades-old P=NP \u2018proof\u2019 finally refuted.\n \n This work shared the Best Paper prize at STOC 2012, and the abstract explains the content simply: \n \nWe solve a 20-year old problem posed by Yannakakis and prove that there exists no polynomial-size linear program (LP) whose associated polytope projects to the traveling salesman polytope, even if the LP is not required to be symmetric. Moreover, we prove that this holds also for the cut polytope and the stable set polytope. These results were discovered through a new connection that we make between one-way quantum communication protocols and semidefinite programming reformulations of LPs.\n \n However, the abstract also emphasizes another qualifier: \u201cthe LP\u2019s polytope projects to the traveling salesman polytope.\u201d This allows a possible loophole via different formulations for which projection to the standard TSP polytope used by Swart is not an issue. Then we need to see how far the logic of this paper (and a FOCS 2012 followup noted on the blog of my Tech colleague Pokutta) extends to alternative formulations. But first we also need to note that other kinds of constraint-management \u201cprograms\u201d besides LP\u2019s have powerful polynomial-time algorithms. \n Programming \n For those of us in computer science the term \u201cprogramming\u201d means something special: it covers the act of writing software for any computer of any type. The term is used in many other ways; one that was new to me is how architects use it. Programming is the term they use when collecting information for the design of a new building. See here for a description. \n  \n Programming also is a term used to talk about various types of problems. \u201cLinear Programming\u201d refers, of course, to the problem of solving a system of linear inequalities over the reals. This is a terrifically powerful problem , with an almost countless number of practical problems that can be reduced to it. Thanks to the brilliant work of George Dantzig, the powerful simplex method was created to solve them. Only later with the likewise great work of Leonid Khachiyan was it proved to be in , polynomial time. \n There are many other types of programming in this sense. Here is a partial list of types that are known to be in polynomial time: \n \n Convex Programming over variables.\n Integer Programming over a fixed number of variables, i.e .\n Quasi-convex Programming over variables.\n \n All of these are in polynomial time in the size of the inputs, although the second requires the number of variables to be fixed, and on the third we are a bit unsure between the 2001 source cited by this Wikipedia page and this 2006 survey . The best algorithms often use some variation of the ellipsoid method of Khachiyan. \n Convex programming means that the objective function has the convexity property: for all constants , \n \n Quasi-convex means that for all values , the subsets are convex as subsets of . This follows from convexity but is not equivalent to it: think of the one-variable function . \n We need to note that all the above problems are closed in the sense that if is a variable one can always add the constraints \n \n for any constants and and still stay in the same type of problems. This trivial observation is the core of the new barrier, and I will explain it in the next section. \n The Barrier \n Okay you are planning to show that factoring is in polynomial time. One obvious idea is to use powerful previous work\u2014a good problem solving rule is to be \u201clazy\u201d and use other powerful methods. That is do not attempt to solve factoring, or any problem, from first principles. Use powerful tools. \n The way this could be done is to try to reduce the search for the factors of a given to one of the above programming problems. Here is an example that does not work , but should give the idea: \n \n \n \n Note that if and must be integers, then the min would be zero when and are nontrivial factors of . If this type of program were known to be in polynomial time, then we would be done. The trouble is that it is not. The function being minimized is not quasi-convex for example. \n But suppose that you found a much more clever encoding into one of the programming types and the proof was correct. Here is the barrier: add the constraint . This means that you are finding not just any factor of , but a factor that is in a given interval. This small change moves factoring to a problem that is -complete under randomized reductions. See this February 2011 StackExchange thread and our own earlier discussion for details on this. \n \n Barrier : If there is a program whose minima are integer factorings, belonging to one of the polynomial-time genres that are closed under adding constraints of the form , then is in randomized polynomial time.\n \n \u2026And a Loophole? \n As observed on the thread, the -hardness result goes away if is forced to be a prime factor. There are ways to enforce this, for instance by changing the objective function of the above example to \n \n Presupposing other parts still work to make extrema occur at integers, a min less than is achievable only by a factoring, and the resulting value is minimized when is the least factor of \u2014which is always prime. \n However, this kind of change may disturb quasi-convexity or other properties the objective function needs for the program to belong to the chosen class. And even so, if the formulation you are attempting to use would extend to cases where any factoring serves as a minimum, then the above conditional barrier comes back into play. \n In summary, if you can reduce factoring to solving a programming problem for and as factors of a given , then you can put into randomized polynomial time. This does not mean that you cannot succeed, nor should you stop trying; it does show that you should be aware that you may be trying to do too much. \n Open Problems \n Can we make the above barrier into a more formal one? I believe that this would be helpful. Perhaps there is a more general barrier result similar to the ones for Traveling Salesman problem that can be proved. \n \n[added to paragraph after bulleted mention of quasi-convex programming, in response to query in comments]"], "link": "http://rjlipton.wordpress.com/2012/09/26/how-not-to-prove-integer-factoring-is-in-p/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://library.msri.org/": 1, "http://arxiv.org/": 2, "http://rjlipton.wordpress.com/": 3, "http://cstheory.stackexchange.com/": 1, "http://www.tue.nl/": 1, "http://en.wikipedia.org/": 5, "http://spokutta.wordpress.com/": 1, "http://www.iammea.org/": 1, "http://dx.doi.org/": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}, {"content": ["A tale of two approaches to solve open problems \n \n  \n \nJeff Kahn and Gil Kalai are among the top problem solvers in the world, and each has won notable prizes in his great career. Gil just recently visited Georgia Tech, and gave a wonderful talk on the interplay of Boolean functions and harmonic analysis. Gil also is involved in our ongoing debate on the feasibility of quantum computers. \n \nToday Ken and I wish to talk about two approaches to problem solving. Neither is better than the other, but they are fundamentally different. \n \nI thought about this topic while listening to Gil\u2019s talk. My favorite among his solved problems is the one he did with Jeff, resolving the Borsuk Conjecture . In 1932 Karol Borsuk asked the following question for which he had proved \u201cyes\u201d in the case : \n \nDie folgende Frage bleibt offen: L\u00e4sst sich jede beschr\u00e4nkte Teilmenge des Raumes in Mengen zerlegen, von denen jede einen kleineren Durchmesser als hat?\n \n \nKen finds Wikipedia\u2019s translation to be optimal: \n \nThe following question remains open: Can every bounded subset of the space be partitioned into sets, each of which has a smaller diameter than ?\n \n \nGil and Jeff showed that the widely-believed positive answer was off a bit\u2014the correct answer is exponential in , actually it is at least for large enough . \n \n The Guess \n \nSuppose one wishes to solve some open problem X. There are many ways to attack a problem, as many ways as there are problems. One critical issue for all open problems is the guess : do you try to prove X is true or try to prove X is false? On exams we often say: \n \nConsider the problem (\u2026) Prove that it is true.\n \n \nUnfortunately real-world problems are not so labeled. They are more like exam questions of the form: \n \nConsider the problem (\u2026) Prove that it is true or supply a counter-example.\n \n \nThese questions are much harder, and I usually avoid them on exams I create for my students. On a exam if you guess the wrong way you may waste valuable time, which could make you miss other questions, and perhaps fail the exam. \n \nOn an open problem\u2014not an exam problem\u2014you do need to make the guess. Are you going to try to prove X or try to disprove X? For some problems we all seem to believe that we know the right guess, but I am not so sure. Just be aware that a wrong guess means that you will never succeed in getting a proof. You can hope that even with a wrong guess your \u201cfailure\u201d will still lead to insights that may advance the field in general, and perhaps lead to a resolution of X anyway. \n \nWe have previously blogged about cases where the math and theory community guessed wrong. The problems in question were solved only after a few very smart people guessed the right way, since initially almost all of the community believed in the other direction. For Gil and Jeff on the Borsuk conjecture it was important that they first guessed right. Yes they were clever and had a deep insight into the problem, but making the right guess strikes me as coming before having that insight. \n \n Help In Guessing Right \n \nAs stated, the conjecture was about arbitrary subsets of . Ken and I know people with brilliant visualization of objects in for high \u2014for instance Harold Kuhn \u201csaw\u201d a tangency in 36-dimensional space from an example involving matrices in Ken\u2019s Princeton senior thesis\u2014but we are not among them, and perhaps Jeff and Gil are not either. However, as noted on the first page of their paper (linked from here ), there was a re-formulation of a major case of Borsuk\u2019s conjecture: \n \n Conjecture. Let be a family of -subsets of such that every two members of have elements in common. Then can be partitioned into parts so that in each part every two members have elements in common.\n \n \nThis is now stated in terms of the objects that a combinatorialist like Gil or Jeff understands well: discrete sets and systems of subsets and partitions, vectors and codes\u2026 Phrasing it this way primed their intuition that this was false. Once they found a construction for a strong counterexample, their proof was short. In the paper they quote Moby Dick on how they were led to it: \n \nHowever contracted, that definition is the result of expanded meditation.\n \n \nThen number theory , part of the landscape of any discrete mathematician, put a finishing touch on their exponential estimate by an application of the Prime Number Theorem. \n \nHence we will talk about what approach you take after you make your guess. This may come hand-in-hand with the insight, but importantly it also often comes before it. It may also be classed as a matter of attitude, of how much novelty one expects is required to reach the end. But we\u2019ll reference the classic divide between \u201cprocedural\u201d and \u201cobject-oriented\u201d programming languages. \n \n The Procedural Approach \n \nOkay, so you have guessed that X is true, and you start out to try to prove X. One method is what I think of as the standard procedure. You bring to bear on X all methods, lemmas, tricks, ideas, and more that you know that seem to be related to X. If you are lucky you will eventually see a pattern that will led to a proof. \n \nThe proof may be short, it may have modest length, it may be high-level, it may be long, it may be quite technical. The point is, you execute it as a series of actions based on objects you know and tools you try to sharpen. In any case you will be happy: you have solved your problem. Congrats. Time to move on to another question. \n \n The Object Approach \n \nSometimes the existing tools are insufficient to solve a problem, even if you have guessed right. You might feel you should work on another problem. The object-based approach basically agrees. \n \nIn object-oriented programming, the initial emphasis is not so much to do what you want your ultimate program to do, but rather to describe and model all the data you will be dealing with. Often this means creating new or compound objects for the data, or making a new class hierarchy of which the concepts you initially considered are only a part. You may do a lot of extra work, coding objects that are not used in the final product. However, it is often easier to do modeling of information than to process it. Once you have modeled a lot of things, then you can start to think about exactly what you want to do with them. \n \nIn some applications, such as simulation and game design, this is called world-building . Ken and I are thinking about this because Shinichi Mochizuki\u2019s new claimed proof of the ABC Conjecture\u2013and much more\u2014appears to be a huge example. Here is a quotation by Minhyong Kim from the New York Times article , which we linked from our own item : \n \n\u201cHe\u2019s taking what we know about numbers and addition and multiplication and really taking them apart,\u201d Dr. Kim said. \u201cHe creates a whole new language\u2014you could say a whole new universe of mathematical objects\u2014in order to say something about the usual universe.\u201d\n \n \n An ABC of New Objects? \n \nMany others commenting around the Web say they are having trouble because they cannot understand Mochizuki\u2019s procedures \u2014and this is because they have to learn his new objects first. Here is part of what Jordan Ellenberg says about them: \n \nMochizuki argues that it is too limiting to think about \u201cthe category of schemes over Spec Z ,\u201d as we are accustomed to do. \u2026 Mochizuki argues that [this is] insufficiently rich to handle certain problems in Diophantine geometry. He wants us instead to think about what he calls the \u201cspecies\u201d of schemes over Spec Z , where a scheme in this sense is not an abstract object in a category, but something cut out by a formula. In some sense this view is more classical than the conventional one, in which we tend to feel good about ourselves if we can \u201cremove coordinates\u201d and think about objects and arrows without implicitly applying a forgetful functor and referring to the object as a space with a Zariski topology or\u2014ptui!\u2014a set of points.\n \n \nWell nothing can be more \u201creal\u201d than a set of points, but so much conceptual building has been done on top of them already that they can be deprecated. Also when you take a first look at Mochizuki\u2019s outlines, the ABC Conjecture is barely prominent\u2014so many other things are defined and dealt with first, in 512 total pages. \n \n Some Problems That Birthed New Objects \n \nLet us think back to a few cases where a proof introduced a new kind of object that spurred research in itself, perhaps even eclipsing the problem the prover originally solved. \n \n \n \n \n The problem: Show 5th-degree equations can/cannot be solved in closed form. \n \n The objects: Groups.\n \n \n \n \n The problem: Show can/cannot be simulated by bounded-width branching programs. \n The objects: Programs over groups.\n \n \n \n \n The problem: Riemann Hypothesis extended to finite fields. \n \n The objects: Varieties over finite fields, and more\u2026\n \n \n \n \n The problem: Improve constant-degree expanders; de-randomize logspace? \n \n The objects: Zig-zag graph products.\n \n \n \n \n The problem: Simulate uses of quantum interference in classical (random) polynomial time. \n \n The objects: Matchgates .\n \n \n \nWe speculate that Andrew Wiles\u2019 work on his brilliant solution of the Fermat Equation is probably not of this type. He used all the tools he could from advanced number theory, but did not really create new objects. Thus we are certainly not knocking the \u201cprocedural\u201d approach, just noting that the expansive meditative alternative is sometimes needed. \n \n Declarative or Functional or Logic-based? \n \nProcedural and object-oriented are not necessarily opposed concepts; for instance C++ and related languages use both. In programming language theory the concepts most directed contrasted to procedural is declarative , which is allied with the idea of functional which was cultivated in John Backus\u2019 famous 1978 Turing Award speech and article , and with logic programming languages such as Prolog. \n \nThe idea of these, however, is more that it should suffice to give a full logical specification of a problem, and then procedures for solving it will largely take care of themselves. There is some echo of this idea in specifications for PolyMath problems , and the notion of a \u201cWatson\u201d for math comes in. However, we preferred to focus on the imperative to create and describe new objects. \n \n Open Problems \n \nWhat are some other good examples of problems that are most known today for creating new objects?"], "link": "http://rjlipton.wordpress.com/2012/09/22/what-is-the-object/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://gilkalai.wordpress.com/": 1, "http://citeseerx.psu.edu/": 1, "http://www.cmu.edu/": 1, "http://rjlipton.wordpress.com/": 5, "https://rjlipton.wordpress.com/": 1, "http://en.wikipedia.org/": 1, "http://quomodocumque.wordpress.com/": 1, "http://www.nytimes.com/": 1}, "blogtitle": "G\u00f6del's Lost Letter and P=NP"}]