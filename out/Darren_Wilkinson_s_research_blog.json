[{"blogurl": "http://darrenjw.wordpress.com\n", "blogroll": [], "title": "Darren Wilkinson's research blog"}, {"content": ["JAGS (Just Another Gibbs Sampler) is a general purpose MCMC engine similar to WinBUGS and OpenBUGS . I have a slight preference for JAGS as it is free and portable, works well on Linux, and interfaces well with R. It is tempting to write a tutorial introduction to JAGS and the corresponding R package, rjags , but there is a lot of material freely available on-line already, so it isn\u2019t really necessary. If you are new to JAGS, I suggest starting with Getting Started with JAGS, rjags, and Bayesian Modelling . In this post I want to focus specifically on the problem of inlining JAGS models in R scripts as it can be very useful, and is usually skipped in introductory material. \n JAGS and rjags on Ubuntu Linux \n On recent versions of Ubuntu, assuming that R is already installed, the simplest way to install JAGS and rjags is using the command \n \nsudo apt-get install jags r-cran-rjags\n \n Now rjags is a CRAN package, so it can be installed in the usual way with install.packages(\"rjags\") . However, taking JAGS and rjags direct from the Ubuntu repos should help to ensure that the versions of JAGS and rjags are in sync, which is a good thing. \n Toy model \n For this post, I will use a trivial toy example of inference for the mean and precision of a normal random sample. That is, we will assume data \n \n with priors on and of the form \n \n Separate model file \n The usual way to fit this model in R using rjags is to first create a separate file containing the model \n \n model {\n for (i in 1:n) {\n  x[i]~dnorm(mu,tau)\n }\n mu~dnorm(cc,d)\n tau~dgamma(a,b)\n }\n \n Then, supposing that this file is called jags1.jags , an R session to fit the model could be constructed as follows: \n \nrequire(rjags)\nx=rnorm(15,25,2)\ndata=list(x=x,n=length(x))\nhyper=list(a=3,b=11,cc=10,d=1/100)\ninit=list(mu=0,tau=1)\nmodel=jags.model(\"jags1.jags\",data=append(data,hyper), inits=init)\nupdate(model,n.iter=100)\noutput=coda.samples(model=model,variable.names=c(\"mu\", \"tau\"), n.iter=10000, thin=1)\nprint(summary(output))\nplot(output)\n \n This is all fine, and it can be very useful to have the model declared in a separate file, especially if the model is large and complex, and you might want to use it from outside R. However, very often for simple models it can be quite inconvenient to have the model separate from the R script which runs it. In particular, people often have issues with naming files correctly, making sure R is looking in the correct directory, moving the model with the R script, etc. So it would be nice to be able to just inline the JAGS model within an R script, to keep the model, the data, and the analysis all together in one place. \n Using a temporary file \n What we want to do is declare the JAGS model within a text string inside an R script and then somehow pass this into the call to jags.model() . The obvious way to do this is to write the string to a text file, and then pass the name of that text file into jags.model() . This works fine, but some care needs to be taken to make sure this works in a generic platform independent way. For example, you need to write to a file that you know doesn\u2019t exist in a directory that is writable using a filename that is valid on the OS on which the script is being run. For this purpose R has an excellent little function called tempfile() which solves exactly this naming problem. It should always return the name of a file which does not exist in a writable directly within the standard temporary file location on the OS on which R is being run. This function is exceedingly useful for all kinds of things, but doesn\u2019t seem to be very well known by newcomers to R. Using this we can construct a stand-alone R script to fit the model as follows: \n \nrequire(rjags)\nx=rnorm(15,25,2)\ndata=list(x=x,n=length(x))\nhyper=list(a=3,b=11,cc=10,d=1/100)\ninit=list(mu=0,tau=1)\nmodelstring=\"\n model {\n for (i in 1:n) {\n  x[i]~dnorm(mu,tau)\n }\n mu~dnorm(cc,d)\n tau~dgamma(a,b)\n }\n\"\ntmpf=tempfile()\ntmps=file(tmpf,\"w\")\ncat(modelstring,file=tmps)\nclose(tmps)\nmodel=jags.model(tmpf,data=append(data,hyper), inits=init)\nupdate(model,n.iter=100)\noutput=coda.samples(model=model,variable.names=c(\"mu\", \"tau\"), n.iter=10000, thin=1)\nprint(summary(output))\nplot(output)\n \n Now, although there is a file containing the model temporarily involved, the script is stand-alone and portable. \n Using a text connection \n The solution above works fine, but still involves writing a file to disk and reading it back in again, which is a bit pointless in this case. We can solve this by using another under-appreciated R function, textConnection() . Many R functions which take a file as an argument will work fine if instead passed a textConnection object, and the rjags function jags.model() is no exception. Here, instead of writing the model string to disk, we can turn it into a textConnection object and then pass that directly into jags.model() without ever actually writing the model file to disk. This is faster, neater and cleaner. An R session which takes this approach is given below. \n \nrequire(rjags)\nx=rnorm(15,25,2)\ndata=list(x=x,n=length(x))\nhyper=list(a=3,b=11,cc=10,d=1/100)\ninit=list(mu=0,tau=1)\nmodelstring=\"\n model {\n for (i in 1:n) {\n  x[i]~dnorm(mu,tau)\n }\n mu~dnorm(cc,d)\n tau~dgamma(a,b)\n }\n\"\nmodel=jags.model(textConnection(modelstring), data=append(data,hyper), inits=init)\nupdate(model,n.iter=100)\noutput=coda.samples(model=model,variable.names=c(\"mu\", \"tau\"), n.iter=10000, thin=1)\nprint(summary(output))\nplot(output)\n \n This is my preferred way to use rjags . Note again that textConnection objects have many and varied uses and applications that have nothing to do with rjags ."], "link": "http://darrenjw.wordpress.com/2012/10/02/inlining-jags-models-in-r-scripts-for-rjags/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://mcmc-jags.sourceforge.net/": 1, "http://www.openbugs.info/": 1, "http://www.ac.uk/": 1, "http://jeromyanglim.co.uk/": 1, "http://cran.r-project.org/": 2}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["I\u2019ve recently taken delivery of a Raspberry Pi mini computer. For anyone who doesn\u2019t know, this is a low cost, low power machine, costing around 20 GBP (25 USD) and consuming around 2.5 Watts of power (it is powered by micro-USB). This amazing little device can run linux very adequately, and so naturally I\u2019ve been interested to see if I can get MCMC codes to run on it, and to see how fast they run.\n \n  \n \nNow, I\u2019m fairly sure that the majority of readers of this blog won\u2019t want to be swamped with lots of Raspberry Pi related posts, so I\u2019ve re-kindled my old personal blog for this purpose. Apart from this post, I\u2019ll try not to write about my experiences with the Pi here on my main blog. Consequently, if you are interested in my ramblings about the Pi, you may wish to consider subscribing to my personal blog in addition to this one. Of course I\u2019m not guaranteeing that the occasional Raspberry-flavoured post won\u2019t find its way onto this blog, but I\u2019ll try only to do so if it has strong relevance to statistical computing or one of the other core topics of this blog.\n \n \nIn order to get started with MCMC on the Pi, I\u2019ve taken the C code gibbs.c for a simple Gibbs sampler described in a previous post (on this blog) and run it on a couple of laptops I have available, in addition to the Pi, and looked at timings. The full details of the experiment are recorded in this post over on my other blog, to which interested parties are referred. Here I will just give the \u201cexecutive summary\u201d.\n \n \nThe code runs fine on the Pi (running Raspbian ), at around half the speed of my Intel Atom based netbook (running Ubuntu ). My netbook in turn runs at around one fifth the speed of my Intel i7 based laptop. So the code runs at around one tenth of the speed of the fastest machine I have conveniently available.\n \n \nAs discussed over on my other blog, although the Pi is relatively slow, its low cost and low power consumption mean that is has a bang-for-buck comparable with high-end laptops and desktops. Further, a small cluster of Pis (known as a bramble ) seems like a good, low cost way to learn about parallel and distributed statistical computing."], "link": "http://darrenjw.wordpress.com/2012/07/07/mcmc-on-the-raspberry-pi/", "bloglinks": {}, "links": {"http://www.ubuntu.com/": 1, "http://feeds.wordpress.com/": 1, "http://darrenjw.wordpress.com/": 2, "http://www.raspbian.org/": 1, "http://darrenjw.posterous.com/": 3, "http://www.raspberrypi.org/": 1, "http://elinux.org/": 1}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["Introduction \n Very often it is desirable to use Metropolis Hastings MCMC for a target distribution which does not have full support (for example, it may correspond to a non-negative random variable), using a proposal distribution which does (for example, a Gaussian random walk proposal). This isn\u2019t a problem at all, but on more than one occasion now I have come across students getting this wrong, so I thought it might be useful to have a brief post on how to do it right, see what people sometimes get wrong, and why, and then think about correcting the wrong method in order to make it right\u2026 \n A simple example \n For this post we will consider a simple target distribution, with density \n \n Of course this is a very simple distribution, and there are many straightforward ways to simulate it directly, but for this post we will use a random walk Metropolis-Hastings (MH) scheme with standard Gaussian innovations. So, if the current state of the chain is , a proposed new value will be generated from \n \n where is the standard normal density. This proposed new value is accepted with probability , where \n \n since the standard normal density is symmetric. \n Correct implementation \n We can easily implement this using R as follows: \n \nmet1=function(iters)\n {\n xvec=numeric(iters)\n x=1\n for (i in 1:iters) {\n  xs=x+rnorm(1)\n  A=dgamma(xs,2,1)/dgamma(x,2,1)\n  if (runif(1)<A)\n  x=xs\n  xvec[i]=x\n }\n return(xvec)\n }\n \n We can run it, plot the results and check it against the true target with the following commands. \n \niters=1000000\nout=met1(iters)\nhist(out,100,freq=FALSE,main=\"met1\")\ncurve(dgamma(x,2,1),add=TRUE,col=2,lwd=2)\n \n  \n If you have a slow computer, you may prefer to use iters=100000 . The above code uses R\u2019s built-in gamma density. Alternatively, we can hard-code the density as follows. \n \nmet2=function(iters)\n {\n xvec=numeric(iters)\n x=1\n for (i in 1:iters) {\n  xs=x+rnorm(1)\n  A=xs*exp(-xs)/(x*exp(-x))\n  if (runif(1)<A)\n  x=xs\n  xvec[i]=x\n }\n return(xvec)\n }\n \n We can run this code using the following commands, to verify that it does work as expected. \n \nout=met2(iters)\nhist(out,100,freq=FALSE,main=\"met2\")\ncurve(dgamma(x,2,1),add=TRUE,col=2,lwd=2)\n \n However, there is a potential problem with the above code that we have got away with in this instance, which often catches people out. We have hard-coded the density for without checking the sign of . Here we get away with it as a negative proposal will lead to a negative acceptance ratio that we will reject straight away. This is not always the case (consider, for example, a distribution). So really we should check the sign of and reject immediately if is not within the support of the target. \n Although this problem often catches people out, it tends not to be a big issue in practice, as it typically leads to an obviously incorrect sampler, or a sampler which crashes, and is relatively simple to debug and fix. \n An incorrect sampler \n The problem I want to focus on here is more subtle, but closely related. It is clear that any should be rejected. With the above code, such values are indeed rejected, and the sampler advances to the next iteration. However, in more complex samplers, where an update like this might be one tiny part of a massive sampler with a very high-dimensional state space, it seems like a bit of a \"waste\" of a MH move to just propose a negative value, throw it away, and move on. Evidently, it seems tempting, therefore, to keep on sampling values until a non-negative value is obtained, and then evaluate the acceptance ratio and decide whether or not to accept. We could code up this sampler as follows. \n \nmet3=function(iters)\n {\n xvec=numeric(iters)\n x=1\n for (i in 1:iters) {\n  repeat {\n  xs=x+rnorm(1)\n  if (xs>0)\n   break\n  }\n  A=xs*exp(-xs)/(x*exp(-x))\n  if (runif(1)<A)\n  x=xs\n  xvec[i]=x\n }\n return(xvec)\n }\n \n As reasonable as this idea may at first seem, it does not lead to a sampler having the desired target, as can be verified using the following commands. \n \nout=met3(iters)\nhist(out,100,freq=FALSE,main=\"met3\")\ncurve(dgamma(x,2,1),add=TRUE,col=2,lwd=2)\n \n  \n So, this sampler seems to be sampling something close to the desired target, but not the same. This raises a couple of questions. First and most important, can we fix this sampler so that it does sample the correct target (yes), and second, can we figure out what target density the incorrect sampler is actually sampling (again, yes)? Let\u2019s start with the issue of how to fix the sampler, as this will also help us to understand what the incorrect sampler is doing. \n Fixing the truncated sampler \n By repeatedly sampling from the proposal until we obtain a non-negative value, we are actually implementing a rejection sampler for sampling from the proposal distribution truncated at zero. This is a perfectly reasonable proposal distribution, so we can use it provided that we use the correct MH acceptance ratio. Now, the truncated density has the same density as the untruncated density, apart from the differing support and a normalising constant. Indeed, this may be why people often assume this method will work, because normalising constants often don\u2019t matter in MH schemes. However, the normalising constant only doesn\u2019t matter if it is independent of the state, and here it is not\u2026 Explicitly, we have \n \n Including the normalising constant we have \n \n where is the standard normal CDF. Consequently, the correct acceptance ratio to use with this proposal is \n \n where we see that the normalising constants do not cancel out. We can modify the previous sampler to use the correct acceptance ratio as follows. \n \nmet4=function(iters)\n {\n xvec=numeric(iters)\n x=1\n for (i in 1:iters) {\n  repeat {\n  xs=x+rnorm(1)\n  if (xs>0)\n   break\n  }\n  A=xs*exp(-xs)/(x*exp(-x))\n  A=A*pnorm(x)/pnorm(xs)\n  if (runif(1)<A)\n  x=xs\n  xvec[i]=x\n }\n return(xvec)\n }\n \n We can verify that this sampler gives leads to the correct target with the following commands. \n \nout=met4(iters)\nhist(out,100,freq=FALSE,main=\"met4\")\ncurve(dgamma(x,2,1),add=TRUE,col=2,lwd=2)\n \n  \n So, truncating the proposal at zero is fine, provided that you modify the acceptance ratio accordingly. \n What does the incorrect sampler target? \n Now that we understand why the naive truncated sampler was wrong and how to fix it, we can, out of curiosity, wonder what distribution that sampler actually targets. Now we understand what proposal we are actually using, we can re-write the acceptance ratio as \n \n from which it is clear that the actual target of this chain is \n \n or \n \n The constant of proportionality is not immediately obvious, but is tractable, and turns out to be a nice undergraduate exercise in integration by parts, leading to \n \n We can verify this using the following commands. \n \nout=met3(iters)\nhist(out,100,freq=FALSE,main=\"met3\")\ncurve(dgamma(x,2,1)*pnorm(x)*2*sqrt(2*pi)/(sqrt(2*pi)+2),add=TRUE,col=3,lwd=2)\n \n  \n Now we know the actual target of the incorrect sampler, we can compare it with the correct target as follows. \n \ncurve(dgamma(x,2,1),0,10,col=2,lwd=2,main=\"Densities\")\ncurve(dgamma(x,2,1)*pnorm(x)*2*sqrt(2*pi)/(sqrt(2*pi)+2),add=TRUE,col=3,lwd=2)\n \n  \n So we see that the distributions are different, but not so different that one would immediate suspect an error on the basis of a sample of output. This makes it a difficult bug to track down. \n Summary \n There is no problem in principle using a proposal with full support for a target with limited support in MH algorithms. However, it is important to check whether a proposed value is within the support of the target and reject the proposed move if it is not. If you are concerned that such a scheme might be inefficient, it is possible to use a truncated proposal provided that you modify the MH acceptance ratio to include the relevant normalisation constants. If you don\u2019t modify the acceptance probability, you will get a sampler which targets the wrong distribution, but it will often be quite similar to the correct target, making it a difficult bug to spot and track down."], "link": "http://darrenjw.wordpress.com/2012/06/04/metropolis-hastings-mcmc-when-the-proposal-and-target-have-differing-support/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://darrenjw.wordpress.com/": 7}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["Introduction \n As I\u2019ve explained previously , I\u2019m gradually coming around to the idea of using Java for the development of MCMC codes, and I\u2019m starting to build up a collection of simple examples for getting started. One of the advantages of Java is that it includes a standard cross-platform GUI library. This might not seem like the most important requirement for MCMC, but can actually be very handy in several contexts, particularly for monitoring convergence. One obvious context is that of image analysis, where it can be useful to monitor image reconstructions as the sampler is running. In this post I\u2019ll show three very small simple Java classes which together provide an application for running a Gibbs sampler on a (non-stationary, unconditioned) Gaussian Markov random field. \n The model is essentially that the distribution of each pixel is defined intrinsically, dependent only on its four nearest neighbours on a rectangular lattice, and here the distribution will be Gaussian with mean equal to the sample mean of the four neighbouring pixels and a fixed (unit) variance. On its own this isn\u2019t especially useful, but it is a key component of many image analysis applications. \n A simple Java implementation \n We will start with the class MrfApp containing the main method for the application: \n MrfApp.java \n \nimport java.io.*;\nclass MrfApp {\n public static void main(String[] arg)\n\tthrows IOException\n {\n\tMrf mrf;\n\tSystem.out.println(\"started program\");\n\tmrf=new Mrf(800,600);\n\tSystem.out.println(\"created mrf object\");\n\tmrf.update(1000);\n\tSystem.out.println(\"done updates\");\n\tmrf.saveImage(\"mrf.png\");\n\tSystem.out.println(\"finished program\");\n\tmrf.frame.dispose();\n\tSystem.exit(0);\n }\n}\n \n Hopefully this code is largely self-explanatory, but relies on a class called Mrf which contains all of the logic associated with the GMRF. \n Mrf.java \n \nimport java.io.*;\nimport java.util.*;\nimport java.awt.image.*;\nimport javax.swing.*;\nimport javax.imageio.ImageIO;\n\n\nclass Mrf \n{\n int n,m;\n double[][] cells;\n Random rng;\n BufferedImage bi;\n WritableRaster wr;\n JFrame frame;\n ImagePanel ip;\n \n Mrf(int n_arg,int m_arg)\n {\n\tn=n_arg;\n\tm=m_arg;\n\tcells=new double[n][m];\n\trng=new Random();\n\tbi=new BufferedImage(n,m,BufferedImage.TYPE_BYTE_GRAY);\n\twr=bi.getRaster();\n\tframe=new JFrame(\"MRF\");\n\tframe.setSize(n,m);\n\tframe.add(new ImagePanel(bi));\n\tframe.setVisible(true);\n }\n \n public void saveImage(String filename)\n\tthrows IOException\n {\n\tImageIO.write(bi,\"PNG\",new File(filename));\n }\n \n public void updateImage()\n {\n\tdouble mx=-1e+100;\n\tdouble mn=1e+100;\n\tfor (int i=0;i<n;i++) {\n\t for (int j=0;j<m;j++) {\n\t\tif (cells[i][j]>mx) { mx=cells[i][j]; }\n\t\tif (cells[i][j]<mn) { mn=cells[i][j]; }\n\t }\n\t}\n\tfor (int i=0;i<n;i++) {\n\t for (int j=0;j<m;j++) {\n\t\tint level=(int) (255*(cells[i][j]-mn)/(mx-mn));\n\t\twr.setSample(i,j,0,level);\n\t }\n\t}\n\tframe.repaint();\n }\n \n public void update(int num)\n {\n\tfor (int i=0;i<num;i++) {\n\t updateOnce();\n\t}\n }\n \n private void updateOnce()\n {\n\tdouble mean;\n\tfor (int i=0;i<n;i++) {\n\t for (int j=0;j<m;j++) {\n\t\tif (i==0) {\n\t\t if (j==0) {\n\t\t\tmean=0.5*(cells[0][1]+cells[1][0]);\n\t\t } \n\t\t else if (j==m-1) {\n\t\t\tmean=0.5*(cells[0][j-1]+cells[1][j]);\n\t\t } \n\t\t else {\n\t\t\tmean=(cells[0][j-1]+cells[0][j+1]+cells[1][j])/3.0;\n\t\t }\n\t\t}\n\t\telse if (i==n-1) {\n\t\t if (j==0) {\n\t\t\tmean=0.5*(cells[i][1]+cells[i-1][0]);\n\t\t }\n\t\t else if (j==m-1) {\n\t\t\tmean=0.5*(cells[i][j-1]+cells[i-1][j]);\n\t\t }\n\t\t else {\n\t\t\tmean=(cells[i][j-1]+cells[i][j+1]+cells[i-1][j])/3.0;\n\t\t }\n\t\t}\n\t\telse if (j==0) {\n\t\t mean=(cells[i-1][0]+cells[i+1][0]+cells[i][1])/3.0;\n\t\t}\n\t\telse if (j==m-1) {\n\t\t mean=(cells[i-1][j]+cells[i+1][j]+cells[i][j-1])/3.0;\n\t\t}\n\t\telse {\n\t\t mean=0.25*(cells[i][j-1]+cells[i][j+1]+cells[i+1][j]\n\t\t\t  +cells[i-1][j]);\n\t\t}\n\t\tcells[i][j]=mean+rng.nextGaussian();\n\t }\n\t}\n\tupdateImage();\n }\n \n}\n \n This class contains a few simple methods for creating and updating the GMRF, and also for maintaining and updating a graphical view of the GMRF as the sampler is running. The Gibbs sampler update itself is encoded in the final method, updateOnce , and most of the code is to deal with edge and corner cases (in the literal rather than metaphorical sense!). This is called repeatedly by the method update for the required number of iterations. At the end of each iteration, the method updateOnce triggers updateImage which updates the image associated GMRF. The GMRF itself is stored in a 2-dimensional array of double s, but an image pixel typically consists of a grayscale value represented by an unsigned byte \u2013 that is, an integer from 0 to 255. So updateImage scans through the GMRF to find the maximum and minimum values and then maps the GMRF values onto the 0 to 255 scale. The image itself is set up by the constructor method, Mrf . This class relies on an additional class called ImagePanel , which is a simple GUI panel for displaying images: \n ImagePanel.java \n \nimport java.awt.*;\nimport java.awt.image.*;\nimport javax.swing.*;\n\nclass ImagePanel extends JPanel {\n\n\tprotected BufferedImage image;\n\n\tpublic ImagePanel(BufferedImage image) {\n\t\tthis.image=image;\n\t\tDimension dim=new Dimension(image.getWidth(),image.getHeight());\n\t\tsetPreferredSize(dim);\n\t\tsetMinimumSize(dim);\n\t\trevalidate();\n\t\trepaint();\n\t}\n\n\tpublic void paintComponent(Graphics g) {\n\t\tg.drawImage(image,0,0,this);\n\t}\n\n}\n \n This completes the application, which can be compiled and run from the command line with \n \njavac *.java\njava MrfApp\n \n This should compile the code and run the application, which will show a GMRF updating for 1000 iterations. When the 1000 iterations are complete, the application writes the final image to a file and then quits. \n Using Parallel COLT \n The above classes are very convenient, as they should work with any standard Java installation. However, in more complex scenarios, it is likely that a math library such as Parallel COLT will be required. In this case it will make sense to make use of features in the COLT library, such as random number generators and 2d matrix objects. We can adapt the above application by replacing the MrfApp and Mrf classes with the following versions (the ImagePanel class remains unchanged): \n MrfApp.java \n \nimport java.io.*;\nimport cern.jet.random.tdouble.engine.*;\n\nclass MrfApp {\n\n public static void main(String[] arg)\n\tthrows IOException\n {\n\tMrf mrf;\n\tint seed=1234;\n\tSystem.out.println(\"started program\");\n  DoubleRandomEngine rngEngine=new DoubleMersenneTwister(seed);\n\tmrf=new Mrf(800,600,rngEngine);\n\tSystem.out.println(\"created mrf object\");\n\tmrf.update(1000);\n\tSystem.out.println(\"done updates\");\n\tmrf.saveImage(\"mrf.png\");\n\tSystem.out.println(\"finished program\");\n\tmrf.frame.dispose();\n\tSystem.exit(0);\n }\n\n}\n \n Mrf.java \n \nimport java.io.*;\nimport java.util.*;\nimport java.awt.image.*;\nimport javax.swing.*;\nimport javax.imageio.ImageIO;\nimport cern.jet.random.tdouble.*;\nimport cern.jet.random.tdouble.engine.*;\nimport cern.colt.matrix.tdouble.impl.*;\n\nclass Mrf \n{\n int n,m;\n DenseDoubleMatrix2D cells;\n DoubleRandomEngine rng;\n Normal rngN;\n BufferedImage bi;\n WritableRaster wr;\n JFrame frame;\n ImagePanel ip;\n \n Mrf(int n_arg,int m_arg,DoubleRandomEngine rng)\n {\n\tn=n_arg;\n\tm=m_arg;\n\tcells=new DenseDoubleMatrix2D(n,m);\n\tthis.rng=rng;\n\trngN=new Normal(0.0,1.0,rng);\n\tbi=new BufferedImage(n,m,BufferedImage.TYPE_BYTE_GRAY);\n\twr=bi.getRaster();\n\tframe=new JFrame(\"MRF\");\n\tframe.setSize(n,m);\n\tframe.add(new ImagePanel(bi));\n\tframe.setVisible(true);\n }\n \n public void saveImage(String filename)\n\tthrows IOException\n {\n\tImageIO.write(bi,\"PNG\",new File(filename));\n }\n \n public void updateImage()\n {\n\tdouble mx=-1e+100;\n\tdouble mn=1e+100;\n\tfor (int i=0;i<n;i++) {\n\t for (int j=0;j<m;j++) {\n\t\tif (cells.getQuick(i,j)>mx) { mx=cells.getQuick(i,j); }\n\t\tif (cells.getQuick(i,j)<mn) { mn=cells.getQuick(i,j); }\n\t }\n\t}\n\tfor (int i=0;i<n;i++) {\n\t for (int j=0;j<m;j++) {\n\t\tint level=(int) (255*(cells.getQuick(i,j)-mn)/(mx-mn));\n\t\twr.setSample(i,j,0,level);\n\t }\n\t}\n\tframe.repaint();\n }\n \n public void update(int num)\n {\n\tfor (int i=0;i<num;i++) {\n\t updateOnce();\n\t}\n }\n \n private void updateOnce()\n {\n\tdouble mean;\n\tfor (int i=0;i<n;i++) {\n\t for (int j=0;j<m;j++) {\n\t\tif (i==0) {\n\t\t if (j==0) {\n\t\t\tmean=0.5*(cells.getQuick(0,1)+cells.getQuick(1,0));\n\t\t } \n\t\t else if (j==m-1) {\n\t\t\tmean=0.5*(cells.getQuick(0,j-1)+cells.getQuick(1,j));\n\t\t } \n\t\t else {\n\t\t\tmean=(cells.getQuick(0,j-1)+cells.getQuick(0,j+1)+cells.getQuick(1,j))/3.0;\n\t\t }\n\t\t}\n\t\telse if (i==n-1) {\n\t\t if (j==0) {\n\t\t\tmean=0.5*(cells.getQuick(i,1)+cells.getQuick(i-1,0));\n\t\t }\n\t\t else if (j==m-1) {\n\t\t\tmean=0.5*(cells.getQuick(i,j-1)+cells.getQuick(i-1,j));\n\t\t }\n\t\t else {\n\t\t\tmean=(cells.getQuick(i,j-1)+cells.getQuick(i,j+1)+cells.getQuick(i-1,j))/3.0;\n\t\t }\n\t\t}\n\t\telse if (j==0) {\n\t\t mean=(cells.getQuick(i-1,0)+cells.getQuick(i+1,0)+cells.getQuick(i,1))/3.0;\n\t\t}\n\t\telse if (j==m-1) {\n\t\t mean=(cells.getQuick(i-1,j)+cells.getQuick(i+1,j)+cells.getQuick(i,j-1))/3.0;\n\t\t}\n\t\telse {\n\t\t mean=0.25*(cells.getQuick(i,j-1)+cells.getQuick(i,j+1)+cells.getQuick(i+1,j)\n\t\t\t  +cells.getQuick(i-1,j));\n\t\t}\n\t\tcells.setQuick(i,j,mean+rngN.nextDouble());\n\t }\n\t}\n\tupdateImage();\n }\n \n}\n \n Again, the code should be reasonably self explanatory, and will compile and run in the same way provided that Parallel COLT is installed and in your classpath. This version runs approximately twice as fast as the previous version on all of the machines I\u2019ve tried it on. \n Reference \n I have found the following book very useful for understanding how to work with images in Java: \n Hunt, K.A. (2010) The Art of Image Processing with Java , A K Peters/CRC Press."], "link": "http://darrenjw.wordpress.com/2012/06/01/gibbs-sampling-a-gaussian-markov-random-field-gmrf-using-java/", "bloglinks": {}, "links": {"http://amzn.to/": 1, "http://feeds.wordpress.com/": 1, "http://darrenjw.wordpress.com/": 1, "https://sites.google.com/": 1}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["I\u2019ve been very quiet on-line in the last few months, due mainly to the fact that I\u2019ve been writing a new undergraduate course on multivariate data analysis. Although there are many books and on-line notes on the general topic of multivariate statistics, I wanted to do something a little bit different from any text I have yet discovered. First, I wanted to have a strong emphasis on using techniques in practice on example data sets of reasonable size. For this, I found Hastie et al (2009) to be very useful, as it covered some interesting example data sets which have been bundled in the CRAN R package, ElemStatLearn . I used several of the data sets from this package as running examples throughout the course. In fact my initial plan was to use Hastie et al as the main course text, but it turned out that this text was in some places overly technical and in many places far too terse to be good as an undergraduate text. I would still recommend the book for researchers who want a good overview of the interface between statistics and machine learning, but with hindsight I\u2019m not convinced it is ideal for typical statistics undergraduate students. \n I also wanted to have a strong emphasis on numerical linear algebra as the basis for multivariate statistical computation. Again, this is a bit different from \u201cold school\u201d multivariate statistics (which reminds me, John Marden has produced a great text available freely on-line on old school multivariate analysis , which isn\u2019t quite as \u201cold school\u201d as the title might suggest). I wanted to spend some time talking about linear systems and matrix factorisations, explaining, for example how the LU decomposition, the Cholesky factorisation and the QR factorisations are related, and why the latter two are both fundamental to multivariate data analysis, and how the singular value decomposition (SVD) is related to the spectral decomposition, and why it is generally better to construct principal components from the SVD of the centred data matrix than the eigen-decomposition of the sample variance matrix, etc. These sorts of topics are not often covered in undergraduate statistics courses, but they are crucial to understanding how to analyse large multivariate data sets in a numerically stable way. \n I also wanted to downplay distribution theory as much as possible, as multivariate distribution theory is quite difficult, and not necessary for understanding most of the essential concepts in multivariate data analysis. Also, it is not obviously very useful. Essentially all introductory courses are based around the multivariate normal distribution, but I have yet to see a real non-trivial multivariate data set for which an assumption of multivariate normality is appropriate. Consequently I delayed the introduction of the multivariate normal until well into the course, and didn\u2019t bother with the Wishart distribution, or testing for multivariate normality. Like much frequentist testing, it is really just a matter of seeing if you have yet collected a large enough sample to reject the null hypothesis \u2013 I just don\u2019t see the point (null)! \n Finally, I wanted to use R to illustrate all of the methods in practice as they were introduced. We use R throughout our undergraduate statistics programme, and I think it is a good language for learning about statistical methods, algorithms and concepts. In most cases I begin by showing how to carry out analyses using \u201celementary\u201d operations (such as matrix manipulations), and then go on to show how to accomplish the same task more simply using higher-level R functions and packages. Again, I think it really helps understanding to first see the mathematical description directly translated into computer code before jumping to high-level data analysis functions. \n There are several aspects of the course that I would like to distil out into self-contained blog posts, but I have a busy summer schedule, and a couple of other things I want to write about before I\u2019ll have a chance to get around to it, so in the mean time, anyone interested is welcome to download a copy of the course notes (PDF, with hyperlinks). This is the student version, containing gaps, but the gaps mainly correspond to bits of standard theory and examples designed to be worked through by hand. All of the essential theory and context and all of the R examples are present in this version of the notes. There are seven chapters: Introduction to multivariate data; PCA and matrix factorisations; Inference, the MVN and multivariate regression; Cluster analysis and unsupervised learning; Discrimination and classification; Graphical modelling; Variable selection and multiple testing."], "link": "http://darrenjw.wordpress.com/2012/05/29/multivariate-data-analysis-using-r/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.ac.uk/": 1, "http://www.uiuc.edu/": 1, "http://istics.net/": 1, "http://www-stat.stanford.edu/": 1}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["This is my 25th blog post, so this seems like a good time to provide an index to those first 25 posts for ease of reference. I\u2019ve covered a range of topics over my first two years of blogging, and managed to average almost one post per month, as suggested in my first post. Due to the rather occasional nature of my posting, most regular readers subscribe to my RSS feed using some kind of RSS feed reader. I use Google Reader for following blogs and other RSS feeds, which I find very convenient as it is web based and therefore synced across all the machines and devices I use, but there are plenty of other options. Alternatively, you can follow me on twitter, where I am @darrenjw , or my Google+ feed , as I always announce new posts on those platforms. \n Blog posts \n \n 1. About this blog\u2026 : quick introduction to the new blog and what to expect. \n 2. Hypercubes in R (getting started with programming in R) : Constructing, rotating and plotting (2d projections of) hypercubes in order to illustrate some elementary R programming concepts. \n 3. Systems biology, mathematical modelling and mountains : My review of an excellent workshop I participated in at BIRS on Multi-scale Stochastic Modeling of Cell Dynamics. \n 4. (Yet another) introduction to R and Bioconductor : A very quick tutorial on basic R concepts and getting started with Bioconductor \u2013 very first steps. \n 5. MCMC programming in R, Python, Java and C : this post showed how to implement a very simple bivariate Gibbs sampler in four different programming languages, and compared the speeds. The post has now been superseded by post number 18. \n 6. The last Valencia meeting on Bayesian Statistics and the future of Bayesian computation : My impressions of Bayes 9, together with some thoughts on Bayesian computing in the context of multicore CPUs, GPUs, clusters and \u201cBig Data\u201d. \n 7. Metropolis-Hastings MCMC algorithms : A quick introduction to the Metropolis algorithm, with example code in R, discussing implementation issues. \n 8. The pseudo-marginal approach to \u201cexact approximate\u201d MCMC algorithms : a simple explanation of the \u201cpseudo-marginal\u201d idea, which has many potential applications in Bayesian inference. \n 9. Introduction to the processing of short read next generation sequencing data : a quick introduction to high-throughput sequencing data, the FASTQ file format, and the use of Unix and command-line tools for initial processing and analysis of FASTQ files. \n 10. A quick introduction to the Bioconductor ShortRead package for the analysis of NGS data : a follow-on from post 9. where I show how to get started with the analysis of FASTQ sequencing data using R and Bioconductor. \n 11. Getting started with parallel MCMC : an introduction to parallel Monte Carlo algorithms and their implementation using C, the GSL, and MPI. \n 12. Calling C code from R : how to call a Gibbs sampler written in C from R. \n 13. Calling Java code from R : how to call a Gibbs sampler written in Java from R. \n 14. Parallel Monte Carlo with an Intel i7 Quad Core : a quick look at the potential speed-ups possible exploiting parallelisation on a laptop with a nice Quad-core CPU. \n 15. MCMC, Monte Carlo likelihood estimation, and the bootstrap particle filter : how the pseudo-marginal idea discussed in post number 8. can be exploited for state-space models by using a simple particle filter to construct an unbiased estimate of marginal likelihood. \n 16. The particle marginal Metropolis-Hastings (PMMH) particle MCMC algorithm : following on from the previous post, an explanation of the full PMMH pMCMC algorithm for simultaneous estimation of parameters and state for state-space models. \n 17. Java math libraries and Monte Carlo simulation codes : a post bemoaning the lack of anything quite like the GSL C library in Java, but highlighting some reasonable alternatives (COLT, Parallel COLT and Apache Commons Math). \n 18. Gibbs sampler in various languages (revisited) : an updated version of post number 5, including detailed timings. I also take the opportunity to include new languages PyPy, Groovy and Scala. \n 19. Faster Gibbs sampling MCMC from within R : How to call MCMC code written in C, C++ and Java from R, with timing details. \n 20. Stochastic Modelling for Systems Biology, second edition : A quick introduction to the 2nd edition of my book, together with a tutorial introduction to the associated CRAN R package, smfsb , for simulation and inference of stochastic kinetic network models and other Markov processes. \n 21. Particle filtering and pMCMC using R : code for particle filtering and particle MCMC for Markov processes, in R. \n 22. Review of \u201cParallel R\u201d by McCallum and Weston : my somewhat critical review of this book. \n 23. Lexical scope and function closures in R : an introduction to notions of variable scope and closure in the context of R. \n 24. Parallel particle filtering and pMCMC using R and multicore : a discussion of the parallelisation of the particle filtering code from post 21. using R\u2019s high-level parallelisation constructs. \n 25. Catalogue of my first 25 blog posts: this post! \n \n Should I ever manage another 25 posts, I\u2019ll do a similar review of the next 25 at post number 50\u2026"], "link": "http://darrenjw.wordpress.com/2011/12/30/catalogue-of-my-first-25-blog-posts/", "bloglinks": {}, "links": {"http://twitter.com/": 1, "https://plus.google.com/": 1, "http://feeds.wordpress.com/": 1, "http://www.co.uk/": 1, "http://darrenjw.wordpress.com/": 25}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["Introduction \n In a previous post I showed how to construct a PMMH pMCMC algorithm for parameter estimation with partially observed Markov processes. The inner loop of a pMCMC algorithm consists of running a particle filter to construct an unbiased estimate of marginal likelihood. This inner loop is the place where the code spends almost all of its time, and so speeding up the particle filter will result in dramatic speedup of the pMCMC algorithm. This is fortunate, since as previously discussed , MCMC algorithms are difficult to parallelise other than on a per iteration basis. Here, each iteration can be speeded up if we can effectively parallelise a particle filter. Particle filters are much easier to parallelise than MCMC algorithms, and so it is tempting to try and exploit this within R. In fact, although it is the case that it is possible to effectively parallelise particle filters in efficient languages using low-level parallelisation tools (say, using C with MPI, or Java concurrency tools), it is not so easy to speed up R-based particle filters using R\u2019s high-level parallelisation constructs, as we shall see. \n Particle filters \n In the previous post we looked at the function pfMLLik within the CRAN package smfsb . As a reminder, the source code is \n \npfMLLik <- function (n, simx0, t0, stepFun, dataLik, data) \n{\n times = c(t0, as.numeric(rownames(data)))\n deltas = diff(times)\n return(function(...) {\n  xmat = simx0(n, t0, ...)\n  ll = 0\n  for (i in 1:length(deltas)) {\n   xmat = t(apply(xmat, 1, stepFun, t0 = times[i], deltat = deltas[i], ...))\n   w = apply(xmat, 1, dataLik, t = times[i + 1], y = data[i,], log = FALSE, ...)\n   if (max(w) < 1e-20) {\n    warning(\"Particle filter bombed\")\n    return(-1e+99)\n   }\n   ll = ll + log(mean(w))\n   rows = sample(1:n, n, replace = TRUE, prob = w)\n   xmat = xmat[rows, ]\n  }\n  ll\n })\n}\n \nThe function itself doesn\u2019t actually run a particle filter, but instead returns a function closure which does (see the previous post for a discussion of lexical scope and function closures in R). There are obviously several different steps within the particle filter, and several of these are amenable to parallelisation. However, for complex models, forward simulation from the model will be the rate-limiting step, where the vast majority of CPU cycles will be spent. Line 9 in the above code is where forward simulation takes place, and in particular, the key function call is the apply call: \n \napply(xmat, 1, stepFun, t0 = times[i], deltat = deltas[i], ...)\n \nThis call applies the forward simulation algorithm stepFun to each row of the matrix xmat independently. Since there are no dependencies between the function calls, this is in principle very straightforward to parallelise on multicore hardware. \n Multicore support in R \n I\u2019m writing this post on a laptop with an Intel i7 quad core chip, running the 64 bit version of Ubuntu 11.10. R has support for multicore processing on this platform \u2013 it is just a simple matter of installing the relevant packages. However, things are changing rapidly regarding multicore support in R right now, so YMMV. Ubuntu 11.10 has R 2.13 by default, but the multicore support is slightly different in the recently released R 2.14. I\u2019m still using R 2.13. I may update this post (or comment) when I move to R 2.14. The main difference is that the package multicore has been replaced by the package parallel . There are a few other minor changes, but it should be easy to adapt what is presented here to 2.14. \n There is a new O\u2019Reilly book called Parallel R . I\u2019ve got a copy of it. It does cover the new parallel package in R 2.14, as well as other parallel R topics, but the book is a bit light weight, to say the least, and I reviewed it on this blog. Please read my review for further details before you buy it. \n If you haven\u2019t used multicore in R previously, then \n \ninstall.packages(c(\"multicore\",\"doMC\"))\n \nshould get you started (again, I\u2019m assuming that your R version is strictly < 2.14). You can test it has worked with: \n \nlibrary(multicore)\nmulticore:::detectCores()\n \nWhen I do this, I get the answer 8 (I have 4 cores, each of which is hyper-threaded). To begin with, I want to tell R to use just 4 process threads, and I can do this with \n \nlibrary(doMC)\nregisterDoMC(4)\n \nReplacing the second line with registerDoMC() will set things up to use all detected cores (in my case, 8). There are a couple of different strategies we could use to parallelise this. One strategy for parallelising the apply call discussed above is to be to replace it with a foreach / %dopar% loop. This is best illustrated by example. Start with line 9 from the function pfMLLik : \n \nxmat = t(apply(xmat, 1, stepFun, t0 = times[i], deltat = deltas[i], ...))\n \nWe can produce a parallelised version by replacing this line with the following block of code: \n \nres=foreach(j=1:dim(xmat)[1]) %dopar% {\n stepFun(xmat[j,], t0 = times[i], deltat = deltas[i], ...)\n}\nxmat=t(sapply(res,cbind))\n \nEach iteration of the foreach loop is executed independently (possibly using multiple cores), and the result of each iteration is returned as a list, and captured in res . This list of return vectors is then coerced back into a matrix with the final line. \n In fact, we can improve on this by using the .combine argument to foreach , which describes how to combine the results from each iteration. Here we can just use rbind to combine the results into a matrix, using: \n \nxmat=foreach(j=1:dim(xmat)[1], .combine=\"rbind\") %dopar% {\n stepFun(xmat[j,], t0 = times[i], deltat = deltas[i], ...)\n}\n \nThis code is much neater, and in principle ought to be a bit faster, though I haven\u2019t noticed much difference in practice. \n In fact, it is not necessary to use the foreach construct at all. The multicore package provides the mclapply function, which is a multicore version of lapply . To use mclapply (or, indeed, lapply ) here, we first need to split our matrix into a list of rows, which we can do using the split command. So in fact, our apply call can be replaced with the single line: \n \nxmat=t(sapply(mclapply(split(xmat,row(xmat)), stepFun, t0=times[i], deltat=deltas[i], ...),cbind))\n \nThis is actually a much cleaner solution than the method using foreach , but it does require grokking a bit more R. Note that mclapply uses a different method to specify the number of threads to use than foreach/doMC . Here you can either use the named argument to mclapply , mc.cores , or use options() , eg. options(cores=4) . \n As well as being much cleaner, I find that the mclapply approach is much faster than the foreach/dopar approach for this problem. I\u2019m guessing that this is because foreach doesn\u2019t pre-schedule tasks by default, whereas mclapply does, but I haven\u2019t had a chance to dig into this in detail yet. \n A parallelised particle filter \n We can now splice the parallelised forward simulation step (using mclapply ) back into our particle filter function to get: \n \nrequire(multicore)\npfMLLik <- function (n, simx0, t0, stepFun, dataLik, data) \n{\n times = c(t0, as.numeric(rownames(data)))\n deltas = diff(times)\n return(function(...) {\n  xmat = simx0(n, t0, ...)\n  ll = 0\n  for (i in 1:length(deltas)) {\n  xmat=t(sapply(mclapply(split(xmat,row(xmat)), stepFun, t0=times[i], deltat=deltas[i], ...),cbind))\n   w = apply(xmat, 1, dataLik, t = times[i + 1], y = data[i,], log = FALSE, ...)\n   if (max(w) < 1e-20) {\n    warning(\"Particle filter bombed\")\n    return(-1e+99)\n   }\n   ll = ll + log(mean(w))\n   rows = sample(1:n, n, replace = TRUE, prob = w)\n   xmat = xmat[rows, ]\n  }\n  ll\n })\n}\n \nThis can be used in place of the version supplied with the smfsb package for slow simulation algorithms running on modern multicore machines. \n There is an issue regarding Monte Carlo simulations such as this and the multicore package (whether you use mclapply or foreach/dopar ) in that it adopts a \u201cdifferent seeds\u201d approach to parallel random number generation, rather than a true parallel random number generator. This probably isn\u2019t worth worrying too much about now, since it is fixed in the new parallel package in R 2.14, but is something to be aware of. I discuss parallel random number generation issues in Wilkinson (2005) . \n Granularity \n The above code is now a parallel particle filter, and can now be used in place of the serial version that is part of the smfsb package. However, if you try it out on a simple example, you will most likely be disappointed. In particular, if you use it for the pMCMC example discussed in the previous post , you will see that the parallel version of the example actually runs much slower than the serial version (at least, it does for me). However, that is because the forward simulator stepFun , used in that example, was actually a very fast simulation algorithm, stepLVc , written in C. In this case, the overhead of setting up and closing down the threads, and distributing the tasks, and collating the results from the worker threads back in the master thread, etc., outweighs the advantage of running the individual tasks in parallel. This is why parallel programming is difficult. What is needed here is for the individual tasks to be sufficiently computationally intensive that the overheads associated with parallelisation are easily outweighed by the ability to run the tasks in parallel. In the context of particle filtering, this is particularly problematic, as if the forward simulator is very slow, running a reasonable particle filter is going to be very, very slow, and then you probably don\u2019t want to be working in R anyway\u2026 Parallelising a particle filter written in C using MPI is much more likely to be successful, as it offers much more fine grained control of exactly how the tasks and processes are managed, but at the cost of increased development time. In a previous post I gave an introduction to parallel Monte Carlo with C and MPI , and I\u2019ve written more extensively about parallel MCMC in Wilkinson (2005) . It also looks as though the new parallel package in R 2.14 offers more control of parallelisation, so that also might help. However, if you are using a particle filter as part of a pMCMC algorithm, there is another strategy you can use at a higher level of granularity which might be useful even within R in some situations. \n Multiple particle filters and pMCMC \n Let\u2019s look again at the main loop of the pMCMC algorithm discussed in the previous post: \n \nfor (i in 1:iters) {\n\tmessage(paste(i,\"\"),appendLF=FALSE)\n\tfor (j in 1:thin) {\n\t\tthprop=th*exp(rnorm(p,0,tune))\n\t\tllprop=mLLik(thprop)\n\t\tif (log(runif(1)) < llprop - ll) {\n\t\t\tth=thprop\n\t\t\tll=llprop\n\t\t\t}\n\t\t}\n\tthmat[i,]=th\n\t}\n \nIt is clear that the main computational bottleneck of this code is the call to mLLik on line 5, as this is the call which runs the particle filter. The purpose of making the call is to obtain an unbiased estimate of marginal likelihood. However, there are plenty of other ways that we can obtain such estimates than by running a single particle filter. In particular, we could run multiple particle filters and average the results. So, let\u2019s look at how to do this in the multicore setting. Let\u2019s start by thinking about running 4 particle filters. We could just replace the line \n \nllprop=mLLik(thprop)\n \nwith the code \n \nllprop=0.25*foreach(i=1:4, .combine=\"+\") %dopar% {\n mLLik(thprop)\n }\n \nNow, there are at least 2 issues with this. The first is that we are now just running 4 particle filters rather than 1, and so even with perfect parallelisation, it will run no quicker than the code we started with. However, the idea is that by running 4 particle filters we ought to be able to get away with each particle filter using fewer particles, though it isn\u2019t trivial to figure out exactly how many. For example, averaging the results from 4 particle filters, each of which uses 25 particles is not as good as running a single particle filter with 100 particles. In practice, some trial and error is likely to be required. The second problem is that we have computed the mean of the log of the likelihoods, and not the likelihoods themselves. This will almost certainly work fine in practice, as the resulting estimate will in most cases be very close to unbiased, but it will not be exactly unbiased, as so will not lead to an \u201cexact\u201d approximate algorithm. In principle, this can be fixed by instead using \n \nres=foreach(i=1:4) %dopar% {\n mLLik(thprop)\n }\nllprop=log(mean(sapply(res,exp)))\n \nbut in practice this is likely to be subject to numerical underflow problems, as it involves manipulating raw likelihood values, which is generally a bad idea. It is possible to compute the log of the mean of the likelihoods in a more numerically stable way, but that is left as an exercise for the reader, as this post is way too long already\u2026 However, one additional tip worth mentioning is that the foreach package includes a convenience function called times for situations like the above, where the argument is not varying over calls. So the above code can be replaced with \n \nres=times(4) %dopar% mLLik(thprop)\nllprop=log(mean(sapply(res,exp)))\n \nwhich is a bit cleaner and more readable. \n Using this approach to parallelisation, there is now a much better chance of getting some speedup on multicore architectures, as the granularity of the tasks being parallelised is now much larger. Consider the example from the previous post, where at each iteration we ran a particle filter with 100 particles. If we now re-run that example, but instead use 4 particle filters each using 25 particles, we do get a slight speedup. However, on my laptop, the speedup is only around a factor of 1.6 using 4 cores, and as already discussed, 4 filters each with 25 particles isn\u2019t actually quite as good as a single filter with 100 particles anyway. So, the benefits are rather modest here, but will be much better with less trivial examples (slower simulators). For completeness, a complete runnable demo script is included after the references. Also, it is probably worth emphasising that if your pMCMC algorithm has a short burn-in period, you may well get much better overall speed-ups by just running parallel MCMC chains . Depressing, perhaps, but true. \n References \n \n McCallum, E., Weston, S. (2011) Parallel R , O\u2019Reilly. \n Wilkinson, D. J. (2005) Parallel Bayesian Computation , Chapter 16 in E. J. Kontoghiorghes (ed.) Handbook of Parallel Computing and Statistics , Marcel Dekker/CRC Press, 481-512. \n Wilkinson, D. J. (2011) Stochastic Modelling for Systems Biology, second edition , Boca Raton, Florida: Chapman & Hall/CRC Press. \n \n Demo script \n \nrequire(smfsb)\ndata(LVdata)\n\nrequire(multicore)\nrequire(doMC)\nregisterDoMC(4)\n\n# set up data likelihood\nnoiseSD=10\ndataLik <- function(x,t,y,log=TRUE,...)\n{\n\tll=sum(dnorm(y,x,noiseSD,log=TRUE))\n\tif (log)\n\t\treturn(ll)\n\telse\n\t\treturn(exp(ll))\n}\n# now define a sampler for the prior on the initial state\nsimx0 <- function(N,t0,...)\n{\n\tmat=cbind(rpois(N,50),rpois(N,100))\n\tcolnames(mat)=c(\"x1\",\"x2\")\n\tmat\n}\n# convert the time series to a timed data matrix\nLVdata=as.timedData(LVnoise10)\n# create marginal log-likelihood functions, based on a particle filter\n\n# use 25 particles instead of 100\nmLLik=pfMLLik(25,simx0,0,stepLVc,dataLik,LVdata)\n\niters=1000\ntune=0.01\nthin=10\nth=c(th1 = 1, th2 = 0.005, th3 = 0.6)\np=length(th)\nll=-1e99\nthmat=matrix(0,nrow=iters,ncol=p)\ncolnames(thmat)=names(th)\n# Main pMCMC loop\nfor (i in 1:iters) {\n\tmessage(paste(i,\"\"),appendLF=FALSE)\n\tfor (j in 1:thin) {\n\t\tthprop=th*exp(rnorm(p,0,tune))\n\t\tres=times(4) %dopar% mLLik(thprop)\n\t\tllprop=log(mean(sapply(res,exp)))\n\t\tif (log(runif(1)) < llprop - ll) {\n\t\t\tth=thprop\n\t\t\tll=llprop\n\t\t\t}\n\t\t}\n\tthmat[i,]=th\n\t}\nmessage(\"Done!\")\n# Compute and plot some basic summaries\nmcmcSummary(thmat)"], "link": "http://darrenjw.wordpress.com/2011/12/29/parallel-particle-filtering-and-pmcmc-using-r-and-multicore/", "bloglinks": {}, "links": {"http://www.amazon.com/": 3, "http://feeds.wordpress.com/": 1, "http://darrenjw.wordpress.com/": 8, "http://www.ac.uk/": 4}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["Introduction \n R is different to many \u201ceasy to use\u201d statistical software packages \u2013 it expects to be given commands at the R command prompt. This can be intimidating for new users, but is at the heart of its power. Most powerful software tools have an underlying scripting language. This is because scriptable tools are typically more flexible, and easier to automate, script, program, etc. In fact, even software packages like Excel or Minitab have a macro programming language behind the scenes available for \u201cpower users\u201d to exploit. \n Programming from the ground up \n It is natural to want to automate (repetitive) tasks on a computer, to automate a \u201cwork flow\u201d. This is especially natural for computational tasks, as all software tools are built from programming language components, anyway. In R, you do stuff by executing a sequence of commands. By putting a bunch of commands one after another into a text file, we can source the file, and script R. Scripting is the simplest form of programming \u2013 automating a sequence of tasks. Indeed, in Unix (including Linux and MacOS), we can put a bunch of Unix shell commands together in a shell script . In Windows, you can put a bunch of terminal commands together in a batch file . \n Next, one can add in simple control structures, to support looping , branching and conditional execution. Looping allows repetition of very similar tasks. Branching and conditional execution allow decisions to be made depending on what has already happened. Most scripting languages support simple control structures \u2013 this allows carrying out of tasks which we could do in principle, but perhaps not in practice, due to the laborious and repetitive nature of some work-flows. We can go a long way with this, but\u2026 \n Although scripting is a simple form of programming, it isn\u2019t \u201creal\u201d programming, or software engineering . Software engineering is about developing flexible, modular, robust, re-usable, generic program components, and using them to build large, complex software systems \u2013 modularity is absolutely key here. Functions and procedures are a first step towards introducing modularity, allowing the development of \u201creal\u201d software. Proper support for these tends to distinguish \u201creal\u201d programming languages from scripting languages (though many modern \u201cscripting\u201d languages have at least some limited support, and the distinction between scripting languages and \u201creal\u201d languages is now very blurred). \n Functions and procedures \n Procedures (or subroutines ) are re-usable pieces of code which can be called from other pieces of code when needed. They may be provided with inputs, but do not have to be. They are usually called for their \u201cside-effects\u201d, such as doing plots, changing global variables, or reading/writing data to/from disk. \n Functions are also re-usable pieces of code, but are mainly used to obtain a return-value that is computed on the basis of the given inputs. \u201cPure\u201d functions do not have any side-effects. Functions and procedures may be combined in a hierarchical way to build large, complex algorithms from much simpler modular components. Note that many languages (including R), do not make a distinction between functions and procedures in the syntax of the language, but conceptually the distinction is really quite important. \n Variable scope \n Almost all programming languages allow the definition of variables which are labels or tags representing or pointing at some value that may be defined and re-defined at run-time. In most modern programming languages, functions can define local variables which can be used in addition to any inputs (formal parameters) of the function \u2013 these are very important for the development of modular, re-usable code components. In particular, they help to avoid unanticipated name clashes in the global name-space. If a function refers to a variable which is neither a formal parameter nor a local variable, then a rule is needed to find which (if any) variable with that label is in scope for the function, so that the program can know what value to use. \n Dynamic scope \n Under dynamic scope, if an \u201cunknown\u201d variable is referred to in a function, the idea is to use the version of the variable that is in scope at the time that the function was called (and apply this rule recursively) \u2013 this is the scoping rule used by the S-PLUS implementation of the S language. Dynamic scope was common among early dynamic programming languages \u2013 including early implementations of LISP (and is still used in Emacs LISP), as it was quite intuitive and natural to implement using a stack-based approach similar to the stack-based approach to passing variables in and out of subroutines commonly used by machine code and assembly programmers. \n Despite being intuitively appealing, at least initially, there are a number of problems with dynamic scope in practice. In particular, we can\u2019t really know by code inspection whether or not a given section of code will run in all situations without actually running the code, as we can\u2019t know whether all variable bindings will resolve correctly. This is an issue even for dynamic languages, but is particularly problematic for strongly typed compiled languages, as it becomes difficult for the compiler to figure out the types of all variables correctly and therefore generate the appropriate byte-code. It is also very difficult for a function to have associated state \u2013 to do this, you must somehow get state variables into global name-space where they then become vulnerable to masking and name clashes. See the Wikipedia page on scope for further details. \n Lexical scope \n Under lexical scoping rules, if an \u201cunknown\u201d variable is referred to in a function, the idea is to use the version that is \u201cin scope\u201d in the enclosing piece of code (and apply this rule recursively) \u2014 this is the scoping rule used by R (as R is built on top of a Scheme interpreter, a LISP derivative which emphasises lexical scope). Variable bindings can be all resolved, checked and verified at compile-time \u2013 this is safer, and in many other ways better. Most modern languages adopt lexical scoping, including most functional languages, such as LISPs (including LISP-STAT) and derivatives. In fact, I first read about lexical scope, function closures and their use in statistical computing in Luke Tierney\u2019s LISP-STAT book (Tierney, 1990) in the early 1990s. That book was published over 20 years ago, so it just goes to show that there is nothing new about these functional programming approaches. In fact, although Tierney\u2019s book describes a now obsolete system, I would nevertheless recommend reading it if you can find a copy, as I think it is still one of the best books on statistical computing ever written. It really puts the recent glut of horrible R-themed books to shame! \n Given that R has been lexically scoped and has supported function closures since day one, it is reasonable to wonder why this programming style is not used more widely in R code. I think it is the difference in scoping rules between S-PLUS and R that has led to a fear of developing any R code which relies on non-local scoping rules. Certainly, in the early days of R, I would use S-PLUS at work and R at home, and I would want my code to work in exactly the same in both places! This is a shame, as lexical scoping is very powerful, and exploited widely in functional programming styles. The use of lexical scope and function closures in R is described quite nicely in Gentleman (2008) , along with many other things. \n To make sure that the concepts are clear, inspect the following piece of code and figure out what the result of the final function call will be. The answer is given below the code, so try not to peek before reading on\u2026 \n \na=1\nb=2\nf<-function(x)\n{\n a*x + b\n}\ng<-function(x)\n{\n a=2\n b=1\n f(x)\n}\ng(2)\n \n No, really, try and figure it out before reading on for the answer! Understanding this example is key to understanding the difference between lexical and dynamic scope. Clearly the obvious answers are 4 and 5. If you didn\u2019t get one of those, go back and try again! So, one of those is the result you get in a dynamically scoped language like S-PLUS, and the other is the result that you get in a lexically scoped language like R. But which is which? Many people when asked what this code does give the answer 5. This is the result for a dynamically scoped language. It is not the answer you get in R. In R, you get the answer 4. This is because f() was defined in the global environment, so it is the global bindings of a and b which count. Although the function g() defines its own local bindings for a and b , these have no impact on the global bindings, and are simply not relevant to the evaluation of f() . \n Function closures \n Some languages (including LISPs and derivatives such as Scheme, Python, and R) have functions as \u201cfirst class objects\u201d, which means that a function is able to return as its value another function. If the function ( fChild ) returned by the function ( fParent ) refers to variables not local to fChild , then scoping rules must apply to the resolution of the variable binding. If the language is lexically scoped, then the binding is determined by the variables in scope within the function fParent . The function fChild therefore has an associated environment , which provides bindings for non-local variable references \u2013 this allows maintaining of state. A function together with its environment is referred to as a function closure , and is a very powerful programming tool. Below is some more code to help illustrate what is going on. Again, try to figure out the result of the final function call before reading on for the answer and explanation\u2026 \n \na=1\nb=2\nf<-function(a,b)\n{\n return( function(x) {\n a*x + b\n })\n}\ng=f(2,1)\ng(2)\n \n Here, the function g() , together with its associated environment, is referred to as a function closure . See the Wikipedia page for closure for further details. So, what is the result of calling g(2) in this case? Again, some people get this wrong, and give the answer 4. This isn\u2019t what you get in R \u2013 in R you get 5, again due to lexical scope. The point is that the function g() is created inside f() , and so it is the variable bindings in scope within f() at the time g() was created which matter. Since f() has a and b as formal arguments, these mask the global variables of the same name, so it is the 2 and 1 that are passed into f() to create g() which matter in the evaluation of g() . This is why function closures are so powerful. They are not simply functions, they are functions together with an associated environment, and the associated environment allows function closures to have associated state. Here the state corresponds to the values of a and b that were used in the creation of g() , but in principle the state can be essentially any data structure. \n Function closures for scientific computing \n Function closures have numerous important applications in a variety of problems in scientific computing that involve dealing in some way with the \u201cfunction environment problem\u201d. There is quite a nice discussion of this issue in Oliveira and Stewart (2006) , in the context of several strongly typed compiled languages. Consider, for example, a function that will numerically integrate a univariate function using (say) the trapezium rule. This integration function might expect that you pass in the function to be integrated, together with the limits of integration, and possibly a step size. Most likely this integration function will expect that the function passed in is univariate. However, in practice many functions have additional parameters (eg. the straight line example, above, which was a function of x , but depending on additional parameters a and b ). This problem is solved by passing in a univariate function closure that contains the necessary environment to evaluate this univariate function correctly. Similar considerations apply for functions that carry out optimisation, solve ODEs by passing in the RHS, etc. \n The smfsb R package \n The second edition of my textbook, Stochastic Modelling for Systems Biology, has recently been published ( Wilkinson, 2011 ). The second edition has an associated R package, smfsb , available from CRAN \u2013 I gave a tutorial introduction in a previous post. The code makes extensive use of lexical scope and function closures, precisely to solve the function environment problem\u2026 \n References \n \n Oliveira, S, Stewart, D.E. (2006) Writing scientific software , CUP. \n Gentleman, R. (2008) R Programming for Bioinformatics , Chapman & Hall/CRC Press. \n Tierney, L. (1990) LISP-STAT , Wiley. \n Wilkinson (2011), Stochastic Modelling for Systems Biology, second edition , Chapman & Hall/CRC Press."], "link": "http://darrenjw.wordpress.com/2011/11/23/lexical-scope-and-function-closures-in-r/", "bloglinks": {}, "links": {"http://amzn.to/": 6, "http://feeds.wordpress.com/": 1, "http://darrenjw.wordpress.com/": 1, "http://en.wikipedia.org/": 2, "http://www.ac.uk/": 2}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["Introduction \n This is the first book review I\u2019ve done on this blog, and I don\u2019t intend to make it a regular feature, but I ordered a copy of \u201cParallel R\u201d a few days ago. It arrived today, and I\u2019m quite disappointed with it, so I wanted to write a quick review to provide some additional information for people thinking of buying a copy. Just to be clear, the book is: \n \n McCallum, E., Weston, S. (2011) Parallel R , O\u2019Reilly. \n \n I generally like O\u2019Reilly books, and own a number of them. I use R a lot, I am very interested in parallel computing (traditionally using C and MPI ), and I\u2019ve dabbled a little with some parallel stuff in R, but don\u2019t consider myself to be an expert. In other words, people like me are probably the target audience for this book, and sure enough I have handed over some of my hard-earned cash to buy a copy. \n The main problem with the book is that it just doesn\u2019t feel finished. It seems as though the authors have rushed the text as quickly as possible and published it without any kind of critical review or reflection. It is very short \u2013 just 108 pages of the main text, and most annoyingly, doesn\u2019t have an index. This wouldn\u2019t be so much of a problem for an electronic version, but selling a technical computing book in dead tree format without an index is really unforgivable. All of my other O\u2019Reilly books contain a decent index, so I\u2019m just baffled as to why this one doesn\u2019t. It really feels like this is the first draft of a manuscript that you would circulate to a few friends and colleagues for comments and suggested improvements. There is the kernel of a decent book here, but most of the current material will be obsolete before a second edition could be put together and published, so the second edition will have to be a complete re-write. \n Chapter by chapter \n Chapter 1 \u2013 Getting started \u2013 5 pages \n A brief introduction to the rest of the book, and has a pointer to the companion website, parallelrbook.com (at the time of writing, it is empty\u2026). \n Chapter 2 \u2013 snow \u2013 30 pages \n This chapter is the most substantial, and arguably the best, chapter of the book. It provides a very reasonable introduction to the snow package for a simple network of workstations , for running embarrassingly parallel jobs on a cluster. \n Chapter 3 \u2013 multicore \u2013 13 pages \n This chapter provides a very brief and superficial introduction to the multicore package, for exploiting modern multicore hardware. It provides a very brief introduction to the high level API ( mclapply , pvec , parallel , collect ). Discussion of the low-level API is almost non-existent (an example function is given which uses some low-level calls). Also, there is no discussion here, or anywhere else, of the foreach package/function, or the doMC back-end. Unfortunately I couldn\u2019t verify this by checking the index (see above), but as there are only 100 or so pages, it didn\u2019t take that long to flick through them all to double-check\u2026 Now I can understand that the book will not cover all obscure parallel packages for R, but foreach/doMC ?! Missing from a book called \u201cParallel R\u201d? Seriously? It is all the more weird as one of the authors (Weston) is an author of foreach/doMC . Go figure\u2026 \n Chapter 4 \u2013 parallel \u2013 8 pages \n This chapter provides an even more brief introduction to the new parallel package for R 2.14. It should be noted that the book went to press before 2.14 was frozen for release, but the content that was there looked OK on the basis of a very quick skim. But at 8 pages, don\u2019t expect too much. \n Chapter 5 \u2013 A primer on MapReduce and Hadoop \u2013 8 pages \n A very brief introduction to the ideas behind MapReduce and Hadoop. Not actually anything to do with R, but necessary for the next chapter. \n Chapter 6 \u2013 R+Hadoop \u2013 18 pages \n I\u2019ve not worked through this chapter in detail, but it looks like a reasonable \u201cgetting started guide\u201d for using Hadoop with R. \n Chapter 7 \u2013 RHIPE \u2013 16 pages \n Again, I\u2019ve not studied this chapter carefully, but it seems like a reasonable introduction to the RHIPE package (it is a package to make it simpler to use R with Hadoop, by hiding Hadoop stuff from the R user). \n Chapter 8 \u2013 Segue \u2013 6 pages \n A very brief introduction to the segue package, which enables running jobs on Amazon\u2019s Elastic MapReduce service. \n Chapter 9 \u2013 New and upcoming \u2013 2 pages \n A very brief mention of some of the things that weren\u2019t covered in the book\u2026 foreach is mentioned here, but no example is given. \n Conclusion \n For people who have absolutely no idea about parallel computing in R, or about what different options are available, then this book does provide a useful overview, together with some simple examples to illustrate the ideas, and try out for themselves. It is generally very brief and superficial, there are some gaping holes, and much of the material will become obsolete very quickly. It is a shame that there is not more discussion of low level functions, or of parallel computing in anything other than a simple embarrassingly parallel context. Admittedly, if your job isn\u2019t embarrassingly parallel, you probably don\u2019t want to use R anyway, but some discussion would still have been nice. And did I mention that there is no index?! I did toy briefly with the idea of sending it back, but I\u2019m not going to. To be fair, there is quite a bit of useful information in the book, and I\u2019d like to work through the Hadoop chapters at some point. So in summary, it\u2019s OK, but don\u2019t expect to love it. \n References \n \n McCallum, E., Weston, S. (2011) Parallel R , O\u2019Reilly."], "link": "http://darrenjw.wordpress.com/2011/11/16/review-of-parallel-r-by-mccallum-and-weston/", "bloglinks": {}, "links": {"http://www.amazon.com/": 2, "http://feeds.wordpress.com/": 1, "http://parallelrbook.com/": 1, "http://darrenjw.wordpress.com/": 1}, "blogtitle": "Darren Wilkinson's research blog"}, {"content": ["In the previous post I gave a quick introduction to the CRAN R package smfsb , and how it can be used for simulation of Markov processes determined by stochastic kinetic networks. In this post I\u2019ll show how to use data and particle MCMC techniques in order to carry out Bayesian inference for the parameters of partially observed Markov processes. \n The simulation model and the data \n For this post we will assume that the smfsb package is installed and loaded (see previous post for details). The package includes the function stepLVc which simulates from the Markov transition kernel of a Lotka-Volterra process by calling out to some native C code for speed. So, for example, \n \nstepLVc(c(x1=50,x2=100),0,1)\n \nwill simulate the state of the process at time 1 given an initial condition of 50 prey and 100 predators at time 0, using the default rate parameters of the function, th = c(1, 0.005, 0.6) . The package also includes some data simulated from this model using these parameters, with and without added noise. The datasets can be loaded with \n \ndata(LVdata)\n \nFor simplicity, we will just make use of the dataset LVnoise10 in this post. This dataset is a multivariate time series consisting of 16 equally spaced observations on both prey and predators subject to Gaussian measurement error with a standard deviation of 10. We can plot the data with \n \nplot(LVnoise10,plot.type=\"single\",col=c(2,4))\n \ngiving: \n  \n The Bayesian inference problem is to see how much we are able to learn about the parameters which generated the data using only the data and our knowledge of the structure of the problem. There are many approaches one can take to this problem, but most are computationally intensive, due to the analytical intractability of the transition kernel of the LV process. Here we will follow Wilkinson (2011) and take a particle MCMC (pMCMC) approach, and specifically, use a pseudo-marginal \u201cexact approximate\u201d MCMC algorithm based on the particle marginal Metropolis-Hastings (PMMH) algorithm. I have discussed the pseudo-marginal approach , using particle filters for marginal likelihood estimation , and the PMMH algorithm in previous posts, so if you have been following my posts for a while, this should all make perfect sense\u2026 \n Particle filter \n One of the key ingredients required to implement the pseudo-marginal MCMC scheme is a (bootstrap) particle filter which generates an unbiased estimate of the marginal likelihood of the data given the parameters (integrated over the unobserved state trajectory). The algorithm was discussed in this post , and R code to implement this is included in the smfsb R package as pfMLLik . For reasons of numerical stability, the function computes and returns the log of the marginal likelihood, but it is important to understand that it is the actually likelihood estimate that is unbiased for the true likelihood, and not the corresponding statement for the logs. The actual code of the function is relatively short, and for completeness is given below: \n \npfMLLik <- function (n, simx0, t0, stepFun, dataLik, data) \n{\n times = c(t0, as.numeric(rownames(data)))\n deltas = diff(times)\n return(function(...) {\n  xmat = simx0(n, t0, ...)\n  ll = 0\n  for (i in 1:length(deltas)) {\n   xmat = t(apply(xmat, 1, stepFun, t0 = times[i], deltat = deltas[i], ...))\n   w = apply(xmat, 1, dataLik, t = times[i + 1], y = data[i,], log = FALSE, ...)\n   if (max(w) < 1e-20) {\n    warning(\"Particle filter bombed\")\n    return(-1e+99)\n   }\n   ll = ll + log(mean(w))\n   rows = sample(1:n, n, replace = TRUE, prob = w)\n   xmat = xmat[rows, ]\n  }\n  ll\n })\n}\n \nWe need to set up the prior and the data likelihood correctly before we can use this function, but first note that the function does not actually run a particle filter at all, but instead stores everything it needs to know to run the particle filter in the local environment, and then returns a function closure for evaluating the marginal likelihood at a given set of parameters. The resulting function (closure) can then be used to run a particle filter for a given set of parameters, simply by passing the required parameters into the function. This functional programming style is consistent with that used throughout the smfsb R package, and leads to quite simple, modular code. To use pfMLLik , we first need to define a function which evaluates the log-likelihood of an observation conditional on the true state, and another which samples from the prior distribution of the initial state of the system. Here, we can do that as follows. \n \n# set up data likelihood\nnoiseSD=10\ndataLik <- function(x,t,y,log=TRUE,...)\n{\n\tll=sum(dnorm(y,x,noiseSD,log=TRUE))\n\tif (log)\n\t\treturn(ll)\n\telse\n\t\treturn(exp(ll))\n}\n# now define a sampler for the prior on the initial state\nsimx0 <- function(N,t0,...)\n{\n\tmat=cbind(rpois(N,50),rpois(N,100))\n\tcolnames(mat)=c(\"x1\",\"x2\")\n\tmat\n}\n# convert the time series to a timed data matrix\nLVdata=as.timedData(LVnoise10)\n# create marginal log-likelihood functions, based on a particle filter\nmLLik=pfMLLik(100,simx0,0,stepLVc,dataLik,LVdata)\n \nNow the function (closure) mLLik will, for a given parameter vector, run a particle filter (using 100 particles) and return the log of the particle filter\u2019s unbiased estimate of the marginal likelihood of the data. It is then very easy to use this function to create a simple PMMH algorithm for parameter inference. \n PMMH algorithm \n Below is an algorithm based on flat priors and a simple Metropolis-Hastings update for the parameters using the function closure mLLik , defined above. \n \niters=1000\ntune=0.01\nthin=10\nth=c(th1 = 1, th2 = 0.005, th3 = 0.6)\np=length(th)\nll=-1e99\nthmat=matrix(0,nrow=iters,ncol=p)\ncolnames(thmat)=names(th)\n# Main pMCMC loop\nfor (i in 1:iters) {\n\tmessage(paste(i,\"\"),appendLF=FALSE)\n\tfor (j in 1:thin) {\n\t\tthprop=th*exp(rnorm(p,0,tune))\n\t\tllprop=mLLik(thprop)\n\t\tif (log(runif(1)) < llprop - ll) {\n\t\t\tth=thprop\n\t\t\tll=llprop\n\t\t\t}\n\t\t}\n\tthmat[i,]=th\n\t}\nmessage(\"Done!\")\n# Compute and plot some basic summaries\nmcmcSummary(thmat)\n \nThis will take a little while to run, but in the end should give a plot something like the following (click for full size): \n  \n So, although we should really run the chain for a bit longer, we see that we can learn a great deal about the parameters of the process from very little data. For completeness, a full runnable demo script is included below the references. Of course there are many obvious extensions of this basic problem, such as partial observation (eg. only observing the prey) and unknown measurement error. These are discussed in Wilkinson (2011) , and code for these cases is included within the demo(PMCMC) , which should be inspected for further details. \n Discussion \n At this point it is probably worth emphasising that there are other \u201clikelihood free\u201d approaches which can be taken to parameter inference for partially observed Markov process (POMP) models. Many of these are implemented in the pomp R package, also available from CRAN, by King et al (2008) . The pomp package is well documented, and has a couple of good tutorial vignettes which should be sufficient to get people started. The API of the package is rather cumbersome, but the algorithms appear to be quite robust. Approximate Bayesian computation (ABC) approaches are also quite natural for POMP models (see, for example, Toni et al (2009) ). This is because \u201cexact\u201d likelihood free procedures break down in the case of low/no measurement error or high-dimensional observations. There are some R packages for ABC, but I am not sufficiently familiar with them to be able to give recommendations. \n If one is able to move away from the \u201clikelihood free\u201d paradigm, it is possible to develop \u201cexact\u201d pMCMC algorithms which do not break down in challenging observation scenarios. The problem here is the intractability of the Markov transition kernel. In the case of nonlinear Markov jump processes, finding very generic solutions seems quite difficult, but for diffusion (approximation) processes based on stochastic differential equations, it seems to be possible to develop irreducible pMCMC algorithms which have very broad applicability \u2013 see Golightly and Wilkinson (2011) for further details of how such techniques can be used in the context of stochastic kinetic models similar to those considered in this post. \n References \n \n Golightly, A., Wilkinson, D. J. (2011) Bayesian parameter inference for stochastic biochemical network models using particle MCMC , Interface Focus , 1 (6):807-820. \n King, A.A., Ionides, E.L., & Breto, C.M. (2008) pomp : Statistical inference for partially observed Markov processes, CRAN . \n Toni, T., Welch, D., Strelkowa, N., Ipsen, A. & Stumpf, M. (2009) Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems , J. R. Soc. Interface 6 (31): 187-202. \n Wilkinson, D. J. (2011) Stochastic Modelling for Systems Biology, second edition , Boca Raton, Florida: Chapman & Hall/CRC Press. \n \n Demo script \n \nrequire(smfsb)\ndata(LVdata)\n\n# set up data likelihood\nnoiseSD=10\ndataLik <- function(x,t,y,log=TRUE,...)\n{\n\tll=sum(dnorm(y,x,noiseSD,log=TRUE))\n\tif (log)\n\t\treturn(ll)\n\telse\n\t\treturn(exp(ll))\n}\n# now define a sampler for the prior on the initial state\nsimx0 <- function(N,t0,...)\n{\n\tmat=cbind(rpois(N,50),rpois(N,100))\n\tcolnames(mat)=c(\"x1\",\"x2\")\n\tmat\n}\n# convert the time series to a timed data matrix\nLVdata=as.timedData(LVnoise10)\n# create marginal log-likelihood functions, based on a particle filter\nmLLik=pfMLLik(100,simx0,0,stepLVc,dataLik,LVdata)\n\niters=1000\ntune=0.01\nthin=10\nth=c(th1 = 1, th2 = 0.005, th3 = 0.6)\np=length(th)\nll=-1e99\nthmat=matrix(0,nrow=iters,ncol=p)\ncolnames(thmat)=names(th)\n# Main pMCMC loop\nfor (i in 1:iters) {\n\tmessage(paste(i,\"\"),appendLF=FALSE)\n\tfor (j in 1:thin) {\n\t\tthprop=th*exp(rnorm(p,0,tune))\n\t\tllprop=mLLik(thprop)\n\t\tif (log(runif(1)) < llprop - ll) {\n\t\t\tth=thprop\n\t\t\tll=llprop\n\t\t\t}\n\t\t}\n\tthmat[i,]=th\n\t}\nmessage(\"Done!\")\n# Compute and plot some basic summaries\nmcmcSummary(thmat)"], "link": "http://darrenjw.wordpress.com/2011/11/12/particle-filtering-and-pmcmc-using-r/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://tinyurl.com/": 1, "http://darrenjw.wordpress.com/": 7, "http://www.ac.uk/": 2, "http://cran.r-project.org/": 2, "http://dx.doi.org/": 4}, "blogtitle": "Darren Wilkinson's research blog"}]