[{"blogurl": "http://blog.vinux.in\n", "blogroll": [], "title": "Fiddling with data and code"}, {"content": ["ggplot2 is one of the most elegant R package for data analysis and visualization.\u00a0 Recently I gave a tutorial on ggplot2 package.\u00a0 You could find my ggplot2 notes here(click the image below). \n  \n You could find my presentation slide below. The presentation made in html5. Use your left/right key to go through the presentation. \n  \n The post R Graphics with ggplot2 appeared first on Fiddling with data and code ."], "link": "http://blog.vinux.in/r-graphics-with-ggplot2/", "bloglinks": {}, "links": {"http://blog.vinux.in": 1, "http://blog.vinux.in/": 1, "http://viz.vinux.in/": 2}, "blogtitle": "Fiddling with data and code"}, {"content": ["The majority of the world\u2019s problem deal with directly or indirectly some kind of optimisation. Instance of optimisation of resources or utility function can be seen our daily life. Here I am talking about standard optimisation problem in statistics, maximum likelihood estimate. This blog is a small episode of my recent work. I used R for this optimisation exercise. \n There are packages available for optimisation in R. The mostly used are: optim() and\u00a0 optimize() are in stats package. A dedicated function mle() available in stats4 package. This is very useful most of the mean part modeling(eg: glm). What we need for this optimisation is to prepare function for the (log)likelihood and gradient (or hessian). We can also specify the algorithm(methods) as any of the following: \u201cNelder-Mead\u201d, \u201cBFGS\u201d, \u201cCG\u201d, \u201cL-BFGS-B\u201d, \u201cSANN\u201d, and \u00a0 \u201cBrent\u201d. This opimisation output give the necessary information for the model estimation. \n The case I was doing is an extension of bivariate garch model. It includes more than 20 parameters. let me focus pure garch model, garch(1,1), as a prototype to explain the algorithm. Here\u00a0the model equation is in the variance part. Also the parameter has constraint to ensure the positive variance. The \u00a0 model equation is given below. \n \n \n The above discussed optimisation function may work for garch(1,1) estimation. But as the number of parameters increases there is not much use with optim functions. The main reason for failing the algorithm is the nonconvex surface with sensitive boundaries. So the solution to write the codes for newton raphson algorithm or BHHH or other modified algorithms. ## Likelihood function for GARCH(1,1)\nPARAM &lt;- c(1,0,0)\nloglik &lt;- function(par=PARAM,xt)\n {\n N &lt;- length(xt) \n omega &lt;- par[1] \n alpha &lt;- par[2]\n beta &lt;- par[3]\n\n h &lt;- omega/(1-alpha-beta)\n e &lt;- xt[1]\n logl &lt;- - 1/2* log(2*pi*h) -e^2/(2*h) \n for(t in 2:N)\n  {  \n  e[t] &lt;- xt[t]\n  h[t] &lt;- omega+ alpha*e[t-1]^2+beta*h[t-1]\n  logl &lt;- logl - 1/2* log(2*pi*h[t]) -e[t]^2/(2*h[t])\n  }\n return(logl)  \n } I wrote BHHH algorithm for my model. I was not able solve it fully, it was not giving the optimum. The reason is the direction for each iteration when it reaches sensitive area is not properly scaled. ie. either it overshoot in some dimension or there is no significant improvement in any direction. So to get appropriate multiplier on optimal direction fails here. ## This function gives the improvement direction: inv(HESSIAN)*Grad\n## Here method of scoring as hessian[ as per Green(2003) ]\ndirection &lt;- function(par=PARAM,xt,vlist=1:3)\n {\n N &lt;- length(xt) \n omega &lt;- par[1] \n alpha &lt;- par[2]\n beta &lt;- par[3]\n\n h &lt;- omega/(1-alpha-beta)\n e &lt;- xt[1]\n logl &lt;- - 1/2* log(2*pi*h) -e^2/(2*h)\n grad &lt;- mat.or.vec(1,3)\n dhdv &lt;- mat.or.vec(N,3)\n hess &lt;- mat.or.vec(3,3)\n\n dhdv[1,] &lt;-c(1,0,h)\n\n for(t in 2:N)\n  {  \n  e[t] &lt;- xt[t]\n  h[t] &lt;- omega+ alpha*e[t-1]^2+beta*h[t-1]\n  dhdv[t,] &lt;- c(1,e[t-1]^2,h[t-1])+ beta*dhdv[t-1]\n  grad &lt;- grad + - (1/(2*h[t]) - e[t]^2/(2*h[t]^2))*dhdv[t,]\n  hess &lt;- hess + 1/(2*h[t]^2)* dhdv[t,] %*% t(dhdv[t,])  \n  }\n  dir &lt;- solve(hess[vlist,vlist])%*% grad[vlist]\n\n return(dir)\n } Now I will discuss about the heuristic part of the modification in the algorithm. In this heuristic, certain number of dimensions are randomly selected and perturbed. Remaining dimensions are left unchanged. This way we can overcome the scaling issue. This given me optimal solution but it is not optimised in the execution perspective. It take slightly more time.( I compared few simple models with my heuristic and existing garch estimation.) ## Estimation using heuristic modification of newton raphson method\nestim &lt;- function(par3=NULL,xt,vlist=1:3,max.iter=200,tol=0.1^8)\n {\n np &lt;- length(vlist) ## number of parameters\n iter &lt;- 0\n madechange &lt;- TRUE\n cnt &lt;- 0\t\n if(is.null(par3)) {\n  init &lt;- runif(3,0,.1)\n  init[1] &lt;- var(xt) \n } else init &lt;- par3  \n\n ## Converting the parameters for the full model\n pm &lt;- function(spar)\n  {\n  par &lt;- PARAM\n  par[vlist] &lt;- spar\n  return(par)\n  }\n ## Creating function for perturbed likelihood\n fl2 &lt;- function(d){ l2 &lt;- -loglik(pm(fest+d*incr),xt); ifelse(is.nan(l2),-10^10,l2)  }\n ## feasibility check for the parameter: Finiteness and positivity\n fcheck &lt;- function(par){ (!is.nan(loglik(pm(par),xt))) &amp;(sum(sign(pm(par)[c(2,3)]&lt;0))==0) }\n ## this function check the parameter is improved after modification or not\n is.improve &lt;- function(par){return(loglik(pm(par),xt) &gt;= loglik(pm(fest),xt)) }\n ## Heuristic selection of improved feasible multiplier\n optd &lt;- function(d){\n  p1 &lt;- fest+d*incr\n  p2 &lt;- fest-d*incr\n  c1 &lt;- fcheck(p1) &amp; is.improve(p1)\n  c2 &lt;- fcheck(p2) &amp; is.improve(p2)\n  if(c1&gt;0) d else if(c1==0 &amp;c2&gt;0) -d else { if(abs(d) &lt;10^-6 ) optd(0) else optd(d/2) }\n } \n\n fest &lt;- init[vlist] ## first estimate\n if(!fcheck(fest)) stop(\"\\n Initial point is not feasible\")\n\n while(madechange &amp; (iter tol) cnt=0 else cnt &lt;- cnt+1\n  madechange &lt;- (cnt &lt; 10)\n  fest &lt;- nest\n  }\n return(pm(nest))\n } This approach looks very silly for regression type problems. But in garch like process tweaking like this is very much require. I am still looking for the better way. Anyway the blogging helped me to relook at my approach again. Please let me know if you have any suggestions. \n The post A heuristic enhancement of optimisation algorithm appeared first on Fiddling with data and code ."], "link": "http://blog.vinux.in/a-heuristic-modification-of-optimisation/", "bloglinks": {}, "links": {"http://blog.vinux.in": 1, "http://blog.vinux.in/": 1}, "blogtitle": "Fiddling with data and code"}, {"content": ["I cannot believe that I lived without emacs!\u00a0 Now I use emacs more than any application. The usage\u00a0 of emacs is going to increase as the days go. A non-emacs user\u00a0 may think that why an editor should get a credit like \u201cemacs is an integral part of my life\u201d. Emacs users may find nothing unusual about my statement. Well! I will brief my emacs story. I will start with my programming life. \n \n \n My programming life \n \n \n My programming life is summarised as below. Even though I learnt multiple languages I have included the languages where I spend more than 30 days(or more than 200 hours) roughly. \n \n 2000-2001:\u00a0 Basic \u00a0and\u00a0 FORTRAN\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ->Windows days \n \n 2002: C and C++\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  ->Windows days  \n 2003: VC++ and OpenGL\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ->Windows days \n 2004: C and C++\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ->Here onwards Linux is my favourite desktop OS \n 2005-2006: JAVA, VB and VFoxpro ( I don\u2019t think anybody uses this one) \n 2007-2009: SAS and Shell Script \n 2010-2011: R \n 2012 : R,\u00a0 Python \u00a0and\u00a0 elisp \n \n \n \n Out of this I consider 2002, 2003 and 2012 are my best years of programming. I should consider my first few years were my golden days of programming. Those days I didn\u2019t had to do anything (No responsibility) other than coding and some mathematics. At that time I never understood the importance of skill of programming. I treated programming as one subject of mathematics. I did programming mainly for fun. I thank my best friend Jais for my consistent interest in coding. We had lot of intellectual discussion and debates. We spent lot of hours to create graphics applications, screen savers, hacking, etc. Now after a long break I am back to real programming lifestyle. \n When I worked for a software company(2005-2006) I did more cut-copy-paste and trial & error methods. This way I never learnt anything deep. Money and movies where other distractions. Weekends were mainly window shopping and parties. The main reason why I didn\u2019t do any real programming was I didn\u2019t get a good friend who is crazy enough on geek stuff. \n \n \n What motivated to learn Emacs \n \n I always like simple applications. When I say simple it means quick execution, transparent functionality and customisable. I really like vi/vim editor and it was my favourite editor till 2011(I don\u2019t consider Windows era). I tried emacs in 2004 and I didn\u2019t like it. It was nightmare at that time because to remember hell number of commands. Again I tried emacs in 2010-2011. This time the motivation was\u00a0 emacs\u00a0 topped as the favourite editor for most of the stat tech guys. But I didn\u2019t learn anything other than C-x C-f and C-x C-c. Now what happened? I was searching a good R editor. I got to know about ESS and ESS is the first reason to learn emacs. So I started using emacs only for R coding. Then I started exploring shortcuts. I found it is very easy to customise. I have copied few shortcuts and started customising my own. Fortunately\u00a0 stackoverflow \u00a0is very helpful on this. Some of the customisations are \n \n auto complete \n execution shortcuts \n folding mode \n \n First two features are very common ins standard editor but, not the third one. I have seen folding mode in Rstudio. After experimenting on customisation I started learning emacs lisp. This made me to think one editor for all programming. \n The main reason to learn emacs is\u00a0 Org-mode . I guess this should be one of the key addon to emacs. Like ggplot2 in R. The advantage of org-mode is Integrating emacs with: \n \n task management \n connect with other applications( HTML, latex beamer, google calendar) \n table creation \n Good for draft/notes preparation, etc. \n \n I am mainly a statistician and coding is my next role. But still for all my data modelling and technical report I depend on emacs.\u00a0 Now I use emacs for latex, python, R, daily planning and blogs(including this one). \n One of the main reason I switched to linux is constant learning. Emacs will ensure the constant learning at best. \n Emacs is good enough for preparing daily planning, latex, R, Python, and other programming. With just a console I can use emacs and do most of my work. I am exploring on other applications emacs like reading mail,news feeds,twitting, and other automation. Already the .el files are available on above but, looking for perfect one. I started exploring on elisp. I will post few of my elisp experiment in future. \n \n \n The post Why Emacs is important to me? : ESS and org-mode appeared first on Fiddling with data and code ."], "link": "http://blog.vinux.in/why-emacs-is-important-to-me-ess-and-org-mode/", "bloglinks": {}, "links": {"http://blog.vinux.in": 1, "https://plus.google.com/": 1, "http://blog.vinux.in/": 1, "http://stackoverflow.com/": 1, "http://orgmode.org/": 1, "http://ess.r-project.org/": 1}, "blogtitle": "Fiddling with data and code"}, {"content": ["Hi! This is Richie again after a long time, precisely after 1 year, 1 month and 27 days. During the last one year I had very less free time compare to the previous years. Weekends, I was watching so many movies and occasionally, reading novels and stories. So never felt like writing a blog. This blog is mainly about a book I read few weeks ago. The book\u2019s name is the same as the title of the blog, written by Mark Haddon that won the 2003 Whitbread Book of the Year. \n \nLast Wednesday morning my bro called me and gave the idea of writing a blog about the book. At first I wondered, because I had given this book to him one month before and he said he had not found it interesting. \n The hero of the novel is Christopher, a 15-year-old autistic boy living in Swindon (somewhere in UK). The story is written in the first-person narrative of the boy. So I give all credit to the boy. I am not going to explain the story because it will be boring if you read the novel without suspense. Why I liked this book is because of a mathematical touch in the storyline and remarkable funny stuffs. When I read this novel I actually thought about my boyhood days and I found a minor similarity in my thinking and that of Christopher\u2019s. \n I will list out some of the interesting stuff i liked in the book \n Prime Numbers . He likes prime number( me too) and in this book the chapter are given in this order 2,3,5,7,11 and so on. He considers that prime numbers are like life \u201c even though they are logical, it\u2019s impossible to work out the rules\u201d. \n \n Monty Hall Problem. This is a famous puzzle problem. And he has used this example to shows that intuition can sometimes get things wrong. \n The problem is \n Suppose you\u2019re on a game show, and you\u2019re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what\u2019s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \u201cDo you want to pick door No. 2?\u201d Is it to your advantage to switch your choice? \n I heard this problem 1.5 years back. One of the professor in IISc asked me this question. I didn\u2019t know it was a famous question till I read this book. \n Sherlock homes. He likes sherlock homes stories(me too). This could be the reason for the title of the book( Because sherlock homes mentioned this sentence, \u201cTo the curious incident of the dog in the night-time.\u201d Book: \u201c The Memoirs of Sherlock Holmes \u201d Story: Silver Blaze ). This may be the reason he is pursing the mystery of the dead dog. He has mentioned that his favorite story is \u201c The hound of Baskervilles \u201d. And he has explained the clues and red herrings of this sherlock holmes book. \n Map. He draws map whenever he goto new places. I also do the similar thing. When I was in Kolkata, I had a big map of the city in the wall. And the same habit I continued in Hyd, Chennai and Bangalore. \n \n White lie. A white lie is not a lie at all. It is where you tell the truth but you do not tell all of the truth. He used this lie to his father many time in the book. Because his father don\u2019t like his detective attempt on dead dog. \n \n* * ** \n There are a couple of new stories I wanted to tell you. Once I finish all clint eastwood\u2019s movies, I will start writing those. Till that time. Bye bye \n \nExternal links \n Monty Hall problem \n Mathematical Association of America book review \n The post The Curious Incident of the Dog in the NightTime appeared first on Fiddling with data and code ."], "link": "http://blog.vinux.in/the-curious-incident-of-the-dog-in-the-nighttime/", "bloglinks": {}, "links": {"http://blog.vinux.in": 1, "http://en.wikipedia.org/": 1, "http://blog.vinux.in/": 1}, "blogtitle": "Fiddling with data and code"}, {"content": ["Hey! I am back, writing a new blog after a long time.\u00a0 \n This time I have something to share. It is about mining in the data and a new \u00a0modelling tool, motivated from natural phenomena, like Neural Networks ( Human Neural System) and Genetic Algorithm(Natural evolution). Don\u2019t worry, I am not going for a boring theoretical stuff. Den wat? I am writing a science fiction. It is not inspired from crazy English movies. May be \u201cThe Calcutta Chromosome\u201d ( Amitav Ghosh , 1995 ) can be the source of inspiration of this venture. \n First we have a look at a bit of data mining , the critical tool in the future business. Nobel Prize winner Dr. Penzias remarks (1999) \u201c Data mining will become much more important, and companies will throw away nothing about their customers because it will be so valuable. If you\u2019re not doing this, you\u2019re out of business.\u201d He believed Data Mining would become the killer applications in the corporation. Dr. Penzias is correct, now there are many companies made tremendous competitive advantages in the market using data mining. Now the nature of competition is shifting away from the classic struggle between companies, the new competition is Data Mining v/s Data Mining. \n Increase response rates on direct mail campaigns, decrease fraudulent claims and predict the likely fluctuations in the market are the main business side applications of Data Mining. Other than this DM is used in Biology, preventing crime, security and Artificial intelligence applications. \n Data mining investigation in \u2018security and criminal detection\u2019 is going to be a one of the critical implementation. The essence of this part you may find in Tom Cruise\u2019s movie \u2018Minority report\u2019(2002). Pre-crime is not science fiction; it is the objective of data mining techniques based on artificial intelligence (AI) technologies. \u201c It\u2019s not going to be a cruise missile or a bomber that will be the determining factor, It\u2019s going to be a scrap of information \u201d defence secretary Donald Rumsfeld said over and over in the days following September 11. After the 9/11 rapid growth is going on this area. Link analysis, intelligent agents, text mining, neural networks and machine network algorithms are the main areas of the development \n Let us think about few DM tools. Artificial Neural Network is inspired by the structure of the brain. Brain contains about 1 x 10 10 basic units called neurons. Each neuron in turn is connected to about 1 x 10 4 other neurons. A neuron is a cell that receives electro chemical signals from its various sources and in turn responds by transmitting electrical impulses to other neurons. The brain learns from experience, this will form a network between neurons. Another tool is Genetic Algorithm. GAs are inspired by Darwinian theory of survival of the fittest. Algorithm starts with a set of solutions for a population are taken and used to form another population. This is motivated by a hope that new population will be better than the old one. \n I will tell you about the story in the next blog. \n The post Your work is to discover your work and then with all your heart to give yourself to it appeared first on Fiddling with data and code ."], "link": "http://blog.vinux.in/first/", "bloglinks": {}, "links": {"http://blog.vinux.in": 1, "http://blog.vinux.in/": 1}, "blogtitle": "Fiddling with data and code"}]