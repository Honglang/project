[{"blogurl": "http://gowers.wordpress.com\n", "blogroll": [], "title": "Gowers's Weblog"}, {"content": ["Recall from earlier posts Gil\u2019s modular conjecture for HAPs. It states that if is large enough and is a function from to that never takes the value 0, then for every there exists a HAP such that mod . It is easy to see that this implies EDP, so it may well be very hard, or even false. However, one can hold out a little hope that, as with some strengthenings of statements, it is in fact easier, because it is in some way more symmetrical and fundamental. Given that, it makes good sense, as Gil has suggested, to try to prove modular versions of known discrepancy theorems, in the hope of developing general techniques that can then be tried out on the modular EDP conjecture. \n A very obvious candidate for a discrepancy theorem that we could try to modularize is Roth\u2019s theorem, which asserts that for any -valued function on there exists an arithmetic progression such that . That gives rise to the following problem. \n Problem. Let be a prime. What is the smallest such that for every function that never takes the value 0, every can be expressed as for some arithmetic progression ? \n In this post I shall collect together a few simple observations about this question. \n \n 1. We can at least prove that such an exists. Indeed, by Szemer\u00e9di\u2019s theorem, if is large enough then there is an arithmetic progression of length on which is constant. Since takes non-zero values and is prime, the sums along initial segments of run through all the numbers mod . \n So the question is really asking whether the result can be proved with a reasonable bound for . \n 2. Roth\u2019s discrepancy result tells us that if and takes only the values , then takes all values mod . That is because there is a progression such that in , and by what one might call the discrete intermediate value theorem, it therefore takes all values in between 0 and or between and . This is reasonably compelling evidence that the conjecture is true with a decent bound, but of course the intermediate-value-theorem argument breaks down completely when can take arbitrary non-zero values mod . \n The known lower bounds for Roth\u2019s discrepancy theorem for APs (which are equal to the upper bounds up to a constant) show that the best possible bound for the modular question is at least . \n 3. To make the problem more symmetrical, and therefore potentially easier to handle, it might be a good idea to begin by tackling the following variant. \n Problem. Let be a prime. What is the smallest such that for every function that never takes the value 0, every can be expressed as for some arithmetic progression in ? \n The big difference here is that arithmetic progressions are allowed to \u201cwrap around\u201d. That means that for every arithmetic progression and every coprime to (it might be convenient to insist that is prime), the sets and are also progressions. I think the bounds in the integer case with functions show that the best bound one could hope for for this modified problem are , but I need to check that. \n 3. One natural approach to proving that a function takes all values mod would be to attempt to show that it takes all values with approximately the same frequency. This would potentially allow us to bring in analytic tools. However, it is not in general true. For example, let be the function that is up to and from to . Then a small calculation shows that if is a mod- AP of common difference , then is never more than . I think can probably be taken to be 2, or maybe even 1. (By I mean the smallest modulus of any number congruent to mod .) Thus, for a positive proportion of common differences , the sums along APs of common difference never exceed , say, in modulus. If is large, this implies that the values of the sums are very far from uniformly distributed, since they are concentrated around 0. On the other hand, is obviously not a counterexample to the conjecture (by which I mean the assertion that the modular statement holds with a good bound). \n 4. That observation does not rule out a proof that goes via showing that the values of sums along APs are approximately uniformly distributed, but it does demonstrate that in order to prove that, we would have to put some conditions on . That is, we would need a two-step argument along the following lines (reminiscent of proofs of Szemer\u00e9di\u2019s theorem, though I would hope that this problem is easier). \n (i) If is quasirandom, or at least \u201ccontains substantial quasirandomness\u201d, then the values of are approximately uniformly distributed mod . \n (ii) If is not quasirandom, or better still \u201cdoes not contain substantial quasirandomness\u201d, then we can deduce in some other way (such as finding a long AP on which is constant, but that may be too much to ask) that the sums take all values. \n 5. The weaker the property we can get away with in (i), the better. However, in order to get started it would be good to find any quasirandomness property that implies that the values of are approximately uniformly distributed. \n 6. One thing that needs deciding here is what we mean by a random progression . The simplest would be to fix some and pick random mod- APs of length . These don\u2019t work for the modular conjecture in general, since the constant function is a counterexample, but they might work for functions with a suitable quasirandomness property. \n 7. The fact that we can\u2019t fix the length of the progressions shows that the modular conjecture differs in an important way from Roth\u2019s discrepancy theorem, where fixing the length is not a problem (especially in the mod- version). This dampens any hopes one might start off with for adapting the proof of Roth\u2019s discrepancy theorem to cope with the modular version. But that might in the end be a good thing, since the point of the modular conjecture is to introduce new techniques that can then be brought to bear on EDP. \n 8. When we are looking for a suitable quasirandomness property, it is tempting to turn to Fourier analysis. However, the following (modification of a standard) example is a bit troubling. Let be defined as follows. For each we randomly decide whether to set equal to or mod (where is the residue between 0 and ). If we now choose a random mod- arithmetic progression of length 4, there is an absolute constant such that with probability at least  does not wrap around, and , and . When this happens . Therefore, the value 0 occurs much too often, but according to the natural Fourier definitions of quasirandomness (which we could get by looking at the function ) this function is highly quasirandom. \n 9. Of course, we are looking at much longer arithmetic progressions. However, it is difficult to think of proofs that work for long arithmetic progressions and fail for short ones. (It is mildly encouraging that at least the above source of examples seems to break down when the progressions become long \u2014 though that should be checked carefully.) \n 10. Hmm \u2026 just after writing that I thought of the following modified example. Instead of choosing randomly whether to set equal to or , let\u2019s do it in the following highly deterministic way: if mod 4 we set equal to , if mod 4 we set it equal to , if mod 4 we set it equal to , and if mod 4 we set it equal to . In addition, let\u2019s suppose that is a multiple of , since I seem to need that to make this example work. (Even if we can\u2019t get rid of this restriction somehow, it will still show that a proof that worked for prime values of would have to make strong use of the fact that was not a multiple of \u2014 and there would be many other examples of a similar kind that would lead to similar requirements \u2014 which would be quite a big restriction on what it could look like.) \n Then if is a multiple of 4 and is a random mod- AP of length , there is a probability 1/16 that and mod 4. (Note that because this makes sense independently of which residue we pick.) If that is the case, we can partition into APs of length 4 such that the sum of along each is zero. This shows that with probability at least 1/16 the sum is 0 mod . \n Is this function quasirandom? According to many definitions, yes, since although we split into cases in a highly structured way, the behaviour of each case of is highly quasirandom. \n I mentioned above that there are many similar examples. To give just one illustration, we could take as a multiple of and let take turns taking values , , , and . Essentially the same argument would work, but now the probability would be . (Actually, I now see that the probabilities can be doubled in both cases, since works just as well as .) \n \n I\u2019ll leave it there. I\u2019ve mentioned some proof strategies, and also discussed some difficulties with them. Does anyone have other proof strategies to suggest? I haven\u2019t mentioned using the polynomial method, but that is an obvious thing to try to do: that is, write down a polynomial that vanishes identically only if the modular conjecture for APs is true, and try to prove that it vanishes identically. It is certainly worth looking into that, though it is a little discouraging that the conjecture is false for APs of any fixed length, since that would necessarily make the polynomials less symmetrical. If the above strategy seems the most promising (or should that be least unpromising?) then does anyone have any thoughts about quasirandomness properties that might do the job? (Gil mentioned something in a recent comment \u2014 it would be good to have that thought in more detail.) \n I\u2019ll end by saying that even if proving the modular conjecture for mod- APs ended up having nothing much to do with EDP, it is a very nice problem on its own, and could make an excellent spin-off Polymath project."], "link": "http://gowers.wordpress.com/2012/09/19/edp27-the-modular-version-of-roths-ap-discrepancy-theorem/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["This short post is designed as a possible way in to EDP for anyone who might be interested in participating but daunted by the idea of reading large amounts of material. One of the natural strategies for proving EDP is to try to formulate and prove stronger statements. At first that sounds paradoxical: isn\u2019t it even harder to prove a stronger statement? But the answer to that question is often no. To give a slightly silly example, suppose you were asked to prove that for every there exists such that for every if is odd and has at least prime factors (counted with multiplicity), then mod , where is Euler\u2019s totient function. You could make the problem easier by proving Euler\u2019s theorem, that mod for every and every that is coprime to . You wouldn\u2019t have as many hypotheses to use, but that\u2019s good, since they can\u2019t be used. Perhaps a better and more relevant example is when you generalize the class of numbers you are working with so as to allow a wider set of methods. For instance, suppose you want to prove that the largest possible product of three positive integers that add to 300 is at most . If you replace positive integers by positive reals, then you suddenly have available methods that you didn\u2019t have before \u2014 for example, you could use compactness plus a lemma that says that if any two numbers are not equal then you can increase the product by replacing both of them by their average. (I\u2019m not saying that\u2019s the easiest proof \u2014 just that it\u2019s a proof that you can\u2019t do without first generalizing the statement.) \n \n In this post I want to mention three strengthenings of EDP. One of them I find interesting but not promising as a way of proving EDP, for reasons that I will explain. The other two look to me also very interesting and much more promising. All of them have been mentioned already, but the point of this post is to collect them in one convenient place. \n Restricting the allowable common differences. \n I know of no -sequence that has bounded discrepancy on all HAPs with common difference either a prime or a power of 2. The sequence has bounded discrepancy on all HAPs of prime common difference, but if you allow powers of 2 as well, then no periodic construction can work, since if the period is for some odd integer , then a HAP of common difference will have a bias of at least 1 in each interval of length , for parity reasons, and this bias will gradually accumulate. \n One can defeat powers of 2 by using the Morse sequence (if you haven\u2019t seen it, then it\u2019s a nice exercise to see how it is generated), but that has unbounded discrepancy on HAPs of odd prime length. (I can\u2019t remember exactly why.) \n Why do I not think that this potential strengthening of EDP is likely to be useful for attacking the problem? Well, one promising feature of EDP is that it seems to generalize to sequences taking other kinds of values, such as complex numbers of modulus 1 or even unit vectors in an arbitrary Hilbert space. However, something that I\u2019ve only just noticed (though others may have spotted it ages ago) is that if one restricts to, say, prime-power common differences, then even the complex version of the problem becomes false. A very simple counterexample is the sequence , where (that is, a primitive sixth root of 1). Then the sum along any HAP with common difference that isn\u2019t a multiple of 6 will be a sum of a GP with common ratio , and will therefore be (uniformly) bounded. In particular, this is true of HAPs with prime-power common difference. \n This simple example also shows that a certain real generalization of EDP is false when you restrict the common differences in this way: you cannot prove unbounded discrepancy for sequences that take values in the set . The counterexample is simply twice the real part of the example above: the sequence repeated over and over again. So if (and I think it\u2019s a big if) EDP is true for HAPs with common differences that are either primes or powers of 2, then any proof must make pretty strong use of the fact that the sequence is a -valued sequence. This rules out a lot of promising techniques, so it appears to make the problem harder rather than easier. \n A discrepancy question about matrices. \n In my earlier post I mentioned what I called the non-symmetric vector-valued EDP. I have subsequently realized (though perhaps a better word is \u201cremembered\u201d since I must have been sort of aware of this at some point) that it is equivalent to another discrepancy statement that I now find more appealing. The statement is the following. \n Conjecture. Let be a function from to and suppose that for every . Then for every real number there exist HAPs and such that . \n If we take a function and define , then for every the above conjecture, if true, gives us HAPs and such that , which proves that the discrepancy of is at least . So the above conjecture implies EDP. But the class of functions of the form with is a very small subset of the class of all functions that take the value 1 along the diagonal, so this conjecture is very much stronger than EDP. \n It is also equivalent to the statement that for every there exists a diagonal matrix of trace at least 1 that can be written as a linear combination , where for each  and are characteristic functions of HAPs and and . (This is the statement that I discussed at length in my previous post.) In one direction this is easy: if a decomposition of that kind exists, then equals the trace of and is therefore greater than 1, but it also equals , which is at most . It follows that there is some such that . \n In the other direction, if no such decomposition of a diagonal matrix exists, then the Hahn-Banach theorem gives us a linear functional that separates the class of diagonal matrices with trace at least 1 from the class of linear combinations of HAP products defined above. It is easy to check that this functional must be a diagonal matrix with a constant diagonal such that the value along the diagonal is at least 1 and such that for any two HAPs and . \n The conjecture is so much stronger than EDP that I think it would be a mistake just to assume that it is true. My guess is that it is true, but I would be very interested if there was a counterexample (even if, like the complex sequence above, it is disappointingly simple \u2014 in fact, if there is a counterexample, then it seems quite likely that it will be fairly simple). And if there isn\u2019t a counterexample, then the fact that it is so much stronger a conjecture than EDP does this time make me think that the result might be easier to prove than EDP itself. \n Until very recently, I had been mainly interested in the dual version of the question (that is, the question about decomposing diagonal matrices), but now it seems to me that the matrix discrepancy question is worth thinking about directly. It is a clean question, and it has the big advantage over the original EDP question that it does not restrict values to the set , so a number of methods can be used that cannot be used directly for EDP. For instance, linear programming can be used to get experimental results: Sasha Nikolov may be going to look into this. \n Given any matrix , one can find vectors and such that , so this matrix question is actually trivially equivalent to the non-symmetric vector-valued question I had formulated earlier. But expressing it in terms of vectors makes it harder to think about rather than easier, and that is what had previously put me off thinking about the question directly. \n There is one class of matrices that is particularly worth mentioning I think. If EDP is true but this matrix question is false, then the best candidates for counterexamples to the matrix problem are probably matrices of high rank, and an obvious class of matrices that tend to have high rank is matrices that are constant on diagonals (that is, Toeplitz matrices ). \n Suppose, then, that our matrix is defined to be for some function such that . Then , where by I mean the number of ways of writing as with and . So we have a class of functions that I\u2019ll call HAP convolutions , and we\u2019d like to show that if is any function with and is any constant, then there exists a HAP convolution such that . This is true if and only if there is an efficient way of writing the function that is 1 at 0 and 0 everywhere else as a linear combination of HAP convolutions. This is another question that could be investigated using linear programming, and perhaps since it concerns functions of just one variable we could get more extensive results than we could for the general matrix problem. \n The modular conjecture. \n In his most recent guest post, Gil Kalai reformulates EDP as a question about sums mod . EDP is trivially equivalent to the following assertion. \n Conjecture. For every there exists such that for every sequence of length and every there exists a HAP such that mod . \n At first that doesn\u2019t look like a very interesting reformulation since it is too obviously equivalent to EDP. But what makes it interesting is that it has a very natural generalization that doesn\u2019t have any obvious counterexamples. \n Stronger Conjecture. For every there exists such that for every sequence of length of non-zero numbers mod and every there exists a HAP such that mod . \n In other words, we replace the condition that the sequence takes values by the much weaker condition that it is never zero. Gil calls this the modular conjecture . (He also presented it in a comment on a much earlier EDP post.) \n As Gil points out in his post, one can write down a polynomial that is identically zero (mod ) if and only if the modular conjecture is true for . It is tempting to try to prove that it is zero by analysing its coefficients. More generally, this approach to the problem appears to open the door to a number of algebraic methods. \n If you want to prove EDP this way, you have to solve the conjecture for some non-zero . (Since is prime, if you can show it for one then you\u2019ve shown it for all.) However, the problem with is interesting in its own right, and in particular it seems to be interestingly different from the problem with non-zero . At the time of writing, I don\u2019t see any way of modifying the EDP examples to obtain exponentially long sequences mod that avoid zero sums \u2014 it seems to me that the upper bound on the length could be significantly smaller. That would be interesting, as it would place constraints on what a proof could look like for non-zero . \n A fourth strengthening. \n This isn\u2019t really meant as part of the body of the post, but more of an afterthought. A strengthening of EDP that has been considered since very early on in the project is where you replace a sequence by a sequence of unit vectors in a Hilbert space. To be precise, one looks at the following statement. \n Conjecture. Let be a sequence of unit vectors in a Hilbert space and let be a real number. Then there exists a HAP such that . \n There is something slightly curious about this conjecture, which is that it is very hard to see how having infinitely many dimensions to play with could help one find a counterexample. In some sense, if you use too many dimensions, then it ought to be the case that there will be a HAP such that the vectors with are pointing in lots of different directions and therefore not cancelling. That makes me wonder whether one can prove that if there is a counterexample to the above conjecture, then there must be a counterexample for some finite-dimensional Hilbert space. Or if that is too much to ask, perhaps there might have to be a counterexample where all the live in some compact set. I think it would be very interesting to think about whether the vague intuition I have just expressed can be made precise. As it stands, it is of course not even close to a proper argument. (I should stress one aspect of what I am saying. If there is any vector sequence of bounded discrepancy, then one can arbitrarily modify for each prime and the sequence will still have bounded discrepancy. So I\u2019m not suggesting that every bounded-discrepancy sequence lives, or almost lives, in finite dimensions, but that it can be used to construct one that does.) \n As I write that, another question occurs to me. For this one I don\u2019t even have a vague intuitive argument. Let\u2019s suppose that we can find a counterexample to the matrix discrepancy question earlier. Suppose also that takes the form for some (not necessarily unit) vectors and in a Hilbert space. Must there be an example where the and lie in a finite-dimensional Hilbert space, or at least in a compact subset of a Hilbert space? \n A combined generalization. \n A second afterthought is that the matrix question has a modular version that might be of some interest. Let be a prime and let be a function from to with for every . Must the sums of on products take all possible values mod ? What if we merely ask for to take non-zero values on the diagonal? \n If the strong modular conjecture is false, then we can turn a counterexample into a diagonal matrix in the obvious way and we get a counterexample to the second question. Indeed, suppose that for every and when . Then , which avoids some value, since is a HAP. But with matrices there are many more ways of trying to avoid particular values, so the strong modular matrix conjecture looks like a much stronger statement. Again there are two cases \u2014 avoiding 0 and avoiding a non-zero value \u2014 of which only the second obviously implies EDP."], "link": "http://gowers.wordpress.com/2012/09/06/edp26-three-generalizations/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["The Polynomial Method \n The polynomial method is another basic combinatorial technique that occasionally works. One way to describe the method is as a way to translate a combinatorial statement into the vanishing of a certain polynomial modulo . \n A demonstration of the method \n Theorem: Every graph (or hypergraph) G with n vertices and 2n+1 edges contains a nontrivial subgraph H with all vertex-degrees divisible by 3. \n (This is a theorem of Noga Alon, Shmuel Friedland, and me from 1984.) \n Before the proof : If we want to get a subgraph with all vertex degrees even then we need n edges (or n+1 edges for hypergraphs). This has a simple linear algebra proof which also gives an efficient algorithm. \n \n From-scratch proof sketch: Associate with every edge e of the graph a variable . Consider the two polynomials \n P=  , and \n Q= \n If the theorem is false then P-Q=0, as polynomials over the field with three elements. This is impossible since P is a polynomial of degree 4n while Q is a polynomial which has a monomial of degree 4n+2 with nonzero coefficient. \n The theorem follows more directly from a theorem of Chevalley-Warning and even more directly from a theorem of Olson, but the above proof serves best our purpose. \n Remarks about the polynomial method: \n 1) The polynomial method has many applications but only in specific cases. It is not nearly as widely applicable as, say, the probabilistic method. \n 2) Good basic references: A. Blokhuis, Polynomials in finite geometries and combinatorics. In Keith Walker, editor, Surveys in Combinatorics, 1993 , pages 35-52. Cambridge University Press, 1993. \n Noga Alon, Combinatorial Nullstellensatz, Combinatorics, Probability and Computing 8 (1999), 7-29. \n 3) The polynomial method is related to the \u201clinear algebra method\u201d in combinatorics. Often, however, direct linear algebraic proofs lead to efficient algorithms while this is not known for applications of the polynomial method. For example, no polynomial algorithm to find the graph H in the above theorem is known, and there is a related complexity class introduced by Christos Papadimitrou . The polynomial method is closely related to arguments coming from the theory of error-correcting codes, and to arguments in TCS related to interactive proofs and PCP. \n The modular EDP. \n The following is an equivalent way to formulate the Erd\u0151s 1932 conjecture that the discrepancy for EDP is unbounded. \n 1) Consider the sequence as a sequence with modulo , where is a prime that we can choose as large as we want. \n 2) Then every number modulo can be expressed as a sum of the sequence along a HAP modulo . \n Translating EDP (in this form) into a statement about polynomials modulo is cumbersome. But one thing we may have going for us is that it suggests a natural extension of EDP where the supposed-to-vanish polynomial is simpler. \n Modular EDP Conjecture: Consider a sequence of non-zero numbers modulo p . Then if n is sufficiently large w.r.t. p , then every number can be expressed as a sum of the sequence along a HAP modulo p . \n As in the original EDP we can consider general sequences or just multiplicative sequences. \n The Polynomial identity required for the modular EDP \n Here is the polynomial identity in n variables we need to prove over when grows to infinity with as slow as we wish. For every , , \n (*) \n These polynomials are not familiar but they are related to generating functions which arise in permutation statistics. In particular, when we look at the product \n  \n and expand it to monomials, the coefficients have a combinatorial meaning in terms of permutations and inversions. \n Given a permutation , and an integer we can ask how many inversions are there between and a smaller integer. This is a number between 1 and . \n The coefficient of  in the above product is the number of permutations where there are integers contributing j inversions. The proposed identity (*) may be expressed in terms of modular properties of such permutation statistics. \n Challenge: Prove the modular EDP using the polynomial method. \n What does the LDH tell us about the modular EDP? \n It is especially easy to apply the large deviation heuristic to the modular version of EDP. Suppose we want to compute the probability that all HAP-sums miss the outcome . \n Given , the probability that is not is . So we are interested in the value of with . (Restricting our attention to multiplicative sequences will divide the exponents on both sides by .) Solving this equation gives us . The LDH heuristic comes with a firm prediction and a weak prediction. In this case the LDH gives \n a) (Firm prediction) There are sequences violating the modular EDP when . \n b) (Weak prediction) There are no such sequences when . \n The firm prediction is correct by the log n discrepancy constructions for EDP and as a matter of fact the LDH itself gives an even stronger prediction of for -sequences. By restricting our attention to sequences we see that the weak prediction is incorrect and LDH for the modular EDP is blind to the special substructure of sequences. Note that the firm conjecture is far from being known when we extend the modular EDP and replace all integers by a random subset of integers, or by square-free integers , or by SCJ-systems of integers etc."], "link": "http://gowers.wordpress.com/2012/09/04/edp25-third-guest-post-by-gil-kalai/", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Gil has a not quite finished third post that will appear soon on this blog. Meanwhile, here are a few thoughts I\u2019ve had recently as I tried to get my brain into EDP mode again. \n The approach to the Erd\u0151s discrepancy problem that, rightly or wrongly, I found most promising when we were last working on it was to prove a certain statement about matrices that can be shown quite easily to imply a positive solution to the problem. In this post, I\u2019m going to treat that matrix statement as the problem, and think about how one might go about trying to prove it. I\u2019ll give the brief explanation of why it implies EDP, but not of what the possible advantages of the approach are (discussions of which can be found in some of the earlier material). \n \n First I\u2019ll need to recall a couple of definitions and some notation. A homogeneous arithmetic progression , or HAP, is a set of the form for some pair of positive integers and . Let me say that is a HAP-function if it is the characteristic function of a HAP. Finally, if and are functions defined on , let denote the function defined on that takes the value at , which can be thought of as an infinite matrix. \n Here, then, is the statement that it would be great to prove. What is nice (but also quite daunting) about it is that it is a pure existence statement. \n Problem. Prove that for every there exists a real matrix decomposition with the following properties. \n (i) is a diagonal matrix and . \n (ii) Each and is a HAP-function. \n (iii) . \n I have stated the problem in this form because (for a reason I will outline below) I am confident that such a decomposition exists. However, it seems to be quite hard to find. \n Why would the existence of an efficient decomposition of a diagonal matrix into products of HAP-functions prove EDP? Well, let be a -sequence and consider the quantity . On the one hand it equals \n , \n and on the other hand it equals \n . \n Since , there must exist such that , from which it follows that there exists a HAP such that . This shows that must have HAP-discrepancy at least for every , and we are done. \n Of course, one can deduce anything from a false premise, so one needs at least some reason to believe that the decomposition exists. Quite a good reason is that the existence of the decomposition can be shown (without too much difficulty) to be equivalent to the following generalization of EDP. \n Problem. Show that if and are any two sequences of vectors in a Hilbert space such that for every , then for every constant there exist HAPs and such that . \n This is a strengthening of EDP, since it clearly implies EDP when the Hilbert space is . However, it doesn\u2019t seem like such a huge strengthening that it might be false even if EDP is true. At any rate, it seems just as hard to find a counterexample to this as it does to EDP itself. (You might ask why I believe that EDP is true. It\u2019s partly blind faith, but the heuristics presented by Gil Kalai in the previous post offer some support, as does \u2014 very weakly \u2014 the experimental evidence.) \n How does one ever solve existence problems? \n Let\u2019s begin by thinking very generally about the situation we\u2019re in: we have to come up with a mathematical object that has certain properties . (Of course, we could combine those into one property, but it is often more natural to think of the object as having several different properties simultaneously.) \n I\u2019m going to list strategies until I can\u2019t (or at least can\u2019t immediately) think of any more that seem to have any chance of success. (An example of a strategy that I think has no chance of success for this problem is to search for a just-do-it proof, the problem being that the constraints appear to be quite delicate, whereas a just-do-it proof works better when one has a number of independent constraints that are reasonably easy to satisfy simultaneously.) \n 1. Use an object you already know, possibly with some small modifications. \n 2. Start with examples that work for a certain constant , and use them as building blocks for larger and more complicated examples that work with a larger constant . \n 3. Use some kind of duality to prove that if an example does not exist, then some other object does exist, and show that in fact the second object does not exist. \n 4. Use a computer to search for small examples, try to discern a pattern in those examples, use that pattern to guess how to construct larger examples, and check that the guess works. \n 5. Strengthen the conditions on the object until there is a unique (or almost unique) object that satisfies those conditions. \n 6. Weaken the conditions on the object (e.g. by forgetting about some of the properties ), try to describe all objects that satisfy the weaker conditions, and then reformulate the problem as that of finding an object of that description that satisfies the stronger conditions. [As a very simple example, if you were required to find a positive solution of some quadratic equation, you would solve the equation without the positivity condition and then pick out a positive root.] \n 7. Assume that you have an object with the required properties, deduce some interesting further properties, find a natural object with the further properties, and hope that it has the original properties as well. [This is a variant of 6.] \n How do the general strategies fare when it comes to EDP? \n I will discuss them briefly at first, before concentrating in more detail on a few of them. \n 1. This doesn\u2019t appear to be the kind of problem where there is already some matrix decomposition (or related mathematical object) waiting in the wings to be modified so that it becomes the example we want. Or if there is, the only way we are likely to discover it is by pinning down more precisely the properties we are interested in so that someone somewhere says, \u201cHang on, that sounds like X.\u201d But I think it is very unlikely that we will discover an example that way. \n 2. I am definitely interested in the idea of starting with small examples and using them to build bigger examples. However, an approach like this needs plenty of thought: non-trivial small examples seem hard to come by, and the only obvious method for building bigger ones is taking linear combinations of smaller ones. (A trivial small example is to take a linear combination of the form where each is the characteristic function of a singleton. This gives us . To get anything interesting, we need the to be HAP-functions for longer HAPs, but then it is very difficult to get the off-diagonal terms to cancel.) \n 3. We are trying to write as an efficient linear combination of objects of a certain form. That is a classic situation for applying the Hahn-Banach theorem: if there is no such linear combination, then there must be a separating functional. What can we say about that functional? \n This approach seems promising at first, but is in fact of no use at all, since if you pursue it, the conclusion you come to is that if there is no such linear combination, then there are sequences of unit vectors and in a Hilbert space and a constant such that for every pair of HAPs and . The reason duality doesn\u2019t help is that we used duality to reformulate EDP as a problem about decomposing diagonal matrices. If we use duality again, we get back to (a generalization of) EDP. \n 4. We have tried this. Moses Charikar and others have written programs to search for the most efficient decompositions (for this and for a related problem). The results are useful in that they give one some feel for what the diagonal matrix needs to be like, but it does not seem to be possible to obtain a large enough matrix to use it to go as far as guessing a formula. For that, more theoretical methods will be necessary (but knowing roughly what the answer needs to look like should make finding those methods quite a bit quicker). \n 5. One difficulty is that we are not looking for a unique object: it would be nice if the properties we were trying to obtain determined the matrix and its decomposition uniquely, since then we could hope to calculate them. An obvious property to add is that the object is in some sense extremal (though what that sense is is not completely obvious). Another thing we can do is insist on various symmetries \u2014 something that has already been tried. Yet another is to restrict the class of HAPs that may be used: as far as we can tell at the moment, we can get away with HAPs with common differences that are all either prime or a power of 2. (I say that simply because it seems to be hard to find a sequence that has bounded discrepancy on all such HAPs. Gil\u2019s heuristics suggest that the discrepancy should grow like for sequences of length .) \n 6. What weaker properties might we go for? One idea is simply to try to find a linear combination of products of HAP-functions that leads to a considerable amount of cancellation off the diagonal, even if there are still off-diagonal terms left. This would be interesting because if one naively tries to find a decomposition that works, it seems to be very hard to get a substantial amount of cancellation (except if very small HAPs are used, in which case the resulting decomposition is not very informative). Another (somewhat related) idea is to look for interesting decompositions but with not particularly large values of . Yet another might be to allow a wider class of functions that HAP-functions. \n 7. I don\u2019t have much idea of how to apply this strategy to the entire decomposition, but there is plenty one can say about the diagonal matrix that ones wishes to decompose, and that information, if it could be expressed neatly, would surely be very useful: it seems much easier to try to decompose a specific diagonal matrix than to try to decompose a diagonal matrix that you have to choose first, when the vast majority of diagonal matrices cannot be efficiently decomposed. \n The proof that the diagonal decomposition implies EDP actually proves a much stronger result. It gives us a function such that such that if is any sequence of real numbers (not necessarily -valued) with , then has unbounded HAP-discrepancy. To put that loosely, the sequence cannot correlate with the square of any sequence of bounded discrepancy. That puts very strong conditions on , but so far we have not fully understood what those conditions are. It seems that we should be able to make progress on this, perhaps even adding reasonable conditions that would allow us determine the function uniquely. \n Creating new examples out of old ones. \n Suppose that and . Clearly if we take a linear combination of and , we get another decomposition of a diagonal matrix, and if the original decompositions were into products of HAPs then so is the new one. \n Can we do anything more interesting? Another possibility is to look for some kind of product. Now Dirichlet convolutions are natural objects to look at in the context of EDP, since the expression can be thought of as follows. You start with a function defined on , then you define the dilate to be the function that takes the value at and 0 at non-multiples of , and finally you take a linear combination of those dilates. If is identically 1, then this is taking a linear combination of HAPs. \n We can\u2019t take a Dirichlet product of two matrices, but we can do something similar: given functions and of two integer variables we can take the function . If and are diagonal (meaning that they are zero if ), then the terms of the sum are zero unless and , so in this case , considered as a function of only, is the Dirichlet convolution of and , also considered as functions of one variable. \n This operation also respects products in the following sense. If and , then \n , \n which equals , where is the Dirichlet convolution of and and is the Dirichlet convolution of and . \n One further obvious fact is that the operation is bilinear in the two matrices that it is applied to. \n So far everything is working swimmingly: out of two decompositions of diagonal matrices into linear combinations of products we build another one that has some number-theoretic significance. However, there is a problem: a Dirichlet convolution of two HAP-functions is not an HAP-function except in trivial cases. \n How much does that matter? Without doing some detailed calculations I don\u2019t know. Here are a couple of simple observations. Let and be two HAP-functions. Then their Dirichlet convolution is a sum of dilates of , one for each point in the HAP of which is the characteristic function. So we can at least decompose the Dirichlet convolution of two HAP-functions as a sum of HAP-functions. The sum of the coefficients is the number of points in the HAP corresponding to \u2014 though since the situation is symmetrical, we can add dilates of instead, so it is better to say that the sum of the coefficients can be taken to be the length of the smaller of the two HAPs. \n That doesn\u2019t sound very efficient, but we must also remember that when we take the Dirichlet convolution of two diagonal matrices, we obtain a diagonal matrix that sums to the product of the sums of the original two matrices. So there may be some gain there to compensate for the loss of efficiency. That is why I say that more detailed calculations are necessary, a point I will return to at some stage. \n What can we say about the diagonal matrix ? \n As I have already mentioned, if a diagonal decomposition of the required kind exists, then we can place strong constraints on the diagonal matrix itself. Writing for , it must have the following property: if is any real-valued function defined on the integers such that , then the HAP-discrepancy of must be at least . (This follows from the proof that the existence of the decomposition for a diagonal matrix with implies that every -sequence has HAP-discrepancy at least .) \n Turning that round, if we have any real-valued sequence of discrepancy at most , then cannot be more than . \n A simple example of the kind of conclusion one can draw from that is that the sum of over all s that are not multiples of 3 is at most 1. This follows from the fact that the sequence has HAP-discrepancy 1. More generally, if is large, then for all small most of the largeness must occur on multiples of . \n In qualitative terms, this suggests that ought to be concentrated at numbers with many factors, and the experimental evidence backs this up, though not quite as cleanly as one might ideally like. (However, \u201cedge effects\u201d could well account for some of the peculiar features that are observed experimentally, so I still think it is worth trying to find a very aesthetically satisfying diagonal matrix .) \n Here is another reason to expect that should be larger for numbers with many factors: they are contained in lots of HAPs. We are trying to find a measure on (at least if we insist that takes non-negative values, which seems a reasonable extra condition to try to impose) such that any function with a large -norm with respect to that measure must have a large HAP-discrepancy. What might make it difficult to find a function with large -norm and small HAP-discrepancy? It would be that we had lots of constraints on the values taken by the functions. And the natural way that we can place lots of constraints on one value is if has many factors and is therefore contained in many HAPs. \n As an example of the opposite phenomenon, suppose we take an arbitrary sequence and alter its values however we like at all primes. What will happen to its HAP-discrepancy? Well, each HAP contains at most one prime, so we cannot have changed the HAP-discrepancy by more than 2. This illustrates fairly dramatically how little it matters what a sequence does at numbers with few factors. \n A first wild guess. \n It is tempting to jump straight from this kind of observation to a guess of a function that might work. For example, what if we took some primes , a collection of subsets of and let be the characteristic measure of the set of all numbers that can be written as for some . That is, takes the value on all such numbers and zero on all other numbers. \n Does a function like that have any chance of working? Because we have ensured that it sums to 1, we would now be looking for a decomposition with absolute values of coefficients summing to at most for a small constant . But before we even think about doing that, we should try to find a sequence that is large on many of the products of primes but that has bounded discrepancy. Only if we find it hard to do that is it worth looking for a decomposition. \n Let me be more precise about what we are looking for. If we can find a decomposition of (the diagonal matrix with entries given by ) as where the and are HAP-functions and , then for every sequence , we have two ways of calculating . One of them gives us , while the other gives us . So if , then there must be some HAP on which has discrepancy at least . \n Therefore, in order to show that this particular will not do, we need to find a sequence such that averages at least 1 on the numbers but has bounded discrepancy. \n Let us make our task easier to think about by looking for a sequence that takes values on the numbers . Can we choose the values elsewhere in such a way that we cancel out any discrepancy that we might accidentally pick up with those values? \n To answer this question, let us think about how HAPs intersect the set, which, since I\u2019m mentioning it quite a bit, I\u2019d better give a name to: I\u2019ll write for the set of numbers with . Let me also write as shorthand for . Any HAP that intersects must have a common difference of the form for some set . The resulting HAP will contain every for which , which is rather pleasantly combinatorial. \n Let\u2019s suppose that we\u2019ve chosen the values everywhere on . What we are trying to do now is choose values off (not necessarily ) in such a way that any HAP-discrepancy contributed by values in is cancelled out by a roughly opposite discrepancy in the values outside . \n An obvious way to do that is to be fairly greedy about it. That is, we look at HAPs that have a substantial intersection with and simply choose a few further values on those HAPs, hoping that our choices won\u2019t mess up too many discrepancies elsewhere. Let\u2019s see if we can get something like that to work. \n Consider, then, the (infinite) HAP with common difference . As already mentioned, this intersects in every such that . So as we trundle along the multiples , we find that the partial sums of the s that we encounter and have already chosen go up and down, and we would like to choose some further values to ensure that they remain bounded. \n At which values of are we still at liberty to choose the value ? It must be a multiple of , and two ways of ensuring that that multiple does not lie in are (i) to make sure that is also a multiple of some with and (ii) to make sure that is a multiple of for some . \n The question now is whether if we do that we will find that the value we choose has an effect on lots of other HAPs at the same time. And it looks rather as though it will. Suppose is quite a large set. Then any multiple that does not belong to is also a multiple of for every , so there is indeed a danger that by choosing a value of we are messing up quite a lot of HAPs and not just the one we were interested in. We could of course try to sort out the HAPs with large first and later correct any damage we had done to smaller sets \u2014 but a large set has a lot of subsets. \n This is another situation where the devil is in the detail. Maybe there is enough flexibility that some kind of careful greedy approach would work \u2014 perhaps we could even choose to be 1 everywhere on \u2014 but so far it seems at least possible that there exists an efficient decomposition of a diagonal matrix with entries that are concentrated on square-free numbers with many prime factors. \n It is sometimes convenient to describe arithmetic progressions of the form as HAPs. Let me do that now, and let me define two HAPs to be adjacent if they are disjoint and their union is also a HAP (in this generalized sense). In other words, two HAPs are adjacent if one is simply a continuation of the other. \n If and are HAP-functions coming from adjacent HAPs and (in which case I\u2019ll say that they are adjacent HAP-functions), then is 1 on the diagonal at all points in the HAP , while off the diagonal it is 1 on and latex P\\times Q\\cup Q\\times P$. If we take lots of products of this form, we can hope to get lots of cancellation off the diagonal, while getting none at all on the diagonal. \n Let\u2019s try to pursue this idea in more detail. Suppose we have some kind of probability distribution on functions of this form. That is, we pick, according to some distribution, a random set and then a random pair of adjacent HAPs and . Now let and be their characteristic functions and let and be two positive integers. What is the expected value of ? \n A necessary condition for this quantity to be non-zero is that should divide both and . So let us condition on this event. If we are clever about the way we choose our probability distribution, we should be able to organize it so that if you choose a random , then typically plenty of its subsets have a similar probability of being chosen. The reason that is potentially important is that if we fix a common difference and pick random pairs of adjacent HAPs and of some given length, then pairs that are close to the diagonal will tend to get positive values (because normally if one of them is in then so is the other, and similarly for ) while pairs that are a little further away tend to get negative values. But if we have many different common differences in operation then we can hope that some of these biases will cancel out: a difference that tends to result in a positive value at one scale might tend to result in a negative value at another scale. \n While writing this I have just remembered a useful trick that we were well aware of during the previous attempt at EDP. It is straightforward to show that EDP is equivalent to the same question for the rationals. That is, one wishes to show that for every function defined on the positive rationals and every constant there is a HAP (the definition is obvious) on which the sum of the function has absolute value at least . The nice thing about this formulation is a much greater symmetry: for every rational , the map is an isomorphism from (considered as a group under addition) to itself. This makes it unnecessary to think about numbers with lots of factors. (It is not hard to get into this nice situation using just integers \u2014 for example, you can just multiply everything by for a very large and you can divide your numbers by whatever you like, within reason \u2014 but it is more natural to use rationals.) \n The hope was that if we thought about functions defined on the rationals, then the rather peculiar properties of the diagonal matrix one wishes to decompose might become less peculiar. However, one would still need to think carefully about ratios of numbers. \n I\u2019m tempted to continue thinking aloud about these possibilities, but I\u2019ll save that up for the comments. \n A second wild guess. \n I\u2019m now going to ignore what I\u2019ve just written about working with the rationals and go back to the question of trying to find a natural function that is biased towards positive integers with many factors. What I\u2019d like to do is build a sequence of non-negative functions , each better than the last, in the hope of eventually finding one that has (or at least doesn\u2019t obviously not have) the property that every sequence such that has large HAP-discrepancy. (To make this non-trivial, I also need .) \n I\u2019ll start with a function I know doesn\u2019t work: let for every and 0 for every . That fails because the sequence has HAP-discrepancy 1 but (at least if is a multiple of 3). That doesn\u2019t satisfy the condition I said, but it does if we multiply it by . \n The problem there was that has a large sum on non-multiples of 3. To correct that and similar problems, it feels natural to give greater weight to numbers with more factors. But how? Well, let\u2019s do it in a rather naive way and simply weight each number by how many factors it has (and therefore how many HAPs it belongs to). This at least has the advantage of being a natural number-theoretic function: it is (up to a constant) the Dirichlet convolution of the constant function 1 with itself, at least until you get beyond . That is, . \n Roughly how big is ? This sum is the number of pairs of positive integers such that (since is the number of pairs of positive integers such that ). Counting the number of pairs with we can crudely estimate this as , which is roughly . On the other hand, the Dirichlet convolution with itself of the function that is up to and 0 thereafter sums to (since each pair of positive integers with contributes 1 to the sum). Since the two functions are equal up to , we find that the vast majority of the second function lies beyond the point where it equals . \n But let\u2019s not worry about that. On average, grows like , which is not too fast a rate of growth, so let\u2019s simply take the function up to . We now want to prove that every sequence such that has HAP-discrepancy at least (for some that tends to infinity with ). Alternatively, we want to find a counterexample, which will induce us to try to improve the function. \n An obvious example to try is the usual one, namely the sequence . To see whether this works, we need to estimate the sum of over all non-multiples of up to . That equals the number of pairs such that neither nor is a multiple of 3 and . And that is roughly times what you get without the divisibility condition. In other words, it is roughly . So to get to equal we need to multiply the sequence by , which gives it a HAP-discrepancy of , a slight improvement on the that we obtained with a constant function. \n This doesn\u2019t prove much \u2014 maybe there are better examples \u2014 but it suggests in a weak way that taking Dirichlet convolutions results in improved functions. To be clear what I mean by \u201cimproved\u201d, I am now looking for a non-negative function defined on such that for large , or perhaps even all , if is any sequence such that , then has HAP-discrepancy at least . The bigger I can get to be, the better I consider the function to be. \n If you take repeated Dirichlet convolutions of the constant function 1, then you get functions , where is the number of -tuples of positive integers such that . (The function is equal to .) Suppose is large, is very large, and is a sequence such that . Does it follow that has large HAP-discrepancy? More ambitiously, does it have HAP-discrepancy that grows exponentially with , provided that is large enough? \n Back to the rationals. \n The following idea is one that feels familiar \u2014 I think something similar has come up already. But there\u2019s nothing like a break from a problem to make an old stale idea seem new and fresh, and sometimes the apparent staleness was an illusion. So here it is again. \n I\u2019ll begin by creating a function on the diagonal that seems to me to have a sporting chance of being efficiently decomposable into HAPs. Let (for generating set) be the set and let be the -fold multiplicative convolution of the characteristic function of with itself. That is, is the number of ways of writing as with and all the and being integers between 1 and . \n Before I think about how one might go about decomposing the diagonal \u201cmatrix\u201d (that is, a function defined on that is zero at unless ) into products of HAP-functions, I want to see whether it looks hard to find a function such that but is of bounded HAP-discrepancy. \n To have any chance of that, we need to understand a bit about . We can get that understanding by representing every rational as a product of (possibly negative) powers of distinct primes. For the rationals where is non-zero, the primes are all at most , so we can think of as a function defined on a lattice of dimension (the number of primes less than or equal to ). Then is the -fold convolution of the characteristic function of regarded as a subset of this lattice. If is large enough, then this convolution will look like a large and spread-out Gaussian, which will have the potentially useful property that it is approximately invariant if you multiply (or divide) by an integer between 1 and . \n If has that property, does it rule out some kind of variant of the example? A key feature of that example is that the function in question is zero on multiples of 3, but in there is no such thing as a multiple of 3 (or rather, everything is a multiple of 3 so the concept ceases to make sense). \n But that argument is far from conclusive. How about defining a function as follows: given a rational in its lowest terms, let be 0 if either or is divisible by 3, and otherwise 1 if mod 3 and if mod 3? \n Let us think what the values of are at , , etc. If is a multiple of 3, then all those values are 0 (since is not a multiple of 3). If is a multiple of but not , then except if is a multiple of . If and are congruent mod 3, then the values of go . If they are not congruent mod 3, then they go . That last argument worked even if , so we have covered all cases. \n What is the density of rationals such that neither numerator nor denominator is a multiple of 3? I haven\u2019t made the question precise, and that allows me to give two different answers. The first answer is that if we pick the numerator and denominator randomly, then the probability that neither is a multiple of 3 is . Also, the probability that they are coprime is , and the probability that they are both coprime and not multiples of 3 can easily be shown by similar methods to be at least an absolute constant. So it seems that our function is supported on a set of rationals of positive density. \n But another way of thinking about it suggests that the support of the function has zero density: every rational can be written as a product of powers of primes, and the probability that the power of 3 that we choose is 0 tends to 0 as we choose more and more rationals. \n The second answer is, I think, the correct one for us, though the question is a bit misleading. The -density of rationals for which the \u201c3-coordinate\u201d is zero does indeed tend to zero, so if we regard as our measure, rather than something more additive, then we do appear to have ruled out examples that are similar to the example. \n Can we use information about the original problem to help us with the dual problem? \n We know that for the integers there is not an efficient HAP-decomposition of a diagonal matrix that is 1 for the first terms and zero afterwards. The proof we have is that if such a decomposition existed, then the sequence would have to have large HAP-discrepancy, which it doesn\u2019t. But can we give a \u201cdirect\u201d proof? I\u2019m not quite sure what I mean by that, except that it should not involve the use of a separating functional. Ideally what I\u2019d like to do is use that example and others like it to work out some rather general constraint that would end up saying a precise version of, \u201cIf an efficiently decomposable matrix equals 1 on the diagonal, then it must have quite a lot of stuff off the diagonal,\u201d and ideally say a fair amount about that \u201cstuff\u201d. \n The hope would be to find a clear obstacle on the dual side to a direct approach to decomposing the identity matrix, and then to show that when we move to the measure on the rationals defined in the previous section, that obstacle goes away and allows us to do what we wished we could do over the integers. The thing that would make life easier would be the ability to divide anything by small integers. \n Actually, maybe it isn\u2019t so important to go for the dual side. Let\u2019s just understand what the existing proof is telling us goes wrong when we try to find an efficient HAP-decomposition. For the decomposition to be efficient, the HAPs we use have to be quite long. And for that to be the case, they must contain roughly as many numbers congruent to 1 mod 3 as they do numbers congruent to 2 mod 3. (They might contain none of either.) Therefore, any HAP product has roughly as many points congruent to or mod 3 as points congruent to or mod 3. Let us call these white points and black points, respectively. By roughly I mean that these two numbers differ by at most a constant. In fact, I think they differ by at most 1. But no points on the diagonal are congruent to or to , so if a product of HAPs includes points on the diagonal that are congruent to or , then it must include at least more black points than white points off the diagonal. \n So, roughly speaking, we can\u2019t build up the non-multiples of 3 on the diagonal without building up a bias towards black points off the diagonal. \n Does that matter? Can\u2019t we correct that unwanted bias by adding or subtracting some other HAP products? No we can\u2019t, unless we also cancel out much of the sum on the diagonal. \n What rescues us if we deal with rationals instead (or more precisely a portion of the rationals weighted by the function )? Also, how easy is it to find an efficient decomposition so that the weight off the diagonal is comparable to the weight on it? I suspect the answer to the second question is that it is not easy, since we have used just the fact that the sequence has bounded discrepancy, but there are many other examples. \n A variant of the HAP problem. \n I stopped the previous section prematurely because I was feeling a bit stuck. Instead I would like to think about a modified problem that may well be easier. EDP asks for a high discrepancy inside some HAP. The set of HAPs has the nice symmetry property that if you dilate a HAP then you get another one. This is particularly good over the rationals, when you can also contract. \n Now let me ask a slightly vague question and then attempt to make it precise. Can we prove EDP for a different \u201cbase collection\u201d of sets? For EDP I\u2019m regarding the base collection as the collection of all sets . If you look at all dilations of those, you get all HAPs. Since I\u2019m having trouble proving EDP for HAPs, I\u2019d like to try to prove it for a different base collection. Of course, the smaller the collection, and the more closely related it is to the set of HAPs, the better, but for now my priority is to be able to prove something . \n I actually have a collection of sets in mind. Let us think of the rationals multiplicatively as an infinite-dimensional lattice, or rather the portion of that lattice that consists of points with only finitely many non-zero coordinates. That is, the point corresponds to the rational , the being integers. I\u2019m not sure exactly what I want to do next, so let me be slightly less precise. If we look at the rationals multiplicatively, as we are doing here, then dilation becomes simply translation. What I\u2019d really like is something like an orthonormal basis, but I\u2019d like it to consist largely of functions that are translates of one another. So far that won\u2019t work, but it looks a lot more hopeful if we allow some kind of \u201cspreading out\u201d (which would be the analogue of taking longer and longer intervals). The most natural type of spreading out is, I would suggest, to take repeated convolutions. So a possible question \u2014 not necessarily the exact one I\u2019ll end up asking \u2014 is this. Suppose we start with the set that we considered earlier. Is it true that every function that has large norm relative to the measure defined by (multiplicatively) convolving  times has a large inner product with some (multiplicative) translate of some iterated convolution of ? \n This problem should be not that hard since it is purely multiplicative, so we can take logs and make it purely additive. If the answer turned out to be yes, then for any -function we would get a large inner product with some iterated convolution of . If we could nicely decompose that iterated convolution into HAPs, we would then be done, though that seems like quite a lot to ask. \n Hmm \u2026 I\u2019ve almost instantly run into trouble. It seems that my \u201cmultiplicative version\u201d of EDP is just plain trivially false. Consider the function that is 1 for products of an even number of primes and -1 for products of odd numbers of primes. (This is the completely multiplicative function , with which we are already very familiar.) From a multiplicative point of view, where we think of as an infinite-dimensional lattice, this is putting 1 at \u201cwhite points\u201d and -1 at \u201cblack points\u201d. If we now take any \u201ccuboid\u201d, by which I mean a set of the form for every , then the sum of on that cuboid is either 0, 1 or -1. I was about to say that I couldn\u2019t see an elegant proof of this, but here\u2019s a fairly quick one. If we remove two adjacent slices from the cuboid, we remove an equal number of white and black squares. \u201cRemoving two adjacent slices\u201d means reducing by 2, provided . If some is even, then by repeating this operation for that we can remove all points, which shows that the numbers of white and black points were originally the same. If all are odd, then we end up reducing the cuboid to a set with just one point, in which case we get . \n If we multiplicatively convolve any two cuboids, we obtain a function that can be expressed as a linear combination of translates of a cuboid, so its inner product with will also be small, as will those of all its translates. (What\u2019s more, those small inner products will themselves alternate in sign as you multiply or divide by primes.) So will have small inner product with everything you build out of products of GPs using multiplicative convolutions. \n It could be fruitful to think about why HAPs have a better chance of giving rise to discrepancy than the kinds of multiplicative sets I\u2019ve just considered. In preparation for that, here is a question to which I know the answer. Let be a nested collection of subsets of such that for every and . Is it possible for there to be a function and a constant such that for every and every ? In the case that for every this is EDP. But for a general nested collection, it is easy to see that bounded discrepancy is possible for that collection and all its dilates. You just take a completely multiplicative function such as and choose in such a way that for every (and such that the are distinct and every integer is equal to for some ). Then the sum is always either -1 or 0, and multiplicativity ensures that the sum on dilates of the is always either 1, -1 or 0. \n The partial sums of grow (it is believed) like , so if we choose our sets greedily \u2014 that is, by taking to be the smallest positive integer we have not yet chosen such that takes the value \u2014 then will contain the first integers, where is around (at most), and look a bit like a random selection of integers within around of for the rest of the set. In other words, will be pretty close to the set , so the system of sets will be pretty close to the set of all HAPs. This suggests that proving EDP is going to be really quite delicate. \n In fact, it will be more delicate still, since we can take a better multiplicative function than . If we take the unique completely multiplicative function that takes to 1 if is congruent to 1 mod 3, to if is congruent to 2 mod 3, and 3 to -1, its partial sums grow logarithmically, so we can say something similar to the above but with around rather than around . \n We can go slightly further with this function. Because it is 1 on the infinite arithmetic progression and on the infinite arithmetic progression , we can choose our sets to be intervals up to roughly plus logarithmically short (non-homogeneous) arithmetic progressions of common difference 3. \n Since these sets are cooked up so as to ensure that the function has bounded discrepancy, they might seem not that interesting. But I think they are important, because if we want to find an efficient decomposition of a diagonal matrix into HAP products, we probably need to understand what it is that makes HAPs so much better than HAPs with highly structured tiny logarithmic perturbations at the end. Otherwise we are in danger of taking seriously arguments that would work just as well for statements that we know to be false. \n As I write, it occurs to me that we might be able to prove a result that would be quite a bit easier than EDP but nevertheless interesting (and in particular interesting enough to be publishable, unless the answer turns out to be disappointingly easy): that there exists a permutation of such that the discrepancy of every sequence on the sets of the form is unbounded. We know it is not true for all permutations \u2014 even ones that in some sense don\u2019t permute by very much \u2014 and we suspect that it is true for the identity permutation but find that very hard to prove. What about if is in some sense \u201cfairly random\u201d? For example, what if we chose a large and randomly permuted the first integers. That would give us sets and all their dilates. Could we prove that the discrepancy on that set system is at least ? What if we drop the condition that the union of the should be all of ? What if instead we ask that for some strictly increasing sequence ? \n I rather like those questions, which makes this a good place to stop the post \u2014 for the length of which I apologize."], "link": "http://gowers.wordpress.com/2012/08/31/edp24-an-attempt-to-get-back-into-the-diagonal-decomposition-approach/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["The Large Deviation Heuristic: an example \u2013 triangle-free graphs \n Here is a very general probabilistic-based heuristic that seems to give good predictions for questions related to EDP. I will refer to this heuristic as \u201cLDH\u201d. (In my polymath5 comments I referred to it as PH \u2013 probabilistic heuristic)). I am thankful to Noga Alon and to Yuval Peres for some helpful help. \nHere is an example: Suppose we want to study the following extremal problem. \n What is the largest number of edges in a graph on n vertices with no triangle. \n If we use the probabilistic method we can ask what is the probability that a random graph in contains no triangle. As long as this probability is positive we know that a triangle-free graph with n vertices and m edges exists. (Being a little careful we can consider instead of where . Looking at random graphs gives us a perfectly correct proof of the assertion that there are triangle-free graphs with vertices and edges for every . \n LDH : \n 1) Estimate naively the probability that a random graph in G(n,m) contains no triangle. \n 2) Choose m so that this estimated probability behaves like 1 over the number of graphs with n vertices and m edges. \n \n So let\u2019s implement this plan. The probability that a random graph in does not contain a specific triangle is . Naively assuming that these probabilities are independent we estimate the probability of not having any triangle as . We want to find so that this probability is roughly . This is the case when roughly . \n LDH prediction for Mantel-Tur\u00e1n problem: The maximum number of edges in a triangle-free graph behaves like . \n In other words, the LDH gives two predictions that we will refer to as the \u201cfirm\u201d prediction and the \u201cweak\u201d prediction. \n A) (The firm prediction.) There exist triangle-free graphs with vertices and edges \n and \n B) (The weak prediction.) There are no (substantially) larger triangle-free graphs. \n Prediction A is correct. There are indeed triangle-free graphs with vertices and edges. (But the LDH does not prove their existence.) Prediction B is miserably false: Actually there are graphs with edges without a triangle. The LDH heuristic ignores the fact that not containing one triangle is not independent of not containing another, and is completely blind to large bipartite graphs. \n Excercise: What is the LDH prediction for the question: How large can a subset of the integers {1,2,\u2026,n} be if it contains no 3-term arithmetic progression? \n We would like to propose the following points regarding the large deviation heuristic: \n \n LDH predictions about the existence of some combinatorial objects are quite often true. (We refer to such predictions as the firm predictions .) \n LDH weak predictions are blind to various structured examples. Sometimes, if we understand the relevant structures we can update the large deviation predictions. (I will come back to this at the end of the post.) \n LDH predictions appear to be quite good for the Erd\u0151s Discrepancy Problem (EDP) and for variations of EDP. This is what we are going to discuss now. \n \n Since LDH is based on a heuristic method to compute probabilities it is quite possible that different heuristics will give different answers but, overall, we did not encounter this. \n Problem 1 : Find natural examples where the LDH firm prediction, namely the prediction for the existence of certain combinatorial objects, fails. \n Problem 2: Find ways to improve the LDH when it fails. (Especially when the answer is known by other methods). \n What does LDH predict for EDP? \n Direct Computation \n We consider the multiplicative version of Erd\u0151s Discrepancy Problem . The number of multiplicative -sequences of length is close to . (These sequences are determined by their values on prime indices and the Prime Number Theorem tells us that the number of primes smaller than behaves like .) We expect that the LDH computations for the general question will give the same answer. \n What we need to compute is: \n What is the value of so that the probability that all partial sums of a random sequence of length belong to the interval satisfies ? \n This question can be formulated in terms of a simple random walk of length on the line. We start at the origin and at each step we go a unit length to the left or to the right with probability 1/2 for each direction. We want to know the probability that the random walk will be confined to the interval and the value of for which this probability is . \n Problem 3 (the answer is known): What is this value ? \n Towards answering problem 2, I asked over Mathoverflow \u201c What is the probability that a random walk of length n will be confined to the interval \u201d and Douglas Zare provided a very detailed answer . Yet, I did not complete the work needed to answer Problem 2. So a few weeks ago I asked Yuval Peres \n What is the probability that the simple random walk of n steps will be confined to the interval , and what is the value of for which this probability is ? \n And a few hours later I received the following reply from Yuval: \n \n Gil, the confinement probability in decays up to a constant like where is known: it is . This is classical and you can find it e.g. in Feller volume 2 or in Spitzer\u2019s book. This holds for all . So the answer to your query is that for a suitable . \n \n So, we get: \n \n The LDH prediction for the EDP is that the maximum discrepancy of a multiplicative sequence of behaves like . (The same prediction applies to general sequences.) \n \n General sequences and positive correlations. \n If we consider general sequences we get the same answer. We need to compute the probability that for a random sequence of length , all HAP are confined to . These HAP are random sequences of lengths , , . The probability that the initial sums of a random sequence of length is confined to is . If we assume that these probabilities are independent we get an estimate of which behaves like when . We get the same outcome. Of course the confinement of different HAPs to are not independent. In fact they seem positively correlated and this strengthens the case for the firm prediction. But I don\u2019t know how such positive correlation can be used to prove that the firm prediction is correct. (Of course, the feeling that small discrepancy on different HAP are positively correlated is rather tentative. We know that for every individual HAP we have with positive probability discrepancy bounded by 1. Yet the probability that this happens for all HAPs is zero.) \n Problem 4: Fix two positive integers . For a function consider the two events \n 1) For a maximal HAP of gap the initial sums of the function are confined to . \n 2) For a maximal HAP of gap the initial sums of the function are confined to . \n Are these two events positively correlated when is large enough? \n Indirect computations \n At an earlier time, I had a version of the LDH based on gaps between vanishing partial sums. Let me discuss it here. We start with the following question: What is a probability that a sequence of of length will not have a vanishing partial sum where ? Another way to ask this question is: \n What is the probability that a simple random walk of length t will not reach zero in the interval ? \n For our purposes all we need to know is that this probability tends to some constant strictly between 0 and 1. The precise value is related to a classic question and let me cite another email by Yuval Peres about it: \n The probability that a simple random walk will not meet 0 in the time interval [s,t], where s=xt, tends as to . This is one of the two classical arcsine laws for random walks that you can find in many sources, including e.g. Durrett\u2019s book or proposition 5.7 page 137 in My Brownian book . There you will see this law applies to all random walks with increments of mean zero and finite variance. More combinatorial arguments for the special case of SRW can be found in Feller vol I, as well as in these slides . \n Given a random walk on the line we will try to estimate the probability that we can find sequence of indices for which the random walk reaches the origin so that the differences between consecutive indices is between and and compute the value of for which this probability is . Since the probability for a random walk of steps to reach the origin between the th and th steps is a certain constant we have the following LDH predictions: \n (Firm prediction) There exists a multiplicative sequence of length such that the gap between two consecutive vanishing partial sums is (up to a multiplicative constant) at most . \n (Weak prediction) This is best possible. \n Some additional gymnastics allow us to move from the prediction regarding multiplicative sequences of length where all the gaps between consecutive zeros behave like to sequences where, in addition, the maximum discrepancy behaves like . The probability that between every two consecutive zeros the maximum discrepancy will be smaller than is indeed small but if is large this too behaves like so the indirect applications of the LDH based on gaps between consecutive zeroes gives the same prediction as the direct prediction above. \n Here too the computation for general sequences gives the same outcome and indicates positive correlation between events which in the heuristic are pretended to be independent. \n (Let me remark that over the polymath5 threads there were several remarks in the direction of trying to show that there are no sequences of length where the differences between consecutive vanishing partial sums are bounded for all HAP. This is weaker than what is required to show that the discrepancy is unbounded.) \n Variations of EDP and related discrepancy problems: \n Variations \n Let me mention briefly several variations of EDP and what LDH says about them. The LDH is responsible for the guesses we propose in the previous post for variations E1-E8 of EDP. \n The weak LDH predictions will not change if we consider sequences where the zero entries occupy the indices divisible by 3. But the prediction fails in this case (the discrepancy can be bounded). \n If we consider only HAPs with prime power differences the LDH prediction is the square root of . I would guess that this is the true behavior. Note that if we consider HAPs with prime differences, we have the same LDH prediction, but since we can have bounded discrepancy the weak LDH prediction is false. \n The firm prediction of the LDH predicts a polylog(n) discrepancy (in fact even discrepancy) when we restrict our attention to square free integers and to random subsets of integers. \n Probabilistic analogs for EDP \n Problem 5: Does the LDH give good predictions for discrepancy problems for random hypergraphs and $on {1,2,\u2026,n}? \n Let be the hypergraph in which we consider precisely one edge which contains every element with probability . Let be the hypergraph obtianed fron by adding as edges to all initial segments of edges in . The LDH predicts that the discrapancy of $\\cal H$ is bounded. When we move from to then we come back to the prediction. Such probabilistic versions take away the number-theoretic aspect from EDP. Still the probabilities of low discrepancy for different edges are not independent and the problem still looks hard. \n LDH and the six standard deviation theorem \n Of course, it is of interest to understand the LDH predictions (both for and ) for the general case when we have probabilities and the edges are random subsets based on these probabilities. The most famous case is when we consider all s to be 1/2 and . Joel Spencer\u2019s Six Standard Deviation Theorem asserts that the discrepancy for random subsets of is at most . Nore generally it was proved that the discrepancy of a random hypergraph with edges behaves like when and like for . \n Roth\u2019s theorem and how the LDH predicts the answer \n Recall that Roth\u2019s theorem is about the discrepancy of the hypergraph whose edges correspond to all arithmetic progressions in {1,2,\u2026,n}. \n The LDH predicts the correct answer, at least roughly. The probability that a maximal AP of gap r will be confined to [-K,K] is . When we have to multiply the th power of , for , where is small and this will give us for . The contribution coming from APs of larger gaps will be of a smaller order of magnitide. \n Problem 6: Are the methods of proving upper bounds for the disrepancy problem for APs relevant for proving better upper bounds for EDP, its extensions, and its variations? \n Large deviations for extremal graph properties revisited \n Graph limits and the work of Chatterjee and Varadhan \n We started this post by trying to answer a classical problem in extremal graph theory using a probabilistic heuristic which is based on large deviation estimates. It would not be irresponsible to say that the heuristic estimates proposed a very poor prediction to the extremal problem we considered. \n Let be a fixed graph and let be the maximum number of edges for a graph on vertices that does not contain as a subgraph. Tur\u00e1n\u2019s 1941 theorem determined the value of when is a complete graph with vertices. Tur\u00e1n\u2019s theorem is the starting point of the wide and deep area of extremal graph theory. The case of triangle-free graphs goes back to Mantel in 1907. \n One of the important discoveries in graph theory which is related both to additive number theory and to probability theory is the notion of limits of graphs. This notion is connected to the famous Szemeredi lemma. The theory of limits of graphs sheds new light on extremal graph theory; in a sense it tells us what the relevant structures for are when is not bipartite. \n A recent paper entitled The large deviation principle for the Erd\u0151s-R\u00e9nyi random graph by Sourav Chatterjee and S. R. S. Varadhan revealed a connection between large deviation for properties of Erd\u0151s-Renyi graphs and graphons \u2013 limits of graphs. (It complements a large body of related results obtained by various other methods.) Here is the abstract. \n Abstract: What does an Erd\u0151s-Renyi graph look like when a rare event happens? This paper answers this question when p is fixed and n tends to infinity by establishing a large deviation principle under an appropriate topology. The formulation and proof of the main result uses the recent development of the theory of graph limits by Lovasz and coauthors and Szemeredi\u2019s regularity lemma from graph theory. As a basic application of the general principle, we work out large deviations for the number of triangles in G(n,p). Surprisingly, even this simple example yields an interesting double phase transition. \n So we can \u201cmorally\u201d understand why the weak prediction of LDH fails for the property of \u201cincluding a triangle\u201d. To obtain a better prediction we also need to condition on various relevant limit structures of the random graph. \n LDH for the triangle-free process \n Consider the following graph process. We start with the empty graph on vertices and add random edges one after the other conditioned on not forming a triangle. Bohman proved that such a process for will lead with substantial probability to a triangle-free graph not containing an independent set of size . This proved a conjecture of Joel Spencer and gave a new proof to a famous result by Jeong Han Kim (with a remarkable history that I won\u2019t describe here). Can we apply the LDH to the triangle-free processes to give a heuristic argument why triangle-free graphs on vertices with edges exist? \n Extremal problems for bipartite graphs \n Problem 7: Let be a fixed bipartite graph. Does the LDH give good predictions for ? \n It turns out that for extremal problems on graphs the LDH gives quite similar predictions to those obtained by the rigorous well known edge-deletion method based on the following proposition: \n Proposition: If, for a random graph with n vertices and 2m edges, the expected number of copies of is smaller than half the number of edges, then . \n For Tur\u00e1n-type problems, the LDH\u2019s predictions are quite similar to those obtained by the edge-deletion method. (I am not aware of a similar trick for discrepancy problems.)The LDH predicts the existence of -free graphs with roughly times more edges than what the edge-deletion method gives. Achieving an improvement of similar kind is a difficult and well-known problem in extremal combinatorics. See the paper: T. Bohman and P. Keevash. The early evolution of the H-free process. Inventiones Mathematicae, 181, 291\u2013336, 2010. \n The LDH prediction are still far from the correct answer. Erd\u0151s conjectured that for any bipartite with degeneracy the Tur\u00e1n number is at most , and that the Turan number is iff the graph is bipartite and 2-degenerate. For more on that see, e.g., N. Alon, M. Krivelevich and B. Sudakov, Turan numbers of bipartite graphs and related Ramsey-type questions , Combinatorics, Probability and Computing 12 (2003), 477-494."], "link": "http://gowers.wordpress.com/2012/08/27/edp23-second-guest-post-by-gil-kalai/", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://front.ucdavis.edu/": 3, "http://feeds.wordpress.com/": 1, "http://www.scribd.com/": 1, "http://www.cmu.edu/": 2, "http://mathoverflow.net/": 2, "http://en.wikipedia.org/": 1, "http://research.microsoft.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["The purpose of this post is to ignite some new activity related to polymath5 about Erd\u0151s\u2019 Discrepancy Problem. This post is thus a polymath5 research thread and comments (also unrelated to the suggestions in this post) are welcome. \n The general form of a discrepancy problem \n Let be a hypergraph, i.e., a collection of subsets of a ground set . The discrepancy of , denoted by is the minimum over all functions of the maximum over all of \n . \n We will mention one additional definition, that of hereditary discrepancy . When is a hypergraph and , the restriction of to is the hypergraph with vertex set whose edges are all sets of the form for edges of . The hereditary discrepancy of is the maximum over all of the discrepancy of restricted to . \n Here is a link for a recent post discussing discrepancy and the famous Beck-Fiala theorem. The Beck-Fiala theorem assert that if every element in is included in at most sets in then . (Of course, the theorem applies also to the hereditary discrepancy.) \n \n The Erd\u0151s\u2019 Discrepancy Problem reviewed \n The problem and links to polymath5 material \n Erd\u0151s Discrepancy Problem (EDP). Is it possible to find a -valued sequence and a constant such that for every and every ? \nA HAP (Homogeneous Arithmetic Progression) is an arithmetic prograssion of the form {k,2k,\u2026,rk}. EDP asks about the sum of a sequence on HAPs. \nGiven a sequence define to be the maximum of sums of subsequences over HAPs which are subsets of {1,2,3,\u2026,n}. EDP asks if we can find a sequence for which is uniformly bounded and we will be interested in finding sequences where grows slowly. \nEDP was extensively discussed and studied in polymath5. Here is the link of the first post . Here are links to all polymath5 posts . Here is the link to polymath5 wiki . \nA sequence is called completely multiplicative if for every and . EDP is of great interest even if we restrict our attention to completely multiplicative sequences. For those we have only to consider partial sums . \n An example where the discrepancy grows like log n. \n The function that takes n to 1 if the last non-zero digit of n in its ternary representation is 1 and -1 if the last non-zero digit is 2 is completely multiplicative and the partial sum up to n is easily shown to be at most . Therefore, the rate at which the worst discrepancy grows, as a function of the length of the homogeneous progression, can be as slow as logarithmic. \n What a random assignment gives us. \n A random sequence (or a random completely multiplicative sequence) gives us discrepancy close to . (There is apparently an additional factor but I am not sure of the precise asymptotic behavior.) \n Two greedy algorithms \n From now on we write instead of . \n Greedy algorithm 1 (for multiplicative functions): Assign the value as to minimize the maximum discrepancy in every partial sum whose terms are now determined. \n Greedy algorithm 1 (for general functions): Assign the value so as to minimize the maximum discrepancy in every HAP whose terms are now determined. \n Problem 1: How does Greedy algorithm 1 perform? \n Empirical observation: (This was claimed in some polymath 5 remarks but I am not sure if there was definite evidence. I would appreciate clarifications.) The discrepancy for sequences based on the greedy algorithm 1 (for multiplicative functions and for general functions) is . \n Interpretation: Greedy algorithm 1 optimizes an \u201cirrelevant task\u201d. \nWe would like to suggest here \n Greedy algorithm 2 (for multiplicative sequences): Assign the value so as to minimize the maximum discrepancy in every partial sum where unassigned entries get the value zero. \n Greedy algorithm 2 (for general sequences ): Assign the value f(n) as to minimize the maximum discrepancy in every partial sum in every HAP where unassigned entries get the value zero. \n Problem 2: How does Greedy algorithm 2 perform? \nOmri Floman ran Greedy 2 on inputs such as N=10000 and got a discrepancy of around 20 (there is a bit of randomness involved in cases of ties). For N=100000 he got about 45. It is unclear what the behavior is. \n Problem 3: Can we find an optimal compromise between Greedy 1 and Greedy 2? \n Problem 4: Can randomization help? \nOf course, since we know that a sequence of length n and discrepancy log n exists if we draw a random sequence there is a probability larger than of reaching such a low discrepancy sequence. What we want to ask is if randomization can lead to a method of getting a low-discrepancy sequence with larger probability, or even better with provable larger probability. (Or even better yet, to a sequence of provable low discrepancy via an effective algorithm.) \n Problem 5: How do Greedy algorithms 1 and 2 perform if we apply them for a random ordering of {1,2,\u2026,n}. \n Extensions of Erd\u0151s\u2019 Discrepancy problems \n Here are some variations of the Erd\u0151s\u2019 Discrepancy Problem along with some guesses for the answers. I will explain where these guesses came from next time. \n E0: Erd\u0151s\u2019 Discrepancy Problem \n Guess : \n E1 : Allow f(n) to attain values which are complex numbers of norm 1. \n Guess , same answer \n (Perhaps we can even consider norm-1 vectors in some Euclidean space or Banach space. If this is too premissive ( ) we may go down to a constant.) \n Square-free integers \n E2 : The EDP for square-free integers \nHere we simply consider sequences where if is not square-free. For this variation the multiplicative version of the problem is not a special case of the general question. (Here multiplicative means that if are coprime.) \n Guess: for both versions, same answer . \n Random subsets of integers \n E3: Instead of square-free integers consider a random dense subset of integers and assume that the sequence vanishes for indices not in the subset. \n Guess: same answer . \n Random sets of primes \n Here we consider multiplicative functions which are non zero only on a random dense sert of primes. \n E4 : The EDP for a random dense subset of primes. \n Guess: same answer \n Problem 6: Find a sequence for problems E2, E3, and E4 with discrepancy , or better or better . Update : see below. It can be shown that sequences with discrepancy exist for all these variations. \n Prime power differences \n E5 : The EDP for HAP with prime power differences. \n Guess: . \n Beurling primes and integers \n Beurling primes were defined by Arne Beurling in 1937 and he also proved a prime number theorem for them. The most general definition is very simple: Consider a sequence of real numbers regarded as \u201cprimes\u201d and consider the (ordered) sequence of their products (multiplicities allowed) as the \u201cintegers\u201d. (We will assume that all products are distinct although for the original purpose of defining a zeta function multiplicities may play a role.) Beurling primes played a role in the polymath4 discussions . \n E6 : The EDP for Beurling primes and integers \n Careless Guess: at most . \n Sune Kristian Jakobsen\u2019s systems of pseudointegers \n One way to think about Beurling primes is to identify with and to reorder the integers according to the ordering of the s. Actually, given the ordering we can recover uniquely the Beurling primes. A much more general notion of \u201cpseudointegers\u201d was suggested over polymath5 by Sune Kristian Jakobsen. See also the overview over Polymath5\u2032s wiki . \nAn ordering of the natural numbers is a SKJ-ordering if it fulfills the following two conditions: \n1) If and then . \nand \n2) for any the set is finite. \n Remark: 1) Sune Kristian considered orderings on sequences of integers (the exponents in the prime factorization). This is equivalent to (but perhaps less provocative than) the formulation here. 2) We can expect that Beurling-orderings are a tiny tiny subset of SKJ-orderings. \nGiven an SKJ-ordering of the natural numbers we can ask the EDP for that ordering. \n E7 : The EDP for Sune Kristian Jakobsen systems of integers. \n Careless Guess: at most polylog n \n Problem 7 : How does Greedy algorithm 2 perform for variations E2 -E7. \nThe answers for E2, E3 and E4 is especially interesting because these are examples where the best upper bounds I know behave like and are obtained from a random assignment. (See problem 5.) \n Problem 8: Prove the assertion of EDP (namely that the discrepancy is unbounded) for an SKJ-pseudointegers of your choice. \n Related Discrepancy Problems \n Discrepancy and hereditary discrepancy \n Recall that for a hypergraph defined on a ser the discrepancy of , denoted by is the minimum over all functions of the maximum over all of \n . \n When is a hypergraph and , the restriction of to is the hypergraph with vertex set whose edges are all sets of the form for edges of . The hereditary discrepancy of is the maximum over all of the discrepancy of restricted to . \n An operation of hypergraphs \n Let be a hypergraph on a vertex set and assume that is ordered, e.g., . Consider the hypergraph obtained from by adding for every set all its initial subsets w.r.t. the ordering. We will consider the operation of moving from to . Note that for EDP all our variations E1-E8 the underlying hypergraph is obtained by this construction . (From a certain natural hypergraphs ). I further guess that for all the variations E1-E8 if we consider the hypergraph (before taking initial segments) then the discrepancy is bounded. \n Probabilistic versions \n Let be a ground set and let be a set of reals. First consider the discrepancy problem for a random hypergraph with edges, whose ith edge is a random set so that every element of belongs to with probability (and all these events are statistically independent). Next we can move from to as described above. \nThe case of taking for the probabilities can be seen as a probabilistic analog of EDP. My guess for the discrepancy of this case is also . I also guess that the discrepancy of itself is bounded. \n Problem 9: How do Greedy algorithms 1 and 2 perform for the random hypergraph . \n Roth\u2019s theorem about the discrepancy of arithmetic progressions \n For EDP the ground set is all natural numbers, or just the set {1,2,\u2026,n} and the hypergraph is the collection of all HAPs(homogeneous arithmetic progressions). Roth considered the hypergraphs of all arithmetic progressions. Roth proved that the discrepancy in this case is at . The existence of with discrepancy of was proved by Beck and the factor was removed by Matousek and Spencer. These works by Beck, Matousek and Spencer may be very relevant to prove the existence of low-discrepancy sequences for EDP and some of its extensions. \n Some results with Noga Alon \n Here are some results proved in collaboration with Noga Alon which are based on the Beck-Fiala theorem and a general argument on moving from to . (It is likely that some of them are known and we will be happy to know.) \n Proposition 1: For the discrepancy of described above is at most . \nThis follows from the following general proposaition: \n Proposition 2: Let be a hypergraph on an element ordered set and let be the maximum degree of a point of , then . \n Proof : Let be a partition of into two nearly equal intervals, a partition of into 2 nearly equal intervals and similarly , etc ( levels). Now define a new hypergraph obtained from by replacing each set in by the following sets (possibly some empty): \n , , , (for all ), . \n Note that the maximum degree of a point in is , hence the discrepancy of is at most by Beck-Fiala\u2019s theorem. Also, each initial segment of is a union of at most pairwise disjoint members of . This gives Proposition 1, and since for the case described in Proposition 2 the degree of vertices for behaves like we obtain Proposition 2 as well. \n Proposition 3: for versions E1-E8 there are examples where the discrepancy is . \n Proof: In all these examples we consider a hypergraph on the ground set A={1,2,\u2026,n}, or on a subest of A with positive density (or for E7 and E8 on another non conventionally ordered set of integers). Then move to the hypergraph of initial subsets of edges of . To apply Proposition 2 and Beck-Fiala\u2019s theorem we need to find an upper bound on the degree of a vertex in the hypergraph $\\cal H$. Consider the case of EDP. (The other cases are similar.) The maximum degree is obtained by an integer that is the product of the first k distinct primes. In this case the degree is smaller than . Note that the proof applies even to hereditary discrepancy. \n Problem 10: Use similar ideas to prove better (even polylog (n)) constructions for EDP and its variations E1 - E8 . \nI extend all my earlier guesses when we move from discrepancy to hereditary discrepancy. \n Proposition 4: The hereditary discrepancy for the hypergraph of HAP on {1,2,\u2026,n} is . \nThe proof is obtained as follows: Consider the first primes, and a hypergraph on {1,2,\u2026,m} of discrepancy . (For example, a Hadamard matrix of order describes such an hypergraph. ) Next, for every edge consider the integer that correspond to products of primes whose indices are in . Then restrict your attention only to these integers. \nPropositions 3 and 4 show that the hereditary discrepancy of the hypergraphs of HAPs in {1,2,\u2026,n} is between and . \n A Quick Outlook at Polymath 5: EDP 1-21 \n Before our quick review of polymath5, let me mention a major difficulty which seems relevant to all approaches: \n If we allow zero entries in our sequence, even a small density of zero entries, then the discrepancy can be bounded. \n For example consider the sequence 1, -1, 0, 1, -1, 0, 1, -1, 0, \u2026 \n Experimentations: Computer experiments played a large role in polymath5. One of the most striking discoveries was a sequence of length 1124 of discrepancy 2. Later a second sequence of length 1124 and discrepancy 2 was found but not a larger sequence. Several people made great contributions with computer experiments. \n Additive Fourier analysis ideas: Using Fourier analysis on our sequence and somehow reaching a contradiction when we assume that the discrepancy is bounded was a suggestion that was dominant in the first few threads and we came back to from time to time. \n Terry Tao\u2019s reduction: Terry Tao found a reduction from the general question to the variation of multiplicative functions with complex norm-1 values ( E1 ). The beautiful proof relies on \u201cmultiplicative\u201d Fourier analysis and it is striking how \u201clittle\u201d the proof uses. \n Problem 11 (Sune Kristian Jakobsen): Does Tao\u2019s reduction apply to arbitrary SKJ-pseudointegers. \n Semi-definite programming: A major turning point was a suggestion by Moses Charikar to use a natural semi-definite relaxation for the problem. This promising avenue was explored over several threads and here too computer experimentation was done. One reason to regard Charikar\u2019s approach (and the related linear programming approach described next) as hopeful is precisely because it offers a convincing way to get around the difficulty demonstrated by the sequence 1, -1, 0, 1, -1, 0, \u2026 \n A generalization and linear programming: The last few threads were centered around a different related relaxation proposed by Tim Gowers. The problem was generalized from a single-sequence problem to a pair-of-sequences problem. (This is a common motif in extremal combinatorics although here the motivation came from functional analysis.) Then relaxation of the problem led to a very appealing linear programming question. \n Mathoverflow questions: Mathoverflow was used to ask several questions related to the project. \n Participation: Polymath 5 attracted much interest and wide participation. Overall, it did not attract researchers with major prior interest in discrepancy theory. \n Discrepancy \n Discrepancy theory is a huge an exciting area. Let me just give references to some books. \nJozsef Beck, William W. L. Chen: Irregularities of Distribution , Cambridge University Press, 1987. And, Jozsef Beck: Irregularities of Distribution (Cambridge Tracts in Mathematics Cambridge Tracts in Mathemat Volume 89)(Paperback) \nBernard Chazele: The Discrepancy Method: Randomness and Complexity \nJiri Matousek: Geometric Discrepancy: An Illustrated Guide (Algorithms and Combinatorics) \nJ. Beck: Combinatorial Games: Tic-Tac-Toe Theory , Cambridge University Press, 2008. \nJ. Beck: A forthcoming book. \nReading these books will prepare you better to deal with EDP and will enrich your life tremendously."], "link": "http://gowers.wordpress.com/2012/08/22/edp22-first-guest-post-from-gil-kalai/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 3, "http://feeds.wordpress.com/": 1, "http://michaelnielsen.org/": 2, "http://en.wordpress.com/": 1, "http://gilkalai.wordpress.com/": 1, "http://mathoverflow.net/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["A few months ago, Gil Kalai got in touch to suggest that it was time to revisit Polymath5, the attempt to prove the Erd\u0151s discrepancy problem. I agreed, but we have both been rather busy in the interim, so it is only now that we are ready. One of the things that Gil did was write three posts, which I shall be putting up as guest posts, the first one today and the others over the next few days. I hope that people who contributed to the project the first time round will consider having another go, and that people who didn\u2019t, but would be interested, will find that they can use the wiki that we created to record our progress to get up to speed with what is already known. Once Gil\u2019s posts are up, I\u2019ll probably write a post myself, and I hope that some serious work might start in early September. I always felt that when the original attempt petered out, it was just a kind of loss of energy that individual mathematicians often feel, rather than the result of hitting a brick wall. And just as having a break from a problem is often useful for individuals, I hope it will turn out to be for this polymath project. If it doesn\u2019t, then maybe I\u2019ll do something I meant to do much earlier, which is write up in paper form the progress that has already been made. (Of course, if I do that, then anybody is free to contribute to the writing.) \n If you want to look at some of the earlier posts, they are collected together in the polymath5 category on this blog."], "link": "http://gowers.wordpress.com/2012/08/22/edp-a-possible-revival/", "bloglinks": {}, "links": {"http://michaelnielsen.org/": 1, "http://gilkalai.wordpress.com/": 1, "http://gowers.wordpress.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["The short answer if you don\u2019t feel like reading a post with some actual mathematics in it is that I don\u2019t know. \n Now for the longer answer. A subset of is called a Sidon set if the only solutions of the equation with are the trivial ones with and or and . Since the number of pairs with is and whenever , it is trivial that if is a Sidon set, then , which gives an upper bound for of around . \n \n There is also a matching lower bound, up to a constant. I think it is due to Erd\u014es. One notes first that the subset of defined by is a Sidon set inside . Indeed, if , then and . Assuming the quadruple is not degenerate, , so we can divide the first equation by the second to deduce that , and that implies that and , so the quadruple is degenerate for a different reason. \n Thus, we can find a Sidon subset of of size , which is the square root of the size of . We can now regard this set as a subset of , and then map each point to the number . It is easy to check that the resulting set is a Sidon subset of . \n Much more is known about this: the correct bound is now known up to a constant. However, that is not the theme of this post. What I want to discuss is whether any kind of converse of this result is true. That is, can one say that a Sidon set of size close to must be constructed in some kind of quadratic way? \n Some very weak evidence in favour of a conclusion like that is that random methods fail miserably to produce Sidon sets of anything like size . Indeed, let\u2019s pick a subset of randomly by choosing each element with probability . The total number of quadruples with taken from is roughly (since apart from a few edge effects and can be chosen more or less freely and they determine ), and each nondegenerate one has a probability . So the expected number of nondegenerate quadruples with is roughly . For this to be smaller than (so we can remove a point from each one and still have plenty of left), we need to be significantly smaller than , which gives a bound of , which gives us . \n There are several other ways to see that random sets of size are very different indeed from Sidon sets, which suggests that Sidon sets of size close to must have interesting structure. But what is that structure? \n It isn\u2019t easy even to state a conjecture here, since before we map the subset of to the integers, we can apply an invertible linear transformation to it. It is hard to think of a way of characterizing the kinds of sets that can arise like this, let alone thinking about how to make the structure looser when the set has size only . (Additive combinatorialists will immediately recognise that I am inspired by Freiman\u2019s theorem here.) \n But today \u2026 make that yesterday as a night has passed since I wrote those two words \u2026 I was at a talk given by Javier Cilleruelo, thanks to which I learnt of an example that places a much more serious constraint on any conjecture one might hope to make. The example is one I sort of knew already, since it is based on a brilliant argument of Imre Ruzsa, who created an infinite Sidon set such that the asymptotic size of is around . (As with the finite case, is very easy; the true answer is conjectured to be ; Ruzsa was the first person to beat and his exponent has not been improved.) What Cilleruelo mentioned in passing was that you can use some of Ruzsa\u2019s ideas in an easy way to get a pretty decent example in the finite case. This is something I hadn\u2019t realized. I presume Ruzsa himself was well aware of it, but I don\u2019t remember it being mentioned in his paper (though I haven\u2019t looked at the paper for many years, so he might have done). What I find particularly interesting is that the example is completely different from the graph-of- type examples. \n Since the construction is simple (to understand, that is) and rather gorgeous, I thought it merited a blog post. And maybe it can also serve as an advertisement for Ruzsa\u2019s amazing paper. \n It begins with the observation that the logarithms of the primes form a Sidon set of reals: if are primes with , then either and or and ; therefore, the same is true if . \n \u201cSo what?\u201d it is tempting to say. After all, the logs of the primes are not integers. Indeed, there are far more of them than there are integers. \n However, at this point a principle comes in that I often feel doesn\u2019t deserve to be as incredibly fundamentally useful as it in fact is: that two distinct integers must differ by at least 1. It\u2019s not too much of an exaggeration to say, for example, that to prove that a concrete number is irrational or transcendental you have to find some way of reducing the problem to this principle. (That is, you say that if your number is rational/algebraic then there must be two distinct integers that have difference less than 1.) For this problem we make a much simpler use of the principle. If , then , since are integers. Since the derivative of is , that tells us that and differ by at least . (One could be a bit more careful here: I think Cilleruelo stated a bound of .) \n Note what we now have that\u2019s better: instead of merely knowing that in nondegenerate cases, we have a lower bound on the difference. This allows us to approximate and discretize. \n So let\u2019s start with all the logs of the primes up to , of which there are about by the prime number theorem. If we multiply those by say , then all the differences between the distinct sums, which were previously at least , are now at least . Therefore, if we move each number to the nearest integer, the differences will be altered by at most 2, so will be at least 1. So we have a Sidon set! It is a subset of the integers up to and it has size around , so this construction gives a Sidon subset of of size around . \n The moral of this is that to prove a structural result about Sidon sets of density close to , then one would probably have to insist on close meaning \u201cwithin a constant of\u201d (which one would then relax to some slow-growing function later). Or alternatively, one would look for a much more general notion of structure, but I don\u2019t see an obvious generalization of the two examples I now know. Another moral is that it\u2019s worth looking for more examples of dense Sidon sets before even trying to make a conjecture."], "link": "http://gowers.wordpress.com/2012/07/13/what-are-dense-sidon-subsets-of-12-n-like/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["As I said would happen in my post about a possible approach to teaching maths to non-mathematicians aged 16-18, I went last Wednesday to Watford Grammar School for Boys to try the approach out. The headmaster there, Martin Post, was remarkably helpful and assembled a usefully varied group of pupils, some from his school, some from the equivalent school for girls , and some from a nearby mixed comprehensive school (I wasn\u2019t told which one) whose pupils receive some of their teaching in scientific subjects from Watford Grammar School. What\u2019s more, some of the people there were doing maths and further maths, some were doing just maths, and some were not doing either. The one thing that was not representative about the group was that they were much brighter than average: for example, the non-mathematicians there had been chosen by their teachers as clever people who could have done maths but decided that they were more interested in other things. For most of the rest of this post, I\u2019ll say what questions I discussed and how the discussions went. All but two of them were taken from the list in the earlier post. \n \n I began with a warm-up: I asked how many times a ball was kicked during the recent European Football Championship. Of course, I made it clear that I wasn\u2019t after an exact answer. My first surprise of the day was that pretty well the moment I had asked the question the room went from almost silent to very far from silent: after a while I got used to this. The conversation was, it seemed, entirely about the question at hand. One person asked whether I was interested in the number of passes or the number of actual kicks. I said the latter. \n After a while, and when I had managed to quieten everyone down, I drew a logarithmic scale on the whiteboard, going up the powers of 10 from 10 to 1,000,000. I then asked people for their answers and marked them roughly on the scale. There was a significant cluster around 100,000 (by which I mean in an interval from about 70,000 to about 120,000). I then asked people how they had arrived at their answers. \n Everybody seemed to know that there had been 31 matches. (Proof: there were four groups of four, so 24 matches at the group stage, and then eight teams left in a knockout competition.) So I wrote that down on the whiteboard. From that point there were two approaches. Unfortunately, I can remember only one of them, which is the one I would have used: to estimate the average amount of time between kicks. The person who proposed this approach thought that was 1-2 seconds, so I asked what we should do with that information. That quickly led to the calculation , which is about . I don\u2019t know whether they were used to doing things like saying \u201c90 is basically 100\u2033 but they quickly got into the spirit of it. \n Next up, I said that sometimes one reads of prophetic dreams, and that in order to get a feel for how seriously to take them I wanted to know how significant it would be if somebody dreamt of the death of someone close to them and that person died the next day. So I asked a rather vague question: what is the probability of that happening? I followed that up by asking what information it would be useful to know. \n One suggestion that came in quite quickly was the probability that someone dies on any particular day. That led to a discussion of whether we were talking about expected deaths (e.g. if someone is very ill or very old) or unexpected deaths. There was general agreement that if someone is expected to die soon, then an apparently prophetic dream would not be too surprising, since both the dream and the death are more likely. So we decided to restrict attention to unexpected deaths. In the end we calculated the probability by simply estimating the number of days in 70 years \u2014 we went for 25,000 \u2014 and taking the reciprocal. \n What else did we need to know? At this point an interesting discussion arose. One person suggested that a relevant parameter was the number of people that a typical person knows. That led to a subdiscussion about how close an acquaintance needed to be to be counted. I suggested that pretty well any identifiable acquaintance would be surprising enough to be worth counting. One other clarification asked for was whether we were talking about reports of prophetic death dreams or true reports of prophetic death dreams. I said the latter. \n I haven\u2019t yet said what the interesting discussion was. It was that some people thought it was relevant how many people people typically know, while other people thought it was not relevant at all. The arguments were roughly as follows. Those who thought it was relevant said that if somebody died, then you needed to know how many people they knew if you wanted to assess how likely it was that one of those people had dreamt of their death the night before. Those who thought it was not relevant argued that if someone dreams of a death, then all you care about is the chances that the person dreamt about will die the next day. \n Did we have a paradox? Somebody gave a reasonable explanation of why we didn\u2019t, which I made a bit more precise by saying that we were conditioning on two different events (the occurrence of a dream and the occurrence of a death), so it wasn\u2019t too surprising that we had to take into account different things. \n But the problem still wasn\u2019t very clear, which itself became clear when people wondered whether we needed to know how many people there were in the whole world. In the end I stated the problem more precisely: roughly how many times per year would we expect somebody in the UK to have a death dream that comes true unexpectedly the next day? We settled on the following method: estimate how many times a death dream occurs per person per year, multiply by the population of the UK, and divide that by 25,000. I asked people how often they had death dreams per year and there were very different answers: some people said they thought 10 was about average and seemed surprised when I said that I thought that for me it was more like one every two years. We settled on 3, which I still found a bit high, but not outrageously so. \n So that led to the calculation . I think we had already multiplied 3 by 50,000,000 to get the number of death dreams per year, so what I actually wrote down was 150,000,000/25,000. So I naughtily changed that to 250,000,000/25,000 which gave me 1,000 and then compensated for the naughtiness by dividing that by 2 to get 500. That confused one or two people, so I explained it again more slowly. \n While I can remember which questions I discussed during the session, I can no longer remember the order in which I covered the next three. However, I think it was either at this point or after the next question I discussed that there was a short break \u2014 not that it is too important. Anyhow, at some point round now I described Terence Tao\u2019s airport-inspired puzzle , which is the following question. You want to get from one end of an airport to the other and your shoelace is undone. If you want to get to your destination as quickly as you can, is it better to tie your shoelace when you are on a moving walkway or when you are on stationary ground? \n The moment I had finished asking the question, the noise switch was in its \u201con\u201d position again, which was fine. After I had managed to quieten the room down again (which was hardish but not impossible), I took a vote. The options I gave them were \u201cfaster on walkway\u201d, \u201cfaster off walkway\u201d, and \u201cit makes no difference\u201d. \u201cFaster on walkway\u201d got a few votes, \u201cfaster off walkway\u201d got one vote, and \u201cit makes no difference\u201d came top, though not by miles. There were also a number of abstentions. \n The next thing I did was invite people to argue for the answers they had voted for. The one person who had voted for \u201cfaster off walkway\u201d did not look at all keen to do this, but after a little while someone else said, \u201cYou would need to know the speeds of the walkways, your walking speed, and the time it takes to tie your shoelace,\u201d which prompted me to add a \u201cnot enough information\u201d option, which was popular, and to which the \u201cfaster off walkway\u201d person transferred her vote. \n Here are some arguments I collected. In favour of the \u201con walkway\u201d answer was that when you tie your shoelace you are moving along rather than wasting time being still. In favour of the \u201cmakes no difference\u201d option was that wherever you tie your shoelace it takes the same amount of time. And when I tried to elicit an argument for the \u201cnot enough information\u201d option, I failed: I asked under what circumstances it would be better to stop on the walkway and under what circumstances it would be better stop off the walkway, but nobody had much to suggest. I also explained that while knowing all the various speeds and times is sufficient to work out the answer, it isn\u2019t obviously necessary: for example, if you are deciding whether it is better to use the walkway or not use it, you don\u2019t need to know how fast it is. \n A notable aspect of the discussion that permeated the whole thing was that people were keen to introduce non-mathematical considerations. For example, they (rightly) pointed out that it would make a difference if the airport was crowded: if the people just ahead of you on the walkway are stationary and you can\u2019t get past, then obviously that\u2019s a good moment to tie your shoelace, and if the people just behind you are in a hurry, then you will annoy them by stopping. So I asked them to assume that the airport was deserted. Other assumptions that had to be spelt out were that your walking speed is unaffected by having a shoelace undone and that there isn\u2019t a robot sensor on the walkway that detects that you are tying your shoelace and stops it. Later on I found out who the mathematicians were and who the non-mathematicians were. It turned out that many of these objections came from the non-mathematicians. They were very helpful to the discussion, since one of the points I wanted to get across was that mathematical modelling involves choosing appropriate simplifications. One of the non-mathematicians begged me to say what the answer was (well before I wanted to do so) so the question had clearly worked its magic. \n If I had been discussing this question as part of a genuine course, I would have waited much longer for someone to come up with a good solution. But as I wanted to get through several questions, I hurried things along a bit. I started by offering the advice to consider extreme cases. To illustrate what I meant by an extreme case, I suggested that maybe the walkways could be very very fast. (Actually, I think that would have helped people guess the answer, but it wasn\u2019t the extreme case I had in mind.) Pretty soon after that, someone suggested considering the case where someone starts tying their shoelace just after getting on a walkway. Since that was exactly the case I was interested in, I basically gave away the answer at that point by suggesting also considering the case where someone starts tying their shoelace just before getting on the walkway. Most people could see that that was a proof that doing it on the walkway was better. One or two had a bit of trouble still, so I drew some pictures of two people and what would happen if one stopped just before and one just after getting on. \n If I\u2019d had more time I would have tried to get everyone to understand why the \u201cyou spend the same amount of time wherever you do it\u201d argument gives the wrong answer. \n Another thing I did at around this stage was what I call the dividing-up-the-pot game. I first asked question 41 from my earlier post, which is this. You and a friend are out for a walk, when you are approached by a stranger, who offers the two of you \u00a31000 on one condition: that you agree how to split it between you. After establishing to your satisfaction that you are not about to be kidnapped, you propose a 50-50 split to your friend. To your astonishment, your friend insists on receiving \u00a3900 with only \u00a3100 going to you, and appears to be prepared to lose all the money rather than accept anything less than this deal. What should you do? \n The main person to make a suggestion suggested that you should call your friend\u2019s bluff, since when the stranger starts to walk away, your friend would be bound to say, \u201cOK OK stop! I\u2019ll settle for \u00a3500!\u201d I wasn\u2019t sure what more to say at this point, so I told them that they would now have a chance to see for themselves. We would have five rounds, in each of which people would be randomly paired and would have the chance to share ten points, on condition that they could agree how to do so. At the end of the five rounds we would see who had the most points. To do the random pairings I had brought along (part of) a pack of playing cards, which I shuffled and dealt out once per round, and each person was paired with the person who had a card of the same colour and same denomination. \n It took a while to get through the rounds, but once it was finished, I asked whether anyone had 30 or more points. Several did. 35 or more? Nobody. 34? Yes, one person. \n I then asked people what their strategies had been. To my surprise, several people had used randomized strategies, the main one being to decide by the toss of a coin (or in one case stone, scissors and paper) who would get all the points. I asked one person why he had adopted that strategy. His response was that the main aim of the game was to come top, which one couldn\u2019t do by sharing points out equally and couldn\u2019t easily do by insisting on lots of points each round. So he judged that the all-or-nothing strategy gave him his best chance of coming top. He turned out to be one of the people doing maths and further maths. I think his strategy got him 30 points. The person who got 34 points got them by, as he put it, \u201cthreatening his opponents.\u201d \n I then started a discussion of what possible other strategies there might be. I had planned to think about a population of strategies and how they would perform against each other, making a link with things like the evolution of morality, but after a few seconds I aborted it because I thought it would be too long and complicated. But this was something else that I would definitely have tried to do if I had devoted more time to this one question. \n Similarly, I had a brief discussion of what real-life situations might be reasonably well modelled by this game. There were some suggestions that I didn\u2019t like and now can\u2019t remember. (The problem with one of them was that there was no analogue of getting zero if you can\u2019t agree.) There was one about haggling, which seemed fine. I gave the example of the Conservatives and Liberal Democrats drawing up their coalition agreement two years ago, and said that any negotiation where there is a range of outcomes that would benefit both parties could be thought of as an instance of this game. If I had had more time I would have held back for much longer before giving these examples. \n Next (or possibly previously), I asked question 37(i), which is the following. In several parts of the UK the police gathered statistics on where road accidents took place, identified accident blackspots, put speed cameras there, and gathered more statistics. There was a definite tendency for the number of accidents at these blackspots to go down after the speed cameras had been installed. Does this show conclusively that speed cameras improve road safety? \n The same person who argued for the randomized strategy in the negotiation game basically knew the answer to this question already. He said no, since if you pick out the extreme cases then you would expect them to be less extreme if you run the experiment again. I decided to move on quickly from this question since there wasn\u2019t a lot more to say. But I told people about a plan I had had, which was to do a bogus telepathy experiment. I would get them to guess the outcomes of 20 coin tosses, which I would attempt to beam to them telepathically. I would then pick the three best performers and the three worst, and would toss the coins again, this time asking the best ones to help me beam the answers to the worst ones. People could see easily that the performances would be expected to improve and that it would have nothing to do with telepathy. One person said, \u201cOh I want to do that,\u201d of the experiment. \n Penultimately, I discussed the following table, slightly modified from an example of Joseph Malkevitch. There are five options, and the orders of preference amongst 55 people are as follows. \n 14532 \u2014 18 people \n 25431 \u2014 12 people \n 32541 \u2014 10 people \n 43251 \u2014 9 people \n 52431 \u2014 4 people \n 53421 \u2014 2 people \n Which option should the group go for? (I gave examples of different film genres for the options in a key below, but we didn\u2019t use those.) \n I asked for relevant observations. The group seemed pretty clued up on voting methods, so I was soon writing up that option 1 would win under first past the post, and someone calculated that option 3 (if I remember correctly) would win under the alternative vote. I then suggested doing pairwise comparisons, and was very gratified when everyone thought that transitivity was obvious (though one person did say that we should consider not just the preferences but the strengths of the preferences), so I could surprise them by showing that it failed in this case. I then gave the simple 123/231/312 example. Some people were keen on the system where you award 5 points for a first preference, 4 for a second and so on. We could probably have discussed this for quite a bit longer. I briefly mentioned Arrow\u2019s theorem, without stating it precisely, and moved on to my final question. \n I told them that for the last couple of years I have been reading Proust. (I asked how many of them had heard of Proust. One had.) More precisely, I have been reading Remembrance of Things Past, Scott Moncrieff\u2019s translation of Proust. The edition I have is in twelve volumes, and my practice has been to read one volume of Proust for every two or three other books I read (my reading taking place before I go to sleep at night). I\u2019ve now read eight of the volumes, and on page 25 of the eighth, I came up against this sentence. (I didn\u2019t tell them that the eighth volume was the second half of \u201cSodom and Gomorrah\u201d, which Scott Moncrieff coyly translates as \u201cCities of the Plain\u201d.) \n But if the course of life, by making Cottard assume, if not at the Verdurins\u2019, where he had, because of the influence that past associations exert over us when we find ourselves in familiar surroundings, remained more or less the same, at least in his practice, in his hospital ward, at the Academy of Medicine, a shell of coldness, disdain, gravity, that became more accentuated while he rewarded his appreciative students with puns, had made a clean cut between the old Cottard and the new, the same defects had on the contrary become exaggerated in Saniette, the more he sought to correct them. \n I displayed it for them and asked how we might go about checking that it was syntactically correct, and, even better, actually understanding it. It was projected on to a whiteboard, so I was in a position to annotate it. Fairly quickly the suggestion came in to use brackets. Somebody also said that we should work from the inside outwards. So I asked where people would like the brackets to go. The first suggestion was to have a bracket opening before \u201cbecause of the influence\u201d and ending after \u201cAcademy of Medicine\u201d. That looked right, but turned out to be wrong: the opening bracket needed to be before \u201cif not at the Verdurins\u2019\u201d. However, eventually we sorted it all out, and I got them, with a bit of prodding, to say that the general principle we were using was to put brackets round bits of text if the sentence still makes sense with those bits removed. (It was because it initially looked as though one could jump from \u201cwhere he had\u201d to \u201ca shell of coldness\u201d that we made the initial mistake.) \n If it had not been inconvenient to do so, I would have rewritten the sentence with indentations, as follows. \n But if the course of life, \n\u2026.by making Cottard assume, \n\u2026\u2026..if not at the Verdurins\u2019, \n\u2026\u2026\u2026\u2026where he had, \n\u2026\u2026\u2026\u2026\u2026.because of the influence that past associations exert over \n\u2026\u2026\u2026\u2026\u2026.us when we find ourselves in familiar surroundings, \n\u2026\u2026\u2026\u2026remained more or less the same, \n\u2026\u2026..at least in his practice, in his hospital ward, at the Academy of \n\u2026\u2026..Medicine, \n\u2026.a shell of coldness, disdain, gravity, \n\u2026\u2026..that became more accentuated while he rewarded his \n\u2026\u2026..appreciative students with puns, \nhad made a clean cut between the old Cottard and the new, \nthe same defects had on the contrary become exaggerated in Saniette, \nthe more he sought to correct them. \n That would have shown more clearly just how many levels of subordinate clauses there were in the sentence. I would also have talked about trees and perhaps broadened the discussion into one about parsing trees for other sentences (which would have required them to decide and try to justify what the natural tree structure of a sentence is). \n I didn\u2019t do this at the time, but it is quite interesting to try to rewrite the sentence to make it more comprehensible. Here is what happens if instead of chopping clauses in two, you try to put subordinate clauses at the end. For example, instead of, \u201cHe decided, because he was in a good mood, to smile at people he would usually have ignored,\u201d you write, \u201cHe decided to smile at people he would usually have ignored, because he was in a good mood.\u201d The sentence above can be reordered as follows. \n But if the course of life had made a clean cut between the old Cottard and the new, by making him assume a shell of coldness, disdain, gravity that became more accentuated while he rewarded his appreciative students with puns, if not at the Verdurins\u2019, where he had remained more or less the same because of the influence that past associations exert over us when we find ourselves in familiar surroundings, at least in his practice, in his hospital ward, at the Academy of Medicine, the same defects had on the contrary become exaggerated in Saniette, the more he sought to correct them. \n It still isn\u2019t easy, but it\u2019s easier. Does it lose something important? I think it loses some of its distinctively Proustian character \u2014 decoding this kind of sentence is part of the peculiar pleasure of reading Proust \u2014 but why it should be pleasurable rather than just a nuisance is hard to say. \n I might add that the sentence is fairly typical of the whole book, though not many sentences go quite that far. And there are similar phenomena at other scales: paragraphs sometimes go on for several pages, and chapters can go on for hundreds of pages. So not only does one have to read twelve volumes of 350-400 pages each, but the reading is slow going. Out of curiosity, I looked up the sentence in the original French. Interestingly, it uses one pair of brackets, which makes it a lot easier to understand. (What I mean by \u201cinterestingly\u201d is that I wonder why Scott Moncrieff decided to do without the brackets.) Here it is. I got it from an online version . \n Mais si la vie, en faisant rev\u00eatir \u00e0 Cottard (sinon chez les Verdurin, o\u00f9 il \u00e9tait, par la suggestion que les minutes anciennes exercent sur nous quand nous nous retrouvons dans un milieu accoutum\u00e9, rest\u00e9 quelque peu le m\u00eame, du moins dans sa client\u00e8le, dans son service d\u2019h\u00f4pital, \u00e0 l\u2019Acad\u00e9mie de M\u00e9decine) des dehors de froideur, de d\u00e9dain, de gravit\u00e9 qui s\u2019accentuaient pendant qu\u2019il d\u00e9bitait devant ses \u00e9l\u00e8ves complaisants ses calembours, avait creus\u00e9 une v\u00e9ritable coupure entre le Cottard actuel et l\u2019ancien, les m\u00eames d\u00e9fauts s\u2019\u00e9taient au contraire exag\u00e9r\u00e9s chez Saniette, au fur et \u00e0 mesure qu\u2019il cherchait \u00e0 s\u2019en corriger."], "link": "http://gowers.wordpress.com/2012/07/07/a-trip-to-watford-grammar-school-for-boys/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 1, "http://feeds.wordpress.com/": 1, "http://en.wikipedia.org/": 2, "http://www.gutenberg.org/": 1, "http://terrytao.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["The formal launch has just taken place at the European Congress of Mathematicians in Krakow of the Forum of Mathematics , which to a first approximation is a new open-access electronic journal. However, the singular \u201cjournal\u201d is misleading, because in some ways it is more like a whole set of journals. But there will be considerable interdependence between the elements of the set, so \u201cjournals\u201d is misleading too. We need an intermediate number between singular and plural. Also, although the journal(s) is/are primarily electronic, there will be a print-on-demand option if anyone wants it. \n What is the Forum of Mathematics? \n Terminological questions aside, how will this new journal-like object work? I think the easiest way of explaining it is to describe the process for submitting an article, which is similar to the process for submitting an article to a conventional maths journal, but with one or two unusual aspects. \n \n First of all, as with any journal you\u2019ll need to decide whether your article is likely to be of the required standard. Except that here there are two standards to worry about, which brings me to the main unusual feature: the Forum of Mathematics, as the journal(s) is called, has two \u201clevels\u201d, called Pi and Sigma. A paper suitable for Sigma should be of the kind of standard you would expect in a leading journal in the area of that paper. For example, if your paper is in combinatorics, then it will be suitable for Forum of Mathematics: Sigma if it is of roughly the standard expected by Combinatorica or JCT A/B. As for Pi, that is for papers that are sufficiently interesting or important that their appeal goes beyond their immediate area of mathematics. Thus, Pi papers will be at the level of leading general mathematics journals and will be an open-access alternative to them. Discussion is still going on about what precisely this means, but it looks as though the aim will probably be for Pi to be a serious competitor for Annals, Inventiones, the Journal of the AMS and the like. Of course, we can only really know what the level and characters of the two journals will be when they have been going for a while. The CUP website says this: \n Pi is the open access alternative to the leading generalist mathematics journals. Papers published will be of a high quality and of real interest to a broad cross-section of all mathematicians. \n It will also be possible for papers submitted to Pi to be reconsidered for Sigma, if the author is willing for this to happen. However, to encourage authors to think hard before submitting to Pi, rather than merely trying their luck with it, there will be a requirement that papers submitted to Pi are accompanied by a justification of a side or two, which should explain why the paper is of more than merely specialist interest. I hope that these justifications will be very useful documents for the editors, and perhaps even for readers of the papers at a later stage, but that kind of detail has not yet been decided. \n The editorial board of the Forum of Mathematics will be divided into \u201cclusters\u201d of people, with each cluster representing a different area of mathematics. When you submit a paper, you will decide which is the appropriate cluster to submit it to, and the editors in that cluster will handle the paper. However, not all papers can be easily classified, and that is where the not-quite-singular-but-not-quite-plural aspect of the Forum of Mathematics becomes apparent. It\u2019s not just papers that are hard to classify, but also editors, and some editors will belong to more than one cluster. Also, there will be plenty of communication between editors of different clusters in an effort to achieve a reasonably uniform standard across the whole of the Forum of Mathematics. \n Otherwise, the journal will be pretty conventional. Once you\u2019ve submitted a paper, it will be processed in basically the same way as papers are processed for any other journal. A small but important difference is that the Forum will not have \u201cissues\u201d. As with many other electronic journals, once a paper is accepted, it goes straight up on the website. It will have a number for reference purposes, and if it is a Sigma paper then it will in some sense \u201cbelong\u201d to the relevant cluster, but it won\u2019t belong to an issue (though there will be \u201cvolumes\u201d for those who want print copies). One of the advantages of this is that the Forum of Mathematics will be able to aim for an absolute standard rather than taking space into account when deciding whether or not to accept papers. \n Speaking as one of the editors, I\u2019d like to say that this is something I feel very strongly about. It is very unclear at this stage how popular the Forum of Mathematics will be. I would hate to be under pressure to lower standards in order to attract more papers, or to turn away good papers because there wasn\u2019t room for them. An electronic journal makes it easy to avoid that kind of pressure, and for the Forum of Mathematics it will be avoided. \n How will the journal be paid for? \n So far, what I\u2019ve been discussing is only rather minor modifications of the normal practice of journals. But I said at the beginning of this post that the Forum of Mathematics will be open access. What does that mean, and how will the finances work? Before I answer this, let me briefly introduce some terminology. \n It is unfortunate that the phrase \u201copen access\u201d has come to mean different things to different people. The result is that if you say that you are in favour of open access, you have to clarify what you mean, since there may be business models that you are not in favour of but that are nevertheless called open access by their proponents. However, there are two types of open access, known as gold and green, that have well-established meanings. Gold open access is where a paper is published in a journal and made freely available online, and the publisher is paid by the author. So a more descriptive name would be author-pays open access. Green open access is where papers are published in the usual way, but also made available on repositories such as the arXiv, so that even if the formatted journal versions are behind a paywall, freely downloadable versions of the papers will still appear when you Google them. \n The advocates of green open access argue that if everyone makes a habit of posting their papers on places like the arXiv, then in time the problems we have with expensive journals will melt away: there will simply no longer be any point in subscribing to them at ridiculous prices. Opponents of green open access, and especially of mandates from funding bodies that people should make their papers available online, use exactly the same argument but with one extra line: if everybody did it, then there would be no point in subscribing to a journal, and therefore the journals as we know them would no longer be financially viable. \n I will resist the temptation to discuss these arguments in more detail, because gold open access is more relevant to the Forum of Mathematics (though papers submitted to the Forum of Mathematics can also be posted on the arXiv, so it is completely sympathetic to green open access). Here there is bad news and good news. The bad news is that it will cost money to have a paper published in the Forum of Mathematics . In other words, it is following a gold open-access model. The good news is hidden in the word \u201cwill\u201d. For the first three years of the journal, Cambridge University Press will waive the publication charges . So for three years the journal will be what Marie Farge (who has worked very hard for a more rational publication system) likes to call diamond open access, a quasi-miraculous model where neither author nor reader pays anything. \n A second piece of good news will seem like good news only if you know what typical author publication charges, or APCs as they are commonly called, are. (Actually, APC officially stands for \u201carticle processing charge\u201d.) Here are a couple of examples. One of the most famous open-access journals is the Public Library of Science, or PLoS. PLoS has several high-quality journals and a big all-encompassing journal called PLoS-ONE that has a high acceptance rate and is basically somewhere where you put a paper if all you want to signal is \u201cthis was a publishable paper\u201d. The APCs are between $2000 and $3000 for the high-quality journals and are $1350 for PLoS-ONE. The details are on this page from their webiste . Another example is the London Mathematical Society\u2019s open-access option. If you want your LMS-journal paper to be freely available after it is published, then you can have that for a fee of $3050 (or \u00a31925) from outside the UK or \u00a32310 from within the UK. The UK figure includes VAT of 20%. \n The proposed charge for the Forum of Mathematics after the initial three years is \u00a3500 or $750. These figures do not include VAT, and are in absolute terms quite a lot of money, but they are nevertheless much cheaper than what appears to be the industry standard at the moment. They will also be waived for people from developing countries (CUP has a list of the countries for which fees will be waived) and for people who can demonstrate a genuine inability to pay. In addition, CUP promises to be transparent about its costs , so one will be able to understand the justification for the fees. I do not know the exact form that this transparency will take: what I would like to see is a web page somewhere that discusses in detail the cost of processing a typical paper, but maybe they won\u2019t go that far. \n A further point is that CUP would like to keep the fees as low as possible, even after the three years are over, and will seek funding to do so. The dream scenario would be a rich donor agreeing to underwrite APCs for several years after the initial three-year period. But there are also smaller things that can be done. For example, some people are at institutions that routinely agree to cover author charges. Such people will be encouraged to pay the author charges if they get a paper into the Forum of Mathematics, since it will not be a problem for them to pay, and the money received will be used to mitigate in one way or another the introduction of APCs in three years\u2019 time. I am being slightly vague here because there are different forms that this mitigation might take: for example, there might be a choice between waiving APCs completely for a further period and half waiving them for longer. \n Some pros and cons of gold open access. \n I have talked to a number of mathematicians about author publication charges, and it is clear that the idea is regarded with deep suspicion by many people. It is also clear that many of the arguments that people have against it are good ones. Before I go into some of those arguments, I would urge you to bear in mind that there are also some very strong arguments against the current subscription system. It seems to me that every system of publication has its drawbacks, so pointing out those drawbacks is not enough: one must argue that they outweigh the drawbacks of the existing system. \n The main drawback of gold open access is that it makes life more difficult for people from less wealthy institutions. More generally, it introduces financial considerations into certain decisions that one would prefer to be made entirely on academic grounds. An example of the kind of consequence this might have if gold open access became the norm is a less well-off UK university telling its academics that it would pay for just the papers needed for the next Research Excellence Framework. In fact, I have even heard a rumour of this kind, and apparently the decision about which papers would be supported was to be taken by the university administration and not by the academics. I very much hope that won\u2019t be typical. \n One of the best ways to avoid unpleasant consequences of that kind is to make the charges small enough that they are relatively painless for universities and funding bodies to pay. Maybe the charges for the Forum of Mathematics could be lower still \u2014 I don\u2019t know \u2014 but the fact that they are much lower than the ones that are prevalent at the moment is at the very least a significant piece of progress. (I would add here that Rob Kirby , who will be the managing editor and who has been campaigning against expensive journals for many years, regards the \u00a3500 price as an absolute bargain.) \n An important point to make is that while libraries continue with their subscriptions to existing journals, APCs are simply an additional cost to universities and funding bodies. However, if we were to switch tomorrow to an author-pays model (that is, stop all subscriptions and do all publishing through gold open access), then the total cost of the publication system would be much less than it is now. Or at least so I have read many times, and I find it plausible. For instance, I have been told that University College London pays over a million pounds a year to Elsevier for its subscriptions. Even if APCs were \u00a32000, a million pounds would be enough for 500 papers. Elsevier has 2000 or so journals, so I would guess that the total number of articles is something like 30,000. If 1% of these articles come from University College London (surely an overestimate \u2014 many of those 2000 journals are not all that high quality and there are many good universities round the world), then that\u2019s 300 papers, so already a significant saving. Improvements to this back-of-envelope calculation are welcome. (Of course, APCs of \u00a3500 alter the calculation dramatically.) Thus, one needs to take a long-term view when thinking about APCs. I see the pain of paying APCs as to some extent a good thing: if academics themselves feel that pain (not quite directly but through their departments or other funders) then they will be less likely to accept outrageous prices than they are now, when the pain of journal prices is felt by librarians. \n Since switching to an author-pays model would save universities a lot of money, one option that looks very attractive on the face of it would be for a large number of universities to club together and agree to fund APCs for journals that were sufficiently cheap and of sufficiently high quality. If that could be organized, then the consortium of universities would have much more bargaining power than libraries do at present: a journal would much rather be able to say, \u201cDon\u2019t worry about APCs \u2014 they are paid for by consortium X,\u201d than have explicit APCs. (Why? Well, which kind of journal would you prefer to submit to?) So with luck there would be significant downward pressure on prices. The difficulty, of course, is of a prisoner\u2019s-dilemma type: while it may be in the collective interest of universities to set up a fund to cover APCs, any individual university would do better by not contributing to the fund. However, if enough universities could be persuaded to start such a fund, then moral pressure could be put on other universities to join in \u2014 especially wealthy universities whose members frequently benefit from the fund. I hope that the Forum of Mathematics will quickly become such a valued part of the mathematical landscape that many of us will make efforts to bring a fund like this into existence (perhaps initially just for FoM, but ideally the fund would grow and support other open-access journals). \n If you are interested in arguments for and against various kinds of open access, I would recommend the writings of Peter Suber , who also has a book coming out soon . \n My plans as an editor. \n Here I am speaking for myself only. The idea of taking on a significant editorial job was not one that would ordinarily have appealed to me, but when David Tranah (whose brainchild all this was) summoned me for a chat in early February, shortly after I had published my first Elsevier post, I realized that it was basically impossible for me to say no. \n Having got myself into that position, I now want to think of ways of doing the work efficiently. Here are a few ideas I have had so far. \n 1. I will follow the practice of many editors these days and ask for quick opinions first, unless I myself already have a quick opinion. I will proceed to a more detailed reference only if the initial opinion suggests that it is worth doing so. \n 2. One reason I have been rather inefficient at this kind of job in the past is that I always feel guilty asking potential referees for a favour. Maybe I shouldn\u2019t, but I do. To counteract that, I would like to compile a list of names of people who volunteer in advance to do a certain amount of work for the discrete maths cluster of the journal (which is the cluster I\u2019ll be part of). What I\u2019d like is for people to tell me roughly how often they are prepared to handle a paper and roughly what topics they are ready to cover. That way, if I have a paper that matches a referee who has not yet reached his/her quota, I\u2019ll be able to ask that person without the slightest embarrassment. If you are reading this and feel like showing your support for the journal by dropping me an email and making a commitment of that kind, I will be very grateful. If you don\u2019t, and if you are a discrete mathematician, you may find that I email you at some point. I won\u2019t hold anyone to the commitments that they make. The main purpose of this is to make it easier to ask for help, but if you\u2019ve said you can handle three papers a year and a long and difficult paper comes along at a bad time, I will understand. \n 3. I plan at some point to write some guidelines for what I personally consider makes a combinatorics paper good enough to publish in Sigma and what gives it that extra Pi-like quality. This probably won\u2019t be easy, but I hope that authors and referees will find it useful to have some explicit criteria (which they will be free to ignore) rather than relying on some vague instinct that \u201cthis paper is good enough for the Forum of Mathematics: Sigma\u201d. \n How does all this relate to the Elsevier boycott? \n Here again I am speaking for myself rather than for CUP, though I hope my opinions will be shared by many others. As it happens, the idea of the Forum of Mathematics predates the Elsevier boycott, so in a sense the answer is that it has nothing to do with the boycott at all. However, I myself see it as potentially a very important development in the campaign for a better system of academic publishing. In particular, it greatly weakens what was previously quite a strong argument for some people against participating in the Elsevier boycott: that the best specialist journal in their area is an Elsevier journal so joining the boycott would harm their career. \n Well, maybe that has been true up to now, but it is about to become less true. If you have a paper that is suitable for the best specialist journal in your area and that journal happens to be very expensive, then you now have the option of submitting your paper to Forum of Mathematics: Sigma instead. Similarly, if you were thinking of submitting a top-notch paper to a top-notch but expensive journal (Inventiones comes to mind here \u2014 a Springer journal, but this discussion is not just about Elsevier), then you can get just as much of a career boost from Forum of Mathematics: Pi but also enjoy the warm glow that comes from knowing that your paper is freely available to all mathematicians. \n At some point I hope to compile an informal list of expensive journals of roughly the same standard as Forum of Mathematics: Sigma, though for that I\u2019ll need help, as I have very little idea of the standards of specialist journals outside my areas of mathematics. \n It is of course unlikely that the Forum of Mathematics will change the face of mathematical publishing in three years. To be a serious direct threat to Elsevier\u2019s mathematics journals, for example, it would need to cause a reduction of their quality by enough to make libraries consider cancelling their subscriptions, which is very difficult when mathematics journals are bundled together with journals from other subjects. However, the Forum of Mathematics can still have a big influence. For one thing, it will demonstrate that a major publishing house can produce a high-quality journal with high-quality formatting and editing with APCs of around \u00a3500. I hope that people in charge of funding bodies who are considering open-access mandates will ask some tough questions of publishers who continue to charge four times that. Secondly, if the Forum of Mathematics is successful, it has the potential to reduce the quality of a number of other journals, which will at least strengthen the hand of librarians who are bargaining with publishers like Elsevier (because cancelling their subscriptions won\u2019t cause quite the inconvenience that it would at the moment). Thirdly, the Forum of Mathematics may encourage other publishers to set up cheaper open-access journals \u2014 one huge gap in the market would still be a place where people could submit papers that were perfectly worthy but not good enough for the Forum of Mathematics. I myself think that for such papers, there is a strong case for not putting in too much effort into formatting and typesetting, since the papers will usually not be read all that much, and since many people can produce a decent typescript themselves. So it ought to be possible to produce such a journal (or journal-type object, if it is like the Forum of Mathematics) with smaller APCs. \n And even if we forget all about price, we still have the huge bonus that the papers published in the Forum of Mathematics will be freely accessible. I hope that once people start to get used to a high-end journal being freely accessible, they will feel all the more keenly the inconvenience of paywalls. \n So I urge you to support the Forum of Mathematics. If you\u2019ve got a good paper, then why not hold on to it until 1st October and submit it to us? If the Forum of Mathematics takes off, then you will be able to look back with pride and say that your paper was one of the very first \u2014 who knows, perhaps even the first \u2014 to appear in it. If your department has not yet thought about author charges, then another thing you can do is initiate a discussion about those \u2014 something I plan to do in Cambridge \u2014 since, as I said above, any author fees that are paid voluntarily during the first three years will help keep the fees lower later on and increase the chances that the venture will be a success, which in turn will raise the prospect of saving a lot of money in the longer term. \n As regular readers of this blog will know, I am keen on the idea of much more radical changes to the way we evaluate and disseminate our work. However, I am also in favour of evolutionary change rather than a sudden collapse of the existing system, and that is what the Forum of Mathematics offers. It is not a solution to all our problems, but neither is anything else. What it is is another way of doing things, and the more of those we have, the greater the chance that some of them will work and become successful and help us move to a cheaper and more open system of publishing."], "link": "http://gowers.wordpress.com/2012/07/02/a-new-open-access-venture-from-cambridge-university-press/", "bloglinks": {}, "links": {"http://www.earlham.edu/": 1, "http://feeds.wordpress.com/": 1, "http://journals.cambridge.org/": 1, "http://www.plos.org/": 1, "http://mitpress.mit.edu/": 1, "http://math.berkeley.edu/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Michael Gove, the UK\u2019s Secretary of State for Education, has expressed a wish to see almost all school pupils studying mathematics in one form or another up to the age of 18 . An obvious question follows. At the moment, there are large numbers of people who give up mathematics after GCSE (the exam that is usually taken at the age of 16) with great relief and go through the rest of their lives saying, without any obvious regret, how bad they were at it. What should such people study if mathematics becomes virtually compulsory for two more years? \n A couple of years ago there was an attempt to create a new mathematics A-level called Use of Mathematics. I criticized it heavily in a blog post , and stand by those criticisms, though interestingly it isn\u2019t so much the syllabus that bothers me as the awful exam questions. One might think that a course called Use of Mathematics would teach you how to come up with mathematical models for real-life situations, but these questions did the opposite, and still do. They describe a real-life situation, then tell you that it \u201cmay be modelled\u201d by some formula, and proceed to ask you questions that are purely mathematical, and extremely easy compared with A-level maths. \n \n One comment on that post particularly interested me, from someone called Joseph Malkevitch, who drew my attention to an article he had written in which he recommended a different kind of question both from the usual sort of symbolic manipulation that most people would think of as mathematics, and from the sterile questions on the Use of Mathematics papers that pretend to show that mathematics is relevant to real life but in fact do nothing of the kind. The main idea I took away from his article was that there is (or could be) a place for questions that start with the real world rather than starting with mathematics. In other words, when coming up with such a question, you would not ask yourself, \u201cI wonder what real world problem I could ask that would require people to use this piece of mathematics,\u201d but rather, \u201cHere\u2019s a situation that cries out to be analysed mathematically \u2014 but how?\u201d \n Inspired by Malkevitch\u2019s article, I decided to write a second post , in which I was more positive about the idea of teaching people how to use mathematics. I gave an example, and encouraged others to come up with further examples. I had a few very nice ones in the comments on that post. \n The difference between then and now is that then there seemed to be a probability of approximately zero that questions of that kind might actually make their way into schools. But it seems that Gove is absolutely serious about getting more people to do mathematics for longer, and that creates an opportunity. Of course, it also creates an opportunity to mess things up badly, and one of my reasons for being interested is that I would like to do what I can to avoid the terrible waste that messing it up would be. \n As an indirect result of the earlier blog posts, I have found myself in a position of some influence. I\u2019m not sure how much, but I am in touch with the Advisory Council for Mathematics Education, or ACME, which, as its name suggests, advises the government on matters of mathematical education, and there appears to be some sympathy in ACME for the idea of a qualification of some kind that involves interesting real-world mathematics problems. I have other reasons for optimism that I won\u2019t go into here. [Update 31/10/12: MEI has just been given government funding to try to develop a qualification of this kind. This is mentioned in a Guardian article today, which is basically accurate apart from the title, which wrongly suggests that the qualification would be an A-level. It wouldn't.] But to translate that into results, what I would really like is a longer list of potentially good questions than the one I have so far, which I shall now give, together with a few brief remarks. I\u2019ll try to group them naturally into topics. \n Fermi estimation. \n This is the process of coming up with estimates of the correct order of magnitude for various real-world quantities. Here are a few examples. \n 1. How much does a cloud weigh? \n 2. How many people could fit into the Isle of Wight? \n 3. How many piano tuners are there in Chicago? [That is a classic example.] \n 4. If the average temperature of the sea were to rise by a degree, then by how much would thermal expansion cause sea levels to rise? \n 5. How many molecules from Socrates\u2019s last breath are in the room? \n Fermi estimation and probability. \n Sometimes estimating a probability boils down to doing a couple of Fermi estimates. The next couple of questions are examples of this. \n 6. You are about to fly to the US. What are the chances that you will die as a result of your flight crashing? And what are the chances that you will die of natural causes while on the flight? \n 7. What are the chances that at some point in the last five years somebody in the UK dreamt that a loved one had died, only for that loved one to die unexpectedly the very next day? \n Sometimes the probability can be more sophisticated. In particular, conditional probability can come in. \n 8. In September 2009 the same six numbers were chosen in two consecutive draws of the Bulgarian State Lottery . Was this conclusive evidence that the draws were manipulated? \n To answer that last question properly, one needs to estimate how many lottery draws of that kind there have ever been, and how many events one would count as suspicious. (For instance, would it be suspicious if for one draw the numbers 1,2,3,4,5,6 were chosen? It would certainly be very remarkable.) That way, one could obtain an estimate for the probability that a \u201csuspicious\u201d draw would have occurred by now purely by chance. So far that is nothing more than Fermi estimation. But let\u2019s suppose that you come up with an estimate that the probability that there would ever have been an occurrence as suspicious as what happened in Bulgaria in the entire history of lotteries of this kind was fairly small \u2014 something like 1 in 30. There is a further step one can take, which is a piece of simple Bayesian analysis. We need to decide also what we think the probability is that somebody would manage, in front of the TV cameras, to get the machine to shake the balls in just the way needed to get the right numbers to come out, and that, given this control, they would go for the same numbers as the previous draw, which was bound to attract attention and in fact meant that many people were winners and the average payout was rather small. It all sounds a bit unlikely. I\u2019m not quite sure how to judge the probability that it would ever have happened, but if we went for, say, 1 in 10, then we\u2019d end up judging that the probability, given that the numbers were the same twice in a row, that that was due to chance rather than manipulation was . And one-in-four probability events do happen. \n More probability and statistics. \n 9. In 1999 a solicitor named Sally Clark was convicted for the murder of her two sons, who had died in 1996 and 1998, both at the age of a few weeks. Roy Meadow, a paediatrician, argued for the prosecution as follows. The probability of a cot death is approximately 1 in 8500. So the probability of two cot deaths is roughly the square of this, or 1 in about 73 million. Therefore it was overwhelmingly likely that the deaths were not due to natural causes. Is this argument valid? \n Again there is a Bayesian argument to make here. There are a lot of people born each year, so there are a lot of opportunities for the 1 in 73 million chance to occur. And one wants to have some idea of how many mothers we might expect to kill two of their children (taking account of any additional circumstances that might be relevant \u2014 such as that Sally Clark wasn\u2019t mentally disturbed etc.). Another major flaw in Roy Meadow\u2019s argument was the assumption that two cot deaths in a single family are independent events. After criticism from the Royal Statistical Society, Sally Clark\u2019s conviction was overturned, but not before she had spent three years in jail. More about this awful story can be read in the Wikipedia article about it . \n 10. The batting averages paradox: how can it be that A has a better average than B in the first half of the season and a better average in the second half of the season, but B has a better average for the entire season? \n 11. How much can we trust opinion polls? \n Related to that is the following question. \n 12. How would you go about obtaining a random sample of 2,000 people eligible to vote at the next General Election? \n 13. How would one go about determining how the average global temperature is changing? \n That last question has an additional interest over random sampling, because the places where temperatures are measured are not random. So one would want to weight the measurements according to some kind of density, and also look out for sources of bias, such as placing the measuring devices in towns \u2014 though if it is changes in global temperatures that we are trying to measure, then some of those biases matter less. \n 14. The average global temperature in 1998 was higher than the average global temperature in 2011. Does this demonstrate that man-made global warming is not a serious threat? \n 15. Obtain figures for how girls and boys do on some public examination, and ask whether the evidence shows that girls are better than boys (if that is what the figures appear to show) at the subject in question. \n What would work best here would be if the distributions were interestingly different: one might have a higher average, but the other have a higher variance and more people at the very top. The idea would be to show that averages do not give a complete picture: for that one ideally wants the whole distribution, though a few extra parameters will be helpful if the whole distribution is too much to ask for. \n 16. \u201cMy grandfather was a chain smoker and he died peacefully in his sleep at the age of 95.\u201d In the light of this kind of argument, why are we so confident that smoking causes lung cancer? \n 17. If you are batting in cricket, you can increase your expected score for any given ball by playing in a riskier way. The trouble is that you also increase your chances of getting out. In a limited-overs game, with a certain number of wickets in hand, how should you decide, when batting, how much risk to take? (Clearly as the end approaches you should play more riskily, but why and by how much?) \n Part of the difficulty with 17 is setting up a simplified probabilistic model. \n Something similar could be asked about football: if a team plays in a more attacking style, the chances are increased that it will score a goal, but the chances are also increased that it will let in a goal. If a team is playing in a knock-out tournament and is a goal behind, how riskily should it play, and how does the answer change as the end of the match approaches? Again, it is not obvious how to model the situation in the first place. \n Questions with a game-theoretic flavour. \n 18. In a leafy suburban street, it turns out that if you convert your front garden into a car parking space, you increase the value of your house. However, you also decrease the values of everybody else\u2019s houses, partly because you make it more difficult for them to park (since they have to leave access to what used to be your front garden) and partly because the street becomes uglier. If enough people convert their front gardens, then everybody ends up worse off. What should be done in a situation like this? \n The above question is just one example of many tragedy-of-commons situations, all of which could be interesting to discuss. I stole it from Tom K\u00f6rner\u2019s article about mathematics in everyday life in the Princeton Companion to Mathematics. \n 19. Suppose you play an iterated prisoner\u2019s-dilemma game. What happens when various strategies are pitted against each other? (For simplicity, let\u2019s say that you both get one point if you cooperate, you get two points if you defect and the other person doesn\u2019t, you both lose a point if you both defect, and you lose two points if the other person defects and you don\u2019t.) \n Here I imagine that pupils are invited to devise strategies, and then they play against each other several times, promising to stick to the strategies they have devised. They keep score, see which strategies do well, then change their strategies if they want to, and so on. (I\u2019m basing this idea on the memory of a fascinating article in the Scientific American about three decades ago where Martin Gardner described an experiment of exactly this kind.) \n 20. The retaliation game. The rules for this are very simple. Two players take turns. Whenever somebody has a turn, they have the option of either stopping the game, or giving themselves a point and taking two points away from the other player. \n I heard about this game from a blog post of Gil Kalai . I think it could form the basis for a very interesting classroom discussion. \n With all the last three questions, a subsidiary question one could (and absolutely should) ask is, \u201cCan you think of real-life situations that are similar to these simple games?\u201d \n 21. A widow dies and leaves the contents of her house to be divided up amongst her three children. The children do not care much about the financial value of the possessions, but they care a lot about the sentimental value. To complicate matters, the sentimental values they attach to the various possessions are quite different. What would be a fair way of dividing the possessions? \n This is similar to one of Joseph Malkevitch\u2019s questions (which was about a divorcing couple). \n 22. In a greengrocer with just one till, it often happens that one customer has a big basket with many items that will take a long time to process, while just behind them is someone who wants to buy one small thing and has the exact change ready. Try to devise a system that would allow the occasional queue jump in a situation like this but that wouldn\u2019t have obvious defects (such as a person with a lot of shopping being overtaken by a very large number of people with only a small amount). \n 23. Five options are put to the vote. Seven people put the options in order of preference. The results are tabulated and shown to the pupils, who are asked which option should be gone for. The table is of course set up so that different voting systems give different results. The discussion can then be generalized almost arbitrarily far. \n See Joseph Malkevitch\u2019s question E on page 90 for a question of exactly this type. \n Perhaps one could make this question more immediate by giving the class an actual choice. For example, perhaps there could be seven DVDs, one of which will be watched. Everybody in the class could put the DVDs in order of preference in a secret ballot, with no conferring allowed, and only after the results were tabulated would they discuss which one should be watched. To make the discussion livelier, three of the DVDs could be very similar (e.g., three different episodes of the Simpsons). \n Questions with a physics flavour. \n Some Fermi estimation problems involve physics. Here are a few more. \n 24. How do speed cameras work? How accurate are they likely to be? (The basic technique I\u2019m talking about is taking two photos in quick succession.) \n 25. Why does a mouse survive a big fall when a human doesn\u2019t? (There are many questions similar to this, such as why elephants have thick legs, ants can carry several times their body weight, etc.) \n 26. How does a Mexican wave get started? \n 27. Somebody pours you a cup of coffee but you aren\u2019t yet in a position to drink it. You take milk, and the milk provided is cold. You want your coffee as warm as possible. When should you put in the milk: now, or just before you drink it, or some time in between? \n 28. You are walking from one end of an airport terminal to the other. The airport has several moving walkways, and you need to stop to tie your shoelace. Assuming you want to get to the other end as quickly as possible, is it better to tie your shoelace while you are on a moving walkway or while you are between walkways? \n This question comes from a blog post of Terence Tao , and the response to it provides us with strong empirical evidence that people find it engaging. \n 29. You have a collection of suitcases, boxes and bags of various sizes, shapes and degrees of squashiness. You want to pack them all into the boot of a car and it\u2019s not obvious whether you can. What is the best method to use? \n This question is of course rather open-ended. The aim would be to elicit principles such as packing big and inflexible things first (and understanding why it is a good idea), overriding that principle if you find that you have an object that fits very snugly into a space (but what exactly does that mean?) and so on. It would also be interesting to model the situation in two dimensions, perhaps having a board with a rectangular hole into which you have to put a whole lot of wooden pieces without overlaps. It would be different from a jigsaw puzzle because the area you had to fill would be greater than the total area of the pieces and the pieces wouldn\u2019t fit neatly together. Or perhaps one could set up a 3D situation where it was just about possible to pack some objects into a box, but only if you were fairly clever about it \u2014 again with the total volume of the objects strictly less than the volume of the box. \n 30. You have probably heard that the distance to the sun is approximately 93 million miles. How on earth can we know something like that? \n That question could lead to a more general discussion of the cosmic distance ladder , which has been beautifully explained by Terence Tao . \n Optimization. \n Some of the questions above already involve optimization. Here are some further optimization questions. \n 31. You have a collection of tasks to perform, each of which has a certain probability of failure. If you ever fail on one of the tasks, then you have to start again at all the tasks. (An example: you want to make a Youtube video in one take in which you successfully perform five tricks of varying difficulty.) In what order should you do the tasks if you want to minimize the expected time it will take to eventually succeed? \n For more on this question, including an entertaining Youtube video, see this comment of Julia Wolf on my second blog post. She got the question from a Google Buzz post of Terence Tao. \n 32. The British organization NICE (National Institute for Health and Clinical Excellence) has the task of deciding which drugs should be approved for use by the National Health Service and which should be paid for. Given that different drugs cost very different amounts, do very different things, and benefit very different people, how should decisions about how much to spend on various drugs (given a fixed total budget) be made? \n Some concepts that I\u2019d like to see arising out of a discussion here are the ideas of marginal utility (not necessarily under that name) and marginal cost \u2014 the rough idea being that if you\u2019ve got an extra pound to spend, then you want to get the most benefit out of that pound. (However, there could be interesting situations where a local optimum is not a global optimum \u2014 for example if there are economies of scale connected with a certain drug.) \n 33. You have a product to sell. How should you price it so as to maximize your profits? \n Again this is a calculus question in disguise. If you decrease the price, you will increase sales (usually, though a side discussion of Giffen goods could be fascinating here) but also increase costs and decrease the average revenue from each sale. You want to stop when the extra revenue from reducing the price is exactly balanced by the extra costs. (If the extra revenue is negative, then the price is already too low.) \n Algorithms. \n 34. Six cards have different numbers written on them and are then laid face down on a table so that you can\u2019t see what the numbers are. You are allowed to select any two cards and ask which has the bigger number. How many questions of this kind do you need to ask before you can put the cards in order? \n Here I would recommend a classroom discussion in which the teacher actually has six cards and invites pupils to ask which pairs they would like compared until they are confident that they know the order. Of course, the discussion can then be generalized considerably. \n 35. You are in a maze. Devise a method that will guarantee that you eventually find a way out. \n There are many further questions here. Suppose the maze is made of hedges, the hedges all look very similar, your memory is very bad, and you can\u2019t keep track of where you have visited by (for example) dropping stones from time to time. How does that affect your method? And what if you no longer want a guarantee that you will escape, but simply a method that on average gets you out fairly fast? Is there a randomized strategy that works quickly on average? (Obviously one would not be looking for a rigorous analysis of such an algorithm, but a heuristic discussion, perhaps with reference to a picture of an actual maze, could still be interesting. For instance, what happens if you simply make random choices whenever you have choices?) \n 36. You are doing a jigsaw puzzle. The pieces are all very similar, so the only practical way of telling whether two pieces fit together is to try to fit them together. The puzzle is well made so it is always obvious that two pieces don\u2019t fit when they don\u2019t. What is a good technique for minimizing the expected number of attempts you will need to make to fit pieces together? For example, do you just want to build up one component, or is it better to build up a lot of small components and then fit those together to make bigger ones, and so on? \n Further questions \n These questions are ones that I\u2019ve thought of since first putting up this post, or otherwise come across, or had suggested in the comments below (possibly with small modifications \u2014 but I\u2019ll link to the comments). \n 37. (i) In several parts of the UK the police gathered statistics on where road accidents took place, identified accident blackspots, put speed cameras there, and gathered more statistics. There was a definite tendency for the number of accidents at these blackspots to go down after the speed cameras had been installed. Does this show conclusively that speed cameras improve road safety? \n (ii) In a certain school, the pupils in year 9 take a maths exam at the end of the year. Those whose scores are in the top half are taught by teacher A in the subsequent year, and those whose scores are in the bottom half are taught by teacher B. At the end of that year they take another exam. If you take the whole year together, then the spread of scores is very similar to the previous year, but the average scores of the pupils in the top stream go down compared with the previous year, while the average scores of the pupils in the bottom stream go up. Does this demonstrate that teacher B is a better teacher than teacher A? \n (iii) A scientist decides to test the effect of coca cola on telepathic powers. He tosses a coin 20 times and records the results. He then takes 100 people and asks them to guess what the results were by writing out a sequence such as HTTHTTTHHTHTHTHHHTTH. As an incentive, he promises a prize of \u00a3100 to the person whose guess is closest to the actual sequence. He then picks the ten people who have done best, gets them all to drink a can of coca cola, and retests them with another sequence of 20 coin tosses. To his surprise, he finds that they do considerably worse the second time. Does this demonstrate that coca cola inhibits telepathic powers? \n By the way, I contemplated making the scientist female, but decided that that would be even more sexist than making him male \u2026 \n 38. Another scientist questions 2000 randomly chosen people about their eating habits and then follows their health over the next ten years. He notices that people who often eat organic food suffer from fewer heart attacks. Does this demonstrate that organic food protects against heart attacks? \n 39. You are taking the trip of a lifetime: a round-the-world cruise. One of the highlights is arriving at Manhattan on a gloriously clear day. Roughly how near do you have to be before you can see the top of the Empire State Building? \n That question is inspired by Tim\u2019s question below , which itself would make a good question. \n 40. You are in the process of buying a washing machine for \u00a3250 at Curry\u2019s (a chain in Britain that sells that kind of thing), and are offered a five-year guarantee for \u00a360. The sales attendant tells you that typical repairs cost at least \u00a3100. Should you go for the insurance? \n This question is stolen (in slightly modified form) from this comment of Gil Kalai . The policy I adopt towards insurance, ever since I read the advice somewhere, is not to take out any insurance unless the result of not doing so could be disastrous, since my expected gain is negative (or I wouldn\u2019t be offered the insurance). If my washing machine breaks within five years, it\u2019s annoying to have to pay the money, but certainly not disastrous, so I definitely don\u2019t insure against it. If this principle were taught to millions in school, it could make a dent in the profits of certain companies \u2026 \n 41. You and a friend are out for a walk, when you are approached by a stranger, who offers the two of you \u00a31000 on one condition: that you agree how to split it between you. After establishing to your satisfaction that you are not about to be kidnapped, you propose a 50-50 split to your friend. To your astonishment, your friend insists on receiving \u00a3900 with only \u00a3100 going to you, and appears to be prepared to lose all the money rather than accept anything less than this deal. What should you do? \n As with the prisoner\u2019s dilemma, this problem could be turned into an interesting and thought-provoking game. You have a series of rounds. In each round you pair up the pupils and offer each pair ten points, provided they can agree how to split them. If they can\u2019t agree, then they get nothing. You offer some kind of incentive \u2014 perhaps a small prize \u2014 to the person who ends up with the most points after fifteen rounds. One could experiment with small variations: does it affect how people play if all the current scores are public knowledge? What about if you know the entire playing history of your opponent? What happens if instead of changing the pairing every round you have several rounds with the same pairing before changing? \n 42. Three people need to get back home after a party. It\u2019s a long walk, but somebody else has a space in their car. They would all prefer not to walk. One of them has an unbiased coin. Devise a method for using the coin to make the decision, in such a way that they all have an equal chance of getting the lift. What happens if the coin cannot be tossed more than ten times? \n 43. A doctor tests a patient for a serious disease that one in ten thousand people have. The test is fairly reliable: if you have the disease, it gives a positive result, whereas if you don\u2019t, then it gives a negative result in 99% of cases. So the only problem with it is that it occasionally gives a false positive. The patient tests positive. How worrying is this? \n I initially resisted putting in this question, partly because it is very well known, and partly because it is a bit close to standard A-level fare. However, the principle behind it is important, and can be stated qualitatively: if the disease itself is much more unlikely than the false positive, then you shouldn\u2019t be too worried about testing positive. What\u2019s more, this principle can be got across without doing too much formal mathematics: you can do things like getting people to imagine a town with 100,000 typical people. Of those people, 10 have the disease, and roughly 1,000 will test positive despite not having the disease. Once you think of it like that, it is much more intuitive that testing positive doesn\u2019t mean that you probably have the disease. \n 44. You drive round a corner and see a red light. You want to get to your destination as fast as possible. What should you do? \n This question was suggested (in a more detailed form) by Anonymous in a comment below . Funnily enough, it is already a favourite question of mine, which I mentioned in the comment thread on Terence Tao\u2019s shoelace-at-airport puzzle. I had left it out here because it seems too hard \u2014 in particular, I don\u2019t know the solution to any version of the problem \u2014 but I\u2019ve changed my mind now. I think it would be good to have questions where you can get somewhere by using a bit of maths (for example looking at specific strategies and specific assumptions about how the traffic lights behave) but can also sense that you would be able to get a whole lot further if you thought harder and/or knew some more maths. One of the main things I would like a course like this to give people is a feel for what mathematics can do . In the end, that is what is likely to be useful to someone who does not want to focus on mathematics. It\u2019s a bit like the internet: to use it effectively, you need a good feel for the kind of thing you can find out with its help and you should know how to use Google. What you don\u2019t need is a good general knowledge, though some general knowledge helps give you a feel for what else might be out there. Similarly, the skill needed to see that a certain situation can be analysed mathematically can, I think, be to a considerable extent decoupled from the skill needed actually to do the analysis. Obviously the more mathematics you know, the easier you will find it to recognise situations where that mathematics is potentially helpful. My claim is that although it is more difficult to recognise characteristically mathematical situations if you aren\u2019t good at maths, it is not impossible. \n 45. A renowned wizard arrives in your town and makes the following offer. In front of you are two envelopes, labelled A and B. You can either open both envelopes and keep the contents, or you can go for just envelope A. But here\u2019s the catch. The wizard claims to be able to predict what people will do, and has been correct every single time so far. If he predicts that somebody will choose just envelope A, then he puts \u00a31000 in envelope A and \u00a3100 in envelope B. But if he predicts that they will choose both envelopes, then he puts nothing in envelope A and \u00a3100 in envelope B. What should you do? \n This question is the famous Newcomb\u2019s paradox . Although it does not involve much mathematics, it has the wonderful quality of leading to heated arguments and challenging people to think clearly enough to find the hidden assumptions behind those arguments. \n 46. In 1972 Diana Sylvester was raped and killed in San Francisco. Despite one or two leads, the police failed to solve the case. However, they kept some DNA, and in 2006 they checked it against a DNA database of 300,000 convicted sex offenders. They discovered that it matched the DNA of John Puckett, who had spent a total of 15 years in jail for two rapes. There was no other evidence linking Puckett to the crime, but the probability that a random person\u2019s DNA would match that of the sample was judged to be 1 in 1,000,000. On that basis, he was found guilty and sentenced to life imprisonment. How reliable was the conviction? \n Thanks to Fergal Daly for drawing my attention to this case . \n 47. How should a government determine tax rates if it wants to maximize the amount of tax that it collects? What about if it has other objectives? \n This is not an easy question, but one could at least hope to raise various issues, such as the effect of tax rates on the incentive to work, to employ others to work, and to spend money, and also the difficulty of measuring this kind of effect. The question was suggested by Richard Baron . \n 48. This is not a question, but it has questions associated with it. I\u2019m fairly sure that there exists software that allows you to invest virtual money in various different stocks and shares (and perhaps other financial products) and see how you do with your investment. One could give everybody in the class a virtual \u00a310,000 and have a competition to see who has the most money three months later. Then one could run a second competition of exactly the same type. People could do as much research as they liked on the investments they were making. It would be instructive to see whether there was any correlation between the results of the two competitions. (My only worry about this question is that it might give some people a taste for the kind of risk taking that has got the world into so much trouble recently, but the moral is supposed to be quite the opposite.) \n 49. In 1985 almost nobody foresaw that a mere four years later a process would start that would result in the collapse of the Soviet Empire. However, Werner Obst, a German economist, analysed various economic trends and predicted that it would happen in around 1990. How impressed should we be by Obst\u2019s insight? \n Something to introduce into the discussion is the well-known fraud where you send lots of people lots of differing predictions of future sporting results, and then offer to sell tips to the few who received correct predictions. Another factor is that there was in fact a long history of predictions of the imminent demise of the Soviet Union \u2014 enough for an entire Wikipedia article . \n 50. You are given a fairly large bunch of common words. Devise a method for creating random sentences out of these words. The sentences need not be true, but they should make grammatical sense. (For example, \u201cThe political dog embarrassed the car,\u201d would be OK, whereas, \u201cTomorrow the Friday in future,\u201d would not be OK.) \n I got the idea for this from a book by Seymour Papert that I was recently given and am in the middle of. He described its effect on a girl, who before this exercise (which was done with a computer and a specially designed programming language) had found grammar pointless and therefore didn\u2019t know any, but who became absorbed by the challenge and ended up inventing several grammatical categories for herself. \n 51. In half an hour\u2019s time you will be given the lyrics for a song. Your task will be to send a message to somebody else (with whom you have been paired), which will tell them what those lyrics are. You could of course send a message that contains the lyrics themselves, but the aim here is to use as few keystrokes as you can. You are allowed to confer with the other person before you receive the song lyrics but not afterwards. \n The idea here is to exploit redundancies in the English language at many different levels. There are obvious ones sch as t\u2019fct tht u cn mss out mny ltrs n rmain cmprhnsbl and slightly less obvious ones such as using context get away missing out entire words. At a higher level still, song lyrics often contain quite a bit of repetition. \n The next four questions were suggested in a comment below by Charles Crissman, to whom many thanks. \n 52. You have a task you want to do that involves standing in a queue, but you also have a time limit. (Charles Crissman\u2019s example is that you arrive at the Department of Motor Vehicles in the US and need to be back at work by the end of your lunch break. One that has happened to me is arriving at Cambridge Station needing to catch the next train and discovering that the queue for tickets is unexpectedly long. Another is cutting things a bit fine at an airport and finding a long queue at the bag drop.) How should you assess whether to join the end of the queue or whether more drastic action is required? \n 53. You run a manufacturing business, and the cost of one of your inputs suddenly increases by \u00a310 per output unit. Should you increase your prices by \u00a310? More? Less? Now suppose instead that your costs decrease by \u00a310/unit. Should you keep prices the same? Drop them? \n 54. You are with a friend one evening and have different ideas about how you would like to spend it. So you decide to toss a coin. However, the coin is slightly bent, and neither of you is confident that the coin is fair. Can you nevertheless use the coin to make a fair decision? \n 55. How would you go about checking whether the coin was biased? \n The situation in 54 gives me another idea for a question. \n 56. You and a friend have to decide between two possible ways of spending the evening: going to the cinema or staying at home and watching a football match. Your friend would prefer to watch the football but is quite interested in the film. You too are interested in the film but absolutely hate football, so your preference is much stronger than that of your friend. What would be a good way of deciding what to do? What if the situation repeats itself every week? \n 57. You are offered the following opportunity. You start with \u00a31, but if you like you can give yourself a chance to increase your money by playing the following game. You toss an unbiased coin. If it ever comes up tails, you lose all your money and the game stops and you will never get the chance to play it again. But if it comes up heads, the amount of money you have multiplies by 3 and you can have another go with your increased stake. What should you do? \n 58. I want to weigh my young daughter on my bathroom scales. The trouble is, she can\u2019t stand still for long enough. So I weigh myself holding her and then myself not holding her and take the difference. However, the scales are accurate to the nearest 100 grams, so all I can tell from this process is that my daughter\u2019s weight lies within some 200-gram range. Is there any way of using the scales to get a more accurate measurement? \n What I have in mind here is repeating the process before and after supper, with and without shoes, etc. etc., to obtain a collection of measurements of which I can take the average. Obviously, it would be unreasonable to expect a 16-year-old who didn\u2019t like maths to think of that idea, but one could give hints such as, \u201cMy wife goes through the same process and gets a different answer. What could we do with those two answers?\u201d \n 59. How much money is it worth spending to keep track of asteroids in case any of them are on a collision course with the Earth? Suppose scientists declared that there was a 1% chance that a particular very large asteroid would collide with the Earth in five years\u2019 time. How much money would it be worth spending to divert it? \n 60. Devise a strategy for never losing at noughts and crosses (=tic tac toe). Try to make it as economical as possible, while still telling you unambiguously what to do for each move. What more could you ask of a strategy? \n The answer to the last part is that you could ask for a strategy that always wins when it is in a winning position. It would be an interesting exercise to try to get people to formulate precisely what a \u201cwinning position\u201d is. \n 61. A political blog gets so many comments that good comments are often drowned out by a sea of stupid ones. To combat this, the owners of the blog decide to introduce a reputation system, so that comments by people who have a good record of being interesting appear at the top of the list. What would be a good way of doing this? \n 62. In a football league of 20 teams each team is supposed to play each other team twice, so it is supposed to play a total of 38 matches. As a result of a players\u2019 strike, each team in fact plays only 10 matches. At the end of the season, decisions have to be made about promotion and relegation, so the teams have to be put in order. The initial proposal is simply to add up the points that the teams have obtained so far, but some teams complain that this is unfair because they have played against much tougher opposition than some other teams with similar numbers of points. Can you devise a fair system that would take the quality of the opposition into account? \n 63. Guardian journalist Zoe Williams recently wrote an article that included the following paragraph: \u201cLess well known is that, mile for mile, it\u2019s more dangerous to be a pedestrian than it is to be a cyclist, and every journey by public transport generates two journeys by foot (most journeys by car will generate at least one journey by foot \u2013 it\u2019s rare to be able to drive directly from one door to another). Pedestrians never object en masse; they don\u2019t self-identify as \u201cpedestrians\u201d and they never say how outrageous it is how many of them die. And yet in 2011, in Greater London, 77 pedestrians were killed (to 16 cyclists); 903 were seriously injured (to 555 cyclists); their deaths were up 33% on the year before, the serious injuries up 6%.\u201d Do the statistics she quotes justify the assertion that mile for mile it is more dangerous to be a pedestrian? Is the assertion likely to be true? (The full article can be found here .) \n 64. As I write this question (on the 12th October 2012), there is a debate about the merits or otherwise of badger culling in order to reduce the spread of bovine tuberculosis. One proposal is to reduce the badger population in certain areas by 70%. But how can one estimate the population in the first place? Pilot studies have been done to try to assess whether culling works. How would you go about designing such a study (assuming you were willing to cull badgers)? \n Some basic facts about these issues can be found on the web. \n 65. If you are a middle-distance or long-distance runner, then you have to worry about two things: speed and endurance. If you run too fast, you won\u2019t be able to keep it up and will be overtaken. If you conserve energy too much, you won\u2019t be able to catch up with the rest of the field. The standard strategy is to run at a roughly constant speed for most of the race and then to speed up at the end in a sprint finish. Is this likely to be a better strategy than aiming for a slightly faster constant speed and no sprint finish? If so, why? And what factors will determine the best strategy? [To keep things simple, it may be better to imagine that you are racing against the clock rather than against other runners.] \n \n How would questions of this kind form the basis for a mathematics class? \n I have very strong views about this. What I emphatically would not like to see is teachers learning \u201cthe right answer\u201d and giving a mini-lecture about it to their classes. Instead, the entire discussion should be far more Socratic . The idea is that the teacher would go into a discussion about a question like this with a good grasp of the issues involved, but would begin by simply asking the question. An initial danger is that nobody would have anything to say, but one way of guarding against that is to discuss questions that people are likely to care about. For example, the question above about whether girls are better than boys at a certain subject is far more likely to encourage people to think critically about statistics than a mathematically equivalent question about a less contentious topic. If the discussion stalled, the teacher\u2019s job would be to give it a little nudge in the right direction. For example, in the unlikely event that nobody had anything to say about girls and boys and their exam results, one could ask a question such as, \u201cDoes the average grade tell us everything we need to know about how good boys are at this subject?\u201d If that still didn\u2019t elicit a response, one could ask something more specific like, \u201cGirls got a higher average score on this paper. Does that mean that the highest score of all must have been achieved by a girl? Does it mean that most girls scored more highly than most boys?\u201d That would start people thinking. If even that failed, one could show the class a couple of silly distributions. For example, one distribution might have all boys with an identical score, and all but one of the girls with a slightly lower score and one girl with a stratospherically high score that lifts the average above the average for boys. (That doesn\u2019t answer the two questions above, but it indirectly gives people a technique \u2014 looking at extreme cases \u2014 for answering questions of this general type.) \n The main point is one I\u2019ve basically made already: the discussions should start from the real-life problem rather than starting from the mathematics. Pupils should not feel that the question is an excuse to force some mathematics on them: they should be interested in the question and should feel the need for the mathematics, the need arising because one can give much better answers if one models the situation mathematically and analyses the model. \n \n For the remainder of this post, I want to consider a few obvious objections to the idea of a course of this kind. \n Objection 1. The last thing we want to do is water down the mathematics curriculum like this. \n Response. I am not proposing a watering down of the mathematics curriculum. This idea is not aimed at everybody, but rather at the (pretty large) cohort of pupils who are intelligent and motivated to learn, but who for one reason or another do not get on well with the traditional mathematics curriculum. One thing one could do with such people is make one further attempt to get them to learn how to rearrange equations, solve quadratics, solve simple questions in trigonometry, and so on. But do they really need that? And even if they do, is it likely that they will become more receptive to that kind of mathematics if it is presented again in basically the way that they already know they dislike? Is it not far more likely that a completely different kind of course of the kind I\u2019ve just described would help at least some of them to lose their dislike of mathematics? (Obviously that is an empirical question, and one that should be empirically tested if an idea like this is taken seriously.) \n Objection 2. Certain very important mathematical skills, such as solving word problems by forming equations and solving those equations, do not seem to figure in the questions above. \n Response. How important a life skill is turning word problems into equations and solving the equations? For some people it is undoubtedly useful a lot of the time, and for many more people it could be useful occasionally. But if you think it should dominate the way mathematics is taught, I recommend the essay What is Mathematics For? by Underwood Dudley. In fact, I recommend it anyway. In it, he rips to shreds the argument that things like algebra and trigonometry are necessary in people\u2019s lives (even their working lives). His conclusion is that mathematics should be defended for its own sake. For example, for most people the great benefit of learning how to solve quadratic equations is not that one day they might actually need to solve a quadratic equation, but rather that it is a wonderful example of a non-obvious idea in mathematics: at first it looks as though you have little choice but to use trial and error, but by the clever trick of completing the square you can solve the problem quickly and systematically. \n But another (possibly related) reason I have not focused on real-world problems that require algebra and trigonometry is that I have found it very hard to come up with good examples. I can come up with examples of some kind, but not ones that are interesting and engaging. \n Here is a boring question, just to give an idea of what I don\u2019t like. You know that you will need two thousand pounds in five years\u2019 time, and a bank account offers you a fixed rate of interest of 5% per year over the next five years. How much do you need to invest so that you will have two thousand pounds at the end of the five years? \n That problem certainly leads to an equation. But it also makes my heart sink. I couldn\u2019t go into a classroom and enthusiastically start a discussion about it. Nor could I expect people who don\u2019t like maths to have an opinion one way or another about what should be done. \n Perhaps part of the problem is that the answer to the question is a number . The answers to the questions in the list above are things like \u201cThe best thing to do is X\u201d or \u201cThe verdict was wrong because Y\u201d. The Fermi estimation problems do ask for numbers, but the numbers are approximate, you are not asked to solve equations, and the questions are somehow fun. \n I am not in principle against questions that naturally lead to algebra, but in practice I find them very hard to come by. \n Objection 3. Future biologists and chemists would be hugely helped by being more competent mathematically. A course like this does not help to develop that competence. \n Response. I have two complementary responses. One is that future chemists and biologists do not have to take a course like this. In general, people would be encouraged to take A-level, or maybe just AS-level, if they are good enough at mathematics to benefit from it. A course of the kind I am describing would be good for a large number of people, but not necessarily the best course for a large percentage of people. \n However, I actually think that future biologists and chemists and even future mathematicians could benefit greatly from a course like this, that encouraged them to think mathematically rather than simply applying some methods they\u2019ve been taught to standard problems. \n Objection 4. It\u2019s not enough to think about what to teach: you must also think about how it would be examined. \n Response. Unfortunately that\u2019s true. However, one thing we have in our favour here is that results on the course would not have the kinds of important consequences that A-level results have. People taking the course would be doing it as a sideline. I would hope that universities would be more interested in a prospective history student (say) if they had done well on a how-to-think-mathematically course, but that it would be regarded as a fringe benefit rather than one of the main criteria on which the student was judged. \n I am told by people I know at ACME that they would be happy to recommend some form of continuous assessment or project work. Normally, I am not keen on that at all, but for a course like this, I think it could work. Suppose, for instance, that a news story breaks that, like the Sally Clark case, has a significant mathematical content. To expect somebody who isn\u2019t all that good at maths to comment intelligently under exam conditions does not seem reasonable. But it seems much more reasonable to give that person a project that is done over several weeks and requires them to look up facts on the internet and draw conclusions from those facts with the help of a bit of mathematical reasoning, perhaps after mathematically similar questions have been discussed in class. \n I actually quite like the idea of a very open-ended exam that feels a bit like a general paper: you could pass the exam with almost no mathematics, just by discussing the questions reasonably intelligently, but to do well you would need to come up with mathematical models, Fermi estimates, abstractions and the like. \n In general, I think that assessing a course of this type would be challenging, but not impossible. And therefore I would not want the need for assessment to distort what is taught (which was the basic problem with the Use of Mathematics A-level). \n Objection 5. You\u2019d never find enough teachers who were capable of teaching a course like this. To do it well, you need to have a very sophisticated understanding of probability, statistics, game theory, physics, multivariable calculus, algorithms, etc. \n Response. This is to my mind by far the most serious objection. The best I can offer is ways of mitigating the problem. A few ideas are the following. \n 1. Produce copious teaching materials \u2014 for example, a book with the questions above, and many more, each with a detailed discussion of the mathematical (and other) issues it raises, and detailed suggestions for how to elicit ideas from the pupils rather than simply \u201ctelling them the answers\u201d. \n 2. Identify a few outstanding teachers, video them giving successful classes on these questions, and make the videos available to other teachers. \n 3. Set up a forum where teachers can exchange ideas and report back on what worked well for them and what worked less well. \n 4. Thoroughly road test questions before letting them loose on the nation\u2019s schoolchildren. In fact, that applies to the entire course: make sure one has something that definitely can work before encouraging too many schools to teach it. \n I am planning to do a bit of testing myself. In just under a month\u2019s time I shall be giving a talk at Watford Grammar School for Boys, and I intend to take one or two of the questions from the list above and see whether I can get a good discussion going. I\u2019ll report back on how I get on. \n If you agree that something like this could work and want to increase the chances of its becoming a reality, then I would be very grateful for any questions you can think of that would go well with the above ones. I don\u2019t mind if they are mathematically very close to questions already on the list, as long as they don\u2019t look too similar on the surface. An ideal question is one that is based on a recent news story, or on an experience that we all have (or can imagine having) from time to time \u2014 such as Terence Tao\u2019s shoelace-in-the-airport question \u2014 since then its real-world relevance cannot be questioned. But any question that is interesting, that does not explicitly mention mathematics, and that can be profitably analysed with the help of some mathematics that is not inappropriately difficult (so I\u2019d rule out differential equations, for example), will be gratefully received. I am told that the more questions I can come up with, and the more varied they are, the greater my chance of convincing ACME that they can convince people further up the chain that this could work."], "link": "http://gowers.wordpress.com/2012/06/08/how-should-mathematics-be-taught-to-non-mathematicians/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 9, "http://www.ams.org/": 1, "http://feeds.wordpress.com/": 1, "http://www.umd.edu/": 2, "http://gilkalai.wordpress.com/": 1, "http://en.wikipedia.org/": 7, "http://www.co.uk/": 5, "http://terrytao.wordpress.com/": 3}, "blogtitle": "Gowers's Weblog"}, {"content": ["This brief post is to update further a recent post that was itself an update on the situation with EPSRC . The good news is that EPSRC postdoctoral fellowships in mathematics are now available for \u201cintradisciplinary research\u201d (as was already the case with the early career and established career fellowships). I am told that a certain amount of work went on behind the scenes to achieve this: we should be very grateful to the mathematicians involved, and grateful also to EPSRC for being prepared to show a degree of flexibility in this instance. I am also told, though only time will tell how true this is, that the interpretation of the word \u201cintradisciplinary\u201d will be generous, so unless your research is extremely narrow, you should be able to present it in a way that will qualify."], "link": "http://gowers.wordpress.com/2012/05/31/epsrc-update-update/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 1, "http://feeds.wordpress.com/": 1, "http://www.ac.uk/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Since time is short, I am going to discuss a couple of Groups questions but in slightly less detail than I have been giving up to now: instead of working through the questions completely, I\u2019ll try to zero in on the most important points. Because there wasn\u2019t a separate Groups course until 2008, I am taking my questions from that year. \n 5E. For a normal subgroup of a group , explain carefully how to make the set of (left) cosets of into a group. \n For a subgroup of a group , show that the following are equivalent: \n (i) is a normal subgroup of ; \n (ii) there exist a group and a homomorphism such that is the kernel of . \n Let be a finite group that has a proper subgroup of index (in other words, ). Show that if , then cannot be simple. [Hint: Let act on the set of left cosets of by left multiplication.] \n \n The first part of this is bookwork, so I shan\u2019t go through it. However, let me remark that there is one how-much-to-write decision to make here. Is it enough to say that we define the product of and to be , or do we need to prove that that operation is well-defined (meaning that if and then )? There is a simple principle that determines the answer to this question: if you are in doubt, and the question says \u201cexplain carefully\u201d, then that word \u201ccarefully\u201d is telling you to go for the more careful option. So it\u2019s pretty clear here that you are intended to prove that the operation is well-defined, rather than merely stating it or, worse still, failing to mention the issue at all. \n Next, we\u2019re asked to prove that kernels of homomorphisms and normal subgroups are the same thing. The proof that kernels of homomorphisms are normal subgroups is easy, but the reverse is rather less so (I discussed this result in detail in this blog post ). The rough answer is that if is a normal subgroup, then it is the kernel of the quotient map from to . So we need to prove that that map is a homomorphism, but that follows directly from the definition of multiplication of cosets. \n Now for the non-bookwork part of the question (though this little problem is often on examples sheets). If you are not comfortable with group actions, you may wonder how defining an action can help you prove that a group is not simple. But the answer to that question is simple (if you\u2019ll excuse the very feeble pun). \n 1. is not simple if and only if has a non-trivial normal subgroup. [Definition of \"simple\".] \n 2. So we want a non-trivial normal subgroup. \n 3. The question is begging us to find a homomorphism with non-trivial kernel, by which I mean a kernel that is not the identity and not the whole of . [Remark: even if the question were not begging us to do that, defining homomorphisms and taking their kernels is still a very good way of coming up with normal subgroups.] \n 4. We\u2019re given a group action. \n 5. But group actions are homomorphisms (or, if you prefer, naturally associated with homomorphisms). So let\u2019s think about the kernel of the group action we are given. [Remark: similarly, it is a good general rule that group actions give us a nice big supply of homomorphisms.] \n What does do to the left cosets? It permutes them. Since has index , there are left cosets. The axioms for group actions tell us that the action on is a homomorphism from to the group of permutations of the left cosets of , which is isomorphic to the permutation group . The action is non-trivial, in the sense that at least some elements of do not perform the identity permutation on the left cosets (proof \u2014 pick and observe that ). \n Now where is this condition that going to come in? Ah, the relevance of that is that it is the size of the group . So we have a non-trivial homomorphism from to , and is strictly larger than . Therefore, it has a non-trivial kernel. And we\u2019re done. \n The eventual proof can be written concisely as follows. \n There are left cosets of , so the group of permutations of these left cosets has size . The left-multiplication action is a homomorphism from to this group, and since , this action must contain elements other than the identity in its kernel. On the other hand, the kernel is not the whole of , since any element permutes the left cosets of non-trivially. Therefore, the action of has a non-trivial kernel, which is a non-trivial normal subgroup, so is not simple. \n \n I have already said that you\u2019ll get more out of these posts if you try the questions first. That is particularly true of this next question: if you solve it for yourself you\u2019ll have really learnt something. \n 7E. Show that every M\u00f6bius map may be expressed as a composition of maps of the form , and (where and are complex numbers). \n Which of the following statements are true and which are false? Justify your answers. \n (i) Every M\u00f6bius map that fixes may be expressed as a composition of maps of the form and (where and are complex numbers). \n (ii) Every M\u00f6bius map that fixes may be expressed as a composition of maps of the form and (where is a complex number). \n (ii) Every M\u00f6bius map may be expressed as a composition of maps of the form and (where is a complex number). \n A quickish way of doing the first part is to observe that the class of maps we are allowed to use is closed under inversion: the first is inverted by , the second by (since we will not take ), and the third by itself. So if we can convert into by applying maps of the three kinds, then doing the inverses of those maps in the reverse order will convert into . \n How can we simplify ? It\u2019s tempting to subtract , because that looks as though it will simplify the fraction. And indeed, \n \n We don\u2019t want to carry too many complicated expressions around, so we might as well rewrite this as , remarking that , by the definition of a M\u00f6bius map. To simplify this, an obvious move is to turn it upside down. That gives us , which we can write as . And now it\u2019s obvious that we can get to by subtracting and dividing by . Or is it? We need not to be zero. That\u2019s a problem, because it is conceivable that is zero. I\u2019ve overlooked the possibility that might be zero. Probably the easiest way of dealing with that is to observe that when the problem is easy. \n OK, let\u2019s move to the three true-or-false problems. \n (i) Under what circumstances does a M\u00f6bius map fix ? The limit as of is , so we need , which implies that . It also implies that , so we have a map of the form , which can obviously be expressed in the desired way. So that\u2019s one TRUE. \n (ii) A M\u00f6bius map fixes if and only if , which implies that implies that neither nor is . Since we\u2019re allowed the transformation , let\u2019s think about rather than about . Can we express using a combination of scalar multiplication and reciprocals? It looks a bit difficult because of that plus sign. \n There are two natural ways that one might think of proceeding from here. We suspect that the result is false, so we could either try to understand which maps can be expressed as combinations of scalar multiplication and reciprocals, or we could try to find some property that distinguishes all such maps from the map (for suitably chosen ). Let\u2019s try both approaches. \n A little experimentation suggests that the only maps you can make out of and are of the form or . Indeed, we can make all these maps, and if you multiply one of them by or take its reciprocal you get another one. But the map is not of that form, so we\u2019re done. \n But why am I so confident that it is not of that form? After all, the representation of a M\u00f6bius map is not unique. It\u2019s not hard to be sure about this, but in the end I think the nicest way of doing it is to go for the second approach. And a natural thing to think about when you\u2019re trying to distinguish between M\u00f6bius maps is fixed points, or more generally fixed sets of points. What can we say about the two maps we\u2019re allowed to use? Multiplying by fixes and < while taking the reciprocal swaps and . So either way, the set maps to itself. Since takes to , it can't be expressed in the desired way. So that's a FALSE. \n (iii) This looks like a FALSE as well, since when we started the question we certainly made use of scalar multiplication. So let\u2019s try to find a special property that compositions of translation and reciprocation have that a general M\u00f6bius map does not need to have. \n Another thought is this. If some M\u00f6bius map is not going to have the property, then some scalar multiplication won\u2019t have the property, assuming, that is, that the property is closed under composition, which it more or less has to be if it is going to be at all natural. So maybe we should look for a property that the map fails to have that all compositions of translations and reciprocation have. \n What might this property be? It\u2019s difficult to see where fixed points might come in, because translations fix just and once you start composing with there is no obvious pattern. But one thing we can say is that translations don\u2019t stretch the complex plane. Could that help? Might there be some sense in which every composition of translations and is either \u201con average\u201d like a translation or \u201con average\u201d like a translation of ? \n It seems difficult to come up with a pithy definition, so let\u2019s switch to the other kind of approach: just getting a feel for what kinds of maps we can generate. \n And here is a general and very useful group theory tip: think about conjugations . Almost always, a conjugation will be more informative than some arbitrary composition. So in a spirit of experimentation, let\u2019s conjugate by and see what we get. And later we might conjugate by . \n The first conjugation takes to to to . That last map can be rewritten as . That\u2019s interesting: it doesn\u2019t look much like a translation. Can we simplify it? We could take the reciprocal, but that\u2019s just undoing what we\u2019ve done. Another possibility would be to subtract : at the moment the limit as is , so doing this subtraction would move it to , which ought to make things look nicer. And indeed, \n . \n It\u2019s looking awfully as though we\u2019ve managed to get scalar multiplication in here. Let\u2019s take the reciprocal. That gets us to . Adding simplifies this to . Which complex numbers can be written as for some complex number . Er \u2026 all of them. So we\u2019ve managed to generate all maps of the form , and therefore, by the introductory result, we get all M\u00f6bius maps. \n I confess that I remember being told by the examiner that year that the answer was TRUE and that almost nobody got it. However, they would have got it if they had followed two basic principles, one general and one slightly more specific. The general one is to do a bit of experimentation: if you want to get a feel for what you can generate, then see what you can generate! The more specific one is that a great start in this process is to look at what you get if you conjugate some of your generators by other generators. \n \n That more specific tip can be generalized: if you are doing a question about M\u00f6bius maps, there is a strong chance that conjugation will make things easier. Here\u2019s a quick example. Suppose you are asked to find a non-trivial M\u00f6bius map that fixes the complex numbers 1 and . You could just define the map to be , get some equations for and find a non-trivial solution. But the calculations would be fairly unpleasant. \n How much less unpleasant they would be if only the fixed points were something nice like and . Then we could just write down an example: the map . Let\u2019s call this map . Is there some way of \u201ctransferring\u201d this example to the one we\u2019re actually asked for? Yes there is \u2014 conjugation. What we do is find a M\u00f6bius map that takes to and to . Then what does do to the point 1? It takes it first to , where it remains fixed before returning to . Similarly, goes to , stays still, then comes back again. Since fixes no other points, fixes no points other than and . \n How easy is it to find a map that takes to and to ? The first condition tells us that , and the second that . So we can just write down an answer: . That will be our . \n The remaining calculations are slightly tedious but basically easy. We need to work out the inverse of (which is easy enough \u2014 just set that expression to be and solve for in terms of ). We then need to work out what happens to if we apply , multiply by 2, and apply . \n You may question whether that was easier than the brute-force approach. So perhaps I should add that in theoretical questions it really is much easier. Suppose the question were to prove that for any two distinct complex numbers and there is a non-trivial M\u00f6bius map that fixes and and that gives you the identity after you do it four times. (That is, it has order 4 in the group of M\u00f6bius maps.) That would be truly horrible if you did it by brute force. However, if you use conjugation, you can observe that multiplication by has the desired property, but with the fixed points and instead. Then you prove that there exists a M\u00f6bius map that takes to and to . And then the conjugation does the job. And since the question merely asked you to prove that the map exists, you don\u2019t even have to work out the conjugation, though it wouldn\u2019t be too unpleasant if you did want to. \n In general, conjugation can be thought of as what you do when you want to move the \u201ccentre of activity\u201d to a nice place. You find a map that takes the nice place to the place you\u2019ve actually been given. Then you do to get to the nice place, followed by a nice transformation (which exists because the place is nice) followed by to get back to the place you were given. It\u2019s this basic idea that explains why expressions of the form occur so frequently in mathematics. Less importantly, but more urgently, it also explains why conjugation occurs so frequently in Tripos questions."], "link": "http://gowers.wordpress.com/2012/05/29/a-look-at-a-few-tripos-questions-x/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Exam day approaches, so I\u2019ve decided to prioritize. Instead of doing question 7C, which isn\u2019t all that interesting (in the sense that it doesn\u2019t give me much scope to emphasize principles of more general use in exams), I\u2019m going to skip it, and instead, if I get time, go through a groups question or two. But first I will do the final Numbers and Sets question because it involves something a lot of people dislike: the inclusion-exclusion principle. It\u2019s worth getting comfortable with this, because it comes up year after year (either in Numbers and Sets or in Probability). You may think that applying the principle requires some ingenuity. The aim of this post is to convince you that it can be done on autopilot. \n 8C. Let be a finite set with elements. How many functions are there from to ? How many relations are there on ? \n Show that the number of relations on such that, for each , there exists at least one with , is . \n Using the inclusion-exclusion principle or otherwise, deduce that the number of such relations for which, in addition, for each , there exists at least one with , is \n . \n \n In my posts from last autumn, I went to great lengths to stress that the formal definitions of \u201cfunction\u201d and \u201crelation\u201d (as subsets of Cartesian products) should not be thought of as what those concepts are really about. I\u2019m not going to revisit those arguments here, but just make the point that if you\u2019re ever asked a question about numbers of functions and relations, then you obviously need to be completely clear about what constitutes a function/relation and when two of them are distinct. Here the formal definitions are absolutely necessary, and much easier to use than any more intuitive concept you might have. \n A function from to , then, is a subset of such that for every there is exactly one with . For each we get choices for , and the choices don\u2019t affect each other, so the answer is . \n Actually, I didn\u2019t really need the Cartesian-products definition there \u2014 all I needed was that for every there is a unique such that , and that any can be chosen. So here is a concise and adequate answer to the question. \n For each there are choices for . Hence, there are possible functions . \n With relations, however, the Cartesian-products definition indisputably makes things easier. Here\u2019s the answer. \n The set has size . It therefore has subsets. Therefore, the number of relations on is . \n One could argue that the last sentence there is unnecessary, but I put it there just to make absolutely clear that I know that relations on are defined (formally) to be subsets of . \n Now we are asked for the number of relations such that for every there is at least one such that . Or rather, we\u2019re given the number, , and asked to justify it. \n That potentially makes things easier, since it gives us two options. We could simply try to work out the number and then check at the end that what we get agrees with the answer given. But we could also try to use the answer as a clue to the derivation of the answer. \n The first approach (to trying to solve the question) would be something like this. We see that for each there is some constraint. Let\u2019s focus on just one . How many ways are there of choosing which are related to that ? Well, there must be at least one, but other than that there aren\u2019t any constraints. In other words, we can choose any non-empty set of s. So the number of ways of choosing which satisfy is the number of non-empty subsets of , which is . Ah, that looks promising. It\u2019s clear that what we choose for one doesn\u2019t affect what we choose for any other , so we get to choose an arbitrary non-empty subset for each , which we can do in ways. \n If we used the answer, we would have thought as follows. What is the significance of that ? Well, is the number of subsets of a set of size , so the most natural structure of size is probably the number of non-empty subsets of a set of size . The fact that we raise this to the power suggests that we are choosing independent non-empty sets. Wait, that\u2019s exactly what we\u2019re doing! \n Finally, here\u2019s the answer written down. \n For each let be the set of such that . The only constraints on the are that each should be a non-empty subset of . Since there are such sets, the number of relations with this property is , as stated. \n It wasn\u2019t essential to define those sets , but on the spur of the moment I felt like it, and I think it did make the answer slightly easier to write down. \n Finally, we get to the interesting part of the question. Again there are two approaches to what we do with the answer we are given: use it as a check at the end, or use it to help us work out how the proof goes. It often happens with applications of the inclusion-exclusion formula that it is hard to see how the expression in front of you relates to the thing you are trying to count. Because of that, I would normally recommend going for the first approach in such questions. For instance, here I can see that is likely to arise from independent choices of a non-empty subset of a set of size , but I don\u2019t quite see where those choices are going to come in. \n Another piece of advice with inclusion-exclusion applications is this. If you can\u2019t immediately see which union of sets you should be applying the result to (or intersection of complements of sets), then just try to solve the problem without the inclusion-exclusion formula until it becomes clear what the sets should be. Later, I shall explain how to recognise them when they appear. \n OK, we\u2019ve got relations such that for each there exists with . How on earth can we count the ones that have the same property the other way round? At first it looks impossible. However, it looks potentially slightly easier to count the ones that don\u2019t work. \n This is another piece of general inclusion-exclusion advice: when you are asked to count something, consider counting its complement. If you want to be really systematic about this, you can actually decide in advance whether the original set or its complement is likely to be a union of sets. Here, for example, we want the relations to satisfy that for every something happens. So that\u2019s an intersection. Since the inclusion-exclusion formula applies to unions, we should look at the complement, so that now we are looking at those relations such that there exists some for which something goes wrong. \n If it isn\u2019t obvious to you that is closely related to intersections and to unions, then I recommend staring at the following four definitions until you are happy that the first two are the same and the third and fourth are the same. is some arbitrary property that can apply to pairs . \n \n \n \n \n Going back to the question, our focus is now on counting the number of relations such that for every there exists with and such that, in addition, there exists such that for no do we have . \n Now the moment we\u2019ve got we have a union, and by this time it\u2019s a safe bet that it\u2019s the union we\u2019re supposed to count. So let us define some sets. But first, I\u2019m getting sick of stating over and over again the property that the relations are already supposed to satisfy, so let\u2019s start with a convenient definition. \n Let us define a relation on to be good if for every there exists with . \n Now for the sets, which, as I have just said, are more or less forced on me. \n For each , let be the set of all good relations such that there is no with . \n The rough thought in the back of my mind is this. Amongst the good relations, there are some that go wrong . Each one that goes wrong goes wrong because of some . So I define to be the set of good relations that go wrong because of . And now I want to work out the size of the union of the , which is complicated because the can overlap. But that\u2019s where the inclusion-exclusion formula will come in. \n Once we\u2019ve got the sets to which the formula will apply, we\u2019ve broken the back of the question, so I\u2019ll say less about the rest. The main thing we need to know in order to apply the formula is the size of the intersection of of the bad sets. So what is the size of ? This is the set of all good relations such that there is no with , no with , and so on. In other words, it is the set of all good relations such that for each the set of all such that is disjoint from the set . The condition that is good tells us that is non-empty, so the number of possibilities for is the number of non-empty subsets of , which is . Aha \u2014 now we see where the in the answer comes from, and why we were indeed destined to count the number of non-empty subsets of a set of size . \n Since for each the constraints on are the same, and do not affect each other, the total number of good relations such that for every is . \n Since the size of depends only on (assuming, as we are, that the are distinct), the sum of these sizes over all -tuples is . Therefore, by the inclusion-exclusion formula, the size of is \n . \n Finally, since what we are actually interested in is the good relations that do not belong to this set, we must subtract this answer from , the number of good relations. That last number conveniently equals when , so our final answer is \n \n as it should be. \n Of course, it wasn\u2019t a coincidence that the number of good relations was the term, and we could have done this problem by using the intersection-of-complements formulation of the inclusion-exclusion formula. But I myself feel more comfortable with the unions version, even when it requires me to take an extra little step at the end. \n I won\u2019t bother to write out a proper answer, since I think it can be extracted quite easily from the account above. But let me repeat some of the key points. \n 1. If you are asked to apply the inclusion-exclusion formula, then don\u2019t try too hard to relate the problem to the answer you are presented with \u2014 if you can, then great, but if you can\u2019t, it is not cause for panic. \n 2. Describe carefully the object you are trying to count. Typically, your description will involve either \u201cfor every\u201d or \u201cthere exists\u201d at some point. If it involves \u201cthere exists such that blah\u201d then it is extremely likely that you should start by writing \u201clet be the set of all such that blah\u201d, after which your task will be to work out the size of the union of the . \n 3. To do this, you will use the inclusion-exclusion formula, and typically you will find that calculating the intersection of any of the (the input you have to provide before using the formula) is easy. \n 4. If the set you are counting involves not \u201cthere exists\u201d but \u201cfor every\u201d, then you will almost certainly want to look at the complement of that set instead (at least if you want to apply the inclusion-exclusion formula in its unions version). That will convert \u201cfor every\u201d into \u201cthere exists\u201d. More precisely, if the set you are counting can be described naturally as \u201cthe set such that for every blah\u201d then you will almost certainly want to say, \u201cLet be the set of all such that not blah\u201d. Now continue as in the \u201cthere exists\u201d case, except that this time when you have calculated the size of the union of the you will need to subtract that from the size of the entire set. \n Let me quickly illustrate this with another example, which is a very standard application of the inclusion-exclusion formula: calculating the number of surjections from a set of size to a set of size , when . \n First, what is \u201cthe entire set\u201d? It\u2019s the set of all functions from to . What is a surjection from to ? It is a function such that for every there exists such that . How did that start? It started \u201cfor every \u201c. Since that\u2019s the \u201cfor every\u201d case, we\u2019d better define to be the set of all functions such that there is no with . And now we\u2019re away. The size of is easy to calculate, since it\u2019s the set of all functions that take values in , of which there are . One application of inclusion-exclusion and one taking of complements later we end up with the answer, which is \n ."], "link": "http://gowers.wordpress.com/2012/05/26/a-look-at-a-few-tripos-questions-ix/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Now for a question on modular arithmetic. As with countability, there is a very high chance of a question on this topic. [Added after the post was written: as usual I wrote down my thoughts about this question as I had them, and I didn't spot the best approach to part (ii) of the question until after I had come up with some less good approaches. So my recommendations evolve through the post, with some of the later ones superseding some of the earlier ones.] \n 6C. (i) Prove Wilson\u2019s theorem: if is prime then (mod ). \n Deduce that if (mod 4) then \n (mod ). \n (ii) Suppose that is a prime of the form . Show that if (mod ) then (mod ). \n (iii) Deduce that if is an odd prime, then the congruence \n (mod ) \n has exactly two solutions (modulo ) if (mod 4), and none otherwise. \n \n The first part is pure bookwork, so I\u2019m just going to write out the answer fairly concisely. I\u2019ll add a couple of small comments in square brackets. \n Every number between 1 and has a unique multiplicative inverse mod . [Legitimate just to state this because it is from an earlier part of the course.] Therefore we can partition the integers into sets of the form . [I didn't spell out that the inverse of is , since that is so obvious that I cannot imagine losing marks for not stating it explicitly.] Each such set consists of two elements unless , or equivalently (mod ). But if and only if if and only if (mod ). [I could have said slightly more or slightly less, but this seems about right: it shows that I understand what is going on but doesn't spell out everything. For example, I clearly used the fact that there are no non-trivial zero divisors mod , but that's easily seen to be equivalent to the existence of multiplicative inverses, so I felt that in a sense I had already implicitly stated it.] If then and the product of the remaining numbers, 1 and -1, is -1. The result follows. \n If you want to see how long that was without the additional comments, then here it is. It\u2019s a sort of model answer, but it shouldn\u2019t be thought of as in any sense unique, since I made a number of small judgments that others might have made differently. But the basic principle is important: you should leave a sceptical examiner in no (reasonable) doubt that you understand the material. \n Every number between 1 and has a unique multiplicative inverse mod . Therefore we can partition the integers into sets of the form . Each such set consists of two elements unless , or equivalently (mod ). But if and only if if and only if (mod ). If then and the product of the remaining numbers, 1 and -1, is -1. The result follows. \n Now for the second part of (i). This is often set as an exercise, so perhaps it was in the year in question. But even if not, the \u201cdeduce that\u201d makes it by no means an impossible question (though one that could be difficult if your brain is freezing up in the middle of an exam). How can we relate the new equation to the equation ? Well, they both say that something equals -1 (mod ). In both cases that thing is a product of numbers. In fact, in both cases it\u2019s a product of numbers. \n The products are not actually equal , though that is not too surprising, since otherwise we wouldn\u2019t need the condition that (mod 4). (It is extremely unlikely that an examiner would be as cruel as to state an irrelevant condition.) \n Another observation is that both products involve the first numbers. The difference between them (in the sense of thing that distinguishes them) is that the remaining numbers are in one case the last numbers and in the other case the first numbers again. So those two need to have equal products. Ah, but the last numbers are just minus the first numbers. So the product of the last numbers is times the product of the first numbers. So they\u2019re equal as long as is even, and that\u2019s where the condition comes in that should be congruent to 1 mod 4. \n The write-up: \n The product is congruent to times the product . Since mod 4, this means that the two products are equal. The result now follows from Wilson\u2019s theorem, since we can replace the second product in by the first to obtain . \n What I wrote there was not what I would write if I were writing a textbook, where the aim is to explain a result to somebody who hasn\u2019t seen it before. Here, the aim is to demonstrate that I know what is going on to someone who has definitely seen it before. That isn\u2019t a licence to be sloppy, but it does allow one to be reasonably concise when it\u2019s clear how to fill in the details. Here, for instance, I couldn\u2019t quite face explaining formally why the two products were the same, so I was slightly informal \u2014 using phrases like \u201cwe can replace\u201d as part of a brief demonstration that I knew how to justify the claim I had made in the previous sentence. \n Now for part (ii). Let me first say how I think about all questions like this. This is a slight digression, because it\u2019s not telling you how to answer the question. However, it is something that\u2019s helpful to bear in mind. \n I can sum it up in one short sentence: the multiplicative group mod is cyclic. (Here must be a prime.) The proof of this fact is rather lovely, because it works despite the fact that there isn\u2019t an easy way of finding a generator. However, it\u2019s too long to write out while solving this question, and it isn\u2019t a fact that you can just quote, since it is more advanced than what you are being asked to prove. \n Why does it imply the result asked for here? Well, if , then we have some number such that the powers run through all the non-zero integers mod . Therefore, if we are given an integer we can \u201ctake logs to base \u201d and write it as . Then mod if and only if mod . Now , so the only multiples of that can be are , , and . Of these, and are not multiples of 4, so those are ruled out too. But if or , then or , so (mod ). \n See here for more examples of results that follow easily from the fact that the multiplicative group mod is cyclic, but now let me think about what the examiner intended for this question. \n We\u2019re given that and that (mod ). We want to prove that (mod ). We can\u2019t see instantly why that should be, so let us begin by writing down what we do know about , which is that it must be . (The justification for this is that , so if , then one of must also be 0. In other words, is essentially the same as it would be if we were talking about real numbers.) \n That reduces our task to that of showing that cannot be congruent to . Now let\u2019s think about why that is the case, using the fact that the multiplicative group is cyclic, but later we shall attempt to prove all the facts we actually use in more elementary ways. So let be our generator again. To show that cannot be , we should think about what is as a power of and what is as a power of . Well, we assume that , so that tells us that . So basically all we know is that is an even power of (which makes sense because the order of in the multiplicative group is even). What about ? Well, and is a square root of , so there\u2019s not much choice for the power: it has to be . But that\u2019s an odd power, so and cannot be the same. \n Can we do anything like this without assuming the existence of a generator ? Did we need to be a generator? Let\u2019s see what we can do with . We don\u2019t know that is a generator, but one crucial fact we used about \u2014 that , applies just as well to , by Fermat\u2019s little theorem. So we know that . What does that tell us about ? That . That is, 1 is an odd power of . But that rules out equalling , since all odd powers of are . \n On reflection, I\u2019m not sure that it helped all that much to know that the multiplicative group is cyclic, though it does for some quite similar questions and is in general a good fact to know. Perhaps a more useful principle for questions like this would be to use Fermat\u2019s little theorem and elementary facts about divisibility. Here is what I\u2019d actually write. \n If then , so . But if , then , which contradicts Fermat\u2019s little theorem. Therefore, . \n (iii) The \u201cdeduce that\u201d here speaks for itself. It\u2019s also clear that we are going to use (i) to prove the case and (ii) to prove the case. Let\u2019s do the case first. \n We certainly know from the first part that there is at least one solution. We also know that there are at most two solutions, by the fact that a polynomial of degree has at most solutions, a result that is valid for any field, with essentially the same proof as the one that works for the reals. (If you\u2019ve got a root then you have a factor . Dividing by that factor you have a polynomial of degree one less.) Finally, we know that if is a solution, then so is , which is different from because and is odd. Here\u2019s what I\u2019d write. \n By part (i) we have at least one solution when . This solution is clearly not zero, so it is distinct from , which is also a solution. And a quadratic equation mod has at most two solutions. So we are done in this case. \n What about when ? I see now that I must have done something that the examiner didn\u2019t intend, because I proved that had no solutions as part of the proof of part (ii). The answer the examiner is clearly looking for is this. \n Now let . If , then , so by part (ii) , which is a contradiction. \n So how do we prove part (ii) without mentioning that cannot be -1? Let\u2019s try applying Fermat\u2019s little theorem straight off. Probably I should have done that earlier. We know that (mod ). If , that tells us that , so it follows straight away that . \n That is a shorter and neater argument than the one I came up with (not that mine was hugely long). Where I slipped up was in not following my own advice, which was partly because I hadn\u2019t formulated it. But here it is again, slightly reformulated. For this kind of question, apply Fermat\u2019s little theorem before you even think. Then see whether what you are being asked to prove follows easily from what you have just written down together with a few simple observations about divisibility."], "link": "http://gowers.wordpress.com/2012/05/24/a-look-at-a-few-tripos-questions-viii/", "bloglinks": {}, "links": {"http://www.tricki.org/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Update 4th June 2012. The petition has now passed 25,000 signatures. It would still be great to push on and reach a significantly higher number by the June 19th deadline. \n As you may know, there is a system in the US for setting up online petitions. Any petition that reaches 25,000 signatures in 30 days will be considered by White House staff. Recently, a petition was set up asking the Obama administration to require publications resulting from research paid for by the US taxpayer to be freely available. If such a requirement were to be put in place, it would be a huge boost to the campaign to make all academic research easily accessible. \n It became possible to sign the petition last Monday, since when there have been (as I write) 14,303 signatures, well over half the number required. Even if the rate of signing goes down, the target of 25,000 by June 19th will probably be reached. However, the organizers want not just to reach the target but to go well beyond it. It is hoped that that, and reported sympathy for the idea within the Obama administration, will give it a real chance of success. So if you can spare two or three minutes (you have to give your email address so that you can receive an email and confirm your identity, and also read a Captcha), then please do the world a service and add your signature. You do not have to be a US citizen to sign. And please pass the message on. \n If you want to read a bit more about the petition and the background to it, then this Guardian article is a good start. \n PS During the writing of this short post, there were six new signatures, so the tally now stands at 14,309."], "link": "http://gowers.wordpress.com/2012/05/24/have-you-signed-the-open-access-petition/", "bloglinks": {}, "links": {"https://wwws.whitehouse.gov/": 1, "http://www.co.uk/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["The obligatory question on countability/uncountability. \n 5C. Define what is meant by the term countable . Show directly from your definition that if is countable, then so is any subset of . \n Show that is countable. Hence or otherwise, show that a countable union of countable sets is countable. Show also that for any , is countable. \n A function is periodic if there exists a positive integer such that, for every , . Show that the set of periodic functions is countable. \n \n If you\u2019ve got countability straight in your head \u2014 by which I mean that you have learnt the statements and proofs of about four key facts, as well, of course, of knowing the precise definitions of countable and uncountable \u2014 then you should find this question very quick and easy. (My impression is that all these 2003 questions are easier than average, which has unfortunately made them not as good an illustration as I had hoped of how to get through an exam.) \n If you read the whole of the first paragraph, you will see a slight implication that you have a choice, since the question says \u201cyour definition\u201d rather than \u201cthe definition\u201d. And indeed you do. \n To begin with, there is the annoying question of whether \u201ccountable\u201d implies \u201cinfinite\u201d. There is no universally agreed standard about this. I myself prefer to say that finite sets are countable, but it is possible that your lecturer used the opposite convention. (The reason I like mine is that I think of countability as a kind of smallness, so I don\u2019t want to insist that my countable sets are also large in some way. Also, there are just too many circumstances where one has to say \u201cfinite or countable\u201d if you insist that countable sets are infinite. However, I do sometimes find myself having to say \u201ccountably infinite\u201d. It\u2019s a bit like the debate about what the symbol means. For me it means \u201cis a subset of\u201d rather than \u201cis a proper subset of\u201d, because I don\u2019t often have to worry about proper subsets, so I\u2019d rather occasionally write for proper subsets than always write for arbitrary subsets.) \n I\u2019m going to assume that \u201ccountable\u201d was defined in your lectures in such a way that finite sets are countable. Even if we grant this, however, you still have a choice, because the following three statements are equivalent (at least for non-empty sets). \n 1. There is an injection from to . \n 2. There is a surjection from to . \n 3. is finite or there is a bijection from to . \n I think it is really the awkwardness of the third definition that tempts people to require countable sets to be infinite \u2014 then one can simply say that a set is countable if there is a bijection between that set and . \n But is that a good argument? Let\u2019s have a look at the first thing we\u2019re asked to do. OK, I\u2019ve just noticed that the question doesn\u2019t even allow the definition that requires a set to be infinite, since then it wouldn\u2019t be true that a subset of a countable set is countable. So it\u2019s clear what the Cambridge convention was in 2003 at least. (I have a funny feeling the lecturer was \u2026 er \u2026 me that year. But I didn\u2019t set the question.) \n Let\u2019s think briefly about which of the three possible definitions will lead to the smoothest proof that a subset of a countable set is countable. And we quickly see that either of the first two definitions works just fine, though the first is slightly easier because you don\u2019t have to argue separately for the empty set (which is problematic because there is no surjection from to ), while the third definition is a bit of a nightmare. Moral: go for the first definition. More general moral: in a situation like this, don\u2019t just write down the first definition that occurs to you. \n Yet another moral, specific to countability, is this: use the formal definitions, and not some intuitive notion of \u201cputting the elements in a list\u201d . After all that, let me deal with the first paragraph of the question. \n A set is countable if there is an injection from to . Suppose that is countable and , and let be an injection. Define a map by setting for every . Then is an injection, so is countable. \n I might well be tempted by the briefer answer where the last two sentences are replaced by \u201cThen the restriction of to is an injection, so is countable.\u201d \n There are all sorts of ways of showing that is countable. A quick one would be this. \n By the fundamental theorem of arithmetic, the function is an injection. Therefore, is countable. \n Now we get a \u201chence or otherwise\u201d. This is a case where the \u201cotherwise\u201d is perfectly OK, but the \u201chence\u201d is quicker, which is why the examiner is (implicitly) recommending it. If we\u2019ve got our countable collection of countable sets, we have an injection from each of those countable sets into . We can then put all those injections together to get an injection from the union into , and composing that with the injection we had from to gives us an injection to . Here\u2019s what I\u2019d actually write. \n Let be countable sets. (If there are only finitely many of them, the proof is similar but easier.) For each , let be an injection. \n A quick pause here, to confess that I was about to write something wrong. What I was about to write was this. \n Define a function as follows. If , then . \n The trouble is that could be in more than one . To sort this out, we instead write the following. \n Define a function as follows. For each , let be minimal such that and define . Then if , we know that and , which implies that , since is an injection. Therefore is an injection. \n We are now asked to prove that is countable for every . Here\u2019s the proof that the examiner clearly had in mind. \n We prove this by induction. It is obviously true when . If it is true for , then it is true for , since , which is a countable union of countable sets. \n I much prefer the following way of proving that sets are countable. Intuitively, a set is countable if every element of that set has a finite description. And the reason it is countable is that there are only finitely many elements that can be described using a description of length at most , so it is a countable union of finite sets. That intuitive idea can usually be turned into a quick and completely rigorous proof. \n Here\u2019s what I\u2019d do for . I\u2019d note that you can describe an element of just by writing down integers, each of which has finitely many digits. So it\u2019s obviously countable. How do I measure the \u201ccomplexity\u201d of an element? A natural way is simply to take the sum of the numbers involved (the maximum would work just as well). So I\u2019d write this. \n For each the set of such that is finite. Since , is countable. \n Yet another proof directly generalizes the proof that is countable. (I like it less because it involves an unnecessary clever idea, but it\u2019s undoubtedly quick to write out.) \n Let be the first primes. By the fundamental theorem of arithmetic, the function that takes to is an injection. Therefore, is countable. \n On to the last part. First, let\u2019s think about what the examiner has in mind. Clearly, we are supposed to observe that there is a one-to-one correspondence between functions with period and elements of . The correspondence takes the function to the element , and in the reverse direction takes the -tuple to the function that takes an integer to , where is the unique integer between 1 and that is congruent to mod . \n Here\u2019s what I might write in order to give the intended answer. \n For each , let be the set of all functions of period . Then it is enough to prove that each is countable, since the set of all periodic functions is the (countable) union of the . \n But the function that takes to is an injection, since once you know the values of a function in at then you know the entire function. Since is countable, there is an injection from to . Composing with we see that is countable, as desired. \n And here is the proof that I prefer. Again, we think about how much information is needed to specify a periodic function, and realize that we\u2019ve pinned it down if we say what the period is and then say what the values are at . So we can write this. \n For each periodic function , write for the period of . Let be the set of all periodic functions such that . Then each set is finite and their union includes all periodic functions. The result follows. \n Finally, for those who like the prime-factorization approach, you could go for this, though it seems to be a bit longer to write out than the previous approach. \n Define a map from the set of all periodic functions that takes a function to the integer , where are the first primes and is the period of . Then is an injection, by the fundamental theorem of arithmetic and the fact that the first values of determine when has period ."], "link": "http://gowers.wordpress.com/2012/05/20/a-look-at-a-few-tripos-questions-vii/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["If you read an earlier post of mine about Elsevier\u2019s updated letter to the mathematical community then you may remember that towards the end of the post I claimed that Elsevier was lobbying heavily to have all mention of open access removed from the documents of Horizon 2020 , Europe\u2019s \u201cFramework Programme for Research and Innovation\u201d, a claim that was then denied by Alicia Wise , who is Elsevier\u2019s \u201cDirector of Universal Access\u201d . \n Leaving aside who is right about this (which may depend rather sensitively on the precise words used to describe what happened, not to mention the interpretation of those words), news has broken today in the THE of potentially important developments. It seems that whatever lobbying Elsevier might have gone in for has been to no avail, because open access will be a very significant aspect of Horizon 2020. \n \n Why is this potentially big news? Well, first of all it comes hard on the heels of the Wellcome Trust\u2019s announcement that it would insist on open access for the research it funds, Harvard University Library\u2019s statement that the current system is unsustainable, the British Government\u2019s announcement that it has plans to make all taxpayer-funded research available online, the decision by TU Munich\u2019s mathematics department to cancel subscriptions to all its Elsevier journals, and an apparently serious suggestion that future rounds of Britain\u2019s Research Excellence Framework will favour open access papers. There seems to be a definite trend here. \n But what also makes it important is the sheer amount of money involved in Horizon 2020. Their budget for the years between 2014 and 2020 is 80 billion Euros. I don\u2019t know what percentage of the world\u2019s scientific papers will be affected by their open access policy (or even what precisely the policy will be, but what some people are reported as saying in the THE article is very promising), but even if it is something small like 2%, there will still be increased pressure to provide the publishing models in which all that research can be published, which will help to speed up the abandonment of the current models that Harvard and TU Munich describe as unsustainable. (It\u2019s amusing that everybody describes what they like as sustainable and what they dislike as unsustainable. I think Elsevier\u2019s prices are sustainable, but that we\u2019d be much better off not sustaining them.) \n Could FRPAA be next?"], "link": "http://gowers.wordpress.com/2012/05/17/horizon-2020-to-promote-open-access/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 2, "http://feeds.wordpress.com/": 1, "http://ec.europa.eu/": 1, "http://www.ac.uk/": 1, "http://en.wikipedia.org/": 1, "http://www.elsevier.com/": 1, "http://www.co.uk/": 4, "http://www.tum.de/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["I\u2019m now going to turn to the Numbers and Sets questions from the same year, 2003. I\u2019ve lost count of the number of times I\u2019ve heard people say that the course is quite easy but the questions on the examples sheets and exams are very hard and \u201cnot very closely related to the course\u201d. There is a grain of truth in that: the new concepts you have to grasp in Numbers and Sets are not as difficult as the new concepts you have to grasp in most of the other courses, so in order to give enough substance to Tripos questions the examiners are almost forced to put in a significant problem-solving element. However, certain styles of problem occur quite regularly, so it\u2019s good to get a bit of practice. And perhaps a detailed discussion of the 2003 questions will be helpful as well. As I did with Analysis I, I\u2019ll start with a post on the Section I questions and then I\u2019ll have separate posts for each of the four Section II questions. The paper, by the way, is Paper 4. \n \n 1. (i) Prove by induction or otherwise that for every , \n \n (ii) Show that the sum of the first positive cubes is divisible by 4 if and only if or (mod 4). \n Part (i) could be an A-level question (I think). Is there a neat way to do it? I\u2019ll think about the \u201cor otherwise\u201d possibility in a moment, but let\u2019s go for the straightforward approach of using induction. \n A non-neat approach would be to prove the well-known formula for the sum of the first positive integers and then prove by induction that the square of that, i.e., , is the sum of the first positive cubes. But let\u2019s see what happens if we just take the statement as it stands and try to prove it by induction. Here\u2019s what I would write to start with. \n The statement is certainly true when , since both sides equal 1. Now suppose that it is true for . Then it will be true for if \n \n Hmm, that looks sort of promising because it we have a difference of two squares. So next I\u2019d write this. \n The right-hand side is a difference of two squares, and therefore equals . \n Now we don\u2019t seem to have much alternative but to work out the sum of the first integers. We might as well use the standard trick. \n But . Therefore, , which shows that the right-hand side is , as required. \n I don\u2019t know whether that was the best thing to do, but the point is that this question can be hacked out pretty quickly one way or another. \n What about the second part? Well, the golden rule of Tripos questions applies, since it is related to the first part. Here goes. \n By Part (i), it is enough to prove that the sum of the first positive integers is even if and only if or 3 (mod 4). To see this, note that has the same parity as when is even and different parity when is odd. Therefore, mod 2 the sums are 1, 1, 0, 0, 1, 1, 0, 0, and so on. \n That completes the question. A quick remark is that what I wrote in that last paragraph might look slightly sloppy, because I didn\u2019t set out a formal inductive argument. However, what I wrote (a) got to the heart of the matter and (b) clearly could be turned into a formal inductive argument. If you feel tempted to leave out details, you should be very sure that you know how to fill in those details, and that the examiner knows that you know how to fill them in, and that the reason you\u2019ve left them out is that they are quite long to write out and genuinely uninteresting. It\u2019s a judgment call, and I\u2019d suggest erring on the safe side. \n What about the \u201cor otherwise\u201d? A fact like that must surely have some reason for being true. It feels like one of those things that is true because of some picture of balls arranged in a \u201cright-angled isosceles tetrahedron\u201d. Except that that\u2019s not quite correct, because the picture looks as though it needs to be four-dimensional. \n Is there something we can count in two different ways, getting the left-hand side if we count one way and the right-hand side if we count the other way? \n To get the right-hand side we could take the Cartesian product of two discrete right-angled triangles. More formally, we could take the set of all quadruples of non-negative integers such that and . But how do we relate that to a bunch of cubes? \n Let\u2019s try to find a nice set of points that gives us the bunch of cubes. We could take the set of all quadruples of positive integers such that and and are all at most . \n Is there a nice bijection between these two sets? I can\u2019t see one. I think I\u2019m going to cheat and use the internet. \n Done that, and although I found a pictorial proof, it was basically just a pictorial version of the above analytic proof, and therefore not very interesting. On to the second question. \n \n 2C. What is an equivalence relation ? For each of the following pairs , determine whether or not is an equivalence relation on . \n (i) , iff is an even integer. \n (ii) , iff . \n (iii) , iff . \n (iv) , iff is times a perfect square. \n A first tip here is not to try to make guesses such as, \u201cThere\u2019s bound to be at least one false statement.\u201d Examiners have sometimes played evil tricks, but more importantly your aim here is to think about the mathematics. And since it\u2019s a Section I question your starting assumption (which occasionally has to be revised) should be that the mathematics is pretty easy. \n Let\u2019s get started. First dilemma: how much can we assume when defining an equivalence relation? Is it enough to say, \u201cAn equivalence relation is a relation that is reflexive, symmetric and transitive?\u201d Or do we need to define those three properties as well? And do we need to say what a relation is? \n These questions don\u2019t have very definite answers. The examiner was probably expecting candidates at least to define the three properties, but would probably have found it quite hard to remove marks for somebody who didn\u2019t do so. Anyhow, since there are three options of increasing explicitness, I think I\u2019ll compromise and go for the second. \n An relation on a set is reflexive if for every . It is symmetric if for every . It is transitive if for every . It is an equivalence relation if it is reflexive, symmetric and transitive. \n I felt free to use the shorthand , since its meaning was very clear. One thing that was important was to demonstrate that I knew how the quantifiers worked. For instance, I didn\u2019t just write, \u201c is symmetric if ,\u201d but I added the words \u201cfor every \u201c. \n Now to the more interesting part of the question. It\u2019s pretty obvious that the first relation is an equivalence relation. Is there a quicker way of proving it than writing out some very obvious calculations? We might consider defining a partition, but perhaps, since the boring method isn\u2019t too laborious, it\u2019s better not to bother to think. Here\u2019s the boring method. \n (i) Let . Then , which is an even integer. If for some integer , then . And if and , then . Since were arbitrary, this shows that is reflexive, symmetric and transitive, so it is an equivalence relation. \n Here\u2019s how one might try to do it with partitions. Even if I don\u2019t necessarily recommend this approach, I definitely recommend, as I\u2019ve said before, at least considering using alternative definitions. \n (i) For every real number in the interval , let be the set . Then the sets form a partition of and if and only if and belong to the same cell of this partition. Therefore, is an equivalence relation. \n Hmm \u2026 I don\u2019t really like that, because I feel an obligation to prove that distinct really are disjoint and that every real number belongs to some . These statements, though not hard to prove, are hard enough to make this approach longer than the boring approach. \n Now let\u2019s look at the second example. There are two possibilities here. If you can\u2019t immediately see whether this is an equivalence relation, then just dive in and do the calculations. You\u2019ll end up writing something like this. \n (ii) Let . Then , which is a positive real number. If , then so is , since . And if and , then . Since is a non-zero real, . This shows that is an equivalence relation. \n If you\u2019re up on your complex numbers, you\u2019ll recognise that the relation is saying that and belong to the same line through the origin. That would allow you to give a partitions proof. \n (ii) Every non-zero complex number can be written uniquely in the form for some and some . It can also be written uniquely in the form with a non-zero real number (positive or negative) and . If and are written in the second form, then , which is real if and only if . Therefore, is an equivalence relation. \n Annoyingly, that again wasn\u2019t as quick as I\u2019d hoped, but it feels like a better explanation of why is an equivalence relation. \n I counselled against trying to second guess the examiner, but here it really does seem likely that (iii) is going to be false, given its similarity to (ii). Why? Well, if the relation in part (iii) is an equivalence relation, then the proof that it is an equivalence relation will presumably be very similar to the proof in part (ii), and it would be very odd for the examiner to ask you to do essentially the same thing twice. This isn\u2019t a completely conclusive argument, but it does suggest that we begin by looking for a counterexample. \n It\u2019s almost always a good idea with a question of this flavour to try the simplest thing you possibly can and work upwards. What\u2019s the simplest non-zero complex number? I\u2019d go for 1. So let\u2019s take 1 for . Now we need such that is real. The simplest example I can think of is 1. But that\u2019s a problem, because we\u2019re now not going to be able to find such that is an integer but is not. So a minimal requirement of our is that it should not equal . \n Let us therefore backtrack to where we chose . We want the simplest non-zero complex number such that is an integer and . How about 2? What does that tell us we need of ? We need to be an integer, but not to be an integer. That\u2019s easy enough. The simplest number that has that property is , and we\u2019re done. \n Here\u2019s what one would actually write out after those thoughts. \n (iii) Let , and . Then and , but . Therefore, is not transitive and so not an equivalence relation. \n One thing not to do is a technique that many people seem to like, which is to write out a proof, get to a step that doesn\u2019t seem to hold in general, and argue that that is a proof that the result fails. Here\u2019s what it might look like in this case. (Here I\u2019m giving a model non-answer, so to speak.) \n Hmm, I was just about to set out on that when I realized that I\u2019d done something a little foolish, which was to assume that if the relation failed to be an equivalence relation then the problem would lie with transitivity. But actually it\u2019s not even reflexive. So a better answer would have been this. \n (iii) Since , the relation is not reflexive and is therefore not an equivalence relation. \n However, let me try to give a model non-answer to the question \u201cIs the relation transitive?\u201d The right approach is to give a simple counterexample. The wrong approach is to write something like this. \n Let . If and , then . This shows that is an integer, but that doesn\u2019t necessarily imply that is an integer. Therefore, is not transitive. \n The trouble with what I\u2019ve just written is that the failure of a proof attempt doesn\u2019t prove that a result is false . It can, however, guide one to a proof. Here, for example, we see that we\u2019re looking for an example where is an integer but not . So it\u2019s pretty natural to think of taking and then , which isn\u2019t the same example as before but it\u2019s very similar. \n On to the fourth part. Here one can profitably use the information that this is a Section I question. This relation is obviously reflexive and symmetric, but there doesn\u2019t seem to be a trivial proof that it is transitive. Why not? Because if and , then , and that doesn\u2019t have to be a perfect square. Again, that\u2019s a useful thought to have, but it doesn\u2019t constitute an answer to the question . For all we know, and have to be very special kinds of perfect squares (because they are differences of squares), and perhaps if you add two of those very special squares together, you always get a perfect square. \n This is where the fact that we\u2019re doing a Tripos question comes in. If a statement like that were true, it would be a piece of serious number theory. This is a fraction of a Section I question, so we probably aren\u2019t expected to come up with a piece of serious number theory (unless it\u2019s very obvious bookwork, which this isn\u2019t). So the result is probably false. So we should try to find a counterexample. \n Our example can\u2019t be too trivial \u2014 in particular, we need all three of and to be distinct to have any hope of its being a counterexample. Help, that means we need some Pythagorean triples. I can only think of 3,4,5 and 5,12,13. Ah, but I see a potentially useful feature of those, which is that they both contain the number 5. Can that somehow be worked into an example? I probably want to set since will be common to my two assumptions. So how about and ? That gives me that , which is fine. And ? I might as well take 12 and see what happens? I get . Oops. OK, then. That\u2019s better, . Now, what is ? It is , which, thankfully, is not plus or minus a perfect square. \n Having thought of that, I can make it slightly neater by switching things round. Here\u2019s what I\u2019d write. \n (iv) Let , and . Then and , but , which is not a perfect square. Therefore, is not transitive and so not an equivalence relation. \n That part strikes me as medium hard for a Section I question \u2014 the kind of thing that many people would have spotted and many people would not have spotted. But maybe I\u2019m wrong about that: you\u2019re more or less forced to look for Pythagorean triples and then the first two such triples that we all know about from school do the job. \n Update: It has subsequently occurred to me that my answer to (iv) was needlessly complicated. I could have used the same Pythagorean triple twice. For instance, setting and we get , , but , which is not a perfect square."], "link": "http://gowers.wordpress.com/2012/05/11/a-look-at-a-few-tripos-questions-vi/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Here is the final analysis question from 2003. \n 12C. State carefully the formula for integration by parts for functions of a real variable. \n Let be infinitely differentiable. Prove that for all and for all , \n \n . \n By considering the function at , or otherwise, prove that the series \n \n converges to . \n \n What is implied by \u201cstate carefully\u201d? It probably means that more is required than just writing \n . \n What else can one put? The main thing is the conditions under which the formula is valid. So I think what is required is something like this. \n Let and be differentiable functions on the interval . Hmm \u2026 I have to confess that I\u2019m not sure what the precise conditions are, or rather what a standard set of precise conditions is. I could go for continuously differentiable since that would guarantee that all the integrals exist. A quick check \u2014 that\u2019s the formulation used by Wikipedia, so it\u2019s probably fairly standard. So here\u2019s what you probably need to say (unless you\u2019ve been given some more general statement in lectures, in which case obviously you should use that). \n Let and be continuously differentiable functions on the closed interval . Then \n . \n Now we have to prove Taylor\u2019s theorem with the integral form of the remainder. I remember that at least one version of Taylor\u2019s theorem always gives me trouble, but I think it\u2019s the one with the mean-value-theorem-ish remainder, and a quick look at this one suggests that all we have to do is integrate the remainder term by parts, which is an obvious enough thing to try even without the huge clue that we have just been told to state the formula for integration by parts. \n Obviously, integrating the remainder term by parts is done in order to produce a new term, and therefore to prove the statement by induction. So let\u2019s write down the statement first. Here is what I would actually write. \n When , the statement we are asked to prove is that . This is true by the fundamental theorem of calculus. \n Now to the rest of the answer. \n Let us now use integration by parts to rewrite the remainder term. Setting and , we have that both and are continuously differentiable. Also, . Therefore our integral is , which equals \n . \n The first term is equal to , which proves the inductive step. \n It\u2019s obvious what the last part is asking us to do: we must simply plug in . That requires us to differentiate infinitely many times. Fortunately, it\u2019s a function where the result is extremely nice. The first derivative is . Then we get , then , then . OK, the pattern is clear now, so let\u2019s do a proper proof by induction. \n I claim that . This is true when , since then the derivative is . If it is true for , then it is true for , since the derivative of is . [It looks like a bit of a cheat to write that, since I haven't shown my working -- things like noticing that two minus signs cancelled out -- but it's hard to see how I could have reached this answer by accident, so the examiner couldn't reasonably remove marks for that. Maybe it would have been better to write slightly more.] \n The question really is holding our hand here. Let\u2019s apply Taylor\u2019s theorem with . The one thing not to do is just calculate the infinite series. The whole point of the question is to prove that you understand that estimating the remainder term is necessary if you want a rigorous proof. Let\u2019s underline that by doing it first. \n We shall prove this result by applying Taylor\u2019s theorem. First let us obtain a bound for the remainder term, which is \n . \n How are we going to estimate that? Well, that looks a lot smaller than and the s cancel out. How can we make that thought precise? Well, . OK, here goes. \n Since for all in the range , the integrand is at most , which implies that the term is at most , which tends to zero. Therefore, by Taylor\u2019s theorem, \n \n By our earlier calculation, , and , so \n \n But , so we are done. \n \n Not much to say about that question, since it was an easy one. But as I\u2019ve already said, if a question is easy, then the examiner wants you to do it properly , and if you don\u2019t then you may well lose an alpha. In this case, doing it properly means stating some conditions on functions that appear in the formula for integration by parts, and more importantly it means bothering to prove that the remainder term tends to zero when you apply Taylors theorem. People who didn\u2019t do the latter would not have got alphas. \n Is there a reasonable \u201cor otherwise\u201d option? That\u2019s a difficult one. If you\u2019re allowed to differentiate a power series term by term, then you can differentiate to get , which is a geometric series (when as it will be here) that sums to . So the original function is, up to a constant, . Looking at what happens when we see that the constant is 0, and we can now plug in . \n But was it reasonable to assume that a power series can be differentiated term by term inside its radius of convergence? It's certainly a different part of the course. My guess is that this proof \u2014 written out a bit less sketchily than I have written it \u2014 would have been accepted even if that result had been merely stated and not itself proved, simply because the examiner would have given some credit for independent thought. But I can't say that with total certainty, because it is a fairly substantial result to assume, whereas the intended approach doesn't ask you to assume anything more than you've just proved."], "link": "http://gowers.wordpress.com/2012/05/08/a-look-at-a-few-tripos-questions-v/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["A natural way that one might hope to bring about a genuine change to the current subscription model where libraries pay through the nose for journals is that (i) we all put our papers on the arXiv and (ii) the libraries conclude, correctly, that the benefits from their very expensive subscriptions do not justify the costs. Bundling across subjects makes this a lot more difficult of course, but it seems that some institutions in Germany do not subscribe to the Freedom Collection (see previous post for a definition), which makes it easier. And now there is an example. The Technical University of Munich mathematics department has put out an announcement that it will cancel all its Elsevier subscriptions by 2013. \n Please, if you are considering submitting a paper to an Elsevier journal without putting it on the arXiv, think of the faculty members of TU Munich who will not be able to get access to your papers (or at least not conveniently), and change your mind. If you do, it will also make it easier for other departments and libraries to make similar decisions."], "link": "http://gowers.wordpress.com/2012/05/04/the-mathematics-department-at-tu-munich-cancels-its-subscriptions-to-elsevier-journals/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "http://www.tum.de/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Elsevier has recently put out a new statement giving details of some changes it has made. In their own words, \n In February, we informed you of a series of important changes that we are making to how the Elsevier mathematics program will be run. In this letter, we would like to update you on where we currently stand, and inform you of some new initiatives we have undertaken based upon the feedback we have received from the community. \n I have known for some time that they were going to make an announcement of this kind, and that it would involve something called \u201cmore flexible subject collections\u201d. During that time I have become clearer in my mind what it is that I don\u2019t like about bundling. So before Elsevier\u2019s announcement, I had in mind some tests that I would apply, to see whether having these new collections would mitigate the problems with bundling. (Spoiler: they don\u2019t.) \n \n Imagine you\u2019re in charge of a university library and you have a limited budget. How do you spend that budget? In a system without any form of bundling, if you had an extra chunk of money, you would look around for the best additional utility you could buy with that money \u2014 where utility would be something like the number of new journal pages you could buy, multiplied by the average benefit that each journal page contributes to your university. In addition, the following would apply. \n (i) If a journal\u2019s quality goes down or its price goes up, then you have the option of subscribing to a different one that is better value for money. \n (ii) If your budget goes down, you can decide which journals you value least (weighted by price) and cancel subscriptions to those. \n (iii) If somebody wants to set up a new journal that\u2019s better than existing journals in some way (for example it might be cheaper, or of a higher standard, or both), then they have a chance of persuading you to subscribe to their new journal, which you can pay for by cancelling subscriptions to less good journals. \n Compare that with the system we have at the moment. What we actually have is this. \n (i) If a (bundled) journal\u2019s quality goes down, you can\u2019t cancel your subscription to it. Your only option is to cancel the entire bundling agreement, which is a very drastic step to take. \n (ii) If your budget goes down, then your options are to cancel a bundling agreement or to cancel subscriptions to journals that are not part of bundling agreements. The latter is much easier, so smaller independent journals are far more vulnerable. \n (iii) If somebody wants to set up a new journal that\u2019s better in some way than an existing journal, they will have great difficulty getting libraries to subscribe to it, because they cannot save money by cancelling subscriptions to bundled journals. \n Somebody from Elsevier \u2014 I think it was Alicia Wise \u2014 defended bundling on the grounds that it protects more obscure journals that might otherwise struggle to find enough subscriptions. It\u2019s possible that that\u2019s the case, but it also protects bad journals, and that is a far stronger effect. If Elsevier were truly interested in protecting good journals in obscure subjects, then it could do what some academic publishers do with books, and use profits from some journals to subsidize others. \n To summarize, bundling makes proper competition between journals impossible. That is what I object to about it, and I judge any move made by Elsevier by whether it makes things better in this respect. With that in mind, let\u2019s look at some more of the Elsevier letter. \n Pricing \n We already indicated that our target is for all of our core mathematics titles to be priced at or below US$11 per article (equivalent to 50-60 cents per normal typeset page), placing us below most University presses, some societies and all other commercial competitors. That will lead to a number of our titles seeing further and significant price reductions from their next volumes. \n Further to this, and in response to feedback from the community for more flexibility around the packages and collections that we offer to libraries, we will take the added step of defining a smaller subject collection (around 15-20 journals) with our key core mathematics titles. The definitive list of journals will be determined shortly but will in any case include journals like Advances in Mathematics, Journal of Algebra, Journal of Number Theory, Journal of Functional Analysis, Journal of Combinatorial Theory A and B, and European Journal of Combinatorics, available with the discount levels offered on our subject collections. \n To clarify what this means, let me briefly explain what the current system is, as I understand it. Elsevier has something called its \u201cFreedom Collection\u201d (I cannot help being reminded of the famous Freedom Fries that were popular in the US soon after the invasion of Iraq), which works roughly as follows: a library buys a number of journals at more or less the list prices \u2014 the ones it really wants \u2014 and then Elsevier offers all the rest, at a very heavy discount. The result of this is that the average price the library pays for its journals is much smaller than the list price. A typical sort of amount for a major library to pay to Elsevier in total is a bit over \u00a31,000,000 per year, of which almost all goes on the \u201ccore collection\u201d and only a tiny fraction on the rest. This page has a more detailed description of how it works, and includes the information that if you terminate your bundling contract, then you no longer have access to back issues of the journals outside the core collection. \n In other words, everything is set up to make it as difficult as possible to leave a bundling arrangement once you\u2019ve agreed to it. \n In the light of all this, what difference does a reduction in list prices make? Very little indeed. It means that if you take the plunge and stop subscribing to the bundle, then it will be cheaper now than it was before. But the list prices are still high enough to make that an unpalatable option, especially given all those back issues you will lose access to. So reducing list prices (which remain very high, even if they are lower than those of many other publishers) does virtually nothing to address the drawbacks of bundling. \n Ah, but what about the \u201cresponse to feedback from the community for more flexibility around the packages and collections that we offer to libraries\u201d? That sounds much more promising, since it adds a new option. \n At first it looks ideal: instead of being under considerable pressure to buy a bundle with lots of second-rate mathematics journals, one can now go for the smaller subject collection with the \u201ckey core mathematics titles\u201d. But who will want to do this? A library such as Cambridge\u2019s or UCL\u2019s currently subscribes to the Freedom Collection. Can it save money by replacing the mathematics part of that collection by the new smaller subject collection? No. I think there are smaller subject collections in other subjects as well. Can a library save money by subscribing to all those instead of to the Freedom Collection? No. (That last answer comes from a conversation that Urs Hartl recently had with Laura Hassink, Elsevier\u2019s VP for strategy and journal services and one of the two signatories to Elsevier\u2019s update \u2014 the other being David Clark. Later I\u2019ll mention some other things that Urs found out.) \n There is one kind of institution that could benefit from the new subject collections, and that is mathematics institutes, since they do not subscribe to journals across all the sciences. To that small extent, this move by Elsevier is welcome, but as an answer to the problems of bundling it is woefully inadequate. \n Open Archives \n In February, we made the archives of 14 core mathematics journals open, from four years after publication, back to 1995, the year when we started publishing digitally. We made more scholarly mathematics content freely available than has ever occurred before. We have now gone further and expanded the open archives back to 1995 for 43 journals in mathematics and related areas. For a full listing of Elsevier journals with an Open Archive, please see our information page . \n This is of course good news, even if one might wish for it to go further still. In particular, it would be nice to have journals available from earlier than 1995, since in mathematics there are plenty of papers we want to consult that go back much further than that. \n Why don\u2019t they go further back? It\u2019s true that there is a natural boundary in 1995, since, as they say, they started publishing digitally only then. However, they have digitized their archive from much earlier than that, and libraries can subscribe to this earlier part of the archive. So why not make that free? This is what Urs Hartl has to say about Laura Hassink\u2019s response to that question: \u201cShe gave two reasons preventing this. First these issues have been bought by some institutions quite recently and it would be unfair to them to open the issues now. I said, I would know a solution for this and she herself guessed that one could solve the problem by refunding the recent buyers. As a response she mentioned the second reason, that there are 1900 issues from societies who had transferred the rights on these issues to Elsevier and want to earn royalties in return.\u201d \n I\u2019ve got a partial solution to the second problem: at least make the journal issues available that are not affected by that problem. So a third excuse is needed \u2026 \n The rest of Elsevier\u2019s letter has less to do with the bundling and pricing issues so I won\u2019t say much here. They have increased the length of time that some of their reviewers receive free access to their journals \u2014 a very minor concession since most of their reviewers will be at universities that subscribe to Science Direct, though it may be useful to some people. (They mention retired mathematicians, for example.) There is a paragraph about access for the developing world, which is welcome and important, but it\u2019s not clear that anything substantial has changed. Finally there are three paragraphs of fluff entitled \u201cSupport to the mathematics community\u201d. \n I\u2019ll finish by mentioning two particularly notable parts of Urs Hartl\u2019s conversation with Laura Hassink. I hope the first in particular will demonstrate why so many people care about the current state of academic publishing and want big changes. \n At the University of M\u00fcnster, which is Urs Hartl\u2019s university, Elsevier increased the price of its Freedom Collection (which is negotiated individually with universities) by 20% per year between 2011 and 2013. Here is the explanation from Urs: \u201cMs Hassink explained to me that in the contract with Elsevier a \u2018committed spend\u2019 of the University of Muenster is fixed. Since the institutes individually canceled titles before 2011 the \u2018committed spend\u2019 was not reached and this caused Elsevier to make up for it in the price increase for the Freedom Collection. I replied that this coincides with what various librarians and colleagues had told me. Namely, that no matter what they tried to do, the total amount they pay for Elsevier did not go down. Ms Hassink agreed that in this particular case with the Freedom Collection in Muenster the price increase had this intention.\u201d \n The second point is to do with the secrecy that surrounds Elsevier\u2019s negotiations with universities. One of the reasons that Laura Hassink gave to Urs about this was that it would be impossible to offer generous discounts to developing countries if all contracts were disclosed. (I think the argument here is that if you admit to giving a discount to one university then they\u2019re all going to want one.) Urs countered that \u201caccording to Florian Breuer\u2019s computations the University of Stellenbosch, South-Africa pays roughly the same than the University of Muenster, Germany.\u201d See also this this comment about the situation in India. \n Interestingly, when I tackled David Clark about this issue (we met a few weeks ago) the reason he gave for non-disclosure was completely different. He claimed that there was a danger that disclosing the agreements would be in breach of competition laws, since it would allow the big publishers to coordinate their prices. If that explanation is correct, then the law is an ass. After all, competition laws are there to protect consumers against monopoly power, whereas (i) the lack of transparency of Elsevier hugely enhances their power and (ii) Elsevier effectively is a monopoly, since if they charge more than you like, you can\u2019t get the same journals from a different source. \n What about another big concession by Elsevier \u2014 the dropping of their support for the Research Works Act? Let me remind you of something they said when they announced this. \n While we continue to oppose government mandates in this area, Elsevier is withdrawing support for the Research Work Act itself. We hope this will address some of the concerns expressed and help create a less heated and more productive climate for our ongoing discussions with research funders. \n They are as good as their word. Horizon 2020 is the European Union\u2019s \u201cFramework Programme for Research and Innovation\u201d. If you scroll down to page 13 of a long document they have put out , you will find the following sentence at the top of the page: \u201cFurther steps will be taken towards Open Access, to ensure that research results are available to those who need them.\u201d At the bottom of the page, it says, \u201cThese measures will also focus on communicating the outcomes of research to policy makers, companies, innovators and other researchers, including by promoting Open Access.\u201d I am told by a source that I trust that Elsevier is lobbying very hard to get all mention of open access removed from the Horizon 2020 documents. They have learnt from their mistakes over the Research Works Act \u2014 now they do their lobbying behind the scenes. Or at least so I have been led to believe: if any Elsevier representative would like to deny it, then please feel free to do so in the comments below and I will add a link here to the denial. Edit: Alicia Wise has provided a denial in this comment below . \n I should say before finishing that this post is my personal reaction to the Elsevier statement: I don\u2019t speak for anyone else, and I don\u2019t guarantee to have got all my facts right. I welcome corrections and alternative views in the comments below. \n At the time of writing, the number of signatories to the boycott at the Cost of Knowledge page has just passed 11,000. The number of mathematicians is creeping up to 2,000. It would be nice to pass that milestone soon, and keep on climbing. Nothing that Elsevier has said gives us any reason to end the boycott. They are behaving much as one would expect: offering minimal concessions that will look as good as possible while keeping their profits intact. I realize that asking them to deal with the objections to bundling and exposing their journals to genuine competition is making a demand they are most unlikely to accede to, since their huge profits are based on stifling this competition. So instead, we must press on with the more positive step of developing alternative models, something I shall report on in the near future. \n Despite Elsevier\u2019s strong dislike for open-access mandates, there does appear to be significant movement in precisely that direction. Three interesting recent news stories are an announcement by the Wellcome Trust that it would insist on open access for the research it funds, a statement by Harvard University Library that the current system is unsustainable, and an announcement by the British Government that it has plans to make all taxpayer-funded research available online. It is difficult to move to new systems, but external pressure of this kind will surely help. \n ( Update: The International Association of Scientific, Technical and Medical Publishers (STM) has put out a press release responding to the British government announcement just mentioned. It\u2019s worth a read. It is clear that the most important word in the press release is \u201csustainable\u201d and that it means \u201ccapable of sustaining the profits of the major publishers\u201d.)"], "link": "http://gowers.wordpress.com/2012/05/02/elseviers-recent-update-to-its-letter-to-the-mathematical-community/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 2, "http://www.co.uk/": 2, "http://ec.europa.eu/": 1, "http://www.sciverse.com/": 1, "http://www.ac.uk/": 1, "http://t.co/": 1, "http://en.wikipedia.org/": 1, "http://www.elsevier.com/": 2, "http://feeds.wordpress.com/": 1, "http://thecostofknowledge.com/": 1, "http://www.uni-muenster.de/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["This post belongs to a series that began here . Next up is a question about integration. \n 11B. Let be continuous. Define the integral . (You are not asked to prove existence.) \n Suppose that are real numbers such that for all . Stating clearly any properties of the integral that you require, show that \n . \n The function is continuous and non-negative. Show that \n . \n Now let be continuous on . By suitable choice of show that \n , \n and by making an appropriate change of variable, or otherwise, show that \n . \n \n The first part of this question is something I recommend practising until you can write it out very fast, since it often comes up. (I\u2019ve just checked and it came up in 2010, though in the slightly more complicated case of a function defined on , and not in 2011, so the probability of its coming up this year is reasonably high.) \n Are there any decisions to make about what to say and what not to say? In particular, does the fact that the function is assumed to be continuous, as opposed to merely integrable, make a difference? Not really \u2014 it just means that we can assume that the function is integrable and concentrate on saying what the integral is. But the two are so closely related that that isn\u2019t much of a saving, if any. So we\u2019d better dive in and write out a chunk of bookwork. \n I was just about to write something when I realized that there is in fact a decision to make. For example, would it be enough to write this? \n The integral is the supremum of over all dissections of the interval , where is the lower sum associated with and the dissection . \n Or should we say what a dissection is and what a lower sum is? (Note that we can get away with not mentioning upper sums, since for continuous functions this really does define the Riemann integral. If we were being asked to define the Riemann integral more generally, we would need to talk about this sup equalling the inf of all upper sums.) This is a bit of a grey area. On the one hand, dissections and lower sums come slightly before the definition of the integral. On the other hand, they aren\u2019t used for much except the definition of the integral, so one can\u2019t really think of them as an earlier part of the course. My guess is that if you were to write a super-brief answer like that, the examiner would be irritated enough to remove a mark or two, and enough irritations like that can end up costing you an alpha. \n I would therefore go for an answer more like this. \n A dissection of an interval is a sequence . If is a dissection of and , then the lower sum is the quantity \n . \n If is continuous, then the integral is the supremum of over all dissections of . \n That doesn\u2019t take too long to write, especially if you\u2019ve practised it. It may be that the notation and terminology you were given in lectures was slightly different from what I\u2019ve just written down. In that case, use what you were given. \n For the next part of the question, the annoying process of reading the mind of the examiner is quite hard. What properties of the integral are we allowed to assume? It\u2019s confusing, because if we argue directly from the definition, then we don\u2019t seem to need any additional properties, whereas if we assume a property such as that if for every , then , then the question becomes rather easy. Or does it? We also need to know that the integral of a constant function is the constant times . \n The wording of the question is clearly suggesting that we don\u2019t have to argue directly from the definition, so it is probably inviting us to use properties of the integral that are typically proved immediately after the definition. Perhaps that year the lecturer proved that the integral of a non-negative function is non-negative, and that the integral of is the integral of plus the integral of . In that case, the following answer would have been appropriate. \n I shall use the fact that the integral of a non-negative function is non-negative, and that the integral of the sum of two continuous functions is the sum of their integrals. I shall also use the fact that the integral of the constant function on the interval is . \n From these facts, we find that , and therefore that \n . \n Similarly, , and therefore \n . \n This proves that . \n I am not completely confident that that is the answer that the examiner was hoping for, but I think it would be difficult to justify removing marks for making those assumptions, given how the question was worded. \n For the next part it is similarly not quite clear what one is allowed to assume. But since we can get away with the same assumptions that we\u2019ve already made, that seems a sensible choice. So I\u2019d write this. \n Since is non-negative and for every , we have the inequalities for every . The result follows by integrating from to . \n Actually, I used there the result that if one function is at most another then the integral of the first is at most the integral of the second. But I feel OK about that, since I basically proved it when doing the previous part (or at least demonstrated that I knew how to prove it). \n In the next part, we have to make \u201ca suitable choice of \u201c. Sometimes such questions are frightening, but not here. How are we going to choose a function that will turn the expression in front of us into something to which we can potentially apply the result we have just established? Well, we\u2019re looking at the integral of a product of with something, so there\u2019s not much choice but to choose to be that thing. Accordingly, we write \n Let for each . \n Now we plug what we\u2019ve got into the previous result, writing this. \n By what we have just proved, \n \n lies between \n \n and \n \n where and are the infimum and supremum of on the interval . \n There I used the time-honoured technique of saying, \u201cI want the minimum and maximum values of in that interval, but this is analysis so to be on the safe side I\u2019ll call them the infimum and supremum.\u201d As it happens, in this case it would have been fine to say minimum and maximum since we have a continuous function defined on a closed bounded interval. The slightly wordy \u201clies between\u201d above is there merely because I wasn\u2019t confident of displaying the full three-way inequality on one line \u2014 it\u2019s not what I would have written in an exam. \n What should we do next? Probably a good move would be to calculate the integral, since it\u2019s clearly not a difficult one and the answer we are aiming for doesn\u2019t involve an integral. So we write \n \n It\u2019s now fairly easy to see why the result is true: since is continuous, both and will be close to once is large (and hence is small. At the same time, is tending to 1. I think we can be pretty brief here. \n Since is continuous, both and tend to as . Since , it follows that the upper and lower bounds obtained above both converge to , which proves the result. \n We now come to the last part of the question. What is the change of variable that the examiner has in mind? It\u2019s quite strange because the function we\u2019re integrating is precisely the same. Still, we could try making a substitution that would change the range of integration from to . The obvious one is to set . I\u2019ll try that. \n Let . Then \n \n I don\u2019t see how that is supposed to help. I\u2019ll come back to that question, but since I don\u2019t really like these look-for-the-magic-key style answers, I think what I\u2019d actually do in an exam is consider what the \u201cor otherwise\u201d possibilities are. For some reason, there are many questions where the method suggested is so overwhelmingly superior to anything else that the words \u201cor otherwise\u201d are almost a joke. It\u2019s as though they\u2019re saying, \u201cIf you don\u2019t see the way you\u2019re supposed to do this, then you\u2019re going to be wasting the next half an hour and a lot of paper.\u201d But this question is not like that. Given what we\u2019ve already shown, it\u2019s enough if we can show that the bit we\u2019ve added on to the integral tends to zero. That is, we want to prove that \n \n How does one show that an integral is small? One way is to calculate it exactly, obtain a formula, and prove that the resulting expression is small. That doesn\u2019t work here, because is an arbitrary continuous function. \n Another method is the very crude one of finding bounds for the integrand and multiplying them by the length of the interval. It\u2019s surprising how often that works. (A slightly more complicated method that also works well is to divide the range of integration up into two parts, one quite wide where the function is very small, and one very narrow where the function may be rather bigger.) \n What bounds do we know here? Well, we don\u2019t know any explicit bound on , but we do know that it is a continuous function defined on a closed bounded interval, so at least it is bounded. What about . That is a positive decreasing function, so on the interval in question its maximum value is . Ah, that looks extremely small when is large. And now we have the ingredients for a very short and completely rigorous proof, which goes like this. \n Since is continuous on the closed bounded interval , there exists such that for every . Also, the function is decreasing and positive, so on the interval it takes a maximum value of . Hence, \n \n for every . Since the right-hand side tends to 0, so does the left-hand side. The result follows by adding to the previous integral. \n \n Further remarks. \n That argument was easy enough that I\u2019m tempted not to look for the magic substitution that solves the problem. But I\u2019m curious to know what was intended, so I\u2019ll press on. \n Looking at where things went wrong before, the difficulty was that the range of integration didn\u2019t relate in the right way to the constant inside the exponential function. So let\u2019s try another important trick in mathematics: make your guess less precise until you know more about what it needs to do. \n In this case, we don\u2019t have to substitute . We could simply substitute and decide later what suits us best. If we do that, then we find that \n \n To relate this to the first integral, we\u2019d like to be able to write as and as . That tells us that should equal , and therefore that should be . In other words, we substitute . Doing that, we get the integral \n . \n Now everything is wonderful apart from one thing: the fact that we\u2019ve got instead of just . Can\u2019t we just define ? No, because then depends on , which isn\u2019t allowed in the first result. \n I\u2019m forced to admit defeat here: I cannot see what the examiner had in mind. \n By the way, if you happen to be the person who set that question, let me make clear that although I have been somewhat critical of the way it was worded, I am also very much aware of how difficult it is to word questions in a way that makes it clear what is intended. I have often written what seemed to me like crystal clear questions that were given some interpretation I didn\u2019t expect by a significant percentage of people. \n Update. I have just been told by someone who has looked at the examiner\u2019s model answer for this question what the examiner\u2019s intended solution was. It was precisely the solution I attempted just now where I substituted and ended up choosing . There\u2019s an important moral here: Tripos questions can sometimes contain mistakes . It\u2019s very unfortunate when they do, but it\u2019s also very hard to avoid. It\u2019s a difficult judgment to make, but if you really really really are confident that what you have written is correct, despite the fact that it disagrees with what the question claims, then you should probably move quickly on to a new question, perhaps putting a little note saying \u201cquestion wrong?\u201d. If it is early on in the exam, you should also inform the invigilator, which will result in the examiner being contacted urgently. In this case it\u2019s an even more difficult judgment to make, since there is the \u201cor otherwise\u201d option, so the question isn\u2019t so much wrong as accidentally misleading. \n To finish off, here\u2019s an example of a fairly common style of misinterpretation, but this time the fault is with the candidate rather than the examiner. Consider the following question. \n Let be a continuous function defined on the interval . Show that is bounded. \n It is not unknown for people to give answers of the following kind to that kind of question. \n Let be the function . Then is continuous. Since for every , it is bounded, as required. \n If you think you might feel that sort of temptation, the following rule of thumb should help: if the question wants you to look at just one example of your choice, it will say so very explicitly; if it doesn\u2019t say so explicitly, then it is asking you to look at a general object. \n The words \u201cany\u201d and \u201carbitrary\u201d make this confusion more likely. Suppose, for example, that the question says, \u201cLet be an arbitrary continuous function defined on the interval .\u201d Is that an invitation to choose your own function? Well, it could just about be interpreted that way, but if there is the slightest room for doubt, then don\u2019t interpret it that way . Similar remarks apply to \u201cLet be any continuous function defined on the interval .\u201d In each case you are being asked to prove a result for all continuous functions and not just one."], "link": "http://gowers.wordpress.com/2012/05/02/a-look-at-a-few-tripos-questions-iv/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["Here\u2019s another one. \n 10F. State without proof the Integral Comparison Test for the convergence of a series of non-negative terms. \n Determine for which positive real numbers the series converges. \n In each of the following cases determine whether the series is convergent or divergent: \n (i) , \n (ii) , \n (iii) . \n \n I don\u2019t know exactly what was referred to in the course as the integral comparison test, but since all the sequences being summed are monotone decreasing I\u2019ll go for a neat statement that assumes that (and coincides with what Wikipedia refers to as the integral test). \n Let be a decreasing real-valued function defined on the interval . Then the series converges if and only if the integral is finite. \n Now the second part, which is pretty standard, and works very quickly if you use the integral test. I\u2019m not sure there\u2019s much more to say, so here\u2019s the answer. \n For every the function is decreasing. The value of is when , and otherwise. When , this equals , which tends to infinity. [We could have argued instead by comparing with the case.] When , it equals , which tends to . Therefore, the series converges if and only if . \n We now have our first dilemma. The series we\u2019re being asked to investigate go from 3 to instead of from 1 to . How much should we say about this? Should we give a more complicated statement of the integral test that allows series that start at ? \n Since this is a minor matter \u2014 after all, the convergence of a series is not affected by the first few terms \u2014 I suggest treating it in a minor way. That doesn\u2019t mean ignoring it completely, but it means just indicating in a minimal way to the examiner that you are aware of the difficulty and know how to deal with it. If you do that, then the examiner cannot reasonably remove any marks. You might, for example, write this. \u201cSince the convergence of a series is not affected by its first few terms, it is clearly enough in the examples below to find a function defined on the interval instead.\u201d \n Note that I haven\u2019t even said exactly what I mean (which would have taken longer), but I\u2019ve written something that I couldn\u2019t have written if I didn\u2019t know what was going on. I stress that that kind of short cut is appropriate only if what you are discussing is a side issue and not the main point of the question. \n Right, let\u2019s try to deal with these series. I myself prefer grouping terms to using the integral test \u2014 I feel that it tells me why the series converges or diverges rather than zapping it with a magic formula \u2014 but the question clearly intends me to use the integral test (even if it doesn\u2019t actually insist on it), so I\u2019d better do that unless it gets really horrible. \n Hmm, can I integrate ? Yes, because it is of the form : I take and . I just need something that differentiates to , and will do very nicely. So the function that differentiates to is . \n I\u2019ll confess that I know (from the grouping terms approach) that the series tends to infinity like , and that that helped me get slightly more quickly to the observation in the previous paragraph. Anyhow, time to write something. \n (i) Let . Then \n . \n Since this tends to infinity, the series diverges. \n What about the next part? Does anything differentiate to the function ? It looks horrible. \n But this is the Cambridge Tripos, and it\u2019s a pure maths question. So it probably isn\u2019t going to involve a horrible calculation. This is a very important clue: it means that it is a good idea to see whether we can find something that works easily. \n A rather natural idea is to use the chain rule, since that worked last time. Aha! We\u2019ve just seen that differentiates to , so we can use the chain rule here. We just need something that differentiates to . Well, does the job, so probably is going to do the job. Quick check: yes, that does indeed differentiate to what we want. \n (ii) Let . Then . This converges to . Therefore, the series converges. \n Now for the third part. Oh dear, that\u2019s a disgusting looking function. How on earth are we supposed to integrate that? \n Here again, don\u2019t forget that you are doing a pure maths Tripos question . It is incredibly unlikely that your examiner will expect you to do a horrible integral. (This is not just for your sake, but also your examiner\u2019s \u2014 marking a large pile of questions that involve a lot of calculation is only marginally preferable to having your fingernails extracted one by one.) So let us use that information. It probably means that the intended method of solving this part is not integrating a function that almost certainly doesn\u2019t have an integral in closed form. \n Incidentally, some people might have been tempted to apply that argument to the function , but I think that in the context \u2014 we were clearly going to apply the integral test at least once, managed to do so for the first part, and observed there that the derivative of is \u2014 it looked a lot less nasty than it would do if you were just presented it out of the blue. \n Going back to this part, if you\u2019ve decided that actually integrating isn\u2019t possible, what else are your options for solving this problem? Well, another great law of mathematics is use what you already know . This law applies with particular force in Tripos questions, where \u201cwhat you already know\u201d often translates to \u201cwhat you have done earlier in the question\u201d. Is there anything from earlier in the question that we might be able to use? \n Well, all three functions we\u2019re supposed to look at are small modifications of . In the first part, the \u201csmall\u201d modification was in fact the zero modification, and the series diverged. In the second part, we made it very slightly smaller, by dividing by , and it converged. So let\u2019s try to think of how the third function relates to . And that is easy to see: it divides it by . So the question we\u2019re facing is whether dividing by does enough to tip the series over from being one that diverges to being one that converges. \n Obviously if we want to decide about that, we would like some idea of what does as . \n A useful tip for many such questions: if you can\u2019t immediately see the answer, take logs. You need a bit of common sense about when to apply this tip, but here, since we\u2019re raising something to a power, taking logs has a good chance of simplifying matters. \n The log of is . What does that do? It tends to 0. (That\u2019s a good example of something that\u2019s more basic than what you\u2019re being asked to prove, and therefore legitimate to assume.) Therefore, tends to 1. But in that case it\u2019s very unlikely that dividing by makes any difference. \n How can we prove quickly that it makes no difference? With convergence and divergence there are two principles that can be used to produce quick proofs \u2014 in conjunction with the comparison test. They are \n (1) if you change finitely many terms in a series, it makes no difference to whether that series converges \n and \n (2) if you multiply the terms of a series by a positive constant, it makes no difference to whether that series converges. \n Armed with those two principles, we have the following very short argument (which is obviously a special case of a more general fact). \n (iii) Since as , . Therefore, for all sufficiently large . It follows that for all sufficiently large . Therefore, by part (i) and the comparison test, diverges. \n Note that I could have argued that the function is bounded, and even tried to calculate a bound. But that would have involved thought and time and gained me an extra zero marks. Better to write something crude and get on to a new question. \n A final remark is that if you pick out the bits of what I wrote above that were supposed to form part of the final answer, you\u2019ll end up with something that can be written out in about three minutes (though obviously you\u2019ll also have to spend some time thinking of the answers). That is often the case with Tripos questions: they are designed to be easy if you\u2019re on top of the material, and quite difficult otherwise. As with the last question, I think this one was quite easy as Tripos questions go, but I can see that the factor might have led a number of people astray. \n \n I\u2019d like to end this post by explaining how I\u2019d tackle a convergence question of another kind that sometimes comes up. What can we say about the convergence of the series \n ? \n I\u2019m not concerned here about the answer, but about how to write a very quick and fully rigorous answer. The answer, incidentally, is obviously that it converges, since the terms decrease roughly like . However, the th term is bigger than , so it\u2019s oversimplifying a bit too much to say, \u201cBy comparison with , the series converges,\u201d even if that is more or less the right reason. \n Here\u2019s my cheating method. You just write down some very easy inequalities that hold when is sufficiently large. For example, if , then and , so . Also, if , then is certainly at least , so . \n Here\u2019s what I\u2019d actually write for the question. It\u2019s quick and ugly and that\u2019s what I like about it. \n If , then and . It follows that the th term is at most . Therefore, by the comparison test and the fact that the first nine terms do not affect convergence, the series converges. \n If your version of the comparison test is a for- -sufficiently-large version, then you don\u2019t even have to say, \u201cand the fact that the first nine terms do not affect convergence.\u201d I\u2019m not sure what your lecturer went for. \n It\u2019s tempting to waste time worrying about things like whether you could get just as good an inequality for smaller , or whether you could improve the constant 12 when . But there\u2019s no need. You just care about whether the series converges. \n If you were very short of time, you could take a gamble and just guess that the number one million is going to work, which, unless the numbers in the question are huge, it will. A safe option would be this. \n If , then the th term is at most . Therefore, by the comparison test, the series converges. \n A slightly riskier option is this. \n For each , the th term is at most . Therefore, by the comparison test, the series converges. \n In general, I don\u2019t recommend this sort of joke approach if you\u2019ve got a few seconds to spare, because if you make the inequality true by miles, you haven\u2019t demonstrated to the examiner all that convincingly that you can back up the statement you are making. With the earlier version, where I said that if then , the examiner can see where the number 10 came from (it was to make equal to 100), so it\u2019s clear that I haven\u2019t just cheated and written something down that was bound to be true and not checked it."], "link": "http://gowers.wordpress.com/2012/04/30/a-look-at-a-few-tripos-questions-iii/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}, {"content": ["This is the second in a series of posts that started here . In the first post I explained what I\u2019m up to. Now let me just continue with some more questions. I\u2019m now on to the harder Section II questions. Here\u2019s the first one I want to look at. Even though it makes the posts shortish, I think I\u2019m going to stick to one long question per post. \n \n 9F. Prove the Axiom of Archimedes. \n Let be a real number in and let be positive integers. Show that the limit \n \n exists, and that its value depends on whether is rational or irrational. \n [ You may assume standard properties of the cosine function provided they are clearly stated. ] \n \n In an exam you have a choice of questions, so with a question like this I\u2019d recommend beginning by spending a minute or two trying to guess whether you\u2019ll be able to do it (and do it reasonably quickly). The first part is bookwork (the Cambridge term for reproducing things that are in your notes), so either you know it or you don\u2019t. What\u2019s less obvious is how you\u2019re going to get on with the second part. \n It\u2019s fairly clear, however, that a good way to proceed is to understand the inner limit first and only then worry about what happens with the outer limit. So what can we say about ? Well, the sequence consists of bigger and bigger powers of some number \u2014 the number , to be precise. That number lies between 0 and 1, so its successive powers will tend to 0 unless it is 1. How can it be 1? Only if is a multiple of , which means that must be a multiple of 2. \n Aha, now we see where rationality and irrationality come in: if is irrational, then can\u2019t be a multiple of 2, so the limit is 0. OK, we\u2019re almost certainly going to be able to finish this off, so let\u2019s get started. \n Prove the Axiom of Archimedes. \n There are various versions of this \u2014 I\u2019m not sure which was given in the course. However, from the thoughts we\u2019ve just had it seems very likely that the intended version is the one that says that as . \n A general point here is that there is almost always a connection between the early theoretical part of a question like this and the more specific what-happens-in-this-example part. So if you can\u2019t relate the axiom of Archimedes to the double limit, you should be worried. In general, this convention of Tripos questions is your friend \u2014 it\u2019s another thing that makes it easier to look for what the examiner wants you to do. \n The thing to remember about the statement that is that it is not trivial. It is trivial if you think of the reals as infinite decimals, but to prove that every real number has a decimal expansion you need \u2026 the axiom of Archimedes. We\u2019re thinking of real numbers as elements of complete ordered fields, so we want to prove that using the axioms for a complete ordered field. \n The obvious axiom to use here is the monotone sequences axiom, since we have a monotone sequence to look at. So here goes. \n Since is decreasing and bounded below, it converges to a limit. Since all the terms of the sequence are positive, the limit cannot be negative. Suppose it is a positive number . Then we can find such that (by the definition of convergence). But then for every , which contradicts the fact that is the limit of the sequence. Therefore, . \n Rest of question. \n We\u2019ve done in our heads part of the rest of the question, so let\u2019s write that down. \n If is irrational, then for every the number is positive and less than 1. \n Now there\u2019s a slight dilemma. It feels obvious that the powers of this number tend to 0, but are we allowed to assume that? Ordinarily I would say yes, but since the first part of the question asks us to prove the axiom of Archimedes and the rest of the question is (it turns out) very easy, it is almost certainly the examiner\u2019s intention that we include a proof, using the axiom of Archimedes, that whenever . \n Here\u2019s how I\u2019d prove that. I find it more convenient to prove the equivalent result that whenever . And here\u2019s what I write for that. \n Let . By the binomial theorem, . But by the Archimedean axiom (unfortunately, it seems that I made the wrong guess about which version to use \u2014 it would have been more convenient to prove that ), so as required. \n How do we prove that ? By saying that whenever and appealing to the axiom of Archimedes for the statement that there actually exists an integer that is at least . \n While we\u2019re at it, let me remind you how that version of the Archimedean axiom is proved. The statement to be proved is that for every real number there is an integer that is greater than . \n We start with the \u201clet\u201d trick: that is, we write, \u201cLet be a real number.\u201d \n We\u2019ve got nothing to go on, so in an effort to generate some information, we assume that the result is false. So we write, \u201cLet be a real number and suppose that for every integer .\u201d \n This can be rephrased as \u201c is an upper bound for the set of all integers.\u201d OK, the integers are a non-empty set and they are bounded above. What does that trigger in our minds? If we\u2019re reasonably up on the course, it should trigger the least upper bound axiom. So we add, \u201cTherefore, there must be a least upper bound for the set of all integers.\u201d \n It\u2019s obvious that such a bound can\u2019t exist: you just pick an integer close to it (which you must be able to do or it wouldn\u2019t be the least upper bound) and add 1. Here\u2019s what I\u2019d write. \u201cSince is the least upper bound, there must be an integer such that . But then , which is a contradiction.\u201d \n OK, we\u2019ve dealt pretty comprehensively with the case where is irrational. What about when is rational? A quick look to see what happens, and we see that what we care about is whether is a multiple of 2. To make things easier to think about, let\u2019s write as . Then when we find that is an integer. \n What\u2019s the neatest way of getting it to be an even integer? Actually, I see that I don\u2019t need this after all: I was being slightly stupid, since for to equal 1, all I need is for to be an integer (by the deep result that ). So here\u2019s what I\u2019d write. \n If is rational, then it is equal to for some integers and , with . If , then is an integer, so for every . Therefore, \n \n whenever is rational. \n \n Even with all the complications I tried to shoehorn in, I\u2019d say that was a pretty easy question. But you have to be a bit careful with easy questions: they make the examiners fussier about your answers. In particular, if you\u2019re in doubt about whether you\u2019re allowed to assume something (a good example in this case being the fact that when ) you should be a little more careful. \n Let me list the general principles we\u2019ve had so far for making this judgment. They are not infallible, but they are a good guide. \n 1. You can assume something as long as it belongs to an earlier part of the course. (Example: it\u2019s OK to assume general facts about limits when proving something about differentiability.) \n 2. If the question is about one thing, then it\u2019s OK to use knowledge about other topics when analysing a concrete example. (Example: if the question is about differentiability in general and then asks you about an example that involves trigonometric functions, then it\u2019s OK to assume facts like that the derivative of is . On the other hand, if the question is about formally deriving the properties of trigonometric functions, then it is definitely not OK to assume those facts.) \n 3. If the question starts with a bit of theory and then asks you to look at a concrete example, then you should not assume facts that can be proved with the help of the theory: the examiner is asking you to relate the theory to the example, and you must demonstrate that you see the connection. (Example: if you\u2019ve been asked to prove the axiom of Archimedes and then find yourself needing the fact that when , then you are almost certainly expected to give a proof, since the proof uses the axiom of Archimedes.)"], "link": "http://gowers.wordpress.com/2012/04/28/a-look-at-a-few-tripos-questions-ii/", "bloglinks": {}, "links": {"http://gowers.wordpress.com/": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "Gowers's Weblog"}]