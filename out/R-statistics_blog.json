[{"blogurl": "http://www.r-statistics.com\n", "blogroll": [], "title": "R-statistics blog"}, {"content": ["In case you tried loading a package that depends on the { rJava } package (by Simon Urbanek), you might came across the following error: \n Loading required package: rJava \nlibrary(rJava) \nError : .onLoad failed in loadNamespace() for \u2018rJava\u2019, details: \ncall: fun(libname, pkgname) \nerror: JAVA_HOME cannot be determined from the Registry \n The error tells us that there is no entry in the Registry that tells R where Java is located. It is most likely that Java was not installed (or that the registry is corrupt). \n This error is often resolved by installing a Java version (i.e. 64-bit Java or 32-bit Java) that fits to the type of R version that you are using (i.e. 64-bit R or 32-bit R). This problem can easily effect Windows 7 users, since they might have installed a version of Java that is different than the version of R they are using. \nYou can pick the exact version of Java you wish to install from this link . If you might (for some reason) work on both versions of R, you can install both version of Java (Installing the \u201cJava Runtime Environment\u201d is probably good enough for your needs). \n(Source: Uwe Ligges ) \n Other possible solutions is trying to re-install rJava. \n If that doesn\u2019t work, you could also manually set the directory of your Java location by setting it before loading the library: \n\n  ?  View Code RSPLUS   Sys. setenv ( JAVA_HOME = 'C: \\\\ Program Files \\\\ Java \\\\ jre7' ) # for 64-bit version \n Sys. setenv ( JAVA_HOME = 'C: \\\\ Program Files (x86) \\\\ Java \\\\ jre7' ) # for 32-bit version \n library ( rJava )  \n\n (Source: \u201cnograpes\u201d from Stackoverflow , which also describes the find.java in the rJava:::.onLoad function)"], "link": "http://www.r-statistics.com/2012/08/how-to-load-the-rjava-package-after-the-error-java_home-cannot-be-determined-from-the-registry/", "bloglinks": {}, "links": {"": 1, "http://stackoverflow.com/": 1, "http://www.java.com/": 2, "http://tolstoy.edu.au/": 1, "http://www.ericbess.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "R-statistics blog"}, {"content": ["This is a guest post written by Branson Owen, an enthusiastic R and data.table user. \n \n Wow, a long time desired feature of data.table finally came true in version 1.8.1! data.table now allowed numeric columns and big number (via bit64) in keys! This is quite a big thing to me and I believe to many other R users too. Now I can hardly think any weakiness of data.table. Oh, did I mention it also started to support character column in the keys (rather than coerce to factor)? \n For people who are not familiar with but interested in data.table package, data.table is an enhanced data.frame for high-speed indexing, ordered joins, assignment, grouping and list columns in a short and flexible syntax. You can take a look at some task examples here: \n \n \n What\u2019s the fastest way to merge/join data.frames in R \n Fastest way to apply functions on groups \n Timings of common tasks using the data.table \n \n \n \n News from datatable-help mailing list: \n * New functions chmatch() and %chin%, faster versions of match() and %in% for character vectors. They are about 4 times faster than match() on the example in ?chmatch. \n * New function set(DT,i,j,value) allows fast assignment to elements of DT. \n\n  ?  View Code RSPLUS   \nM = matrix ( 1 , nrow = 100000 , ncol = 100 ) \nDF = as. data . frame ( M ) \nDT = as. data . table ( M ) \n system. time ( for ( i in 1 : 1000 ) DF [ i,1L ] & lt ;- i ) # 591.000s \n system. time ( for ( i in 1 : 1000 ) DT [ i,V1 := i ] ) # 1.158s \n system. time ( for ( i in 1 : 1000 ) M [ i,1L ] & lt ;- i ) # 0.016s \n system. time ( for ( i in 1 : 1000 ) set ( DT,i,1L,i ) ) # 0.027s  \n\n * Numeric columns (type \u2018double\u2019) are now allowed in keys and ad hoc by. Other types which use \u2018double\u2019 (such as POSIXct and bit64) can now be fully supported. \n For advanced and creative users, it also officially supported list columns awhile ago (rather than support it by accident). For example, your column could be a list of vectors, where each of the vector has different length. This can allow very flexible and creative ways to manipulate data. \n The code example below use \u201cfunction column\u201d, i.e. a list of functions \n\n  ?  View Code RSPLUS   > DT = data. table ( ID = 1 : 4 ,A = rnorm ( 4 ) ,B = rnorm ( 4 ) ,fn = list ( min , max ) ) \n > str ( DT ) \nClasses \u2018data. table \u2019 and 'data.frame' : 4 obs. of 4 variables : \n$ ID : int 1 2 3 4 \n$ A : num - 0.7135 - 2.5217 0.0265 1.0102 \n$ B : num - 0.4116 0.4032 0.1098 0.0669 \n$ fn : List of 4 \n..$ : function ( ..., na. rm = FALSE ) \n..$ : function ( ..., na. rm = FALSE ) \n..$ : function ( ..., na. rm = FALSE ) \n..$ : function ( ..., na. rm = FALSE ) \n \n > DT [ ,fn [ [ 1 ] ] ( A,B ) , by = ID ] \nID V1\n [ 1 , ] 1 - 0.71352508 \n [ 2 , ] 2 0.40322625 \n [ 3 , ] 3 0.02648949 \n [ 4 , ] 4 1.01022266  \n\n [ref] https://r-forge.r-project.org/tracker/index.php?func=detail&aid=1302&group_id=240&atid=978"], "link": "http://www.r-statistics.com/2012/05/data-table-version-1-8-1-now-allowed-numeric-columns-and-big-number-via-bit64-in-keys/", "bloglinks": {}, "links": {"": 2, "http://stackoverflow.com/": 1, "https://r-forge.r-project.org/": 1, "http://zvfak.blogspot.com/": 1, "http://www.ericbess.com/": 2, "http://cran.r-project.org/": 1}, "blogtitle": "R-statistics blog"}, {"content": ["This post is about speeding up your R code using the JIT (just in time) compilation capabilities offered by the new (well, now a year old ) {compiler} package . Specifically, dealing with the practical difference between enableJIT and the cmpfun functions. \n If you do not want to read much, you can just skip to the example part. \n As always, I welcome any comments to this post, and hope to update it when future JIT solutions will come along. \n \n Prelude: what is JIT \n Just-in-time compilation (JIT) : \n is a method to improve the runtime performance of computer programs. Historically, computer programs had two modes of runtime operation, either interpreted or static (ahead-of-time) compilation. Interpreted code is translated from a high-level language to a machine code continuously during every execution, whereas statically compiled code is translated into machine code before execution, and only requires this translation once. \nJIT compilers represent a hybrid approach, with translation occurring continuously, as with interpreters, but with caching of translated code to minimize performance degradation . It also offers other advantages over statically compiled code at development time, such as handling of late-bound data types and the ability to enforce security guarantees. \n JIT in R \n To this date, there are two R packages that offers Just-in-time compilation to R users: the {jit} package (through The Ra Extension to R), and the {compiler} package (which now comes bundled with any new R release, since R 2.13). \n The\u00a0{jit} package \n The\u00a0 {jit} package , \u00a0created by Stephen Milborrow , provides just-in-time compilation of R loops and arithmetic expressions in loops, enabling such code to run much faster (speeding up loops between 10 to 20 times faster ). However, the drawback is that in order to use {jit}, you will need to use it through \u201c the Ra Extension to R \u201c\u00a0( Ra is like R , only that it allows using the {jit} package). Sadly, the {jit} package will have no effect under standard R . \u00a0The package was not updated since\u00a02011-08-27, and I am curious to see how its future might unfold (either continue on, merge with some other project, or sadly will go out of use). \n The\u00a0{compiler} package \n The {compiler} package , created by\u00a0 Luke Tierney ,\u00a0offers a byte-code compiler for R: \n A byte code compiler translates a complex high-level language like Lisp into a very simple language that can be interpreted by a very fast byte code interpreter, or virtual machine. The internal representation of this simple language is a string of bytes, hence the name byte code. The compilation process eliminates a number of costly operations the interpreter has to perform, including variable lookup and setting up exits for nonlocal transfers of control. It also performs some inlining and makes the language tail-recursive. This means that tail calls are compiled as jumps and therefore iterations can be implemented using recursion. \u00a0(the\u00a0 source \u00a0of this\u00a0quote) \n The compiler produces code for a virtual machine that is then executed by a virtual machine runtime system. \u00a0The virtual machine is a stack based machine. Thus instructions for the virtual machine take arguments off a stack and may leave one or more results on the stack. Byte code objects consists of an\u00a0integer vector representing instruction opcodes and operands, and a generic vector representing a\u00a0constant pool. The compiler is implemented almost entirely in R, with just a few support routines\u00a0in C to manage compiled code objects. \nThe virtual machine instruction set is designed to allow much of the interpreter internals to be\u00a0re-used. In particular, for now the mechanism for calling functions of all types from compiled code\u00a0remains the same as the function calling mechanism for interpreted code.\u00a0 \u00a0(the\u00a0 source\u00a0 of this\u00a0quote) \n From the perspective of using JIT with R, the above means that the {compiler} package does not offer a jit compiler to a machine code, but it does offer it in order to turn it into byte code. \n The byte compiler was first introduced with R 2.13, and starting with R 2.14 , all of the standard functions and packages in R were pre-compiled into byte-code. \u00a0The benefit in speed depends on the specific function but code\u2019s\u00a0performance can improve up by a factor of 2x times or more. \n In some early experiments,\u00a0 Dirk Eddelbuettel \u00a0(our community\u2019s\u00a0 R\u2019s HPC guru) looked at what\u00a0the {compiler} package can offer us when using the\u00a0 cmpfun() function. \u00a0e showed that the performance gain for various made-up functions \u00a0can range between 2x to 5x times faster running time. \u00a0This is great for the small amount of work (e.g: code modification) it requires on our part, but just in order to give it some perspective, using Ra can speed up your code to be up to 25x times faster (as was also mentioned by Tierney himself in slides from 2010 , see slide 20). \u00a0Moreover, by combining C/C++ code with R code (for example, through\u00a0the { Rcpp } and { Inline } packages) you can improve your code\u2019s running time by a factor of 80 (or even almost 120 to the worst manual implementation) relative to interpreted code. \u00a0But to be fair to R, the code that is used for such examples is often\u00a0unrealistic code examples that is often not representative of real R work. Thus, effective speed gains can be expected to be smaller for all of the above solutions. \n If you want to learn more on the {compiler} package,\u00a0Prof Tierney wrote a massive\u00a0 100+ page paper on R\u2019s byte-code compiler ,\u00a0which also includes points for future research and development. \n Using the {compiler} package as a JIT for R \n Description \n (From the manual \u201c A Byte Code Compiler for R \u201c) \n JIT compilation, through the\u00a0{compiler} package,\u00a0can be enabled from within an active R session by calling the enableJIT() function with a non-negative integer\u00a0argument (from 0 to 3), or by starting R with the environment variable R_ENABLE_JIT set to a non-negative\u00a0integer. \u00a0The possible values of the argument to enableJIT and their meanings are: \n \n 0 \u00a0- turn off JIT \n 1 \u00a0- compile closures before they are called the first time \n 2 \u00a0- same as 1, plus compile closures before duplicating (useful for packages that store closures in\u00a0lists, like lattice) \n 3 \u00a0- same as 2, plus compile all for(), while(), and repeat() loops before executing. \n \n R may initially be somewhat sluggish if JIT is enabled and base and recommended packages have\u00a0not been pre-compiled as almost everything will initially need some compilation. \n Example \n The following example is an adaptation of the code taken from the ?compile help file . \n Let us start by defining two functions we will use for our example: \n\n  ?  View Code RSPLUS   \n ##### Functions ##### \n \nis. compile <- function ( func ) \n { \n\t # this function lets us know if a function has been byte-coded or not \n\t #If you have a better idea for how to do this - please let me know... \n  if ( class ( func ) != \"function\" ) stop ( \"You need to enter a function\" ) \n last_2_lines <- tail ( capture. output ( func ) , 2 ) \n  any ( grepl ( \"bytecode:\" , last_2_lines ) ) # returns TRUE if it finds the text \"bytecode:\" in any of the last two lines of the function's print \n } \n \n # old R version of lapply \nslow_func <- function ( X, FUN, ... ) { \n FUN <- match. fun ( FUN ) \n if ( ! is. list ( X ) ) \n X <- as. list ( X ) \n rval <- vector ( \"list\" , length ( X ) ) \n for ( i in seq ( along = X ) ) \n rval [ i ] <- list ( FUN ( X [ [ i ] ] , ... ) ) \n names ( rval ) <- names ( X )   # keep `names' ! \n return ( rval ) \n } \n \n # Compiled versions \n require ( compiler ) \nslow_func_compiled <- cmpfun ( slow_func )  \n\n Notice how in the last line of code we manually byte-compile our slow function. Next, let\u2019s define a function that will run the slow function (the raw and the complied versions) many times, and measure the time it takes to run each: \n\n  ?  View Code RSPLUS   \nfo <- function ( ) for ( i in 1 : 1000 ) slow_func ( 1 : 100 , is. null ) \nfo_c <- function ( ) for ( i in 1 : 1000 ) slow_func_compiled ( 1 : 100 , is. null ) \n \n system. time ( fo ( ) ) \n system. time ( fo_c ( ) ) \n \n # > system.time(fo()) \n # user system elapsed \n # 0.54 0.00 0.57 \n # > system.time(fo_c()) \n # user system elapsed \n # 0.17 0.00 0.17  \n\n We see in this example how using the byte compiler on \u201cslow_func\u201d gave us a bit over 3x times speed gain. What will happen if we used the cmpfun() function on the function \u201cfo\u201d itself? Let\u2019s check: \n\n  ?  View Code RSPLUS   fo_compiled <- cmpfun ( fo ) \n system. time ( fo_compiled ( ) ) # doing this, will not change the speed at all: \n # user system elapsed \n # 0.58 0.00 0.58  \n\n We can see that we did not get any speed gain from this operation, why is that? The reason is that the slow function (slow_func) is still not compiled: \n\n  ?  View Code RSPLUS   is. compile ( slow_func ) \n # [1] FALSE \nis. compile ( fo ) \n # [1] FALSE \nis. compile ( fo_compiled ) \n # [1] TRUE  \n\n The cmpfun() function only compiled the wrapping function fo (fo_compiled), but not the functions nested within it (slow_func). And this is where the enableJIT() function kicks in: \n\n  ?  View Code RSPLUS   enableJIT ( 3 ) \n system. time ( fo ( ) ) \n # user system elapsed \n # 0.19 0.00 0.18  \n\n We can see that \u201csuddenly\u201d fo has become much faster. The reason is because turning the JIT on (using enableJIT(3)), basically started turning any function we run into byte-code. So if we now check again, we find that: \n\n  ?  View Code RSPLUS   is. compile ( fo ) \n # [1] TRUE # when it previously was not compiled, only fo_compiled was... \nis. compile ( slow_func ) \n # [1] TRUE # when it previously was not compiled, only slow_func_compiled was...  \n\n This means that if you want to have as little modification to your code as possible , instead of going through every function you use and run cmpfun() on it, you can simply run the following code once, in the beginning of your code: \n\n  ?  View Code RSPLUS   require ( compiler ) \nenableJIT ( 3 )  \n\n And you will get the speed gains the byte compiler has to offer your code. \n On a side note, we can now turn the JIT back off using \u201cenableJIT(0)\u201d, but it has still already compiled the inner functions (for example fo and slow_func). If we want to un-compile them, we will have to re-create these functions again (run the code that produced them in the first place). \n To conclude : in this post I have discussed the current state of just-in-time compilation of R code, and shown how to use the {compiler} package for using JIT in R."], "link": "http://www.r-statistics.com/2012/04/speed-up-your-r-code-using-a-just-in-time-jit-compiler/", "bloglinks": {}, "links": {"http://dirk.eddelbuettel.com/blog": 1, "": 7, "http://blog.revolutionanalytics.com/": 1, "http://dirk.eddelbuettel.com": 1, "http://onertipaday.blogspot.com/": 1, "http://www.sonic.net/": 7, "http://www.ericbess.com/": 7, "http://www.uiowa.edu/": 6, "http://en.wikipedia.org/": 2, "http://cran.r-project.org/": 3, "http://stat.ethz.ch/": 4}, "blogtitle": "R-statistics blog"}, {"content": ["This is a guest post by Garrett Grolemund (mentored by Hadley Wickham ) \n Lubridate is an R package that makes it easier to work with dates and times. The newest release of lubridate (v 1.1.0) comes with even more tools and some significant changes over past versions. Below is a concise tour of some of the things lubridate can do for you. At the end of this post, I list some of the differences between lubridate (v 0.2.4) and lubridate (v 1.1.0). If you are an old hand at lubridate, please read this section to avoid surprises! \n Lubridate was created by Garrett Grolemund and Hadley Wickham. \n Parsing dates and times \n Getting R to agree that your data contains the dates and times you think it does can be a bit tricky. Lubridate simplifies that. Identify the order in which the year, month, and day appears in your dates. Now arrange \u201cy\u201d, \u201cm\u201d, and \u201cd\u201d in the same order. This is the name of the function in lubridate that will parse your dates. For example, \n\n  ?  View Code RSPLUS   library ( lubridate ) \nymd ( \"20110604\" ) ; mdy ( \"06-04-2011\" ) ; dmy ( \"04/06/2011\" ) \n ## \"2011-06-04 UTC\" \n ## \"2011-06-04 UTC\" \n ## \"2011-06-04 UTC\"  \n\n Parsing functions automatically handle a wide variety of formats and separators, which simplifies the parsing process. \n If your date includes time information, add h, m, and/or s to the name of the function. ymd_hms() is probably the most common date time format. To read the dates in with a certain time zone, supply the official name of that time zone in the tz argument. \n\n  ?  View Code RSPLUS   arrive <- ymd_hms ( \"2011-06-04 12:00:00\" , tz = \"Pacific/Auckland\" ) \n ## \"2011-06-04 12:00:00 NZST\" \nleave <- ymd_hms ( \"2011-08-10 14:00:00\" , tz = \"Pacific/Auckland\" ) \n ## \"2011-08-10 14:00:00 NZST\"  \n\n Setting and Extracting information \n Extract information from date times with the functions second(), minute(), hour(), day(), wday(), yday(), week(), month(), year(), and tz(). You can also use each of these to set (i.e, change) the given information. Notice that this will alter the date time. wday() and month() have an optional label argument, which replaces their numeric output with the name of the weekday or month. \n\n  ?  View Code RSPLUS   second ( arrive ) \n ## 0 \nsecond ( arrive ) <- 25 \narrive\n ## \"2011-06-04 12:00:25 NZST\" \nsecond ( arrive ) <- 0 \nwday ( arrive ) \n ## 7 \nwday ( arrive, label = TRUE ) \n ## Sat  \n\n Time Zones \n There are two very useful things to do with dates and time zones. First, display the same moment in a different time zone. Second, create a new moment by combining a given clock time with a new time zone. These are accomplished by with_tz() and force_tz(). \n For example, I spent last summer researching in Auckland, New Zealand. I arranged to meet with my advisor, Hadley, over skype at 9:00 in the morning Auckland time. What time was that for Hadley who was back in Houston, TX? \n\n  ?  View Code RSPLUS   meeting <- ymd_hms ( \"2011-07-01 09:00:00\" , tz = \"Pacific/Auckland\" ) \n ## \"2011-07-01 09:00:00 NZST\" \nwith_tz ( meeting, \"America/Chicago\" ) \n ## \"2011-06-30 16:00:00 CDT\"  \n\n So the meetings occurred at 4:00 Hadley\u2019s time (and the day before no less). Of course, this was the same actual moment of time as 9:00 in New Zealand. It just appears to be a different day due to the curvature of the Earth. \n What if Hadley made a mistake and signed on at 9:00 his time? What time would it then be my time? \n\n  ?  View Code RSPLUS   mistake <- force_tz ( meeting, \"America/Chicago\" ) \n ## \"2011-07-01 09:00:00 CDT\" \nwith_tz ( mistake, \"Pacific/Auckland\" ) \n ## \"2011-07-02 02:00:00 NZST\"  \n\n His call would arrive at 2:00 am my time! Luckily he never did that. \n \n Time Intervals \n You can save an interval of time as an Interval class object. This is quite useful! For example, my stay in Auckland lasted from June 4 to August 10 (which we\u2019ve already saved as arrive and leave). We can create this interval in one of two ways: \n\n  ?  View Code RSPLUS   auckland <- interval ( arrive, leave ) \n ## 2011-06-04 12:00:00 NZST--2011-08-10 14:00:00 NZST \nauckland <- arrive %--% leave\n ## 2011-06-04 12:00:00 NZST--2011-08-10 14:00:00 NZST  \n\n My mentor at the University of Auckland, Chris, traveled to various conferences that year including the Joint Statistical Meetings (JSM). This took him out of the country from July 20 until the end of August. \n\n  ?  View Code RSPLUS   jsm <- interval ( ymd ( 20110720 , tz = \"Pacific/Auckland\" ) , ymd ( 20110831 , tz = \"Pacific/Auckland\" ) )  \n\n Will my visit overlap with and his travels? Yes. \n\n  ?  View Code RSPLUS   int_overlaps ( jsm, auckland ) ## TRUE  \n\n Then I better make hay while the sun shines! For what part of my visit will Chris be there? \n\n  ?  View Code RSPLUS   setdiff ( auckland, jsm ) ## 2011-06-04 12:00:00 NZST\u20132011-07-20 NZST  \n\n Other functions that work with intervals include int_start, int_end, int_flip, int_shift, int_aligns, union, intersect, and %within%. \n Arithmetic with date times \n Intervals are specific time spans (because they are tied to specific dates), but lubridate also supplies two general time span classes: durations and periods. Helper functions for creating periods are named after the units of time (plural). Helper functions for creating durations follow the same format but begin with a \u201cd\u201d (for duration) or, if you prefer, and \u201ce\u201d (for exact). \n\n  ?  View Code RSPLUS   minutes ( 2 ) # period \n ## 2 minutes \ndminutes ( 2 ) # duration \n ## 120s (~2 minutes)  \n\n Why two classes? Because the timeline is not as reliable as the number line. The durations class will always supply mathematically precise results. A duration year will always equal 365 days. Periods, on the other hand, fluctuate the same way the timeline does to give intuitive results. This makes them useful for modelling clock times. For example, durations will be honest in the face of a leap year, but periods may return what you want: \n\n  ?  View Code RSPLUS   leap_year ( 2011 ) \n ## FALSE \nymd ( 20110101 ) + dyears ( 1 ) \n ## \"2012-01-01 UTC\" \nymd ( 20110101 ) + years ( 1 ) \n ## \"2012-01-01 UTC\" \n \nleap_year ( 2012 ) \n ## TRUE \nymd ( 20120101 ) + dyears ( 1 ) \n ## \"2012-12-31 UTC\" \nymd ( 20120101 ) + years ( 1 ) \n ## \"2013-01-01 UTC\"  \n\n We can use periods and durations to do basic arithmetic with date times. For example, if I wanted to set up a reoccuring weekly skype meeting with Hadley, it would occur on: \n\n  ?  View Code RSPLUS   meetings <- meeting + weeks ( 0 : 5 ) \n ## [1] \"2011-07-01 09:00:00 NZST\" \"2011-07-08 09:00:00 NZST\" \n ## [3] \"2011-07-15 09:00:00 NZST\" \"2011-07-22 09:00:00 NZST\" \n ## [5] \"2011-07-29 09:00:00 NZST\" \"2011-08-05 09:00:00 NZST\"  \n\n Hadley travelled to conferences at the same time as Chris. Which of these meetings would be affected? The last two. \n\n  ?  View Code RSPLUS   meetings % within % jsm\n ## FALSE FALSE FALSE FALSE TRUE TRUE  \n\n How long was my stay in Auckland? \n\n  ?  View Code RSPLUS   auckland / edays ( 1 ) \n ## 67.08333 \nauckland / edays ( 2 ) \n ## 33.54167 \nauckland / eminutes ( 1 ) \n ## 96600  \n\n And so on. Alternatively, we can do modulo and integer division. Sometimes this is the more sensible than division \u2013 it is not obvious how to express a remainder as a fraction of a month because the length of a month constantly changes. \n\n  ?  View Code RSPLUS   auckland %/% months ( 1 ) \n ## 2 \nauckland %% months ( 1 ) \n ## 2011-08-04 12:00:00 NZST--2011-08-10 14:00:00 NZST  \n\n Modulo with an interval returns the remainder as a new (smaller) interval. We can turn this or any interval into a generalized time span with as.period(). \n\n  ?  View Code RSPLUS   as. period ( auckland %% months ( 1 ) ) \n ## 6 days and 2 hours \nas. period ( auckland ) \n ## 2 months, 6 days and 2 hours  \n\n Vectorization \n The code in lubridate is vectorized and ready to be used in both interactive settings and within functions. As an example, I offer a function for advancing a date to the last day of the month \n\n  ?  View Code RSPLUS   last_day <- function ( date ) { \n ceiling_date ( date , \"month\" ) - days ( 1 ) \n } \n # try last_day(ymd(20000101) + months(0:11))  \n\n Changes in lubridate (v 1.1.0) \n To comply with changes in base R, We\u2019ve moved lubridate from an S3 class system to an S4 system. Most of these changes are contained \u201cunder the hood.\u201d But some changes will affect how lubridate behaves and compromise previously written code. This disruption is unavoidable: previous versions of lubridate will not work on future versions of R. To see a complete list of changes, see the lubridate news file here . \n Changes between version 0.2.6 and 1.1.0 can be grouped into three main categories: \n \n \n Subtracting two dates must now follow the default behavior of R, which is to create a difftime object. Previous versions of lubridate created an Interval object. If you receive an error involving an Interval object, please check that it is not in fact a difftime. \n \n \n Intervals now have a direction. This means the order of the dates in new_interval(), interval() and %\u2013% matters. You can flip the direction with int_flip and coerce all intervals to be postive with int_standardize \n \n \n Lubridate no longer automatically coerces time span input to the correct class when performing math. This was poor programming because the user can and should explicitly control the class of their input with as.interval(), as.period(), and as.duration(). It also made lubridate needlessly chatty with messages and led to unintuitive results as we added increased functionality. When coercion is needed to perform an operation, lubridate now returns an informative error message. \n \n \n Further Resources \n To learn more about lubridate, including the specifics of periods and durations, please read the original lubridate paper at http://www.jstatsoft.org/v40/i03/ . Questions about lubridate can be addressed to the lubridate google group . The development page for lubridate can be found at http://github.com/hadley/lubridate ."], "link": "http://www.r-statistics.com/2012/03/do-more-with-dates-and-times-in-r-with-lubridate-1-1-0/", "bloglinks": {}, "links": {"": 17, "http://www.jstatsoft.org/": 1, "http://had.co.nz/": 1, "http://www.ericbess.com/": 17, "http://github.com/": 1, "http://groups.google.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "R-statistics blog"}, {"content": ["This post shows how to print a prettier nested pivot table, created using the {reshape} package (similar to what you would get with Microsoft Excel), so you could print it either in the R terminal or as a LaTeX table. This task is done by bridging between the cast_df object produced by the {reshape} package, and the tabular function introduced by the new {tables} package. \n Here is an example of the type of output we wish to produce in the R terminal: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n \n \n \n  ozone  solar. r   wind   temp  \n month mean sd  mean  sd  mean  sd  mean sd \n 5  23.62 22.22 181.3  115.08 11.623 3.531 65.55 6.855 \n 6  29.44 18.21 190.2  92.88 10.267 3.769 79.10 6.599 \n 7  59.12 31.64 216.5  80.57 8.942 3.036 83.90 4.316 \n 8  59.96 39.68 171.9  76.83 8.794 3.226 83.97 6.585 \n 9  31.45 24.14 167.4  79.12 10.180 3.461 76.90 8.356 \n \n \n \n \n Or in a latex document: \n  \n Motivation: creating pretty nested tables \n In a recent post we learned how to use the {reshape} package (by Hadley Wickham ) in order to aggregate and reshape data (in R) using the melt and cast functions. \n The cast function is wonderful but it has one problem \u2013 the format of the output. As opposed to a pivot table in (for example) MS excel, the output of a nested table created by cast is very \u201cflat\u201d. That is, there is only one row for the header, and only one column for the row names. So for both the R terminal, or an Sweave document, when we deal with a more complex reshaping/aggregating, the result is not something you would be proud to send to a journal. \n The opportunity: the {tables} package \n The good news is that Duncan Murdoch have recently released a new package to CRAN called {tables} . The {tables} package can compute and display complex tables of summary statistics and turn them into nice looking tables in Sweave (LaTeX) documents. For using the full power of this package, you are invited to read through its detailed (and well written) 23 pages Vignette . However, some of us might have preferred to keep using the syntax of the {reshape} package, while also benefiting from the great formatting that is offered by the new {tables} package. For this purpose, I devised a function that bridges between cast_df (from {reshape}) and the tabular function (from {tables}). \n The bridge: between the {tables} and the {reshape} packages \n The code for the function is available on my github (link: tabular.cast_df.r on github ) and it seems to works fine as far as I can see (though I wouldn\u2019t run it on larger data files since it relies on melting a cast_df object.) \n Here is an example for how to load and use the function: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n \n \n \n ###################### \n # Loading the functions \n ###################### \n # Making sure we can source code from github \n source ( \"http://www.r-statistics.com/wp-content/uploads/2012/01/source_https.r.txt\" ) \n \n # Reading in the function for using tabular on a cast_df object: \nsource_https ( \"https://raw.github.com/talgalili/R-code-snippets/master/tabular.cast_df.r\" ) \n \n \n \n ###################### \n # example: \n ###################### \n \n ############ \n # Loading and preparing some data \n require ( reshape ) \n names ( airquality ) <- tolower ( names ( airquality ) ) \nairquality2 <- airquality \nairquality2$temp2 <- ifelse ( airquality2$temp > median ( airquality2$temp ) , \"hot\" , \"cold\" ) \naqm <- melt ( airquality2, id = c ( \"month\" , \"day\" , \"temp2\" ) , na. rm = TRUE ) \n colnames ( aqm ) [ 4 ] <- \"variable2\" \t # because otherwise the function is having problem when relying on the melt function of the cast object \n head ( aqm, 3 ) \n # month day temp2 variable2 value \n #1  5 1 cold  ozone 41 \n #2  5 2 cold  ozone 36 \n #3  5 3 cold  ozone 12 \n \n ############ \n # Running the example: \ntabular. cast_df ( cast ( aqm, month ~ variable2, c ( mean , sd ) ) ) \ntabular ( cast ( aqm, month ~ variable2, c ( mean , sd ) ) ) # notice how we turned tabular to be an S3 method that can deal with a cast_df object \nHmisc :: latex ( tabular ( cast ( aqm, month ~ variable2, c ( mean , sd ) ) ) ) # this is what we would have used for an Sweave document \n \n \n \n \n And here are the results in the terminal: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n \n \n \n > \n > tabular. cast_df ( cast ( aqm, month ~ variable2, c ( mean , sd ) ) ) \n \n  ozone  solar. r   wind   temp  \n month mean sd  mean  sd  mean  sd  mean sd \n 5  23.62 22.22 181.3  115.08 11.623 3.531 65.55 6.855 \n 6  29.44 18.21 190.2  92.88 10.267 3.769 79.10 6.599 \n 7  59.12 31.64 216.5  80.57 8.942 3.036 83.90 4.316 \n 8  59.96 39.68 171.9  76.83 8.794 3.226 83.97 6.585 \n 9  31.45 24.14 167.4  79.12 10.180 3.461 76.90 8.356 \n > tabular ( cast ( aqm, month ~ variable2, c ( mean , sd ) ) ) # notice how we turned tabular to be an S3 method that can deal with a cast_df object \n \n  ozone  solar. r   wind   temp  \n month mean sd  mean  sd  mean  sd  mean sd \n 5  23.62 22.22 181.3  115.08 11.623 3.531 65.55 6.855 \n 6  29.44 18.21 190.2  92.88 10.267 3.769 79.10 6.599 \n 7  59.12 31.64 216.5  80.57 8.942 3.036 83.90 4.316 \n 8  59.96 39.68 171.9  76.83 8.794 3.226 83.97 6.585 \n 9  31.45 24.14 167.4  79.12 10.180 3.461 76.90 8.356 \n \n \n \n \n And in an Sweave document: \n  \n Here is an example for the Rnw file that produces the above table: \n cast_df to tabular.Rnw \n I will finish with saying that the tabular function offers more flexibility then the one offered by the function I provided. If you find any bugs or have suggestions of improvement, you are invited to leave a comment here or inside the code on github . \n (Link-tip goes to Tony Breyal for putting together a solution for sourcing r code from github.)"], "link": "http://www.r-statistics.com/2012/01/printing-nested-tables-in-r-bridging-between-the-reshape-and-tables-packages/", "bloglinks": {}, "links": {"https://github.com/": 2, "http://had.co.nz/": 1, "http://tonybreyal.wordpress.com/": 2, "http://www.r-statistics.com/": 4, "http://cran.r-project.org/": 2, "http://www.mail-archive.com/": 1}, "blogtitle": "R-statistics blog"}, {"content": ["The followings introductory post is intended for new users of R. \u00a0It deals with interactive visualization using R through the iplots package. \n This is a guest article by Dr.\u00a0 Robert I. Kabacoff , the founder of (one of) the first online R tutorials websites:\u00a0 Quick-R . Kabacoff has recently published the\u00a0book\u00a0\u201d R in Action \u201c, providing a detailed walk-through for the R language based on\u00a0various examples for illustrating R\u2019s features (data manipulation, statistical methods, graphics, and so on\u2026). In\u00a0 previous guest post s by Kabacoff we introduced data.frame objects in R \u00a0and dealt with the\u00a0 Aggregation and Restructuring of data \u00a0(using base R functions and the reshape package). \n For readers of this blog, there is a \u00a038% discount \u00a0off\u00a0 the \u201cR in Action\u201d book \u00a0(as well as all other eBooks, pBooks and MEAPs at\u00a0 Manning publishing house ), simply by using\u00a0the code\u00a0 rblogg38\u00a0 when reaching checkout. \n Let us now talk about Interactive Graphics with the iplots Package: \n \n \n Interactive Graphics with the iplots Package \n \n The base installation of R provides limited interactivity with graphs. You can modify graphs by issuing additional program statements, but there\u2019s little that you can do to modify them or gather new information from them using the mouse. However, there are contributed packages that greatly enhance your ability to interact with the graphs you create\u2014playwith, latticist, iplots, and rggobi. In this article, we\u2019ll focus on functions provided by the iplots package. Be sure to install it before first use. \n While playwith and latticist allow you to interact with a single graph, the iplots package takes interaction in a different direction. This package provides interactive mosaic plots, bar plots, box plots, parallel plots, scatter plots, and histograms that can be linked together and color brushed. This means that you can select and identify observations using the mouse, and highlighting observations in one graph will automatically highlight the same observations in all other open graphs. You can also use the mouse to obtain information about graphic objects such as points, bars, lines, and box plots. \n The iplots package is implemented through Java and the primary functions are listed in table 1. \n Table 1 iplot functions \n \n \n \n \n \n Function \n \n \n \n \n Description \n \n \n \n \n ibar() \n Interactive bar chart \n \n \n ibox() \n Interactive box plot \n \n \n ihist() \n Interactive histogram \n \n \n imap() \n Interactive map \n \n \n imosaic() \n Interactive mosaic plot \n \n \n ipcp() \n Interactive parallel coordinates plot \n \n \n iplot() \n Interactive scatter plot \n \n \n \n To understand how iplots works, execute the code provided in listing 1. \n Listing 1 iplots demonstration \n \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n \n \n \n library ( iplots ) \n attach ( mtcars ) \ncylinders <- factor ( cyl ) \ngears <- factor ( gear ) \ntransmission <- factor ( am ) \nihist ( mpg ) \nibar ( gears ) \niplot ( mpg, wt ) \nibox ( mtcars [ c ( \"mpg\" , \"wt\" , \"qsec\" , \"disp\" , \"hp\" ) ] ) \nipcp ( mtcars [ c ( \"mpg\" , \"wt\" , \"qsec\" , \"disp\" , \"hp\" ) ] ) \nimosaic ( transmission, cylinders ) \n detach ( mtcars ) \n \n \n \n \n Six windows containing graphs will open. Rearrange them on the desktop so that each is visible (each can be resized if necessary). A portion of the display is provided in figure 1. \n  \n Figure 1 An iplots demonstration created by listing 1. Only four of the six windows are displayed to save room. In these graphs, the user has clicked on the three-gear bar in the bar chart window. \n Now try the following: \n \n Click on the three-gear bar in the Barchart (gears) window. The bar will turn red. In addition, all cars with three-gear engines will be highlighted in the other graph windows. \n Mouse down and drag to select a rectangular region of points in the Scatter plot (wt vs mpg) window. These points will be highlighted and the corresponding observations in every other graph window will also turn red. \n Hold down the Ctrl key and move the mouse pointer over a point, bar, box plot, or line in one of the graphs. Details about that object will appear in a pop-up window. \n Right-click on any object and note the options that are offered in the context menu. For example, you can right-click on the Boxplot (mpg) window and change the graph to a parallel coordinates plot (PCP). \n You can drag to select more than one object (point, bar, and so on) or use Shift-click to select noncontiguous objects. Try selecting both the three- and five-gear bars in the Barchart (gears) window. \n \n The functions in the iplots package allow you to explore the variable distributions and relationships among variables in subgroups of observations that you select interactively. This can provide insights that would be difficult and time-consuming to obtain in other ways. For more information on the iplots package, visit the project website at http://rosuda.org/iplots/ . \n Summary \n In this article, we explored one of the several packages for dynamically interacting with graphs, iplots. This package allows you to interact directly with data in graphs, leading to a greater intimacy with your data and expanded opportunities for developing insights. \n \n \n This article first appeared as\u00a0chapter 16.4.4 from the \u201c R in action \u201c \u00a0book, and is published with permission from\u00a0 Manning publishing house . \u00a0Other books in this serious which\u00a0you might be interested in are (see the beginning of this post for a discount code): \n \n Machine Learning in Action by Peter Harrington \n \n \n Gnuplot in Action (Understanding Data with Graphs) by Philipp K. Janert"], "link": "http://www.r-statistics.com/2012/01/interactive-graphics-with-the-iplots-package-from-r-in-action/", "bloglinks": {}, "links": {"http://www.statmethods.net/": 2, "http://www.r-statistics.com/": 4, "http://affiliate.manning.com/": 7, "http://rosuda.org/": 1}, "blogtitle": "R-statistics blog"}, {"content": ["Merging two data.frame objects in R is very easily done by using the merge function. While being very powerful, the merge function does not (as of yet) offer to return a merged data.frame that preserved the original order of, one of the two merged, data.frame objects. \nIn this post I describe this problem, and offer some easy to use code to solve it. \n \n Let us start with a simple example: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n \n \n \n  x <- data. frame ( \n   ref = c ( 'Ref1' , 'Ref2' ) \n   , label = c ( 'Label01' , 'Label02' ) \n   ) \n y <- data. frame ( \n   id = c ( 'A1' , 'C2' , 'B3' , 'D4' ) \n  , ref = c ( 'Ref1' , 'Ref2' , 'Ref3' , 'Ref1' ) \n  , val = c ( 1.11 , 2.22 , 3.33 , 4.44 ) \n   ) \n \n ####################### \n # having a look at the two data.frame objects: \n > x\n ref label\n 1 Ref1 Label01\n 2 Ref2 Label02\n > y\n id ref val\n 1 A1 Ref1 1.11 \n 2 C2 Ref2 2.22 \n 3 B3 Ref3 3.33 \n 4 D4 Ref1 4.44 \n \n \n \n \n If we will now merge the two objects, we will find that the order of the rows is different then the original order of the \u201cy\u201d object. This is true whether we use \u201csort =T\u201d or \u201csort=F\u201d. You can notice that the original order was an ascending order of the \u201cval\u201d variable: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n \n \n \n > merge ( x, y, by = 'ref' , all. y = T , sort = T ) \n ref label id val\n 1 Ref1 Label01 A1 1.11 \n 2 Ref1 Label01 D4 4.44 \n 3 Ref2 Label02 C2 2.22 \n 4 Ref3  < NA > B3 3.33 \n > merge ( x, y, by = 'ref' , all. y = T , sort = F ) \n ref label id val\n 1 Ref1 Label01 A1 1.11 \n 2 Ref1 Label01 D4 4.44 \n 3 Ref2 Label02 C2 2.22 \n 4 Ref3  < NA > B3 3.33 \n \n \n \n \n This is explained in the help page of ?merge: \n The rows are by default lexicographically sorted on the common columns, but for \u2018sort = FALSE\u2019 are in an unspecified order.\n \n \n Or put differently: sort=FALSE doesn\u2019t preserve the order of any of the two entered data.frame objects (x or y); instead it gives us an \nunspecified (potentially random) order. \n However, it can so happen that we want to make sure the order of the resulting merged data.frame objects ARE ordered according to the order of one of the two original objects. In order to make sure of that, we could add an extra \u201cid\u201d (row index number) sequence on the dataframe we wish to sort on. Then, we can merge the two data.frame objects, sort by the sequence, and delete the sequence. (this was previously mentioned on the R-help mailing list by Bart Joosen ). \n Following is a function that implements this logic, followed by an example for its use: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n \n \n \n ############## function: \n\tmerge. with . order <- function ( x,y, ..., sort = T , keep_order ) \n\t { \n\t\t # this function works just like merge, only that it adds the option to return the merged data.frame ordered by x (1) or by y (2) \n\t\tadd. id . column . to . data <- function ( DATA ) \n\t\t { \n\t\t\t data. frame ( DATA, id... = seq_len ( nrow ( DATA ) ) ) \n\t\t } \n\t\t # add.id.column.to.data(data.frame(x = rnorm(5), x2 = rnorm(5))) \n\t\torder. by . id ... and . remove . it <- function ( DATA ) \n\t\t { \n\t\t\t # gets in a data.frame with the \"id...\" column. Orders by it and returns it \n\t\t\t if ( ! any ( colnames ( DATA ) == \"id...\" ) ) stop ( \"The function order.by.id...and.remove.it only works with data.frame objects which includes the 'id...' order column\" ) \n \n\t\t\tss_r <- order ( DATA$id... ) \n\t\t\tss_c <- colnames ( DATA ) != \"id...\" \n\t\t\tDATA [ ss_r, ss_c ] \t\t\n\t\t } \n \n\t\t # tmp <- function(x) x==1; 1\t# why we must check what to do if it is missing or not... \n\t\t # tmp() \n \n\t\t if ( ! missing ( keep_order ) ) \n\t\t { \n\t\t\t if ( keep_order == 1 ) return ( order. by . id ... and . remove . it ( merge ( x = add. id . column . to . data ( x ) ,y = y,..., sort = FALSE ) ) ) \n\t\t\t if ( keep_order == 2 ) return ( order. by . id ... and . remove . it ( merge ( x = x,y = add. id . column . to . data ( y ) ,..., sort = FALSE ) ) ) \n\t\t\t # if you didn't get \"return\" by now - issue a warning. \n\t\t\t warning ( \"The function merge.with.order only accepts NULL/1/2 values for the keep_order variable\" ) \n\t\t } else { return ( merge ( x = x,y = y,..., sort = sort ) ) } \n\t } \n \n ######### example: \n >  merge ( x. labels , x. vals , by = 'ref' , all. y = T , sort = F ) \n ref label id val\n 1 Ref1 Label01 A1 1.11 \n 2 Ref1 Label01 D4 4.44 \n 3 Ref2 Label02 C2 2.22 \n 4 Ref3  < NA > B3 3.33 \n >  merge. with . order ( x. labels , x. vals , by = 'ref' , all. y = T , sort = F ,keep_order = 1 ) \n ref label id val\n 1 Ref1 Label01 A1 1.11 \n 2 Ref1 Label01 D4 4.44 \n 3 Ref2 Label02 C2 2.22 \n 4 Ref3  < NA > B3 3.33 \n >  merge. with . order ( x. labels , x. vals , by = 'ref' , all. y = T , sort = F ,keep_order = 2 ) # yay - works as we wanted it to... \n ref label id val\n 1 Ref1 Label01 A1 1.11 \n 3 Ref2 Label02 C2 2.22 \n 4 Ref3  < NA > B3 3.33 \n 2 Ref1 Label01 D4 4.44 \n \n \n \n \n Here is a description for how to use the keep_order parameter: \n keep_order can accept the numbers 1 or 2, in which case it will make sure the resulting merged data.frame will be ordered according to the original order of rows of the data.frame entered to x (if keep_order=1) or to y (if keep_order=2). If keep_order is missing, merge will continue working as usual. If keep_order gets some input other then 1 or 2, it will issue a warning that it doesn\u2019t accept these values, but will continue working as merge normally would. Notice that the parameter \u201csort\u201d is practically overridden when using keep_order (with the value 1 or 2).\n \n \n The same code can be used to modify the original merge.data.frame function in base R, so to allow the use of the keep_order, here is a link to the patched merge.data.frame function (on github). If you can think of any ways to improve the function (or happen to notice a bug) please let me know either on github or in the comments. (also saying that you found the function to be useful will be fun to know about ) \n Update : Thanks to KY\u2019s comment, I noticed the ?join function in the {plyr} library. This function is similar to merge (with less features, yet faster), and also automatically keeps the order of the x (first) data.frame used for merging, as explained in the ?join help page: \n Unlike merge, (join) preserves the order of x no matter what join type is used. If needed, rows from y will be added to the bottom. Join is often faster than merge, although it is somewhat less featureful \u2013 it currently offers no way to rename output or merge on different variables in the x and y data frames."], "link": "http://www.r-statistics.com/2012/01/merging-two-data-frame-objects-while-preserving-the-rows-order/", "bloglinks": {}, "links": {"https://github.com/": 1, "http://www.mail-archive.com/": 1}, "blogtitle": "R-statistics blog"}, {"content": ["The followings introductory post is intended for new users of R. \u00a0It deals with the restructuring of data: what it is and how to perform it using base R functions and the {reshape} package. \n This is a guest article by Dr.\u00a0 Robert I. Kabacoff , the founder of (one of) the first online R tutorials websites:\u00a0 Quick-R . Kabacoff has recently published the\u00a0book\u00a0\u201d R in Action \u201c, providing a detailed walk-through for the R language based on\u00a0various examples for illustrating R\u2019s features (data manipulation, statistical methods, graphics, and so on\u2026). The previous guest post by Kabacoff introduced data.frame objects in R . \n For readers of this blog, there is a \u00a038% discount \u00a0off\u00a0 the \u201cR in Action\u201d book \u00a0(as well as all other eBooks, pBooks and MEAPs at\u00a0 Manning publishing house ), simply by using\u00a0the code\u00a0 rblogg38\u00a0 when reaching checkout. \n Let us now talk about the\u00a0Aggregation and Restructuring of data in R: \n \n \n Aggregation and Restructuring \n R provides a number of powerful methods for aggregating and reshaping data. When you aggregate data, you replace groups of observations with summary statistics based on those observations. When you reshape data, you alter the structure (rows and columns) determining how the data is organized. This article describes a variety of methods for accomplishing these tasks. \n We\u2019ll use the mtcars data frame that\u2019s included with the base installation of R. This dataset, extracted from Motor Trend magazine (1974), describes the design and performance characteristics (number of cylinders, displacement, horsepower, mpg, and so on) for 34 automobiles. To learn more about the dataset, see help(mtcars). \n Transpose \n The transpose (reversing rows and columns) is perhaps the simplest method of reshaping a dataset. Use the t() function to transpose a matrix or a data frame. In the latter case, row names become variable (column) names. An example is presented in the next listing. \n Listing 1 Transposing a dataset \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n \n \n \n > cars <- mtcars [ 1 : 5 , 1 : 4 ] \n > cars \n     mpg cyl disp hp\nMazda RX4   21.0  6 160 110 \nMazda RX4 Wag  21.0  6 160 110 \nDatsun 710   22.8  4 108 93 \nHornet 4 Drive  21.4  6 258 110 \nHornet Sportabout 18.7  8 360 175 \n > t ( cars ) \n  Mazda RX4 Mazda RX4 Wag Datsun 710 Hornet 4 Drive Hornet Sportabout\nmpg   21   21    22.8    21.4    18.7 \ncyl   6   6    4.0    6.0     8.0 \ndisp  160   160   108.0   258.0    360.0 \nhp   110   110    93.0    110.0    175.0 \n \n \n \n \n Listing 1 uses a subset of the mtcars dataset in order to conserve space on the page. You\u2019ll see a more flexible way of transposing data when we look at the reshape package later in this article. \n Aggregating data \n It\u2019s relatively easy to collapse data in R using one or more by variables and a defined function. The format is \n \n \n \n \n 1\n \n \n \n aggregate ( x, by , FUN ) \n \n \n \n \n where x is the data object to be collapsed, by is a list of variables that will be crossed to form the new observations, and FUN is the scalar function used to calculate summary statistics that will make up the new observation values. \n As an example, we\u2019ll aggregate the mtcars data by number of cylinders and gears, returning means on each of the numeric variables (see the next listing). \n Listing 2 Aggregating data \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n \n \n \n > options ( digits = 3 ) \n > attach ( mtcars ) \n > aggdata <- aggregate ( mtcars , by = list ( cyl,gear ) , FUN = mean , na. rm = TRUE ) \n > aggdata\n Group.1 Group.2 mpg cyl disp hp drat wt qsec vs am gear carb\n 1   4   3 21.5  4 120 97 3.70 2.46 20.0 1.0 0.00  3 1.00 \n 2   6   3 19.8  6 242 108 2.92 3.34 19.8 1.0 0.00  3 1.00 \n 3   8   3 15.1  8 358 194 3.12 4.10 17.1 0.0 0.00  3 3.08 \n 4   4   4 26.9  4 103 76 4.11 2.38 19.6 1.0 0.75  4 1.50 \n 5   6   4 19.8  6 164 116 3.91 3.09 17.7 0.5 0.50  4 4.00 \n 6   4   5 28.2  4 108 102 4.10 1.83 16.8 0.5 1.00  5 2.00 \n 7   6   5 19.7  6 145 175 3.62 2.77 15.5 0.0 1.00  5 6.00 \n 8   8   5 15.4  8 326 300 3.88 3.37 14.6 0.0 1.00  5 6.00 \n \n \n \n \n In these results, Group.1 represents the number of cylinders (4, 6, or and Group.2 represents the number of gears (3, 4, or 5). For example, cars with 4 cylinders and 3 gears have a mean of 21.5 miles per gallon (mpg). \n When you\u2019re using the aggregate() function , the by variables must be in a list (even if there\u2019s only one). You can declare a custom name for the groups from within the list, for instance, using by=list(Group.cyl=cyl, Group.gears=gear). \n The function specified can be any built-in or user-provided function. This gives the aggregate command a great deal of power. But when it comes to power, nothing beats the reshape package. \n The reshape package \n The reshape package is a tremendously versatile approach to both restructuring and aggregating datasets. Because of this versatility, it can be a bit challenging to learn. \n We\u2019ll go through the process slowly and use a small dataset so that it\u2019s clear what\u2019s happening. Because reshape isn\u2019t included in the standard installation of R, you\u2019ll need to install it one time, using install.packages(\u201creshape\u201d). \n Basically, you\u2019ll \u201cmelt\u201d data so that each row is a unique ID-variable combination. Then you\u2019ll \u201ccast\u201d the melted data into any shape you desire. During the cast, you can aggregate the data with any function you wish. The dataset you\u2019ll be working with is shown in table 1. \n Table 1 The original dataset (mydata) \n \n \n \n \n \n ID \n \n \n \n \n Time \n \n \n \n \n X1 \n \n \n \n \n X2 \n \n \n \n \n 1 \n 1 \n 5 \n 6 \n \n \n 1 \n 2 \n 3 \n 5 \n \n \n 2 \n 1 \n 6 \n 1 \n \n \n 2 \n 2 \n 2 \n 4 \n \n \n \n \n In this dataset, the measurements are the values in the last two columns (5, 6, 3, 5, 6, 1, 2, and 4). Each measurement is uniquely identified by a combination of ID variables (in this case ID, Time, and whether the measurement is on X1 or X2). For example, the measured value 5 in the first row is uniquely identified by knowing that it\u2019s from observation (ID) 1, at Time 1, and on variable X1. \n Melting \n When you melt a dataset, you restructure it into a format where each measured variable is in its own row, along with the ID variables needed to uniquely identify it. If you melt the data from table 1, using the following code \n \n \n \n \n 1\n2\n \n \n \n library ( reshape ) \nmd <- melt ( mydata, id = ( c ( \"id\" , \"time\" ) ) ) \n \n \n \n \n You end up with the structure shown in table 2. \n Table 2 The melted dataset \n \n \n \n \n \n ID \n \n \n \n \n Time \n \n \n \n \n Variable \n \n \n \n \n Value \n \n \n \n \n 1 \n 1 \n X1 \n 5 \n \n \n 1 \n 2 \n X1 \n 3 \n \n \n 2 \n 1 \n X1 \n 6 \n \n \n 2 \n 2 \n X1 \n 2 \n \n \n 1 \n 1 \n X2 \n 6 \n \n \n 1 \n 2 \n X2 \n 5 \n \n \n 2 \n 1 \n X2 \n 1 \n \n \n 2 \n 2 \n X2 \n 4 \n \n \n \n \n Note that you must specify the variables needed to uniquely identify each measurement (ID and Time) and that the variable indicating the measurement variable names (X1 or X2) is created for you automatically. \n Now that you have your data in a melted form, you can recast it into any shape, using the cast() function. \n Casting \n The cast() function starts with melted data and reshapes it using a formula that you provide and an (optional) function used to aggregate the data. The format is \n \n \n \n \n 1\n \n \n \n newdata <- cast ( md, formula , FUN ) \n \n \n \n \n Where md is the melted data, formula describes the desired end result, and FUN is the (optional) aggregating function. The formula takes the form \n \n \n \n \n 1\n \n \n \n rowvar1 + rowvar2 + \u2026 ~ colvar1 + colvar2 + \u2026 \n \n \n \n \n In this formula, rowvar1 + rowvar2 + \u2026 define the set of crossed variables that define the rows, and colvar1 + colvar2 + \u2026 define the set of crossed variables that define the columns. See the examples in figure 1. (click to enlarge the image) \n  \n Figure 1 Reshaping data with the melt() and cast() functions \n Because the formulas on the right side (d, e, and f) don\u2019t include a function, the data is reshaped. In contrast, the examples on the left side (a, b, and c) specify the mean as an aggregating function. Thus the data are not only reshaped but aggregated as well. For example, (a) gives the means on X1 and X2 averaged over time for each observation. Example (b) gives the mean scores of X1 and X2 at Time 1 and Time 2, averaged over observations. In (c) you have the mean score for each observation at Time 1 and Time 2, averaged over X1 and X2. \n As you can see, the flexibility provided by the melt() and cast() functions is amazing. There are many times when you\u2019ll have to reshape or aggregate your data prior to analysis. For example, you\u2019ll typically need to place your data in what\u2019s called long format resembling table 2 when analyzing repeated measures data (data where multiple measures are recorded for each observation). \n Summary \n Chapter 5 of R in Action reviews many of the dozens of mathematical, statistical, and probability functions that are useful for manipulating data. In this article, we have briefly explored several ways of aggregating and restructuring data. \n \n This article first appeared as\u00a0chapter 5.6 from the \u201c R in action \u201c \u00a0book, and is published with permission from\u00a0 Manning publishing house . \u00a0Other books in this serious which\u00a0you might be interested in are (see the beginning of this post for a discount code): \n \n Machine Learning in Action by Peter Harrington \n \n \n Gnuplot in Action (Understanding Data with Graphs) by Philipp K. Janert"], "link": "http://www.r-statistics.com/2012/01/aggregation-and-restructuring-data-from-r-in-action/", "bloglinks": {}, "links": {"http://www.statmethods.net/": 2, "http://www.r-statistics.com/": 3, "http://www.manning.com/": 1, "http://affiliate.manning.com/": 8}, "blogtitle": "R-statistics blog"}, {"content": ["R-bloggers.com is now two years young. The site is an (unofficial) online R journal written by bloggers who agreed to contribute their R articles to the site. \nIn this post I wish to celebrate R-bloggers\u2019 second birthmounth by sharing with you: \n \n Links to the top 20 posts of 2011 \n Statistics on \u201chow well\u201d R-bloggers did this year \n An invitation for sponsors/supporters to help keep the site alive \n \n \n 1. Top 24 R posts of 2011 \n R-bloggers\u2019 success is largely owed to the content submitted by the R bloggers themselves. \u00a0The R community currently has almost 300\u00a0active R bloggers \u00a0(links to the blogs are clearly visible in the right navigation bar on the\u00a0 R-bloggers homepage ). \u00a0In the past year, these bloggers wrote over 2800 posts about R. \n Here is a\u00a0 list of the top visited posts \u00a0on the site in 2011: \n \n How much of r is written in r \n Cpu and gpu trends over time \n Select operations on r data frames \n Getting started with sweave r latex eclipse statet texlipse \n Delete rows from r data frame \n Amanda cox on how the new york times graphics department uses r \n Hipster programming languages \n Opendata r google easy maps \n New r generated video has stackoverflow posting behavior changed over time \n SNA visualising an email box with r \n 100 prisoners 100 lines of code \n Google ai challenge languages used by the best programmers \n Basics on markov chain for parents \n Top 10 algorithms in data mining \n A million random digits review of reviews \n Character occurrence in passwords \n Setting graph margins in r using the par function and lots of cow milk \n The new r compiler package in r 2 13 0 some first experiments \n Tutorial principal components analysis pca in r \n Making guis using c and r with the help of r net \n \n 2. Statistics \u2013 how well did R-bloggers do this year \n There are several\u00a0matrices\u00a0one can consider when evaluating the success of a website. \u00a0I\u2019ll present a few of them here and will begin by talking about the visitors to the site. \n This year,\u00a0 the site was visited by \u00a0over 665,000 \u201cUnique Visitors.\u201d \u00a0There was a total of over 1.4 million visits and over 2.8 million page-views. \u00a0People have surfed the site from over 200 countries, with the greatest number of visitors coming from the United States (~40%) and then followed by the\u00a0United Kingdom (6.9%), Germany (6.6%), Canada (4.7%), France (3.3%), and other countries. \n The site has received between 15,000 to 45,000 visits a week in the past few months, and I suspect this number will remain stable in the next few months (unless something very interesting will happen). \n  \n I believe this number will stay constant thanks to visitors\u2019 loyalty: 55% of the site\u2019s visits came from returning users. \n Another indicator of reader loyalty is the number of subscribers to R-bloggers as counted by feedburner, which includes both RSS readers and e-mail subscribers. \u00a0The range of subscribers is estimated to be between 5600 to 5900. \n Thus, I am very happy to see that R-bloggers continues to succeed in offering a real service to the global R users community. \n 3. Invitation to sponsor/advertise on R-bloggers \n This year I was sadly accused by google adsense of click fraud (which I did not do, but have no way of proving my innocence). \u00a0Therefor, I am no longer able to use google adsense to sustain R-bloggers high monthly bills, and I turned to rely on direct \u00a0sponsoring of R-bloggers. \n If you are interested in sponsoring/placing-ads/supporting R-bloggers, then you are welcome to\u00a0 contact me . \n Happy new year! \nYours, \n Tal Galili"], "link": "http://www.r-statistics.com/2012/01/top-20-r-posts-of-2011-and-some-r-bloggers-statistics/", "bloglinks": {}, "links": {"http://www.r-statistics.com/": 3, "http://www.r-bloggers.com/": 22}, "blogtitle": "R-statistics blog"}, {"content": ["The followings introductory post is intended for new users of R. \u00a0It deals with R data frames: what they are, and how to create, view, and update them. \n This is a guest article by Dr. Robert I. Kabacoff , the founder of (one of) the first online R tutorials websites: Quick-R . \u00a0Kabacoff has recently published the\u00a0book\u00a0\u201d R in Action \u201c, providing a detailed walk-through for the R language based on\u00a0various examples for illustrating R\u2019s features (data manipulation, statistical methods, graphics, and so on\u2026) \n  \n For readers of this blog, there is a \u00a038% discount off the \u201cR in Action\u201d book (as well as all other eBooks, pBooks and MEAPs at Manning publishing house ), simply by using\u00a0the code rblogg38 when reaching checkout. \n Let us now talk about data frames: \n \n \n Data Frames \n \nA data frame is more general than a matrix in that different columns can contain different modes of data (numeric, character, and so on). It\u2019s similar to the datasets you\u2019d typically see in SAS, SPSS, and Stata. Data frames are the most common data structure you\u2019ll deal with in R. \n The patient dataset in table 1 consists of numeric and character data. \n Table 1: A patient dataset \n \n \n \n \n \n PatientID \n \n \n \n \n AdmDate \n \n \n \n \n Age \n \n \n \n \n Diabetes \n \n \n \n \n Status \n \n \n \n \n 1 \n 10/15/2009 \n 25 \n Type1 \n Poor \n \n \n 2 \n 11/01/2009 \n 34 \n Type2 \n Improved \n \n \n 3 \n 10/21/2009 \n 28 \n Type1 \n Excellent \n \n \n 4 \n 10/28/2009 \n 52 \n Type1 \n Poor \n \n \n \n Because there are multiple modes of data, you can\u2019t contain this data in a matrix. In this case, a data frame would be the structure of choice. \n A data frame is created with the data.frame() function: \n \n \n \n \n 1\n \n \n \n mydata <- data. frame ( col1, col2, col3,\u2026 ) \n \n \n \n \n where col1, col2, col3, \u2026 are column vectors of any type (such as character, numeric, or logical). Names for each column can be provided with the names function. \n The following listing makes this clear. \n Listing 1 Creating a data frame \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n \n \n \n > patientID <- c ( 1 , 2 , 3 , 4 ) \n > age <- c ( 25 , 34 , 28 , 52 ) \n > diabetes <- c ( \"Type1\" , \"Type2\" , \"Type1\" , \"Type1\" ) \n > status <- c ( \"Poor\" , \"Improved\" , \"Excellent\" , \"Poor\" ) \n > patientdata <- data. frame ( patientID, age, diabetes, status ) \n > patientdata\n patientID age diabetes status\n 1   1 25  Type1 Poor\n 2   2 34  Type2 Improved\n 3   3 28  Type1 Excellent\n 4   4 52  Type1 Poor \n \n \n \n \n Each column must have only one mode, but you can put columns of different modes together to form the data frame. Because data frames are close to what analysts typically think of as datasets, we\u2019ll use the terms columns and variables interchangeably when discussing data frames. \n There are several ways to identify the elements of a data frame. You can use the subscript notation or you can specify column names. Using the patientdata data frame created earlier, the following listing demonstrates these approaches. \n Listing 2 Specifying elements of a data frame \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n \n \n \n > patientdata [ 1 : 2 ] \n patientID age\n 1   1 25 \n 2   2 34 \n 3   3 28 \n 4   4 52 \n > patientdata [ c ( \"diabetes\" , \"status\" ) ] \n diabetes status\n 1  Type1 Poor\n 2  Type2 Improved\n 3  Type1 Excellent \n 4  Type1 Poor\n > patientdata$age  #age variable in the patient data frame \n [ 1 ] 25 34 28 52 \n \n \n \n \n The $ notation in the third example is used to indicate a particular variable from a given data frame. For example, if you want to cross-tabulate diabetes type by status, you could use the following code: \n \n \n \n \n 1\n2\n3\n4\n5\n \n \n \n > table ( patientdata$diabetes, patientdata$status ) \n \n  Excellent Improved Poor\n Type1   1   0  2 \n Type2   0   1  0 \n \n \n \n \n It can get tiresome typing patientdata$ at the beginning of every variable name, so shortcuts are available. You can use either the attach() and detach() or with() functions to simplify your code. \n attach, detach, and with \n The attach() function adds the data frame to the R search path. When a variable name is encountered, data frames in the search path are checked in order to locate the variable. Using a sample (mtcars) data frame, you could use the following code to obtain summary statistics for automobile mileage (mpg), and plot this variable against engine displacement (disp), and weight (wt): \n \n \n \n \n 1\n2\n3\n \n \n \n summary ( mtcars $mpg ) \n plot ( mtcars $mpg, mtcars $disp ) \n plot ( mtcars $mpg, mtcars $wt ) \n \n \n \n \n This could also be written as \n \n \n \n \n 1\n2\n3\n4\n5\n \n \n \n attach ( mtcars ) \n summary ( mpg ) \n plot ( mpg, disp ) \n plot ( mpg, wt ) \n detach ( mtcars ) \n \n \n \n \n The detach() function removes the data frame from the search path. Note that detach() does nothing to the data frame itself. The statement is optional but is good programming practice and should be included routinely. \n The limitations with this approach are evident when more than one object can have the same name. Consider the following code: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n \n \n \n > mpg <- c ( 25 , 36 , 47 ) \n > attach ( mtcars ) \n \nThe following object ( s ) are masked _by_ \u2018. GlobalEnv \u2019 : mpg\n > plot ( mpg, wt ) \nError in xy. coords ( x, y, xlabel, ylabel, log ) : \n \u2018x\u2019 and \u2018y\u2019 lengths differ\n > mpg\n [ 1 ] 25 36 47 \n \n \n \n \n Here we already have an object named mpg in our environment when the mtcars data frame is attached. In such cases, the original object takes precedence, which isn\u2019t what you want. The plot statement fails because mpg has 3 elements and disp has 32 elements. The attach() and detach() functions are best used when you\u2019re analyzing a single data frame and you\u2019re unlikely to have multiple objects with the same name. In any case, be vigilant for warnings that say that objects are being masked. \n An alternative approach is to use the with() function. You could write the previous example as \n \n \n \n \n 1\n2\n3\n4\n5\n \n \n \n with ( mtcars , { \n summary ( mpg, disp, wt ) \n plot ( mpg, disp ) \n plot ( mpg, wt ) \n } ) \n \n \n \n \n In this case, the statements within the {} brackets are evaluated with reference to the mtcars data frame. You don\u2019t have to worry about name conflicts here. If there\u2019s only one statement (for example, summary(mpg)), the {} brackets are optional. \n The limitation of the with() function is that assignments will only exist within the function brackets. Consider the following: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n \n \n \n > with ( mtcars , { \n stats <- summary ( mpg ) \n stats\n } ) \n Min. 1st Qu. Median Mean 3rd Qu. Max .\n 10.40 15.43 19.20 20.09 22.80 33.90 \n > stats\nError : object \u2018stats\u2019 not found \n \n \n \n \n If you need to create objects that will exist outside of the with() construct, use the special assignment operator <<- instead of the standard one (<-). It will save the object to the global environment outside of the with() call. This can be demonstrated with the following code: \n \n \n \n \n 1\n2\n3\n4\n5\n6\n7\n8\n9\n \n \n \n > with ( mtcars , { \n nokeepstats <- summary ( mpg ) \n keepstats <<- summary ( mpg ) \n } ) \n > nokeepstats\nError : object \u2018nokeepstats\u2019 not found\n > keepstats\n Min. 1st Qu. Median Mean 3rd Qu. Max .\n  10.40 15.43 19.20 20.09 22.80 33.90 \n \n \n \n \n Most books on R recommend using with() over attach(). I think that ultimately the choice is a matter of preference and should be based on what you\u2019re trying to achieve and your understanding of the implications. \n Case identifiers \n In the patient data example, patientID is used to identify individuals in the dataset. In R, case identifiers can be specified with a rowname option in the data frame function. For example, the statement \n \n \n \n \n 1\n2\n \n \n \n patientdata <- data. frame ( patientID, age, diabetes, status,\n row. names = patientID ) \n \n \n \n \n specifies patientID as the variable to use in labeling cases on various printouts and graphs produced by R. \n Summary \n One of the most challenging tasks in data analysis is data preparation. R provides various structures for holding data and many methods for importing data from both keyboard and external sources. One of those structures is data frames, which we covered here. Your ability to specify elements of these structures via the bracket notation is particularly important in selecting, subsetting, and transforming data. \n R offers a wealth of functions for accessing external data. This includes data from flat files, web files, statistical packages, spreadsheets, and databases. Note that you can also export data from R into these external formats. We showed you how to use either the attach() and detach() or with() functions to simplify your code. \n This article first appeared as\u00a0chapter 2.2.4 from the \u201c R in action \u201c \u00a0book, and is published with permission from\u00a0 Manning publishing house ."], "link": "http://www.r-statistics.com/2011/12/data-frame-objects-in-r-via-r-in-action/", "bloglinks": {}, "links": {"http://www.statmethods.net/": 2, "http://affiliate.manning.com/": 6}, "blogtitle": "R-statistics blog"}]