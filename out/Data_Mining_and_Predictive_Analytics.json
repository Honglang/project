[{"blogurl": "http://abbottanalytics.blogspot.com\n", "blogroll": [], "title": "Data Mining and Predictive Analytics"}, {"content": ["Data preparation in data mining and predictive analytics (dare I also say Data Science?) rightfully focuses on how the fields in ones data should be represented so that modeling algorithms either will work properly or at least won't be misled by the data. These data preprocessing steps may involve filling missing values, reigning in the effects of outliers, transforming fields so they better comply with algorithm assumptions, binning, and much more. In recent weeks I've been reminded how important it is to know your records. I've heard this described in many ways, four of which are: the unit of analysis the level of aggregation what a record represents unique description of a record For example, does each record represent a customer? If so, over their entire history or over a time period of interest? In web analytics, the time period of interest may be a single session, which if it is true, means that an individual customer may be in the modeling data multiple times as if each visit or session is an independent event. Where this especially matters is when disparate data sources are combined. If one is joining a table of customerID/Session data with another table with each record representing a customerID, there's no problem. But if the second table represents customerID/store visit data, there will obviously be a many-to-many join resulting in a big mess. This is probably obvious to most readers of this blog. What isn't always obvious is when our assumptions about the data result in unexpected results. What if we expect the unit of analysis to be customerID/Session but there are duplicates in the data? Or what if we had assumed customerID/Session data but it was in actuality customerID/Day data (where ones customers typically have one session per day, but could have a dozen)? The answer is just like we need to perform a data audit to identify potential problems with fields in the data, we need to perform record audits to uncover unexpected record-level anomalies. We've all had those data sources where the DBA swears up and down that there are no dups in the data, but when we group by customerID/Session, we find 1000 dups. So before the joins and after joins, we need to do those group by operations to find examples with unexpected numbers of matches. In conclusion: know what your records are supposed to represent, and verify verify verify. Otherwise, your models (who have no common sense) will exploit these issues in undesirable ways!"], "link": "http://abbottanalytics.blogspot.com/feeds/3596816964105151437/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Within the time allotted for any empirical modeling project, the analyst must decide how to allocate time for various aspects of the process. As is the case with any finite resource, more time spent on this means less time spent on that . I suspect that many modelers enjoy the actual modeling part of the job most. It is easy to try \"one more\" algorithm: Already tried logistic regression and a neural network? Try CART next. Of course, more time spent on the modeling part of this means less time spent on other things. An important consideration for optimizing model performance, then, is: Which tasks deserve more time, and which less? Experimenting with modeling algorithms at the end of a project will no doubt produce some improvements, and it is not argued here that such efforts be dropped. However, work done earlier in the project establishes an upper limit on model performance. I suggest emphasizing data clean-up (especially missing value imputation) and creative design of new features (ratios of raw features, etc.) as being much more likely to make the model's job easier and produce better performance. Consider how difficult it is for a simple 2-input model to discern \"healthy\" versus \"unhealthy\" when provided the input variables height and weight alone. Such a model must establish a dividing line between healthy and unhealthy weights separately for each height. When the analyst uses instead the ratio of weight to height, this becomes much simpler. Note that the commonly used BMI (body mass index) is slightly more complicated than this, and would likely perform even better. Crossing categorical variables is another way to simplify the problem for the model. Though we deal with a process we call \"machine learning\", is is a pragmatic matter to make the job as easy as possible for the machine. The same is true for handling missing values. Simple global substitution using the non-missing mean or median is a start, but think about the spike that creates in the variable's distribution. Doing this over multiple variables creates a number of strange artifacts in the multivariate distribution. Spending the time and energy to fill in those missing values in a smarter way (possibly by building a small model) cleans up the data dramatically for the downstream modeling process."], "link": "http://abbottanalytics.blogspot.com/feeds/5941038578556401865/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["I've called myself a data miner for about 15 years, and the field I was a part of as Data Mining (DM). Before then, I referred to what I did as \"Pattern Recognition\", \"Machine Learning\", \"Statistical Modeling\", or \"Statistical Learning\". In recent years, I've called what I do Predictive Analytics (PA) more often and even co-titled my blog with both Data Mining and Predictive Analytics. That stated, I don't have a good noun to go along with PA. A \"predictive analytist\" (as if I myself were a \"predictor\")? A \"predictive analyzer\"? I often call someone who does PA a Predictive Analytics Professional. But the according to google, the trending on data mining is down. Pattern recognition? Down. Machine Learning? Flat or slightly up. Only Predictive Analytics and it's closely-related sibling, Business Analytics, are up. Even the much-touted Data Science has been relatively flat, though has been spiking Q4 the past few years.    Data Mining    Pattern Recognition    Machine Learning    Predictive Analytics    Business Analytics The big winner? Big Data of course! It has exploded this year. Will that trend continue? It's hard to believe it will continue, but this wave has grown and it seems that every conference related to analytics or databases is touting \"big data\".     Big Data      Data Science  I have no plans of calling what I do \"big data\" or \"data science\". The former term will pass when data gets bigger than big data. The latter may or may not stick, but seems to resonate more with theoreticians and leading-edge types than with practitioners. For now, I'll continue to call myself a data miner and what I do predictive analytics or data mining."], "link": "http://abbottanalytics.blogspot.com/feeds/8936982854004182349/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 2, "http://3.blogspot.com/": 1, "http://1.blogspot.com/": 4}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Every so often, an article or survey will appear stressing the importance of data preparation as an early step in the process of data mining. One often-overlooked part of data preparation is to clearly define the problem, and, in particular, the target variable. Often, a nominal definition of the target variable is given. As an example, a common problem in banking is to predict future balances of a loan customer. The current balance is a matter of record and a host of explanatory variables (previous payment history, delinquency history, etc.) are available for model construction. It is easy to move forward with such a project without considering carefully whether the raw target variable is the best choice for the model to approximate. It may be, for instance, that it is easier to predict the logarithm of balance, due to a strongly skewed distribution. Or, it might be that it is easier to predict the ratio of future balances to the current balance. These two alternatives result in models whose output are easily transformed back into the original terms (by exponentiation or multiplication by the current balance, respectively). More sophisticated targets may be designed to stabilize other aspects of the behavior being studied, and certain other loose ends may be cleaned up as well, for instance when the minimum or maximum target values are constrained. When considering various possible targets, it helps to keep in mind that the idea is to stabilize behavior, so that as many observations as possible align in the solution space. If retail sales include a regular variation, such as by day of the week or month of the year, then that might be a good candidate for normalization: Possibly we want to model retail sales divided by the average for that day of the week, or retail sales divided by a trailing average for that day of the week for the past 4 weeks. Some problems lend themselves to decomposition, such as profit being modeled by predicting revenue and cost separately. One challenge to using multiple models in series this way is that their (presumably independent) errors will compound. Experience indicates that it is difficult in practice to tell which technique will work best in any given situation without experimenting, but performance gains are potentially quite high for making this sort of effort."], "link": "http://abbottanalytics.blogspot.com/feeds/3359995071745036850/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Recently published research, What Makes Paris Look like Paris? , attempts to classify images of street scenes according to their city of origin. This is a fairly typical supervised machine learning project, but the source of the data is of interest. The authors obtained a large number of Google Street View images, along with the names of the cities they came from. Increasingly, large volumes of interesting data are being made available via the Internet, free of charge or at little cost. Indeed, I published an article about classifying individual pixels within images as \"foliage\" or \"not foliage\", using information I obtained using on-line searches for things like \"grass\", \"leaves\", \"forest\" and so forth. A bewildering array of data have been put on the Internet. Much of this data is what you'd expect: financial quotes, government statistics, weather measurements and the like- large tables of numeric information. However, there is a great deal of other information: 24/7 Web cam feeds which are live for years, news reports, social media spew and so on. Additionally, much of the data for which people once charged serious bucks is now free or rather inexpensive. Already, many firms augment the data they've paid for with free databases on the Web. An enormous opportunity is opening up for creative data miners to consume and profit from large, often non-traditional, non-numeric data which are freely available to all, but (so far) creatively analyzed by few."], "link": "http://abbottanalytics.blogspot.com/feeds/7989574061939665598/comments/default", "bloglinks": {}, "links": {"http://graphics.cmu.edu/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Applying inferential statistics to criminology is not new, but it appears that the market has been maturing. See, for instance, a recent article, \"Police using \u2018predictive analytics\u2019 to prevent crimes before they happen\" , published by Agence France-Presse on The Raw Story (Jul-29-2012). Setting aside obvious civil liberties questions, consider the application of this technology. My suspicion is that targeting police efforts by geographic locale and day-of-week/time-of-day using this approach will decrease the overall level of crime, but by how much is not clear. This is typical of problems faced by businesses: It is not enough to predict what we already know, nor is it enough to trot out glowing but artificial technical measures of performance. Knowledge that real improvement has occurred requires more. For instance, at least some effect of police effort on the street does not decrease crime, but merely moves to new locations. Were I mayor of a small town approached by the vendor of such a solution, I'd want to see some sort of experimental design which made apples-to-apples comparison between our best estimates of what happens with the new tool, and what happens without it. Only once this firm measure of benefit has been obtained could one reasonably weigh it against the financial and political costs."], "link": "http://abbottanalytics.blogspot.com/feeds/8231445947986807967/comments/default", "bloglinks": {}, "links": {"http://www.rawstory.com/": 2}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["I came across this photo today and couldn't resist.  John Elder and I worked at the company Barron Associates, Inc. (BAI) in Charlottesville, VA in the 80s (John was employee #1). Hopefully you can identify John in the back right and me in the front right, though it will take some good pattern matching to do so! The Founder and President of the company was Roger Barron. Both John and I were introduced to statistical learning methods at BAI and of course went on to careers in the field now known as Data Mining or Predictive Analytics (among other things). I write about my experience with BAI in the forthcoming book Journeys to Data Mining: Experiences from 15 Renowned Researchers , Ed. by Dr Mohamed Medhat Gaber, published by Springer. Authors in the book include John (thanks to John for recommending my inclusion in the book), Gregory Piatetsky-Shapiro, Mohammed J. Zaki, and of course several others. The photo appeared as I was searching for descriptions of our field back in the 80s and was looking in particular for the Barron and Barron paper \" Statistical Learning Networks: A Unifying View \", where \"Statistical Learning Networks\" was the phrase of interesting, along with \"Models\", \"Classifiers\", and \"Neural Networks\". We used to refer to the field as \"Pattern Recognition\" and \"Artificial Intelligence\". It's interesting to note that pattern recognition on Wikipedia contains a list of \"See Also\" terms that includes the more modern terms such as data mining and predictive analytics. I will post within a couple days on the pattern recognition terms of the day and how they are changing."], "link": "http://abbottanalytics.blogspot.com/feeds/2586639083645339978/comments/default", "bloglinks": {}, "links": {"http://www.barron-associates.com/": 1, "http://3.blogspot.com/": 1, "http://www.springer.com/": 1, "http://www.yale.edu/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["The New York Times Magazine article \" How Companies Learn Your Secrets \" by Charles Duhigg with the key descriptions of Target, pregnancy, predictive analytics (blogged on here and here ) certainly generated a lot of buzz; if you are unable to see the NYTimes Magazine article, the Forbes summary is a good summary. However, few know that Eric Siegel booked Andy Pole for the October 2010 Predictive Analytics World conference as a keynote speaker. The full video of that talk is here . In this talk, Mr. Pole discussed how Target was using Predictive Analytics including descriptions of using potential value models, coupon models, and...yes...predicting when a woman is due (if you aren't the patient type, it is at about 34:30 in the video). These models were very profitable at Target, adding an additional 30% to the number of woman suspected or known to be pregnant over those already known to be (via self-disclosure or baby registries). The fact that this went on for over a year after the Predictive Analytics World talk and before the fallout tells me that it didn't cause significant problems for Target prior to the attention brought to the subject related to the NYTimes article. After watching the talk, what struck me most was that Target was applying a true \"360 deg\" customer view by linking guests through store visits, web, email, and mobile interactions. In addition, close attention was paid to linking the interactions so that coupons made sense: they didn't print coupons to those who had just purchased items they score high for couponing, and they identify which mediums don't generate responses and stop using those channels. I suspect what Target is doing is no different than most retailers, but this talk was an interesting glimpse into how much they value the different channels and try to find ways to provide customers with what they are most interested in, and suppress what they are not interested in."], "link": "http://abbottanalytics.blogspot.com/feeds/3310413943604233328/comments/default", "bloglinks": {}, "links": {"http://www.predictiveanalyticsworld.com/": 1, "http://abbottanalytics.blogspot.com/": 2, "http://www.nytimes.com/": 1, "http://rmportal.performedia.com/": 1, "http://www.forbes.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["This past week at Predictive Analytics World / Toronto (PAW) has been a great time for connecting with thought leaders and practitioners in the field. Sometimes there are unexpected pleasures as well, which is certainly the case this time. One of the exhibitors for the eMetrics conference, co-locating with PAW at the venue, was Unilytics , a web analytics company. At their booth there was a cylindrical container filled with crumpled dollar bills with a sign soliciting predictions of how many dollar bills were in the container (the winner getting all the dollars). After watching the announcement of the winner, who guessed $352, only $10 off from the actual $362, I thought this would be the perfect opportunity for another Wisdom of Crowds test,just like the one conducted 9 months ago and blogged on here .   Two Unilytics employees at the booth, Gary Panchoo and Keith MacDonald, were kind enough to indulge me and my request to compute the average of all the guesses. John Elder was also there, licking his wounds from finished a close second as his guess, $374 was off by $12, a mere $2 away from the winning entry! The results of the analysis are here (summary statistics created by JMP Pro 10 for the mac). In summary, the results are as follows: Dollar Bill Guess Scores Method Guess Value Error Actual 362  Ensemble/Average (N=61) 365 3 Winning Guess (person) 352 10 John Elder 374 12 Guess without outlier (2000), 3rd place 338 24 Median, 19th place 275 87  So once again, the average of the entries (the \"Crowds\" answer) beat the single best entry. What is fascinating to me about this is not that the average won (though this in of itself isn't terribly surprising), but rather how it won. Summary statistics are below. Note that the Median is 275, far below the mean. Not too how skewed the distribution of guesses are (skew = 3.35). The fact that the guesses are skewed positively for a relatively small answer (362) isn't a surprise, but the amount of skew is a bit surprising to me. What these statistics tell us is that while the mean value of the guesses would have been the winner, a more robust statistic would not, meaning that the skew was critical in obtaining a good guess. Or put another way, people more often than not under-guessed by quite a bit (the median is off by 87). Or put a third way, the outlier (2000) which one might naturally want to discount because it was a crazy guess was instrumental to the average being correct. In the prior post on this from July 2011, I trimmed the guesses, removing the \"crazy\" ones. So when should we remove the wild guesses and when shouldn't we? (If I had removed the 2000, the \"average\" still would have finished 3rd). I have no answer to when the guesses are not reasonable, but wasn't inclined to remove the 2000 initially here. Full stats from JMP are below, with the histogram showing the amount of skew that exists in this data. Distribution of Dollar Bill Guesses - Built with JMP Summary Statistics Statistic Guess Value Mean 365 Std Dev 299.80071 Std Err Mean 38.385548 Upper 95% Mean 441.78253 Lower 95% Mean 288.21747 N 61 Skewness 3.3462476 Median 275 Mode 225 2% Trimmed Mean 331.45614 Interquartile Range 185.5 Note: The mode shown is the smallest of 2 modes with a count of 3. Quantiles Quantile Guess Value 100.0% maximum 2000 99.5% 2000 97.5% 1546.8 90.0% 751.6 75.0% quartile 406.5 50.0% median 275 25.0% quartile 221 10.0% 145.6 2.5% 98.2 0.5% 96 0.0% minimum 96"], "link": "http://abbottanalytics.blogspot.com/feeds/7921607058871493431/comments/default", "bloglinks": {}, "links": {"http://jmp.com/": 1, "http://www.predictiveanalyticsworld.com/": 1, "http://abbottanalytics.blogspot.ca/": 1, "http://unilytics.com/": 2, "http://1.blogspot.com/": 1, "http://4.blogspot.com/": 1, "http://www.amazon.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Ruben's comment that referred to spam reminded me of an old Dilbert comic which conveys the misconception about database marketing (e-marketing) and spam.  I know Ruben well and know he was poking fun, though I still have to correct folks who after finding out I do \"data mining\" actually comment that I'm responsible for spam. Answer: \"No, I'm the reason you don't get as much spam!\""], "link": "http://abbottanalytics.blogspot.com/feeds/7597606632613430602/comments/default", "bloglinks": {}, "links": {"http://dilbert.com/": 2, "http://abbottanalytics.blogspot.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Sometimes folks ask me what I'm doing, so I thought I'd share a few things on my plate right now: Courses and Conferences 1. Reading several papers for the KDD 2012 Conference Industrial / Government Track 2. Preparing for the Predictive Analytics World / Toronto \" Advanced Methods Hands-on: Predictive Modeling Techniques \" workshop on April 27. I'm using the Statsoft Statistica package. 3. Starting preparation for a talk at the Salford Analytics and Data Mining Conference 2012 , \" A More Transparent Interpretation of Health Club Surveys \" on May 24. It will highlight use of the CART software package in the analysis. This was work that motivated interviews with New York Times reporter Charles Duhigg , and ended with a mention (albeit *very* briefly) in the fascinating new book by Duhigg, \" The Power of Habit: Why We Do What We Do in Life and Business \". 4. Working through data exercises for the next UCSD-Extension Text Mining Course on May 11, 18, and 25th. I'm using KNIME for this course. Consulting Approximately 80% of my time is spent on active consulting. While I can't describe most of the work I'm doing, my current clients are in the following domains: 1. Web Analytics and email remarketing for retail via a great startup company, Smarter Remarketer headed by Angel Morales (Founder and Chief Innovation Officer), Howard Bates (CEO), and me (Founder and Chief Scientist). 2. Customer Acquisition, web/online and offline (2 clients) 3. Tax Modeling (2 clients) 4. Data mining software tool selection for a large health care provider. Here's to productive application of predictive analytics!"], "link": "http://abbottanalytics.blogspot.com/feeds/1677792147286778148/comments/default", "bloglinks": {}, "links": {"http://www.predictiveanalyticsworld.com/": 1, "http://charlesduhigg.com/": 1, "http://www.salforddatamining.com/": 2, "http://www.smarterhq.com": 1, "http://predictiveanalyticsworld.com/": 1, "http://extension.ucsd.edu/": 1, "http://www.amazon.com/": 1, "http://sigkdd.org/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Every data mining project begins with defining what problem will be solved. I won't describe the CRISP-DM process here, but I use that general framework often when working with customers so they have an idea of the process. Part of the problem definition is defining the target variable. I argue that this is the most critical step in the process that relates to the data, and more important than data preparation, missing value imputation, and the algorithm that is used to build models, as important as they all are. The target variable carries with it all the information that summarizes the outcome we would like to predict from the perspective of the algorithms we use to build the predictive models. Yet this can be misleading is many ways. I'm addressing one way we can be fooled by the target variable here, and please indulge me to lead you down the path. Let's say we are building fraud models in our organization. Let's assume that in our organization, the process for determining fraud is first to identify possible fraud cases (by tips or predictive models), then assign the case to a manager who determines which investigator will get the case (assuming the manager believes there is value in investigating the case), then assign the case to an investigator, and if fraud is found, the case is tried in court, and ultimately a conviction is made or the party is found not guilty. Our organization would like to prioritize which cases should be sent to investigators using predictive modeling. It is decided that we will use as a target variable all cases that were found to be fraudulent, that is, all cases that had been tried and a conviction achieved. Let's assume here that all individuals involved are good at their jobs and do not make arbitrary or poor decisions (which of course is also a problem!) Let's also put aside for a moment the time lag involved here (a problem itself) and just consider the conviction as a target variable. What does the target variable actually convey to us? Of course our desire is that this target variable conveys fraud risk. Certainly when the conviction has occurred, we have high confidence that the case was indeed fraudulent, so the \"1\"s are strong and clear labels for fraud. But, what about the \"0\"s? Which cases do they include? --cases never investigated (i.e., we suspect they are not fraud, but don't know) --cases assigned to a manager who never assigned the case (he/she didn't think they were worth investigating). --cases assigned to an investigator but the investigation has not yet been completed, or was never completed, or was determined not contain fraud --cases that went to court but was found \"not guilty\" Remember, all of these are given the identical label: \"0\" That means that any cases that look on the surface to be fraudulent, but there were insufficient resources to investigate them, are called \"not fraudulent. That means cases that were investigated but the investigator was taken off the case to investigate other cases are called \"not fraudulent\". It means too that court cases that were thrown out of court due to a technicality unrelated to the fraud itself are called \"not fraud\". In other words, the target variable defined as only the \"final conviction\" represents not only the risk of fraud for a case, but also the investigation and legal system. Perhaps complex cases that are high risk are thrown out because they aren't (at this particular time, with these particular investigators) worth the time. Is this what we want to predict? I would argue \"no\". We want our target variable to represent the risk, not the system. This is why when I work on fraud detection problems, the definition of the target variable takes time: we have to find measures that represent risk and are informative and consistent, but don't measure the system itself. For different customers this means different trade-offs, but usually it means using a measure from earlier in the process. So in summary, think carefully about the target variable you are defining, and don't be surprised when your predictive models predict exactly what you told them to!"], "link": "http://abbottanalytics.blogspot.com/feeds/5247622072255095889/comments/default", "bloglinks": {}, "links": {"ftp://ftp.software.ibm.com/software/analytics/spss/support/Modeler/Documentation/14/UserManual/CRISP-DM.pdf": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["This is part II of my thoughts on the New York Times article \"How Companies Learn Your Secrets\" . In the first post, I commented on the quote \u201cIt\u2019s like an arms race to hire statisticians nowadays,\u201d said Andreas Weigend, the former chief scientist at Amazon.com. \u201cMathematicians are suddenly sexy.\u201d Comments on this can be seen in Part I here . In this post, the next portion of the article I found fascinating can be summarized by the section that says Habits aren\u2019t destiny \u2014 they can be ignored, changed or replaced. But it\u2019s also true that once the loop is established and a habit emerges, your brain stops fully participating in decision-making. So unless you deliberately \ufb01ght a habit \u2014 unless you \ufb01nd new cues and rewards \u2014 the old pattern will unfold automatically. Habits are what predictive models are all about. Or putting as a question, \"is customer behavior predictable based on their past behavior?\" The Frawley, Piatetsky-Shapiro, Mattheus definition of knowledge discovery in databases (KDD) as follows: Knowledge discovery is the nontrivial extraction of implicit, previously unknown,and potentially useful information from data. (PDF of the paper can be found here ) This quote has often been applied to data mining and predictive analytics, and rightfully so. We believe there are patterns hidden in the data and want to characterize those patterns with predictive modeols. Predictive models usually work best when individuals don't even realize what they are doing so we can capture their behavior solely based on what they want to do rather than behavior influence by how they want to be perceived, which is exactly how the Target models were built. So what does this have to do with the NYTimes quote? The \"habits\" that \"unfold automatically\" as described in the article was fascinating precisely because predictive models rely on habits; we wish to make the connection between past behavior and expected result as captured in the data that are consistent and repeatable (that is, habitual!). These expected results could be \"is likely to respond to a mailing\", \"is likely purchase a product online\", \"is likely to commit fraud\", or in the case of the article, \"is likely to be pregnant\". Duhigg (and presumably Pole describing it to Duhigg) characterizes this very well. The behavior Target measured was shoppers purchasing behavior when they were to give birth some weeks or months in the future, and nothing more. They had to apply broadly to thousands of \"Guest IDs\" for models to work effectively. The description of what Andy Pole did for target is an excellent summary of what predictive modelers can and should do. The approach included domain knowledge, understanding of what predictive models can do, and most of all a forensic mindset. I quote again from the article: \"Target has a baby-shower registry, and Pole started there, observing how shopping habits changed as a woman approached her due date, which women on the registry had willingly disclosed. He ran test after test, analyzing the data, and before long some useful patterns emerged. Lotions, for example. Lots of people buy lotion, but one of Pole\u2019s colleagues noticed that women on the baby registry were buying larger quantities of unscented lotion around the beginning of their second trimester. Another analyst noted that sometime in the first 20 weeks, pregnant women loaded up on supplements like calcium, magnesium and zinc. Many shoppers purchase soap and cotton balls, but when someone suddenly starts buying lots of scent-free soap and extra-big bags of cotton balls, in addition to hand sanitizers and washcloths, it signals they could be getting close to their delivery date.\" (emphases mine) To me, the key descriptive terms in the quote from the article are \" observed \", \" noticed \" and \" noted \". This means the models were not built as black boxes; the analysts asked \"does this make sense?\" and leveraged insights gained from the patterns found in the data to produce better predictive models. It undoubtedly was iterative; as they \"noticed\" patterns, they were prompted to consider other patterns they had not explicitly considered before (and maybe had not even occurred to them before). But it was these patterns that turned out to be the difference-makers in predicting pregnancy. So after all my preamble here, the key take-home messages from the article are: 1) understand the data, 2) understand why the models are focusing on particular input patterns, 3) ask lots of questions (why does the model like these fields best? why not these other fields?) 4) be forensic (now that's interesting or that's odd...I wonder...), 5) be prepared to iterate, (how can we predict better for those customers we don't characterize well) 6) be prepared to learn during the modeling process We have to \"notice\" patterns in the data and connect them to behavior. This is one reason I like to build multiple models: different algorithms can find different kinds of patterns. Regression is a global predictor (one continuous equation for all data), whereas decision trees and kNN are local estimators. So we shouldn't be surprised that we will be surprised, or put another way, we should expect to be surprised. The best models I've built contain surprises, and I'm glad they did!"], "link": "http://abbottanalytics.blogspot.com/feeds/5655764893300528372/comments/default", "bloglinks": {}, "links": {"http://abbottanalytics.blogspot.com/": 1, "http://www.nytimes.com/": 1, "http://www.kdnuggets.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["There have been a plethora of tweets about the New York Times article \"How Companies Learn Your Secrets\" , mostly focused on the story of how Target can predict if a customer is pregnant. The tweets I've seen on this most often have a reaction that this is somewhat creepy or invasive. I may write more on this topic at some future time (which probably means I won't!) because I don't find it creepy at all that a company would try to understand my behavior and infer the cause of that behavior. But I digress\u2026 The parts of the article I find far more interesting include these: \u201cIt\u2019s like an arms race to hire statisticians nowadays,\u201d said Andreas Weigend , the former chief scientist at Amazon.com. \u201cMathematicians are suddenly sexy.\u201d and Habits aren\u2019t destiny \u2014 they can be ignored, changed or replaced. But it\u2019s also true that once the loop is established and a habit emerges, your brain stops fully participating in decision-making. So unless you deliberately \ufb01ght a habit \u2014 unless you \ufb01nd new cues and rewards \u2014 the old pattern will unfold automatically. Part I will address the first question, and next week I'll post the second, much longer part. First, mathematics and predictive analytics\u2026 The first quote is a tremendous statement and one that all of us in the field should take notice of. While college students enrollment with STEM majors continues to decline, we have fewer and fewer candidates (as a percentage) to choose from. But I don't think this is necessarily hopeless. I just finished teaching a text mining course, and one woman in the course told me that she never liked mathematics, yet it was obvious that she not only did data mining, but she understood it and was able to use the techniques successfully. There is something different about statistics, data mining and predictive analytics. It isn't math, it's forensic. It's a like solving a puzzle rather than proving a theorem or solving for \"x\". Almost every major retailer, from grocery chains to investment banks to the U.S. Postal Service, has a \u201cpredictive analytics\u201d department devoted to understanding not just consumers\u2019 shopping habits but also their personal habits, so as to more efficiently market to them. Really? I appreciate the statement of how widespread predictive analytics is. But I think it overstates the case. I've personally done work for retailers and other major organizations without predictive analytics departments. Now they may have several individuals who are analysts, but they aren't organized as a department. More often, they are part of the \"marketing\" department with an \"analyst\" title. This matters because collaboration is key in building predictive models well. One thing I try to encourage with all of my customers is building a collaborate environment where ideas, insights, and lessons learned are exchanged. With most customers, this is something they already do or are eager to do. With a few it has been more challenging. \u201cBut Target has always been one of the smartest at this,\u201d says Eric Siegel, a consultant and the chairman of a conference called Predictive Analytics World . \u201cWe\u2019re living through a golden age of behavioral research. It\u2019s amazing how much we can figure out about how people think now.\u201d I completely agree with Eric that we live in a world now where we finally have enough data, enough accessible data, the technical ability, and the interest in understanding that data. These are indeed good times to be in predictive analytics! We need both kinds of analysts: the mathematically astute one, and those that don't care about the match, but understand deeply how to build and use predictive models. We need to develop both kinds of analysts, but there are far more of the latter, and they can do the job."], "link": "http://abbottanalytics.blogspot.com/feeds/6691242882961042174/comments/default", "bloglinks": {}, "links": {"http://www.nytimes.com/": 1, "http://predictiveanalyticsworld.com/": 1, "http://www.weigend.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["By far, the most visited post of 2011 was the \" What Do Data Miners Need to Learn \" post from June. The top five visited posts that were first posted in 2011 are (with actual ranks for all posts): 1. What Do Data Miners Need to Learn 2. Statistical Rules of Thumb, Part III 3. Statistical Rules of Thumb, Part II 4. Number of Hidden Layer Neurons to Use 5. Statistics: The Need for Integration The top six viewed posts in 2011 originally created prior to 2011 were: 1. Why Normalization Matters with K-Means (2009) 2. Free and Inexpensive Data Mining Software (2006) 3. Data Mining Data Sets (2008) 4. Can you Learn Data Mining in Undergraduate or Graduate School (2009) 5. Quotes from Moneyball (2007) 6. Business Analytics vs. Business Intelligence (2009) The \"Free Data Mining Tools\" post is understandably relatively popular, even after 5 years. The Moneyball quotes has a particularly high bounce rate. I'm most surprised that the K-Means normalization post has remained popular for so long."], "link": "http://abbottanalytics.blogspot.com/feeds/4047565637769503625/comments/default", "bloglinks": {}, "links": {"http://abbottanalytics.blogspot.com/": 12}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["I just read a fascinating book review in the Wall Street Journal Physics Envy: Models Behaving Badly . The author of the book, Emanuel Derman (former head of Quantitative Analsis at Goldman Sachs) argues that the financial models involved human beings and therefore were inherently brittle: as human behavior changed, the models failed. \"in physics you're playing against God, and He doesn't change His laws very often. In finance, you're playing against God's creatures.\" I'll agree with Derman that whenever human beings are in the loop, data suffers. People change their minds based on information not available to the models. I also agree that human behavioral modeling is not the same as physical modeling. We can use the latter to provide motivation and even mathematics for human behavioral modeling, but we should not take this too far. A simple example is this: purchase decisions sometimes depend not on the person's propensity to purchase alone, but also on whether or not they had an argument that morning, or if they just watched a great movie. There is an emotional component that data cannot reflect. People therefore behave in ways that on the surface are contradictory, seemingly \"random\", which is way response rates of 1% can be \"good\". However, I bristle a bit at the the emphasis on the physics analogy. In closed systems, models can explain everything. But once one opens up the world, even physical models are imperfect because they often do not incorporate all the information available. For example, missile guidance is based on pure physics: move a surface on a wing and one can change the trajectory of the missile. There are equations of motion that describe exactly where the missile will go. There is no mystery here. However, all operational missile guidances systems are \"closed loop\"; the guidance command sequence is not completely scheduled but is updated throughout the flight. Why? To compensate for unexpected effects of the guidance commands, often due to ballistic winds, thermal gradients, or other effects on the physical system. It is the closed-loop corrections that make missile guidance work. The exact same principal applies to your car's cruise control, chasing down a fly ball in baseball, or even just walking down the street. For a predictive model to be useful long-term, it needs updating to correct for changes in the population the models are applied to, whether the models be for customer acquisition, churn, fraud detection, or any model. The \"closed-loop\" typical in data mining is called \"model updating\" and is critical for long-term modeling success. The question then becomes this: can the models be updated quickly enough to compensate for changes in the population? If a missile can only be updated at 10Hz (10x / sec.) but uncertainties effect the trajectory significantly in milliseconds, the closed-loop actions may be insufficient to compensate. If your predictive can only be updated monthly, but your customer behavior changes significantly on a weekly basis, your models will be behind perpetually. Measuring the effectiveness of model predictions is therefore critical in determining the frequency of model updating necessary in your organization. To be fair, until I read the book I have no quibble with the arguments. The arguments here are based solely on the book review and some ideas they prompted in my mind. I'd welcome comments from anyone who has read the book already. The book can be found on amazon here . UPDATE: Aaron Lai wrote an article for CFA Magazine on the same topic, also quoting Derman. I commend the article to all (note: this is a PDF file download)."], "link": "http://abbottanalytics.blogspot.com/feeds/7559753344179494645/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://online.wsj.com/": 1, "https://sites.google.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["As I perused Statistical Rules of Thumb again, as I do from time to time, I came across this gem. (note: I live in CA, so get no money from these amazon links). Van Belle uses the term \"Graph\" rather than \"Visualize\", but it is the same idea. The point is to visualize in addition to computing summary statistics. Summaries are useful, but can be deceiving; any time you summarize data you will lose some information unless the distributions are well behaved. The scatterplot, histogram, box and whiskers plot, etc. can reveal ways the summaries can fool you. I've seen these as well, especially variables with outliers or that are bi- or tri-modal. One of the most famous examples of this effect is Anscombe's Quartet . I'm including the Wikipedia image of the plots here:   All four datasets have the same mean x values, y values, x standard deviation, y standard deviation, x-y pearson correlation coefficient, and regression line of y, so the summaries don't tell the differences in the data. I use correlations a lot to get the gist of the relationships in the data, and I've seen how correlations can deceive. In one project, we had 30K data points with a correlation of 0.9+. When we removed just 100 of these data points (the largest magnitudes of x and y), the correlation shrunk to 0.23. Most data mining software has ways to visualize data easily now. Avail yourself to them to avoid subsequent surprises in your data."], "link": "http://abbottanalytics.blogspot.com/feeds/7514769420610119420/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://2.blogspot.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["I was at the Federal Building downtown San Diego for a consulting job, and met some representatives for a life and disability insurance company who were giving away a big-screen HD TV for the individual who came closest to guessing the number of M&Ms (chocolate and peanut butter filled) in a container. Because they do this often, I won't show the specific container they use. I offered to make a guess of the total, but only if I could see all of the guesses so far. I was drawing from the Wisdom of Crowds example from Chapter 1 of the book where a set of independent guesses tend to outperform even an expert's best guess. I've done the same experiment many times in data mining courses I've taught, and have found the same phenomenon. I collected data from 77 individuals (including myself) shown here (sorted for convenience, but this makes no difference in the analysis): 37 625 772 784 875 888 903 929 983 987 1001 1015 1040 1080 1080 1124 1245 1250 1450 1500 1536 1596 1600 1774 1875 1929 1972 1976 1995 2000 2012 2033 2143 2150 2200 2221 2235 2251 2321 2331 2412 2500 2500 2550 2571 2599 2672 2714 2735 2777 2777 2803 2832 2873 2931 3001 3101 3250 3333 3362 3500 3500 3501 3501 3583 3661 3670 3697 3832 3872 4280 4700 4797 5205 5225 5257 9886 10000 187952 Note there are a few flakey ones in the lot. The last two were easy to spot (so I put them at the bottom of my list). The idea of course is to just take the average of the guesses. Average all: 4932 Average all without 37 and 187932: 2626 Then I looked at the histogram and decided that the guesses close to 10000 were also too flaky to include:  So I removed all data points greater than 8000, which took away 2 samples, leaving this histogram and a mean of 2436.  So now for the outcome: Actual Count: 2464 Average of trimmed sample: 2436 (error 28) Best individual guess: 2500 (error 36) So amazingly, the average won, though I wouldn't have been disappointed at all if it finished 3rd or 4th because it still would have been a great guess. Wisdom of Crowds wins again! PS I reported to the insurance agents a guess of 2423 because I had omitted my original guess (provided before looking at any other guesses--2550 if you must know) and my co-worker's guess of 3250, so these helped bring up the mean a bit. The Average would have lost (barely) if I had not included them. PPS So how will they split the winnings since two guessed the same value? I won't recommend the saw approach. I hope they ask each of the two guessers to either modify their guess, and require they modify their guess by at least one. PPPS Note: the charts were done using JMP Pro 9 for the Macintosh"], "link": "http://abbottanalytics.blogspot.com/feeds/2516252329487969788/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 1, "http://2.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://www.jmp.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["I've been asked by several folks recently what they need to learn to succeed in data mining and predictive analytics. This is a different twist on the question I also get, namely what degree should one get to be a good (albeit \"green\") data miner. Usually, the latter question gets the answer \"it doesn't matter\" because I know so many great data miners without a statistics or mathematics degree. Understandably, there are many non-stats/math degrees that have a very strong statistics or mathematics component, such as psychology, political science, and engineering to name a few. But then again, you don't necessarily have to load up on the stats/math courses in these disciplines either. So the question of \"what to learn\" applies across majors whether undergraduate or graduate. Of course statistics and machine learning courses are directly applicable. However, the answer I've been giving recently to the question what do new data miners need to learn (assuming they will learn algorithms) have centered around two other topics: databases and business. I had no specific coursework or experience in either when I began my career. In the 80s, databases were not as commonplace in the DoD world where I began my career; we usually worked with flat files provided to us by a customer, even if these files were quite large. Now, most customers I work with have their data stored in databases or data marts, and as a result, we data miners often must lean on DBAs or an IT layer of people to get at the data. This would be fine except that (1) the data that is provided to data miners is often not the complete data we need or at least would like to have before building models, (2) we sometimes won't know how valuable data is until we look at it, and (3) communication with IT is often slow and laden with political issues inherent in many organizations. On the other hand, IT is often reticent to give analysts significant freedom to query databases because of the harm they can do (wise!) because data miners have in general a poor understanding of how databases work and which queries are dangerous or computationally expensive. Therefore, I am becoming more of the opinion that a masters program in data mining, or a data mining certificate program should contain at least one course on databases, which should contain at least some database design component, but for the most part should emphasize a users perspective). It is probably more realistic to require this for a degree than a certificate, but could be included in both. I know that for me, in considering new hires, this would be provide a candidate an advantage for me if he or she had SQL or SAS experience. For the second issue, business experience, there are some that might be concerned that \"experience\" is too narrow for a degree program. After all, if someone has experience in building response models, what good would that do for Paypal if they are looking for building fraud models? My reply is \"a lot\"! Building models on real data (meaning messy) to solve a real problem (meaning identifying a target variable that conveys the business decision to be improved) requires a thought process that isn't related to knowing algorithms or data. Building \"real-world\" models requires a translation of business objectives to data mining objectives (as described in the Business Understanding section of CRISP-DM , pdf here ). When I have interviewed young data miners in the past, it is those who have had to go through this process that are better prepared to begin the job right away, and it is those who recognize the value here who do better at solving problems in a way that impacts decisions rather than finding cool, innovative solutions that never see the light of day. (UPDATE: the crisp-dm.org site is no longer up--see comments section. The CRISP-DM 1.0 document however can still be downloaded here , with higher resolution graphics, by the way!) My challenge to the universities who are adding degree programs in data mining and predictive analytics, or are offering Certificate programs is then to include courses on how to access data (databases), and how to solve problems (business objectives, perhaps by offering a practicum with a local company)."], "link": "http://abbottanalytics.blogspot.com/feeds/1809764257291892486/comments/default", "bloglinks": {}, "links": {"http://www.crisp-dm.org/": 1, "ftp://ftp.software.ibm.com/software/analytics/spss/support/Modeler/Documentation/14/UserManual/CRISP-DM.pdf": 1, "http://www.crisp-dm.org": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["In the linkedin.com Artificial Neural Networks group, a question arose about how many hidden neurons one should choose. I've never found a fully satisfactory answer to this, but there is quite a lot of guesses and rules of thumb out there. I've always like Warren Sarle's neural network FAQ that includes a discussion on this topic. There is another reference on the web that I agree with only about 50%, but the references are excellent: http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html . My personal preference is to use software that experiments with multiple architectures and selects the one that performs best on held-out data. Better still are the algorithms that also select (i.e. prune) inputs as well. As I teach in my courses, I've spent far too many hours in my life selection neural network architectures and re-training, so I'd much rather let the software do it for me."], "link": "http://abbottanalytics.blogspot.com/feeds/2384287810495853873/comments/default", "bloglinks": {}, "links": {"http://linkd.in/": 1, "ftp://ftp.sas.com/pub/neural/FAQ3.html#A_hu": 1, "http://www.faqs.org/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["A while back, Will Dwinnell posted on two books, one of which is one of my favorites as well:  Will mentioned a few general topics covered in the book, but I thought I would mention two specific ones that I agree with wholeheartedly. 7.3: Always Graph the Data In this section he quotes E.R. Tufte as follows (Abbott quoting van Belle quoting Tufte): Graphical Excellence is that which gives the viewer the greatest number of ideas in the shortest time with the least ink in the shortest space. I'm not so sure I agree with the superlatives, I certainly agree with the gist that excellence in graphics is parsimonious, clear, insightful, and informationally rich. Contrast this to another rule of thumb: 7.4: Never use a Pie Chart well, that's not exactly rocket science; pie charts have lots of detractors...The only thing worse than a pie chart is a 3-D pie chart! 7.6: Stacked Barcharts are Worse than Bargraphs. Perhaps the biggest problem with stacked bar graphs (such as the one here) is that you cannot see clearly the comparison between the colored values in the bins.   (a good summary of why they are problematic is in Stephen Few's Newletter, which you can download here ) I have found that data shown in a chart like this can be shown better in a table, perhaps with some conditional formatting (in Excel) or other color coding to push the eye toward the key differences in values. For continuous data, this often means binning a variable (akin to the histogram) and creating a cross-tab. The key is clarity--make the table so that the key information is obvious."], "link": "http://abbottanalytics.blogspot.com/feeds/6274289495370726630/comments/default", "bloglinks": {}, "links": {"http://www.perceptualedge.com/": 1, "http://abbottanalytics.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://www.edwardtufte.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Rexer Analytics, a data mining consulting firm, is conducting their 5th annual survey of the analytic behaviors, views and preferences of data mining professionals. I urge all of you to respond to the survey and help us all understand better the nature of the data mining and predictive analytics industry. The following text contains their instructions and overview. If you want to skip the verbage and just get on with the survey, use code RL3X1 and go here . Your responses are completely confidential: no information you provide on the survey will be shared with anyone outside of Rexer Analytics. All reporting of the survey findings will be done in the aggregate, and no findings will be written in such a way as to identify any of the participants. This research is not being conducted for any third party, but is solely for the purpose of Rexer Analytics to disseminate the findings throughout the data mining community via publication, conference presentations, and personal contact. To participate, please click on the link below and enter the access code in the space provided. The survey should take approximately 20 minutes to complete. Anyone who has had this email forwarded to them should use the access code in the forwarded email. Survey Link: www.RexerAnalytics.com/Data-Miner-Survey-2011-Intro2.html Access Code: RL3X1 If you would like a summary of last year\u2019s or this year\u2019s findings emailed to you, there will be a place at the end of the survey to leave your email address. You can also email us directly (DataMinerSurvey@RexerAnalytics.com) if you have any questions about this research or to request research summaries. Here are links to the highlights of the previous years\u2019 surveys. Contact us if you want summary reports from any of these years. -- 2010 survey highlights: http://www.rexeranalytics.com/Data-Miner-Survey-Results-2010.html -- 2009 survey highlights: http://www.rexeranalytics.com/Data-Miner-Survey-Results-2009.html -- 2008 survey highlights: http://www.rexeranalytics.com/Data-Miner-Survey-Results-2008.html -- 2007 survey highlights: http://www.rexeranalytics.com/Data-Miner-Survey-Results.html Thank you for your time. We hope this research program continues to provide useful information to the data mining community. Sincerely, Karl Rexer, PhD"], "link": "http://abbottanalytics.blogspot.com/feeds/3060120501331422949/comments/default", "bloglinks": {}, "links": {"http://www.RexerAnalytics.com/": 1, "http://abbottanalytics.blogspot.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["This post was first posted on Predictive Models are not Statistical Models \u2014 JT on EDM My friend and colleague James Taylor asked me last week to comment on a question regarding statistics vs. predictive analytics. The bulk of my reply is on James' blog ; my fully reply is here, re-worked from my initial response to clarify some points further. I have always love reading the green \"Sage\" books, such as Understanding Regression Assumptions (Quantitative Applications in the Social Sciences) or Missing Data (Quantitative Applications in the Social Sciences) because they are brief, cover a single topic, and are well-written. As a data miner though, I am also somewhat amused reading them because they are obviously written by statisticians with the mindset that the model is king . This means that we either pre-specify a model (the hypothesis) or require the model be fully interpretable, fully representing the process we are modeling. When the model is king, it's as if there is a model in the ether that we as modelers must find, and if we get coefficients in the model \"wrong\", or if the model errors are \"wrong\", we have to rebuild the data and then the model to get it all right. In data mining and predictive analytics, the data is king . These models often impute the models from the data (decision trees do this), or even if they only fit coefficients (like neural networks), it's the accuracy that matters rather than the coefficients. Often, in the data mining world, we won't have to explain precisely why individuals behave as they do so long as we can explain generally how they will behave. Model interpretation is often related to describing trends (sensitivity or importance of variables). I have always found David Hand's summaries of the two disciplines very useful, such as this one here ; I found that he had a healthy respect for both disciplines."], "link": "http://abbottanalytics.blogspot.com/feeds/3276486919908409952/comments/default", "bloglinks": {}, "links": {"http://www.amazon.com/": 2, "http://citeseerx.psu.edu/": 1, "http://jtonedm.com/": 2}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["Sometimes, the output of analytical tools can be voluminous and complicated. Making sense of it sometimes requires, well, analysis. Following are two examples of applying our tools to their own output. Model Deployment Verification From time to time, I have deployed predictive models on a vertical application in the finance industry which is not exactly \"user friendly\". I have virtually no access to the actual deployment and execution processes, and am largely limited to examination the production mode output, as implemented on the system in question. As sometimes happens, the model output does not match my original specification. While the actual deployment is not my individual responsibility, it very much helps if I can indicate where the likely problem is. As these models are straightforward linear or generalized linear models (with perhaps a few input data transformations), I have found it useful to calculate the correlation between each of the input variables and the difference between the deployed model output and my own calculated model output. The logic is that input variables with a higher correlation with the deployment error are more likely to be calculated incorrectly. While this trick is not a cure-all, it quickly identifies in 80% or more of cases the culprit data elements. Model Stability Over Time A bedrock premise of all analytical work is that the future will resemble the past. After all, if the rules of the game keep changing, then there's little point in learning them. Specifically in predictive modeling, this premise requires that the relationship between input and output variables must remain sufficiently stable for discovered models to continue to be useful in the future. In a recent analysis, I discovered that models universally exhibited a substantial drop in test performance, when comparing out-of-time to (in-time) out-of-sample. The relationships between at least some of my candidate input variables and the target variable are presumably changing over time. In an effort to minimize this issue, I attempted to determine which variables were most susceptible. I calculated the correlation between each candidate predictor and the target, both for an early time-frame and for a later one. My thinking was that variables whose correlation changed the most across time were the least stable and should be avoided. Note that I was looking for changes in correlation, and not whether correlations were strong or weak. Also, I regarded strengthening correlations just as suspect as weakening ones: The idea is for the model to perform consistently over time. In the end, avoiding the use of variables which exhibited \"correlation slide\" did weaken model performance, but did ensure that performance did not deteriorate so drastically out-of-time. Final Thought It is interesting to see how useful analytical tools can be when applied to the analytical process itself. I note that solutions like the ones described here need not use fancy tools: Often simple calculations of means, standard deviation and correlations are sufficient."], "link": "http://abbottanalytics.blogspot.com/feeds/576044526316813303/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Data Mining and Predictive Analytics"}, {"content": ["I'd like to revisit an issue we covered here, way back in 2007: Statistics: Why Do So Many Hate It? . Recent comments made to me, both in private conversation (\"Statistics? I hated that class in college!\"), and in print prompt me to reconsider this issue. One thing which occurs to me is that many people have a tendency to think of statistics in an isolated way. This world view keeps statistics at bay, as something which is done separately from other business activities, and, importantly, which is done and understood only by the statisticians. This is very far from the ideal which I suggest, in which statistics (including data mining) are much more integrated with the business processes of which they are a part. In my opinion, this is a strange way to frame statistics. As an analog, imagine if, when asked to produce a report, a business team turned to their \"English guy\", with the expectation that he did all the writing . I am not suggesting that everyone needs to do the heavy lifting that data miners do, but that people who don't accept some responsibility for data mining's contribution to the business process. Managers, for example, who throw up their hands with the excuse that \"they are not numbers people\" forfeit control over an important part of their business function. It is healthier for everyone involved, I submit, if statistics moves away from being a black art, and statisticians become less of an arcane priesthood."], "link": "http://abbottanalytics.blogspot.com/feeds/8117537045903714336/comments/default", "bloglinks": {}, "links": {"http://abbottanalytics.blogspot.com/": 1}, "blogtitle": "Data Mining and Predictive Analytics"}]