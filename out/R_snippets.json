[{"blogurl": "http://rsnippets.blogspot.com\n", "blogroll": [], "title": "R snippets"}, {"content": ["Recently I wanted to reproduce Figure 2 from Watts and Strogatz (1998) . The task using igraph is simple but an interesting task was annotation of the resulting plot. Watts-Strogatz model generates graphs that have so called small-world network property. Such networks should have low average path length and high clustering coefficient. The algorithm has three parameters: number of nodes in the graph, initial number of neighbors of each node distributed on a ring and rewiring probability. Interestingly in Watts-Strogatz model having small but positive values of rewiring probability generates graphs having desired properties - and this is exactly depicted on Figure 2 in their article . I decided to replicate it. To enhance it I wanted to plot median and 5 and 95 percentile of distribution of average path length and clustering coefficient as a function of rewiring probability. Here you have the code that generates the graph (warning: it takes about 1 minute to run): library ( igraph )  set.seed ( 1 )   avg.stat <-  function ( nei, p )  {   result <-  replicate ( 1000 , {   wsg <- watts.strogatz.game ( 1 , 100 , nei, p )    c ( average.path.length ( wsg ) ,   transitivity ( wsg ))   })   apply ( result, 1 , quantile , probs = c ( 0.5 , 0.05 , 0.95 ))  }   nei <-  6  p <-  2  ^  - seq ( 0 , 10 , len =  21 )  result <-  sapply ( p, avg.stat, nei = nei )  result <-  t ( result /  rep ( avg.stat ( nei, 0 )[ 1 , ] , each =  3 ))  par ( mar = c ( 3.2 , 2 , 0.2 , 0.2 ) , mgp = c ( 2 , 1 , 0 ))  matplot ( p, result, type =  \"l\" , log  =  \"x\" , xaxt =  \"n\" , ylab =  \"\" ,   lty = rep ( c ( 1 , 2 , 2 ) , 2 ) , col = rep ( c ( 1 , 2 ) , each = 3 ))  axis ( 1 , at =  2  ^  -( 0 : 10 ) ,   labels  =  c ( 1 , parse ( text  =  paste ( 2 , 1 : 10 , sep =  \"^-\" ,           collapse = \";\" ))))  legend ( \"bottomleft\" , c ( \"average path length\" , \"clustering coefficient\" ) ,   lty = 1 , col = c ( 1 , 2 )) The result of the procedure is the following picture:  It looks very similar to what is shown in the article (apart from adding lines depicting 5 and 95 percentile of distributions of both graph characteristics). However, the interesting part was to properly annotate X-axis on the plot. Of course you can use expression function to get it but then the problem is that you have to do it ten times. Interestingly parsing a string containing those ten expressions separated by semicolons works just as needed."], "link": "http://rsnippets.blogspot.com/feeds/5891984552617589972/comments/default", "bloglinks": {}, "links": {"http://www.nature.com/": 2, "http://3.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Last week I have posted about using simulation metamodeling to verify results of analytical solution of the model. After posting it I realized that the solution presented there can be improved by using knowledge of simulation model structure. The story is about calculation of the variance of the random variable given by formula: q ( p -2+ q / N ) where p and N are model parameters and q is a random variable that has binomial distribution with N Bernoulli trials and probability of success 2- p . As I have written the properly specified formula for the metamodel has the following form: v ~ ( p + I ( p ^  2 )  + I ( p ^  3 )  + I ( p ^  4 ))  * ( N + I ( 1  / N )) where v is an estimate of the variance. Such a model was discussed in my last post (it is described in more detail there so I omit the repetition in this post).  However one can easily notice that for p =1 and p =2 the variance of the random variable is equal to 0. This is because in such case q has no variance (it is equal to 1 and 0 respectively).  In estimation of the metamodel one can take this into account in two ways: the weight of cases where p equals 1 or 2 might be increased (so we use Weighted Least Squares) a restriction on values of prediction in points where p equals 1 or 2 can be made to ensure that it is exactly 0 (so we use Constrained Least Squares) The code given below estimates: original model and two versions of corrected models:  library ( plyr )  library ( limSolve )   sim <-  function ( param )  {   q <- rbinom ( 1000 , param $ N, 2 - param $ p )   c ( v =  var (( param $ p -  2  +  q / param $ N )  * q ))  }   set.seed ( 1 )  data.set <- expand.grid ( p = seq ( 1 , 2 , len =  101 ) , N =  1 : 10 )  data.set <- ddply ( data.set, . ( p,N ) , sim,.progress =  \"text\" )   lm.coef <-  coef ( lm ( v ~ ( p + I ( p ^  2 )  + I ( p ^  3 )  + I ( p ^  4 ))      * ( N + I ( 1  / N )) , data = data.set ))   weights  <-  ifelse ( data.set $ v == 0 , 100000 , 1 )  wlm.coef <-  coef ( lm ( v ~ ( p + I ( p ^  2 )  + I ( p ^  3 )  + I ( p ^  4 ))       * ( N + I ( 1  / N )) , data = data.set,      w eights = weights ))   A <-  with ( data.set, cbind ( 1 , p, p ^ 2 , p ^ 3 , p ^ 4 , N, 1  / N,  p * N, p / N, p ^  2  * N, p ^  2  / N,  p ^  3  * N, p ^  3  / N, p ^  4  * N, p ^  4  / N ))  colnames ( A )  <-  names ( lm.coef )  B <- data.set $ v E <- A [ B == 0 , ]  F <- B [ B == 0 ]  lsei.coef <- lsei ( A, B, E, F )$ X  Now we can compare the coefficients from these models to their analytical values given in the last post:  true.coef <-  c ( 32 , - 88 , 88 , - 38 , 6 , - 8 , - 26 , 20 ,      75 , - 18 , - 79 , 7 , 36 , - 1 , - 6 )  coef.dev <-  cbind ( lm.coef, wlm.coef, lsei.coef )  - true.coef print ( coef.dev, digits =  2 )  dotchart ( coef.dev [ , 1 ] , pch =  19 )  points ( coef.dev [ , 2 ] , 1 : nrow ( coef.dev ) , col = \"red\" , pch = 19 )  abline ( v = 0 , col = \"gray\" , lty = 2 )   The code first tabulates the deviation of estimates from true values:      lm.coef wlm.coef lsei.coef (Intercept) -0.6056 0.060  0.060 p    1.5061 -0.235 -0.235 I(p^2)  -1.3576 0.323  0.323 I(p^3)   0.5243 -0.185 -0.185 I(p^4)  -0.0731 0.038  0.038 N    0.0088 -0.060 -0.060 I(1/N)   1.1510 0.354  0.354 p:N   -0.0065 0.173  0.173 p:I(1/N)  -3.0646 -0.965 -0.965 I(p^2):N  -0.0102 -0.182 -0.182 I(p^2):I(1/N) 2.9942 0.953  0.953 I(p^3):N  0.0115 0.084  0.084 I(p^3):I(1/N) -1.2730 -0.404 -0.404 I(p^4):N  -0.0029 -0.014 -0.014 I(p^4):I(1/N) 0.1991 0.062  0.062   and next plots the visualization of the differences (on the plot black points represent OLS estimates and red - WLS):    It can be seen that: the precision of estimation is improved by using additional information on problem structure; the estimates from WLS and CLS are identical to the third decimal place."], "link": "http://rsnippets.blogspot.com/feeds/862396955362413991/comments/default", "bloglinks": {}, "links": {"http://rsnippets.blogspot.com/": 2, "http://3.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["I am one of the organizers of ESSA2013 conference that will take place in September 2013 in Warsaw, Poland. The conference scope is social simulation and in particular methods of statistical analysis of simulation output (metamodeling). As we have just issued Call for Papers for the conference so I decided to post a simple example of a metamodel. Recently I had to calculate variance of a random variable given as: q ( p -2+ q / N ), where p and N are model parameters and q is a random variable that has binomial distribution with N Bernoulli trials and probability of success 2- p . After some lengthily calculations I came up with the following formula for the variance:  However the result is so long that I was not sure that it was correct, so I decided to verify it using metamodeling. First I have generated the data for the simulation with the following code and visualized it. The data covers a grid where p is from interval [1;2] and N spans from 1 to 10. In each grid point 100000 simulations are performed and variance of q ( p -2+ q / N ) is calculated: library ( plyr )  library ( lattice )   sim <-  function ( param )  {   q  <-  rbinom ( 100000 , param $ N, 2  - param $ p )   c ( v =  var (( param $ p -  2  +  q  / param $ N )  *  q ))  }   set.seed ( 1 )  data.set <- expand.grid ( p = seq ( 1 , 2 , len =  201 ) , N =  1 : 10 )  data.set <- ddply ( data.set, . ( p,N ) , sim, .progress =  \"text\" )  levelplot ( v ~ p + N, data = data.set,    col.regions = terrain.colors, xlab =  \"p\" , ylab =  \"N\" )  Here is the resulting plot:  Next I have estimated the linear regression model with parameters reflecting analytical specification given above: summary ( lm ( v ~( p + I ( p ^  2 )  + I ( p ^  3 )  + I ( p ^  4 ))      * ( N + I ( 1  / N )) , data = data.set ))  The output looks as follows: Call: lm(formula = v ~ (p + I(p^2) + I(p^3) + I(p^4)) * (N + I(1/N)),   data = data.set)   Residuals:   Min   1Q  Median   3Q  Max  -0.0116801 -0.0007195 0.0000063 0.0007608 0.0098659   Coefficients:     Estimate Std. Error t value Pr(>|t|)  (Intercept) 32.089136 0.235380 136.33 <2e-16 *** p    -88.232888 0.655760 -134.55 <2e-16 *** I(p^2)   88.225183 0.674590 130.78 <2e-16 *** I(p^3)  -38.095417 0.303854 -125.37 <2e-16 *** I(p^4)   6.014949 0.050597 118.88 <2e-16 *** N    -8.012673 0.027781 -288.42 <2e-16 *** I(1/N)  -26.082527 0.303365 -85.98 <2e-16 *** p:N   20.034053 0.077398 258.84 <2e-16 *** p:I(1/N)  75.213337 0.845166 88.99 <2e-16 *** I(p^2):N  -18.033947 0.079621 -226.50 <2e-16 *** I(p^2):I(1/N) -79.203708 0.869434 -91.10 <2e-16 *** I(p^3):N  7.014852 0.035863 195.60 <2e-16 *** I(p^3):I(1/N) 36.085047 0.391618 92.14 <2e-16 *** I(p^4):N  -1.002405 0.005972 -167.85 <2e-16 *** I(p^4):I(1/N) -6.013089 0.065212 -92.21 <2e-16 *** --- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1   Residual standard error: 0.002197 on 1995 degrees of freedom Multiple R-squared: 0.9999,  Adjusted R-squared: 0.9999  F-statistic: 1.976e+06 on 14 and 1995 DF, p-value: < 2.2e-16  As we can see the model fit is almost perfect and the estimates are very close to analytical results. This reassured me that my calculation was correct. In this way you could use simulation to verify analytical results. Of course simulation metamodeling has much broader applications - especially when we do not have analytical results. I hope to organize a special session during ESSA2013 dedicated to simulation metamodeling using GNU R. So if you are interested in such topics and willing to promote GNU R in simulation society you are welcome to come to Warsaw."], "link": "http://rsnippets.blogspot.com/feeds/8879520546799073596/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://essa2013.org/": 1, "http://www.essa2013.org/": 2}, "blogtitle": "R snippets"}, {"content": ["When producing regression or classification trees (standard rpart or ctree from party package) in GNU R I am often unsatisfied with the default plots they produce. One of many possible solutions is to export a tree plot to Asymptote . The code I have prepared generates an Asymptote file based on generated ctree object. Here is the procedure that does the conversion. treeAsy <-  function ( tree,   # ctree to be plotted       off.f,  # tree plot fixed shift       off.v,  # tree plot variable shift       file.name, # output file name       preamble )  {  # preamble for asy  minv <-  + Inf   maxv <-  - Inf   response <-  names ( tree@responses@variables )   plot.node <-  function ( root,      nest =  0 ,    # level in a tree      p Offset = 0 ,   # plotting offset      condition =  \"root\" , # split condition text      id =  \"root\" )  {   # block name in asy    if  ( length ( root $ prediction )  >  1 )  {     stop ( \"Only single prediction value supported\" )    }    if  ( root $ prediction < minv ) minv <<- root $ prediction   if  ( root $ prediction > maxv ) maxv <<- root $ prediction  child.l <-  \"\"   child.r <-  \"\"    if  (! root $ terminal )  {     if  ( class ( root $ psplit )  ==  \"orderedSplit\" )  {      varN <- root $ psplit $ variableName    point <- root $ psplit $ splitpoint    left <-  paste ( varN, \"$\\\\leq$\" , point, sep = \"\" )     right <-  paste ( varN, \"$>$\" , point, sep = \"\" )     }  else  {      stop ( \"Only orderedSplit supported\" )     }       add <-  \"add(new void(picture pic, transform t) {  blockconnector operator --=blockconnector(pic,t);\\n \"    child.l <-  paste ( plot.node ( root $ left, nest +  1 ,    p Offset - off.f - 1 / off.v ^ nest,    left, paste ( id, \"l\" ,sep = \"\" )) ,    add,id, \"--Down--Left--Down--\" ,id, \"l;\\n});\\n\\n\" , sep = \"\" )    child.r <-  paste ( plot.node ( root $ right, nest +  1 ,    p Offset + off.f + 1 / off.v ^ nest,    right, paste ( id, \"r\" ,sep = \"\" )) ,    add,id, \"--Down--Right--Down--\" ,id, \"r;\\n});\\n\\n\" , sep = \"\" )    }    paste ( \"block \" , id, \" = rectangle(Label(\\\"\" ,   condition, \"\\\"),  pack(Label(\\\"n=\" , sum ( root $ weights ) , \"\\\"),   Label(\\\"\" , response, \"=\" ,    format ( root $ prediction ) , \"\\\")),  (\" , p Offset, \",\" , - nest, \"), lightgray, col(\" ,   root $ prediction, \"));\" ,   \"\\ndraw(\" , id, \");\\n\\n\" ,   child.l, child.r, sep = \"\" )   }   treestruct <- plot.node ( tree@tree )     cat ( file = file.name,   preamble,   \"\\nimport flowchart;\\n\" ,   \"pen col(real x) {  real minv = \" , minv, \";  real maxv = \" , maxv, \";  real ratio = 1 - (x - minv) / (maxv - minv);  return rgb(1, ratio, ratio); }\\n\\n\" ,   treestruct, \"\\n\" , sep = \"\" )   shell ( paste ( \"asy -f png\" , file.name ))  }  Each node on the plot contains: the condition leading to it, number of observations and response variable prediction (also intensity of red indicates its relative value). In order to keep the example simple it is very simplified. Currently handles only regression trees with continuous predictors and will generate errors if variable names contain TeX special characters (like &). Additionally you can control the tree layout only manually by setting variables off.f and off.v or by manipulating picture size in the preamble (and one could write a code to layout the plot automatically). The code produces png output as I needed this format to show the picture on blog, but of course you can generate eps or pdf file which is probably a more suitable option. And there is the example of the code use based on standard ctree example: library ( party )   airq <-  subset ( airquality, ! is.na ( Ozone ))  airct <- ctree ( Ozone ~ ., data = airq )  treeAsy ( airct, - 0.25 , 1.4 , \"tree.asy\" ,  \"size(22cm,12cm, keepAspect=false);\" )  It gives the following output:    Which is much nicer for me in comparison to default plot generated by plot ( airct ) :"], "link": "http://rsnippets.blogspot.com/feeds/33754867875661427/comments/default", "bloglinks": {}, "links": {"http://asymptote.sourceforge.net/": 1, "http://1.blogspot.com/": 2}, "blogtitle": "R snippets"}, {"content": ["Recently I have discussed with my friend from WLOG Solutions an implementation of banking cash management engine in GNU R. The code made a nice use of S4 classes so I thought it would be worth showing as an example. The problem Every commercial bank needs to provide its customers with access to cash via hundreds of cash access points like ATMs or branches. Bank managers facing this problem have to handle three conflicting objectives: (1) they have to ensure that there is enough cash in cash access points to maintain liquidity, (2) they want to minimize the amount of cash frozen because it is not working for bank and (3) they want to minimize transportation costs from central vault to access points. This is a complex optimization problem which in particular involves the need to predict cash balance in access point every day using historic data. When designing a solution providing forecasts of cash balances in access points a case for typical application of object oriented approach arises. For each cash point we supply historical data having the same structure and want to obtain a balance prediction. However different access points have different customer usage characteristics and will require different statistical forecasting models. For example in ATM one can only withdraw the money but in branch you can as well make a deposit. Therefore we model the bank using S4 classes. The implementation There is a Bank class that can have many CashPoints associated with it. CashPoint is a virtual class that has two implementing classes ATM and Branch. This structure is shown on figure below. Each CashPoint holds its historical balances and has a givePrediction() method that provides a forecast. This method will be implemented differently in ATM and Branch classes.  The example code implementing this structure is given on listing below. First using setGeneric() function we create a generic function givePrediction() that will dispatch appropriate methods following the class of its arguments. Next we create definitions of Bank, ATM and Branch classes of S4 type using setClass() function and create formal method for givePrediction() function for those classes. In our example for ATMs we use linear regression and for Branches simple mean as balance predictors. Notice this method defined for CashPoint class will be invoked if it will not be overridden by appropriate methods in subclasses (it is not possible to create an object of class CashPoint as it is defined virtual). The code is run by invoking givePrediction() function on a new Bank class instance. The constructor of Bank class reads bank structure data from bank_model.csv file that contains the list of cash points with their ids and types (ATM or Branch). Next it invokes creation of CashPoint-s. Each cash point is initialized with data from branch_balances_data.csv which contains three columns: BranchId, Date, Balance. An appropriate subset of data is first selected using BranchId column. Date and Balance are retained in balances field and contain historical data for this cash point. After creation of an object of type Bank its givePrediction() method is invoked which calls automatically via S4 class system either ATM or Branch givePrediction() method according to the run-time type of cash point. And here is the code. I hope you will find this simple example of S4 classes application useful.  setGeneric ( \"givePrediction\" , function ( object )  {   standardGeneric ( \"givePrediction\" )  })   setClass ( \"Bank\" , representation ( cashPoints =  \"list\" ))  setMethod ( \"initialize\" , \"Bank\" , function ( .Object ){   BankModel <- read.table ( file = \"bank_model.csv\" ,   sep =  \";\" , header =  TRUE , stringsAsFactors =  FALSE )   .Object@cashPoints <-  apply ( BankModel, 1 , function ( cp )  {    new ( cp [ 2 ] , cp [ 1 ])    })    names ( .Object@cashPoints )  <-  apply ( BankModel, 1 ,    paste , collapse =  \"_\" )    return ( .Object )  })  setMethod ( \"givePrediction\" , \"Bank\" , function ( object ){    return ( sapply ( object@cashPoints, \"givePrediction\" ))  })   setClass ( \"CashPoint\" , representation ( id =  \"character\" ,  balances =  \"data.frame\" , \"VIRTUAL\" ))  setMethod ( \"initialize\" , \"CashPoint\" , function ( .Object, cashPointId ){   .Object@id <- cashPointId  balances <- read.table ( file = \"branch_balances_data.csv\" ,   sep =  \";\" , header =  TRUE )   .Object@balances <-  subset ( balances,   balances $ BranchId == .Object@id, - BranchId )   .Object@balances $ Date <- as.Date ( .Object@balances $ Date )    return ( .Object )  })  setMethod ( \"givePrediction\" , \"CashPoint\" , function ( object ){    stop ( \"no givePrediction method for this class\" )  })   setClass ( \"Branch\" , contains =  \"CashPoint\" )  setMethod ( \"givePrediction\" , \"Branch\" , function ( object ){    return ( mean ( object@balances $ Balance ))  })   setClass ( \"ATM\" , contains =  \"CashPoint\" )  setMethod ( \"givePrediction\" , \"ATM\" , function ( object )  {   LM <-  lm ( Balance ~ as.numeric ( Date ) , data = object@balances )   prediction <-  predict ( LM,   data.frame ( Date =  1  +  max ( object@balances $ Date )))    return ( unname ( prediction ))  })   print ( givePrediction ( new ( \"Bank\" )))  To run the code you need to create bank_model.csv and branch_balances_data.csv files. Here you have sample truncated contents (you need to copy-paste the text given below and save in GNU R working directory under appropriate names). bank_model.csv CashPointId;CashPointType CashPoint_1;ATM CashPoint_2;Branch CashPoint_3;ATM CashPoint_4;Branch CashPoint_5;ATM branch_balances_data.csv BranchId;Date;Balance CashPoint_1;2012-12-01;423000 CashPoint_1;2012-12-02;312000 CashPoint_1;2012-12-03;220000 CashPoint_1;2012-12-04;123000 CashPoint_2;2012-12-01;223000 CashPoint_2;2012-12-02;212000 CashPoint_2;2012-12-03;320000 CashPoint_2;2012-12-04;223000 CashPoint_3;2012-12-01;323000 CashPoint_3;2012-12-02;312000 CashPoint_3;2012-12-03;270000 CashPoint_3;2012-12-04;223000 CashPoint_4;2012-12-01;323000 CashPoint_4;2012-12-02;412000 CashPoint_4;2012-12-03;320000 CashPoint_4;2012-12-04;373000 CashPoint_5;2012-12-01;223000 CashPoint_5;2012-12-02;192000 CashPoint_5;2012-12-03;150000 CashPoint_5;2012-12-04;133000"], "link": "http://rsnippets.blogspot.com/feeds/3009898958805283410/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://www.wlogsolutions.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Since I have a vacation this time I decided to implement some entertaining graphics. I have chosen to animate a Cassini oval . The task is can be accomplished using polar equation:  The implementation of the animation is given by the following code: library ( animation )   xy <-  function ( angle, a, b )  {   ca <- cos ( 2 * angle )  r2 <- a ^  2  *  ca + sqrt ( a ^ 4 * ca ^ 2 -  ( a ^ 4 - b ^ 4 ))   c ( sqrt ( r2 )  *  cos ( angle ) , sqrt ( r2 )  *  sin ( angle ))  }   go <-  function ()  {   angle <-  seq ( 0 , 2  *  pi , len =  1000 )   par ( mar = rep ( 0 , 4 ))   b <-  1  +  0.5  *  seq ( 0 , 1 , len =  31 )  ^  3  b <-  c ( b, 1.5 , rev ( b ))   for  ( i in b )  {   coord <-  t ( sapply ( angle, xy, a =  1 , b = i ))    plot ( coord, type =  \"l\" ,   xlim =  c (- 2 , 2 ) , ylim =  c (- 1.25 , 1.25 ))    polygon ( coord, col  =  \"gray\" )   }  }   ani.options ( interval =  0.1 )  saveGIF ( go ())  Parameter a is fixed to 1 and b changes from 1 to 1.5 . In order to achieve smooth animation the sequence defining changes of b is not uniform but is more dense near 1 . And here is the result:"], "link": "http://rsnippets.blogspot.com/feeds/5325549481354625769/comments/default", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1, "http://1.blogspot.com/": 2}, "blogtitle": "R snippets"}, {"content": ["In my last post I have discussed how to work around GNU R scoping rules using environment function. This time let us look at a practical example using recode function from car package. First let us look at how recode works:  library ( car )  x <-  rep ( 1 : 3 , 2 )  recode ( x, \"1='a'\" ) transforms 1  2 3  1 2  3 into \"a\"  \"2\"  \"3\"  \"a\"  \"2\"  \"3\" . In further codes we will want to replicate this result using several different approaches sticking to the same definition of variable x . Interestingly the string recodes is split and evaluated inside recode function so we can use the following code to get the same result:  a <-  1 b <-  \"a\" recode ( x, \"a[1]=b[1]\" ) Now we can change b variable to get different recoding results without the change of recode call. Note that this use of recode is not following its help page as documentation not support using variables inside recodes string. Let us now try writing a simple wrapper around recode that does the same stuff. Unfortunately:  wrong.recode.one <-  function ( v, from, to )  {   recode ( v, \"from[1]=to[1]\" )  }  wrong.recode.one ( x, 1 , \"a\" )   does not work and produces error. Due to lexical used in GNU R from and to variables are not within recode function scope. Here are two ways to work around it:  recode.one <-  function ( v, from, to )  {    environment ( recode )  <-  environment ()   squeezeBlanks <- car ::: squeezeBlanks  recode ( v, \"from[1]=to[1]\" )  }  recode.one ( x, 1 , \"a\" )   recode.one2 <-  function ( v, from, to )  {    environment ( recode )  <-  environment ()   recode ( v, \"from[1]=to[1]\" )  }  environment ( recode.one2 )  <-  environment ( recode )  recode.one2 ( x, 1 , \"a\" )  The first function recode.one moves recode into its own lexical scope. Unfortunately it also has to move squeezeBlanks function into its scope to work properly as recode calls it. To avoid this recode.one2 is put into environment ( recode ) environment so squeezeBlanks will be in its lexical scope. Those examples helped me better understand how scoping works in GNU R, but they are dangerous. To see this one can look at the following code:  a <-  1 is.fac <-  \"a\" recode ( x, \"a[1]=is.fac[1]\" )  Such call produces incorrect result 0  2 3  0 2  3 because is.fac is defined inside recode function so its evaluation is done inside recode body. Another lesson that one has to be careful when hacking in GNU R."], "link": "http://rsnippets.blogspot.com/feeds/8898873234439600437/comments/default", "bloglinks": {}, "links": {"http://rsnippets.blogspot.com/": 1, "http://www.blogger.com/": 1}, "blogtitle": "R snippets"}, {"content": ["By design GNU R uses lexical scoping. Fortunately it allows for at least two ways to simulate dynamic scoping.  Let us start with the example code and next analyze it:  x <-  \"global\"  f1 <-  function ()  cat ( \"f1:\" , x, \"\\n\" )  f2 <-  function ()  cat ( \"f2:\" , evalq ( x, parent.frame ()) , \"\\n\" )  fun <-  function ()  {   x <-  \"f1\"   f1 ()   f2 ()    environment ( f1 )  <-  environment ()   f1 ()  }  The difference between functions f1 and f2 is the following. In f1 standard lexical scoping to find x  will be used . In f2 evaluation of x is done in parent environment in the calling stack. This could be called a one level dynamic scoping, because in parent environment x is found using lexical scoping. If they are both called from global environment their behavior is identical. > f1 ()  f1: global  > f2 ()  f2: global  However if they are called from within a function the results will differ: > fun ()  f1: global  f2: f1  f1: f1  We can see that f1 selects a variable from its lexical scope (global environment) and f2 from calling function fun .  An interesting thing is that function's f1 lexical scope can be changed by assigning new environment to function f1 within function fun . This forces fun environment into lexical search path of f1 and is another way to simulate one level dynamic scoping.  The second method is useful when a function we want to call is defined externally (for example within a package) and we are unable to change it. The drawback is that called function may stop working because within its internals it might call functions or seek variables that are undefined within changed environment search path - so one has to be cautious with it. In my next post I plan show an application of this idea on some practical example."], "link": "http://rsnippets.blogspot.com/feeds/4516191675193214036/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "R snippets"}, {"content": ["In my last post I have shown a solution to classical sorting problem in R. So I thought that this time it would be nice to generate a strategy for playing Mastermind using R. It was shown by D.E. Knuth that Mastermind code can be broken in at most five guesses. The algorithm is to always choose guess that minimizes the maximum number of remaining possibilities . Here is the R code that implements it. The game is played using six colors and codes have length four. First part of the code prepares the data for calculations: # vector with possible peg colors  set <-  letters [ 1 : 6 ]   # matrix all possible codes  full <- as.matrix ( expand.grid ( set, set, set, set ))   # compare guess to hidden code  # black - hit with correct position  # white - hit with incorrect position  guessFit <-  function ( hidden, guess )  {   count <-  function ( pattern )  {     sapply ( set, function ( x )  {  sum ( x == pattern )  })    }   black <-  sum ( full [ hidden, ]  == full [ guess, ])   white <-  sum ( pmin ( count ( full [ hidden, ]) ,       count ( full [ guess, ])))  - black   paste ( black, white, sep = \";\" )  }   # prepare matrix with all possible  # guess-hidden combinations evaluation  # this is slow: 5 minutes  all.fit <-  mapply ( guessFit, rep ( 1 : nrow ( full ) , nrow ( full )) ,   rep ( 1 : nrow ( full ) , each =  nrow ( full )))  dim ( all.fit )  <-  c ( nrow ( full ) , nrow ( full ))  We want to prepare matrix all.fit of all possible guess-hidden code combinations in advance in order to avoid calling guessFit in the main algorithm (WARNING: it takes ~5 minutes on my laptop). Having generated it we can reference codes using their position (row number) in full matrix. Now let us move on to the main function: # apply mini-max rule  minimax <-  function ( possible, indent =  1 )  {    # if there is only one possibility we are done    if  ( length ( possible )  ==  1 )  {     if  ( indent > worst )  {     worst <<- indent    }     cat ( full [ possible, ] , \"| *\\n\" )     return ( 1 )    }     if  ( indent ==  1 )  {     cat ( \"1: \" )    }    # for each possible guess find worst case size of set   splits <-  sapply ( 1 : nrow ( full ) , function ( guess )  {     max ( table ( all.fit [ guess, possible ]))  })    # choose guess that minimizes maximal size of set   best.guess <- which.min ( splits )   out.split <-  split ( possible, sapply ( possible, guessFit,    guess = which.min ( splits )))    cat ( full [ best.guess, ] , \"|\" , length ( possible ) , \"\\n\" )    # recursively construct the decision tree    for  ( i in  1 : length ( out.split ))  {     # ask additional questions if an exact hit was not obtained     if  ( names ( out.split )[ i ]  !=  paste ( ncol ( full ) , 0 , sep =  \";\" ))  {      cat ( indent +  1 , \":\" , rep ( \" \" , indent ) ,      names ( out.split )[ i ] , \"|\" , sep = \"\" )     minimax ( out.split [[ i ]] , indent +  1 )     }    }  }  It recursively constructs the decision tree solving the game and outputs it using cat . At each level of the tree first number of the question asked is printed, next the chosen guess and finally either number of remaining options or a star * indicating a hit. Additionally in variable worst we keep the number of questions that have to be asked in the worst case. Finally we run the prepared code: sink ( \"rules.txt\" )  # save output to a file  worst <-  0  minimax ( 1 : nrow ( full ))  # this is slow: 2 minutes  cat ( \"\\nQuestions in worst case:\" , worst, \"\\n\" )  sink () I redirect output to a file because the resulting tree is quite big (1710 lines) and we can actually see that the game can be solved in five questions in a worst case. Finally - the code was prepared to make it easy to experiment with the code by changing number of colors and pegs only by changing variables set and full ."], "link": "http://rsnippets.blogspot.com/feeds/6683771328442388389/comments/default", "bloglinks": {}, "links": {"http://rsnippets.blogspot.com/": 1, "http://www.up.pt/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "R snippets"}, {"content": ["Some time ago I read a nice post Solving easy problems the hard way where linear regression is used to solve an interesting puzzle. Following the idea I used rpart to find optimal decision tree sorting five elements. It is well known that in order to sort five elements it is enough to use seven comparisons. Interestingly it is possible to adjust rpart function to find the optimal decision tree.  To see this first let us start with generating the appropriate training data using the following code:  library ( gtools )  perm5 <- permutations ( 5 , 5 )  comb5 <- combinations ( 5 , 2 )   data.set <- data.frame ( perm5c =  apply ( perm5, 1 ,   paste , collapse =  \"\" ))  for  ( k in  1 : nrow ( comb5 ))  {   i <- comb5 [ k, 1 ]   j <- comb5 [ k, 2 ]   val <-  factor ( apply ( perm5, 1 , function ( v )  {     which ( i == v )  <  which ( j == v )  }))   data.set [ , paste ( \"v\" , i, j, sep =  \"\" )]  <- val  }  We get a data set with 120 rows. Each row contains a column perm5c representing one permutation of the set from 1 to 5 and 10 columns representing pairwise comparison results between variables. We want to find a decision tree that partitions this data set into one-element leaves with seven decisions in the worst case.  Simply applying rpart  in the following way:  tree.model <- rpart ( perm5c ~ ., data  = data.set,   control  = rpart.control ( minsplit =  1 , cp =  0 ) )  plot ( tree.model )  unfortunately generates a tree where ten decisions are neeeded in the worst case.  The trick to solve this problem is that you can supply rpart with your own split evaluation function. In this case the appropriate splitting rule should favour balanced splits (so for example 30-30 split is better than 40-20). Here is the code that does the work:  library ( rpart )  temp1 <-  function ( y, wt, parms )  {   lab <-  ifelse ( length ( y )  ==  1 ,   as.integer ( paste ( perm5 [ y, ] , collapse = \"\" )) , NaN )    list ( label = lab, deviance  =  length ( y )  -  1 )  }   # prefer balanced splits  temp2 <-  function ( y, wt, x, parms, continuous )  {    list ( goodness =  nrow ( perm5 )^ 2 - sum ( table ( x )^ 2 ) ,   direction =  unique ( x ))  }   temp3 <-  function ( y, offset , parms, wt )  {    list ( y = y, parms = 0 , numresp = 1 , numy = 1 ,    summary =  function ( yval, dev, wt, ylevel, digits ) {       \"Summary\"  })  }   tree.model <- rpart ( perm5c ~ ., data  = data.set,   control  = rpart.control ( minsplit =  1 , cp =  0 ) ,  method =  list ( eval  = temp1, split  = temp2, init = temp3 ))  plot ( tree.model )   If you look at the output after printing tree.model you can see which paths lead to which classifications. As you can see on the figure below at most seven comparisons are needed:  Additionally it is easy to modify the code to produce optimal trees for other numbers of elements to be sorted."], "link": "http://rsnippets.blogspot.com/feeds/9001206809473879990/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://www.cerebralmastication.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Recently I have stumbled on a problem with split function applied on list of factors. The issue is that it might produce wrong splits when splitting factors contain dots. Here is the example of the problem. Invoking the following code: df <- data.frame ( x = rep ( c ( \"a\" , \"a.b\" ) , 3 ) ,      y =  rep ( c ( \"b.c\" , \"c\" ) , 3 ) ,      z =  1 : 6 )   split ( df, df [ , - 3 ])  produces: $a.b.c  x y z 1 a b.c 1 2 a.b c 2 3 a b.c 3 4 a.b c 4 5 a b.c 5 6 a.b c 6  $a.b.b.c [1] x y z <0 rows> (or 0-length row.names)  $a.c [1] x y z <0 rows> (or 0-length row.names)  And we can see that incorrect splits were produced. The issue is that split uses interaction to combine list of factors passed to it. One can see this problem by invoking: > interaction ( df [ , - 3 ])  [1] a.b.c a.b.c a.b.c a.b.c a.b.c a.b.c Levels: a.b.c a.b.b.c a.c  The problem might be not a huge issue in interactive mode, but in production code such behavior is a problem. There are three obvious ways to improve how split works: Rewriting split internals to avoid this problem; Allow passing sep parameter to split that would be further passed to interaction ; Warning if resulting number of levels in combined factor does not equal the multiplication of number of levels of combined factors (assuming drop = F option). Until this issue is solved there is a workaround using split and two other options using by and dlply (from plyr package):  #Workaround split ( df, lapply ( df [ , - 3 ] , as.integer ))   #Alternative 1  by ( df, df [ , - 3 ] , identity )   #Alternative 2  library ( plyr )  dlply ( df,. ( x,y ))"], "link": "http://rsnippets.blogspot.com/feeds/2231438050743926742/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "R snippets"}, {"content": ["I always like to read new posts at chartsnthings as they always inspire me with new ideas for data visualization. Yesterday I have read an article on choices of car brands by members of parliament in Poland in Gazeta.pl . It contains a simple table graph created using Tableau that was interesting to replicate in GNU R. The source data ( cars.txt ) contains club name each member of parliament belongs to and name of the car she owns. Here is the head of the data set: cars <- read.table ( \"cars.txt\" ,  head = T, sep = \"\\t\" , quote  = \"\" )  head ( cars )  # club        brand  # 1 PO       Citroen C8  # 2 PO Opel Astra IV Combi 1,4 Turbo 2011  # 3 PO      Mercedes E 300  # 4 PO     Chevrolet Blazer  # 5 PO      Mercedes C 202  # 6 PO       Mercedes G  In order to do the cross tabulation of clubs against car brands first I leave only brand name in the data, next I order both factors by categories count and plot the data: library ( ggplot2 )  library ( plyr )   # leave only car brand  cars $ brand <-  factor ( sapply ( cars $ brand,   function ( x )  {  strsplit ( as.character ( x ) , \" \" )[[ 1 ]][ 1 ]  }))   # order clubs and brands by counts  cars $ club <-  ordered ( cars $ club,  names ( sort ( table ( cars $ club ))))  cars $ brand <-  ordered ( cars $ brand,  names ( sort ( table ( cars $ brand ) , decreasing =  TRUE )))   # transform the data for plotting  scars <- ddply ( cars, . ( brand, club ) , .fun =  nrow )   ggplot ()  +   geom_point ( data = scars,  aes ( x = brand, y = club, colour =  log ( V1 )) ,  shape = 15 , size =  4 )  +   scale_colour_gradient ( low = \"#AFE9AF\" , high = \"#0B280B\" )  +   opts ( panel.background = theme_blank () ,  legend.position = \"none\" ,  axis.title.x = theme_blank () ,  axis.title.y = theme_blank () ,  axis.text.x = theme_text ( angle = - 90 ) ,  axis.text.y = theme_text ( colour = \"black\" ))  Here is the final plot:"], "link": "http://rsnippets.blogspot.com/feeds/8425386638666478350/comments/default", "bloglinks": {}, "links": {"http://chartsnthings.tumblr.com/": 1, "http://wyborcza.pl/": 1, "http://2.blogspot.com/": 1, "http://bogumilkaminski.pl/": 1, "http://www.tableausoftware.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Recently I had a discussion with a student about variability of results obtained from cross-validation procedure. While the subject is well known there are not many examples on the web showing it, so I have written its simple presentation. Results from cross-validation are reported as a standard by rpart procedure ( printcp and plotcp ) and optimal cp is selected for tree pruning. Many people I have talked to think that because each time rpart is run on the same data-set the same tree is obtained that also printcp and plotcp results do not change. However, it should be remembered that x-val relative error returned by them is based on random sampling and is not constant. Therefore two runs of rpart might indicate different values of optimal cp . Here is the code that illustrates this situation using Participation data from Ecdat package: library ( Ecdat )  library ( rpart )  data ( Participation )   set.seed ( 1 )  xerror <-  t ( replicate ( 8192 ,  rpart ( lfp ~ ., data = Participation )$ cptable [ , 4 ]))  tree.size <-  factor (   rpart ( lfp ~ ., data = Participation )$ cptable [ , 2 ]  + 1 )  colnames ( xerror )  <- tree.size par ( mfrow =  c ( 1 , 2 ))  boxplot ( xerror, xlab =  \"size of tree\" ,  ylab =  \"X-val Relative Error\" )  plot ( tree.size [ apply ( xerror, 1 , which.min )] , xlab =  \"size of tree\" ,  ylab =  \"# minimal\" )  The resulting plot is the following:  We can see that using x-val criterion tree of size 5 is selected in around 2/3 of cases and size 6 is found best otherwise. The other issue is why there is no variability of x-val for tree size 1 and almost no variability at size 2. The answer is that for those tree sizes the split in every cross-validation fold is made on nominal variable (for example foreign for tree size 1) at the same cut-point and all resulting trees are identical (one outlier for tree size 2 is due single different split). For tree sizes 5 and 6 continuous variables enter the tree ( age and lnnlinc ) and cut-points start moving, so the resulting trees in different cross-validation runs are different."], "link": "http://rsnippets.blogspot.com/feeds/3945150295469204541/comments/default", "bloglinks": {}, "links": {"http://1.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Recently I was writing a code allowing to plot multiple ggplot2 plots on one page. I wanted to replicate standard behavior of plot function that plots graphs in sequence according to mfrow / mfcol option in par . The solution lead me to think of emulating C-like local static variables in R. There are several solutions to this problem but I think that a nice one is by adding attributes to a function. Here is a simple example: f <-  function ( x )  {   y <-  attr ( f, \"sum\" )    if  ( is.null ( y ))  {    y <-  0    }   y <- x + y   attr ( f, \"sum\" )  <<- y   return ( y )  }   It can be applied as follows: > for  ( i in  1 : 5 )  cat ( i, \": \" , f ( i ) , \"\\n\" , sep = \"\" )  1: 1 2: 3 3: 6 4: 10 5: 15 As it can be seen attribute \"sum\" is static but it can be thought of as local because it is not stored directly as a variable in global environment. And here is the application of the concept to the problem of plotting several qplots in a sequence: library ( ggplot2 )  library ( grid )   # setup the ploting grid and plotting sequence mplot.setup <-  function ( nrow, ncol, by.row =  TRUE )  {   attributes ( mplot.seq )  <<-  list ( nrow = nrow, ncol = ncol,   pos =  0 , by.row = by.row )   grid.newpage ()   pushViewport ( viewport ( layout = grid.layout ( nrow, ncol )))  }    # plot at given grid location mplot <-  function ( graph, row, col )  {    print ( graph, vp =  viewport ( layout.pos.row =  row,         layout.pos.col =  col ))  }   # plot the at the next position in the sequence  mplot.seq <-  function ( graph )  {   pos <-  attr ( mplot.seq, \"pos\" )   nrow <-  attr ( mplot.seq, \"nrow\" )    ncol <- attr ( mplot.seq, \"ncol\" )    if  ( attr ( mplot.seq, \"by.row\" ))  {     col <- 1 +  ( pos %% ncol )    row <- 1 +  (( pos %/% ncol ) %% nrow )    }  else  {     row <- 1 +  ( pos %% nrow )    col <- 1 +  (( pos %/% nrow ) %% ncol )    }    attr ( mplot.seq, \"pos\" )  <<- pos +  1   mplot ( graph, row, col )  }   # application example  mplot.setup ( 2 , 4 , FALSE )  for  ( i in  1 : 4 )  {   mplot.seq ( qplot ( iris [ ,i ] , xlab =  names ( iris )[ i ]))   mplot.seq ( qplot ( iris [ , 5 ] , iris [ ,i ] , geom =  \"boxplot\" ,  xlab =  \"Species\" , ylab =  names ( iris )[ i ])  + coord_flip ())  } The following plot is produced by the above code:"], "link": "http://rsnippets.blogspot.com/feeds/5266838632675384479/comments/default", "bloglinks": {}, "links": {"http://en.wikipedia.org/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Last week I published a post on plotting tables in ggplot2 . So the next natural step is to port cdplot to allow simple visualization of categorical variables against a numerical predictor. First part of the story covers binary variables. In this case the solution does not require using cdplot as one can use gam smoother. Here is the code that uses Participation data set from Ecdat package to visualize relationship between age and lfp :  library ( ggplot2 )  library ( Ecdat )  library ( mgcv )  data ( Participation )  cdens <-  with ( Participation,  cdplot ( lfp ~ age, plot =  FALSE , ylevels =  2 : 1 ))  ggplot ()  + geom_smooth ( data = Participation,  aes ( x = age, y = as.integer ( lfp )  -  1 ) ,  method = \"gam\" , formula = y ~ s ( x ) , family = \"binomial\" ,  lwd = 2 , colour = \"black\" )  +   stat_function ( fun = cdens [[ 1 ]] , lwd =  1 , colour =  \"red\" )  +   theme_bw ()  + xlab ( \"age\" )  + ylab ( \"Pr(lfp = 1)\" )   On the plot below the black line is plotted using gam and the red one is derived from cdplot :    For categorical variables having more than two levels the situation is a bit more complex because it is not so easy to use gam -based solution. But using cdplot works in this case also. Here is the code using iris data as an example:  library ( ggplot2 )  cdens <- cdplot ( iris $ Sepal.Length, iris $ Species, plot  = F )  x <-  seq ( min ( iris $ Sepal.Length ) , max ( iris $ Sepal.Length ) ,  length.out =  100 )  y <-  c ( cdens [[ 1 ]]( x ) , cdens [[ 2 ]]( x ) , rep ( 1 , length ( x )))  type <-  ordered ( rep ( levels ( iris $ Species ) , each =  length ( x )) ,  levels = rev ( levels ( iris $ Species )))  x <-  rep ( x, 3 )  qplot ( x, y, geom = \"area\" , fill = type, position = \"identity\" ,  xlab = \"Sepal.Length\" , ylab = \"Species\" )  + theme_bw ()   and the plot it generates looks as follows:"], "link": "http://rsnippets.blogspot.com/feeds/8626460622311952167/comments/default", "bloglinks": {}, "links": {"http://rsnippets.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://3.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Recently I wanted to recreate assocplot using ggplot2 . In the end I propose a simple way to visualize data arranged two-way tables using geom_tile . I used Titanic data set as an example combining age and sex dimensions to get two-way data. I plot residuals of Chi-squared test (like in assocplot ) on the left and probability of survival on the right. A nice feature of geom_tile is that nicely highlights missing data (children were not crew members). Here is a code generating the plots:  library ( ggplot2 )  library ( grid )  library ( reshape2 )   m <- acast ( melt ( unclass ( Titanic )) , Class ~ Age + Sex, sum )  names ( dimnames ( m ))  <-  c ( \"Class\" , \"Age_Sex\" )  df <- melt ( unclass ( chisq.test ( m )$ res ) , value.name =  \"residuals\" )  g1 <- ggplot ( df, aes ( x = Class, y = Age_Sex ))  +   geom_tile ( aes ( fill = residuals ))  +   scale_fill_gradientn ( colours = c ( \"blue\" , \"white\" , \"red\" ))  +   theme_bw ()   m <- acast ( melt ( unclass ( Titanic )) , Class ~ Age + Sex,     function ( x )  { x [ 2 ]  /  sum ( x )})  names ( dimnames ( m ))  <-  c ( \"Class\" , \"Age_Sex\" )  df <- melt ( m, value.name =  \"survived\" )  g2 <- ggplot ( df, aes ( x = Class, y = Age_Sex ))  +   geom_tile ( aes ( fill = survived ))  +   scale_fill_gradient ( low =  \"blue\" , high =  \"red\" )+ theme_bw ()   grid.newpage ()  pushViewport ( viewport ( layout = grid.layout ( 1 , 2 )))  print ( g1, vp =  viewport ( layout.pos.row =  1 , layout.pos.col =  1 ))  print ( g2, vp =  viewport ( layout.pos.row =  1 , layout.pos.col =  2 ))  And the result:"], "link": "http://rsnippets.blogspot.com/feeds/6701952555145602546/comments/default", "bloglinks": {}, "links": {"http://1.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["Recent blog post on Animations in R inspired me to write a code that generates animations of simulation model. For this task I have chosen Schelling's segregation model. Having written the code I have found that one year ago a similar code has been proposed . However, the implementation model is different so I thought it is a nice comparison. Here is the code implementing the model: # 0 - empty  # 2 - first agent type color  # 4 - second agent type color   # initialize simulation  # size  - square size  # perc.full - percentage of lots to be occupied  init <-  function ( side, perc.full )  {   size <-  floor ( side ^  2  * perc.full /  2 )   state <-  matrix ( 0 , side, side )   occupied <-  sample ( side ^  2 , 2  * size )   state [ occupied ]  <-  c ( 2 , 4 )    return ( state )  }   # plot simulation state  # state - simulation state  # i  - simulation iteration  do.plot <-  function ( state, i )  {   side <-  dim ( state )[ 1 ]   x <-  rep ( 1 : side, side )   y <-  rep ( 1 : side, each = side )    par ( fin = c ( 4 , 4 ) , fig = c ( 0 , 1 , 0 , 1 ))    plot ( x , y, axes = F, xlab = \"\" , ylab = \"\" , col = state,    main =  paste ( \"Step\" , i ) , pch =  19 , cex =  40  / side )  }   # perform one step of simulation  # state  - simulation state  # threshold - percent of required agents of the same color  #    in neighborhood  # radius - neighborhood radius  sim.step <-  function ( state, threshold, radius )  {   mod.1 <-  function ( a, b )  {  1  +  (( a -  1 )  %% b )  }   div.1 <-  function ( a, b )  {  1  +  (( a -  1 )  %/% b )  }    unhappy <-  rep ( NA , length ( state ))   side <-  dim ( state )[ 1 ]   check <-  (- radius ):( radius )      #find unhappy agents    for  ( n in  which ( state >  0 ))  {    x <- div.1 ( n, side )    y <- mod.1 ( n, side )    x.radius <- mod.1 ( check + x, side )    y.radius <- mod.1 ( check + y, side )    region <- state [ y.radius, x.radius ]    similar <-  sum ( region == state [ n ])  -  1   total <-  sum ( region >  0 )  -  1    unhappy [ n ]  <-  ( similar < total * threshold )    }   vunhappy <-  which ( unhappy )    # move unhappy agents   vunhappy <- vunhappy [ sample.int ( length ( vunhappy ))]   empty <-  which ( state ==  0 )    for  ( n in vunhappy )  {    move.idx <- sample.int ( length ( empty ) , 1 )    state [ empty [ move.idx ]]  <- state [ n ]    state [ n ]  <-  0   empty [ move.idx ]  <- n   }    return ( state )  }   library ( animation )   # simple wrapper for animation plotting  go <-  function ()  {   s <- init ( 51 , 0.75 )    for  ( i in  1 : 50 )  {    do.plot ( s, i )    last.s <- s   s <- sim.step ( s, 0.6 , 1 )     if  ( identical ( last.s, s ))  {  break  }    }    for  ( j in  1 : 4 )  {    do.plot ( s, i )    }   ani.options ( interval =  5  /  ( i +  2 ))  }   saveGIF ( go ())  and a sample animation it generates:"], "link": "http://rsnippets.blogspot.com/feeds/7080698944982546122/comments/default", "bloglinks": {}, "links": {"http://2.blogspot.com/": 1, "http://www.r-bloggers.com/": 2}, "blogtitle": "R snippets"}, {"content": ["Recently I have calculated Banzhaf power index . I required generation of all subsets of a given set. The code given there was a bit complex and I have decided to write a simple function calculating it. As an example of its application I reproduce Figure 3.5 from Hastie et al. (2009) . The figure shows RSS for all possible linear regressions for prostate cancer data on training subset. The standard approach for such a problem in R is to use leaps package, but I simply wanted to test my function generating all subsets of the set. Here is the code with all.subsets function generating all subsets and its application to prostate cancer data: library ( plyr )  library ( ggplot2 )   all.subsets <-  function ( set )  {   n <-  length ( set )   bin <- expand.grid ( rlply ( n, c ( F, T )))   mlply ( bin, function ( ... )  { set [ c ( ... )]  })  }   file.url <-  \"http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data\"  data.set <- read.table ( file.url, sep =  \"\\t\" , header =  TRUE )  varlist <- all.subsets ( names ( data.set )[ 2 : 9 ])   get.reg <-  function ( vars )  {    if  ( length ( vars )  ==  0 )  {    vars =  \"1\"    }   vars.form <-  paste ( \"lpsa ~ \" , paste ( vars, collapse = \" + \" ))    lm ( vars.form, data = data.set, subset = train )  }   models <- llply ( varlist, get.reg )  models.RSS <- ldply ( models, function ( x )  {    c ( RSS =  sum ( x $ residuals ^  2 ) , k =  length ( x $ coeff ))  })   min.models <- ddply ( models.RSS, . ( k ) , function ( x )  {   x [ which.min ( x $ RSS ) , ]})  qplot ( k, RSS, data  = models.RSS, ylim =  c ( 0 , 100 ) ,  xlab =  \"Subset Size k\" , ylab =  \"Residual Sum-of-Squares\" , )  +  geom_point ( data  = min.models, aes ( x = k, y = RSS ) , colour =  \"red\" )  +  geom_line ( data  = min.models, aes ( x = k, y = RSS ) , colour =  \"red\" )  +  theme_bw ()  And here is the plot it generates:"], "link": "http://rsnippets.blogspot.com/feeds/4013128096872833341/comments/default", "bloglinks": {}, "links": {"http://rsnippets.blogspot.com/": 1, "http://1.blogspot.com/": 1, "http://www-stat.stanford.edu/": 2}, "blogtitle": "R snippets"}, {"content": ["Recently I have read a post on Comparing all quantiles of two distributions simultaneously on R-bloggers . In the post author plots two conditional density plots on one graph. I often use such a plot to visualize conditional densities of scores in binary prediction. After several times I had a problem with appropriate scaling of the plot to make both densities always fit into the plotting region I have written a small snippet that handles it. Here is the code of the function. It scales both x and y axes appropriately:  # class: binary explained variable  # score: score obtained from prediction model  # main, xlab, col, lty, lwd: passed to plot function  # lx, ly: passed to legend function as x and y  cdp <-  function ( class, score,     main =  \"Conditional density\" , xlab =  \"score\" ,      col  =  c ( 2 , 4 ) , lty =  c ( 1 , 1 ) , lwd =  c ( 1 , 1 ) ,     lx =  \"topleft\" , ly =  NULL )  {   class <-  factor ( class )    if  ( length ( levels ( class ))  !=  2 )  {     stop ( \"class must have two levels\" )    }    if  (! is.numeric ( score ))  {     stop ( \"score must be numeric\" )    }   cscore <-  split ( score, class )   cdensity <-  lapply ( cscore, density )   xlim <-  range ( cdensity [[ 1 ]]$ x, cdensity [[ 2 ]]$ x )   ylim <-  range ( cdensity [[ 1 ]]$ y, cdensity [[ 2 ]]$ y )    plot ( cdensity [[ 1 ]] , main = main, xlab = xlab, col  =  col [ 1 ] ,    lty = lty [ 1 ] , lwd = lwd [ 1 ] , xlim = xlim, ylim = ylim )    lines ( cdensity [[ 2 ]] , col  =  col [ 2 ] , lty = lty [ 2 ] , lwd = lwd [ 2 ])    legend ( lx, ly, names ( cdensity ) ,    lty = lty, col  =  col , lwd = lwd )  }  As an example of its application I compare its results to standard cdplot on a simple classification problem: data ( Participation, package =  \"Ecdat\" )  data.set <- Participation data.set $ age2 <- data.set $ age ^ 2  glm.model <-  glm ( lfp ~ ., data  = data.set, family = binomial ( link = probit ))  par ( mfrow =  c ( 1 , 2 ))  cdp ( data.set $ lfp, predict ( glm.model ) , main =  \"cdp\" )  cdplot ( factor ( data.set $ lfp )  ~  predict ( glm.model ) ,   main =  \"cdplot\" , xlab =  \"score\" , ylab =  \"lfp\" )  Here is the resulting plot:"], "link": "http://rsnippets.blogspot.com/feeds/4934028633868818120/comments/default", "bloglinks": {}, "links": {"http://1.blogspot.com/": 1, "http://www.r-bloggers.com/": 2}, "blogtitle": "R snippets"}, {"content": ["Last week I analyzed Shapley-Shubik power index in R. I got several requests to write a code calculating Banzhaf power index . Here is the proposed code. Again I use data from Warsaw School of Economics rector elections (the details are in my last post ). I give the code for calculation of Shapley-Shubik and Banzhaf power indices below. # Constituency list  const <-  c ( 30 , 22 , 27 , 27 , 41 , 2  +  11 , 38  +  5 , 1  +  9 )   # Shapley-Shubik power index  library ( gtools )  perms <- permutations ( 8 , 8 )  outcome <-  apply ( perms, 1 , function ( x )  {   x [ sum ( cumsum ( const [ x ]) < 107 )  +  1 ]  })  sspi <- prop.table ( table ( outcome ))   # Banzhaf power index  subsets <-  length ( const )  -  1  subs <-  matrix ( FALSE , 2 ^ subsets, subsets )  for  ( i in  1 : subsets )  {   subs [ ,i ]  <-  rep ( c ( rep ( FALSE , 2  ^  ( i -  1 )) ,       rep ( TRUE , 2  ^  ( i -  1 ))) ,       2  ^  ( subsets - i ))  }  banzhaf <-  function ( i )  {   other <- const [- i ]   part.sum <-  apply ( subs, 1 , function ( x )  {  sum ( other [ x ])  }  )    sum (( part.sum <  106.5 )  &  (( part.sum + const [ i ])  >  106.5 ))  }  bpi <- prop.table ( sapply ( 1 : 8 , banzhaf ))   # power index comparison  names ( bpi )  <-  c ( \"C_1\" , \"C_2\" , \"C_3\" , \"C_4\" , \"C_5\" ,      \"C_67\" , \"C_89\" , \"C_1011\" )  barplot ( rbind ( bpi, sspi, const /  sum ( const ))[ , order ( const )] ,    col = c ( 2 , 4 , 1 ) , beside =  TRUE ,   legend.text =  c ( \"Banzhaf\" , \"Shapley-Shubik\" , \"Votes\" ) ,   args.legend = list ( x = \"top\" ))  Calculating Banzhaf power index is more complex to implement in R in comparison to Shapley-Shubik power index but the code is faster. At the end of the code I plot comparison of both power indices. It is interesting to note that the results are very similar. Banzhaf power index slightly favors smaller constituencies but the difference is negligible. Again we can see that although constituency #2 has over 50% more votes than combined constituencies #6 and #7 (22 vs. 13) it has exactly the same power according to both indices."], "link": "http://rsnippets.blogspot.com/feeds/8527700089873536899/comments/default", "bloglinks": {}, "links": {"http://rsnippets.blogspot.com/": 2, "http://3.blogspot.com/": 1, "http://en.wikipedia.org/": 2}, "blogtitle": "R snippets"}, {"content": ["This spring we have Rector Elections at Warsaw School of Economics. One of my collegues Tomasz Szapiro agreed to start in the elections. This induced me to write Shapley-Shubik Power Index calculation snippet in R. Rector elections in Warsaw School of Economics are organized in eleven constituencies representing different communities of our school. Each constituency is represented by different number of electors. I have written a simple R code calculating relative power of electors representing those constituencies. To reduce the volume of calculations I have joined some constituencies (6 and 7, 8 and 9, 10 and 11). Here is the code performing the Shapley-Shubik Power Index calculations: library ( gregmisc )    # number of electors in each constituency  const <-  c ( 30 , 22 , 27 , 27 , 41 , 2 +  11 , 38  +  5 , 1  +  9 )   perms <- permutations ( 8 , 8 )  outcome <-  apply ( perms, 1 , function ( x )  {   x [ sum ( cumsum ( const [ x ]) < 107 )  +  1 ]  })   sspi <- prop.table ( table ( outcome ))  names ( sspi )  <-  c ( \"C_1\" , \"C_2\" , \"C_3\" , \"C_4\" , \"C_5\" ,      \"C_67\" , \"C_89\" , \"C_1011\" )  plot ( sspi /  ( const /  sum ( const )) ,   xlab = \"constituency\" , ylab = \"power index / votes\" )  The plot generated by the code shows relative power of constituency to number of its votes. Here it goes:  Interestingly relative power index of almost all constituencies is balanced. However, power index of constituency #2 is very low in comparison to the fraction of electors it possesses."], "link": "http://rsnippets.blogspot.com/feeds/6852004754745502440/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://www.szapiro.net/": 1}, "blogtitle": "R snippets"}, {"content": ["Last week I have compared synchronous and asynchronous implementation of NetLogo Voting model. An interesting afterthought is that synchronous model implementation can be easily made much faster using vectorization. The two versions of the Voting synchronous code are given here: step.syn.slow <-  function ( space )  {   base.x <-  c (- 1 , - 1 , - 1 , 0 , 0 , 1 , 1 , 1 )   base.y <-  c (- 1 , 0 , 1 , - 1 , 1 , - 1 , 0 , 1 )   size <-  nrow ( space )   new.space <- space   for  ( x in  1 : size )  {     for  ( y in  1 : size )  {     nei8 <-  1  +  (( cbind ( x + base.x, y + base.y )  -  1 )  %% size )     nei.pref <-  sum ( space [ nei8 ])      if  ( nei.pref >  0 )  { new.space [ x, y ]  <-  1  }      if  ( nei.pref <  0 )  { new.space [ x, y ]  <-  - 1  }     }    }    return ( new.space )  }   step.syn.fast <-  function ( space )  {    size <-  nrow ( space )   shift.back  <-  c ( 2 : size, 1 )   shift.forward <-  c ( size, 1 :( size - 1 ))     shift.space <- space [ , shift.back ]  +       space [ shift.back, shift.back ]  +       space [ shift.forward, shift.back ]  +       space [ , shift.forward ]  +       space [ shift.back, shift.forward ]  +       space [ shift.forward, shift.forward ]  +       space [ shift.back, ]  +       space [ shift.forward, ]   space [ shift.space >  0 ]  <-  1   space [ shift.space <  0 ]  <-  - 1   return ( space )  }  To compare their execution speed I run the following test: run <-  function ( size, reps )  {   space <-  2  *  matrix ( rbinom ( size ^ 2 , 1 , 0.5 ) , nrow = size )  -  1   slow <- system.time ( replicate ( reps, step.syn.slow ( space )))   fast <- system.time ( replicate ( reps, step.syn.fast ( space )))    cbind ( \"slow\"  = slow [ 3 ] , \"fast\"  = fast [ 3 ])  }   run ( 32 , 512 )  # Result #   slow fast  # elapsed 6.02 0.06   run ( 512 , 32 )  # Result #   slow fast  # elapsed 95.77 1.98  As it could be predicted vectorized version of the code is much faster for small and large problem sizes. Unfortunately such a vectorization is probably impossible to implement in R for asynchronous model."], "link": "http://rsnippets.blogspot.com/feeds/7882466547695928500/comments/default", "bloglinks": {}, "links": {"http://rsnippets.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["This time I have implemented NetLogo Voting model to verify how agent activation scheme influences the results. The code executing the simulation is given below. It simulates two types of voter preferences encoded as 1 and - 1 . In this way average preference equal to 0 indicates 50/50 split. Voters are arranged on square grid with vertical and horizontal wrapping. nei8 <-  function ( x, y, size )  {   base.x <-  c (- 1 , - 1 , - 1 , 0 , 0 , 1 , 1 , 1 )   base.y <-  c (- 1 , 0 , 1 , - 1 , 1 , - 1 , 0 , 1 )   1  +  (( cbind ( x + base.x, y + base.y )  -  1 )  %% size )  }   step.syn <-  function ( space )  {   size <-  nrow ( space )   new.space <- space   for  ( x in  1 : size )  {     for  ( y in  1 : size )  {     nei.pref <-  sum ( space [ nei8 ( x, y, size )])      if  ( nei.pref >  0 )  { new.space [ x, y ]  <-  1  }      if  ( nei.pref <  0 )  { new.space [ x, y ]  <-  - 1  }     }    }    return ( new.space )  }   step.asyn <-  function ( space )  {   size <-  nrow ( space )   old.space <- space  all.x <-  rep ( 1 : size, size )   all.y <-  rep ( 1 : size, each = size )   dec.seq <- sample.int ( length ( all.x ))     for  ( i in dec.seq )  {    x <- all.x [ i ]    y <- all.y [ i ]    nei.pref <-  sum ( space [ nei8 ( x, y, size )])     if  ( nei.pref >  0 )  { space [ x, y ]  <-  1  }    if  ( nei.pref <  0 )  { space [ x, y ]  <-  - 1  }   }    return ( space )  }   simulate <-  function  ( size, is.syn, do.plot )  {   x <-  rep ( 1 : size, size )   y <-  rep ( 1 : size, each = size )    space <-  2  *  matrix ( rbinom ( size ^ 2 , 1 , 0.5 ) , nrow = size )  -  1   rep <-  0    while  ( TRUE )  {    rep <- rep +  1    old.space <- space    if  ( is.syn )  {     space <- step.syn ( space )     }  else  {     space <- step.asyn ( space )     }     if  ( do.plot )  {      par ( pin = c ( 3 , 3 ))      plot ( x , y, axes =  FALSE , xlab = \"\" , ylab = \"\" ,      col = space +  2 , pch =  15 , cex =  40  / size )     }     if  ( all ( old.space == space ))  {      return ( c ( rep, mean ( space )))     }     if  ( rep  >  100 )  {      return ( c ( NA , mean ( space )))     }    }  }  It uses either step.syn function which assumes that all voters make decision at the same time in each time step and step.asyn where all voters are activated in random order. To reproduce graph similar to NetLogo original the do.plot option should be set to TRUE . For example simulate ( 32 , FALSE , TRUE ) generates the following picture:  In order to compare synchronous and asynchronous voter activation regimes I have run the following code: s.r <-  replicate  ( 1024 , simulate ( 32 , TRUE , FALSE ))  as.r <-  replicate  ( 1024 , simulate ( 32 , FALSE , FALSE ))   par ( mfrow =  c ( 1 , 2 ))  boxplot ( cbind ( \"synchronous\" = s.r [ 1 , ] , \"asynchronous\" = as.r [ 1 , ]) ,   main =  \"time\" )  boxplot ( cbind ( \"synchronous\" = s.r [ 2 , ] , \"asynchronous\" = as.r [ 2 , ]) ,   main =  \"mean preference\" )  As shown on the picture below the distribution of mean voter preferences is very similar. However convergence speed of both methods varies greatly. Moreover under synchronous activation around 0.5% of simulations do not reach stable state."], "link": "http://rsnippets.blogspot.com/feeds/1787511085842886203/comments/default", "bloglinks": {}, "links": {"http://ccl.northwestern.edu/": 1, "http://3.blogspot.com/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["There are two limitations of Watts-Strogatz network generator in igraph package: (1) it works only for undirected graphs and (2) rewiring algorithm can produce loops or multiple edges. You can use simplify function of such a graph, but then number of edges in the graph is reduced. Below I give ws.graph function that generates directed graph without these problems: library ( igraph ) library ( colorspace ) resample <- function ( x , ... ) {  x [ sample.int ( length ( x ) , ... )] } ws.graph <- function ( n , nei , p ) {  stopifnot ( nei < n )  edge.list <- vector ( \"list\" , n )  for ( v in 0 : ( n - 1 )) {   edge.end <- union (( v + 1 : nei ) %% n ,        ( v + ( - 1 : - nei )) %% n )   rewire <- ( runif ( length ( edge.end )) < p )   edge.end <- edge.end [ ! rewire ]   rewired <- resample ( setdiff ( 0 : ( n - 1 ) ,    c ( edge.end , v )) , sum ( rewire ))   edges <- rep ( v , 4 * nei )   edges [ c ( F , T )] <- c ( edge.end , rewired )   edge.list [[ v + 1 ]] <- edges  }  graph ( unlist ( edge.list )) }  n <- 8 nei <- 2 p.levels <- c ( 0 , 0.25 , 0.5 , 1 ) reps <- 2 ^ 16 m <- matrix ( 0 , nrow = n , ncol = n ) m <- list ( m , m , m , m ) for ( i in 1 : reps ) {  for ( j in seq_along ( p.levels )) {   g <- ws.graph ( n , nei , p.levels [ j ])   m [[ j ]] <- m [[ j ]] + get.adjacency ( g )  } } x <- rep ( 1 : n , n ) y <- rep ( 1 : n , each = n ) par ( mfrow = c ( 2 , 2 ) , mar = c ( 5 , 5 , 2 , 2 )) for ( i in 1 : 4 ) {  mc <- as.vector ( m [[ i ]]) / reps  mc <- cbind ( mc , mc , mc )  mc <- 1 - mc  plot ( x , y , col = hex ( RGB ( mc )) , pch = 19 , ylab = \"\" ,   xlab = paste ( \"p =\" , round ( p.levels [ i ] , 4 )) , cex = 1.5 ) } This is the resulting plot:   As expected increasing rewiring probability to 1 makes edge probability distribution more uniform."], "link": "http://rsnippets.blogspot.com/feeds/8934357498159547132/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1}, "blogtitle": "R snippets"}, {"content": ["This week I reimplemented part of Conic Sections 1 model from NetLogo. In the model turtles seek to to be in target distance from center. My code takes only one center point, so only circles can be obtained. Apart from turtle location plot given in NetLogo implementation I added: plot showing maximal difference between turtle distance and target distance; decreasing turtle step size. Here is the plot showing final simulation state, but it is also nice to watch the simulation run:   Below is the code generating the simulation: # n: number of turtles # p.x, p.y: location of center # range: turtles have random position from [0,range] #  and will move in random angle a # step: how fast turtles move # target: target distance from center # time: simulation time init <- function ( n , p.x , p.y , range , step , target , time ) {  sim <- list (   turtles = data.frame ( x = runif ( n , max = range ) ,        y = runif ( n , max = range ) ,        a = runif ( n , max = 2 * pi )) ,   p.x = p.x , p.y = p.y , step = step , target = target ,   time = time , max.dist = rep ( NA , time ))  # Calculate turtle distance from center  sim $ turtles $ dist <- sqrt (( sim $ turtles $ x - p.x ) ^ 2 +        ( sim $ turtles $ y - p.y ) ^ 2 )  return ( sim ) } step <- function ( sim ) {  x <- sim $ turtles $ x  y <- sim $ turtles $ y  # Remember last distance and save current distance  o.dist <- sim $ turtles $ dist  n.dist <- sqrt (( x - sim $ p.x ) ^ 2 + ( y - sim $ p.y ) ^ 2 )  sim $ turtles $ dist <- n.dist  # For turtles that are too far and are moving out  # or too close and are moving in randomly change direction  w.dist <- (( n.dist < o.dist ) & ( n.dist < sim $ target )) |     (( n.dist > o.dist ) & ( n.dist > sim $ target ))  sim $ turtles $ a [ w.dist ] <- runif ( sum ( w.dist ) , max = 2 * pi )  sim $ turtles $ x <- x + sin ( sim $ turtles $ a ) * sim $ step  sim $ turtles $ y <- y + cos ( sim $ turtles $ a ) * sim $ step  return ( sim ) } do.plot <- function ( sim ) {  rng <- quantile ( c ( sim $ turtles $ x , sim $ turtles $ y ) ,      c ( 0.05 , 0.95 ))  rng <- round ( rng , - 1 ) + c ( - 10 , 10 )  par ( mai = rep ( 0.5 , 4 ) , mfrow = c ( 1 , 2 ))  plot ( sim $ turtles $ x , sim $ turtles $ y , pch = \".\" ,   xlim = rng , ylim = rng , xlab = \"\" , ylab = \"\" ,   main = \"Turtle location\" )  points ( sim $ p.x , sim $ p.y , col = \"red\" , pch = 20 , cex = 2 )  plot ( sim $ max.dist , type = \"l\" ,   ylim = c ( 0 , max ( sim $ max.dist , na.rm = TRUE ) + 5 ) ,   xlab = \"\" , ylab = \"\" , main = \"Max difference from target\" ) } run <- function ( sim ) {  for ( i in 1 : sim $ time ) {   sim <- step ( sim )   sim $ step <- sim $ step * 127 / 128   sim $ max.dist [ i ] <- max ( sim $ turtles $ dist ) - sim $ target   do.plot ( sim )  } } sim <- init ( 4096 , 128 , 128 , 256 , 2 , 128 , 512 ) set.seed ( 0 ) run ( sim )"], "link": "http://rsnippets.blogspot.com/feeds/7522025470733118524/comments/default", "bloglinks": {}, "links": {"http://ccl.northwestern.edu/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "R snippets"}]