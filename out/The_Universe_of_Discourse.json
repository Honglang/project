[{"blogurl": "http://blog.plover.com\n", "blogroll": [], "title": "The Universe of Discourse"}, {"content": ["My earlier article about my\nhabits using Git attracted some comment, most of which was\nfavorable. But one recurring comment was puzzlement about my seeming\nwillingness to rewrite published history. In practice, this was not\nat all a problem, I think for three reasons: \n\n \n Rewriting published history is not nearly as confusing as\npeople seem to think it will be.\n I worked in a very small shop with very talented developers, so\nthe necessary communication was easy.\n Our repository setup and workflow were very well-designed and\nunusually effective, and made a lot of things easier, including this one.\n \n\nThis article is about item 3. Here's what they do at my previous\nworkplace to avoid most of the annoyances of people rewriting\npublished history. \n\nIf there are N developers, there are N +1 repositories. \n\nThere is a master repository to which only a few very responsible\npersons can push. It is understood that history in this repository\nshould almost never be rewritten, only in the most exceptional\ncircumstances. We usually call this master repository\n gitbox . It has only a couple of branches, typically\n master and deployed . \nYou had better not push incomplete work to master , because \nif you do someone is likely to deploy it.\nWhen you deploy a new version\nfrom master , you advance deployed up to\n master to match. \n\nIn addition, each developer has their own semi-public repository,\nnamed after them, which everyone can read, but which nobody but them\ncan write. Mine is mjd , and that's what we call it when\ndiscussing it, but my personal git configuration calls it\n origin . When I git push origin master I am pushing\nto this semi-public repo. \n\nIt is understood that this semi-public repository is my sandbox and I\nam free to rewrite whatever history I want in it. People building\natop my branches in this repo, therefore, know that they should be\nprepared for me to rewrite the history they see there, or to contact\nme if they want me to desist for some reason. \n\nWhen I get the changes in my own semi-public repository the way I want\nthem, then I push the changes up to gitbox. Nothing is\nconsidered truly \"published\" until it is on the master repo. \n\nWhen a junior programmer is ready to deploy to the master repository,\nthey can't do it themselves, because they only have read access on the\nmaster. Instead, they publish to their own semi-private repository,\nand then notify a senior programmer to review the changes. The senior\nprogrammer will then push those changes to the master repository and\ndeploy them. \n\n \n\nThe semi-public mjd repo has lots of benefits. I can rewrite\nmy branches 53 times a day (and I do!) but nobody will\ncare. Conversely, I don't need to know or care how much my co-workers\nvacillate. \n\nIf I do work from three or four different machines, I can use the\n mjd repo to exchange commits between them. At the end of the\nday I will push my work-in-progress up to the mjd repo, and\nthen if I want to look at it later that evening, I can fetch the\nwork-in-progress to my laptop or another home computer. \n\nI can create and abandon many topic branches without cluttering up the\nmaster repository's history. If I want to send a change or a new test\nfile to a co-worker, I can push it to mjd and then point them\nat the branch there. \n\nA related note: There is a lot of FUD around the rewriting of\npublished history. For example, the \"gitinfo\" robot on the #git IRC\nchannel has a canned message: \n\n \n  Rewriting public history is a very bad idea. Anyone else who\n  may have pulled the old history will have to git pull\n  --rebase and even worse things if they have tagged or\n  branched, so you must publish your humiliation so they know\n  what to do. You will need to git push -f to force the push.\n  The server may not allow this. See receive.denyNonFastForwards \n  (git-config) \n \n\nI think this grossly exaggerates the problems. Very bad!\nHumiliation! The server may deny you! But dealing with a rebased\nupstream branch is not very hard. It is at worst annoying: you have to\nrebase your subsequent work onto the rewritten branch and move any\nrefs that pointed to that branch. If you don't have any subsequent\nwork, you might still have to move refs, if you have any that point to\nit, but you might not have any. \n\n[ Thanks to Rik Signes for helping me put this together. ]"], "link": "http://blog.plover.com/2012/08/26#git-vacillation", "bloglinks": {}, "links": {"http://blog.plover.com/": 1}, "blogtitle": "The Universe of Discourse"}, {"content": ["A monk asked Li Fu, \"Master, how do we know that the Peano axioms are\nconsistent?\" \n\nLi Fu said, \"The axioms are consistent because they have a model.\""], "link": "http://blog.plover.com/2012/08/25#koan", "bloglinks": {}, "links": {}, "blogtitle": "The Universe of Discourse"}, {"content": ["In an article earlier this\nweek , I explored some oddities of defining a toplogy in terms of\nclosed sets rather than open sets, mostly as a result of analogous\nasymmetry in the ZF\nset theory axioms . \n\n\n\n\n\n\nLet's review those briefly. The relevant axioms concern the\noperations by which sets can be constructed. There are two that are\nimportant. First is the axiom of union, which says that if is a family\nof sets, then we can form , which is the union of all\nthe sets in the family. \n\nThe other is actually a family of axioms, the\nspecification axiom schema. It says that for any one-place predicate \n and any set we can construct the subset of for\nwhich holds: \n\n\n \n\nBoth of these are required. The axiom of union is for making bigger sets out of\nsmaller ones, and the specification schema is for extracting smaller sets from bigger\nones. (Also important is the axiom of pairing, which says that if\n and are sets, then so is the two-element set ;\nwith pairing and union we can construct all the finite sets. But we\nwon't need it in this article.) \n\nConspicuously absent is an axiom of intersection. If you have a\nfamily of sets, and you want a set of every element that is in\n some member of , that is easy; it is what the axiom of union gets\nyou. But if you want a set of every element that is in every \nmember of , you have to use specification. \n\nLet's begin by defining this compact notation:\n\n \n\nfor this longer formula:\n\n \n\nThis is our intersection of the members of , taken \"relative to\n \", as we say in the biz. It gives us all the elements of \nthat are in every member of . The is mandatory in\n , because ZF makes it mandatory when you construct a\nset by specification. If you leave it out, you get the Russell paradox. \n\nMost of the time, though, the is not very important. When\n is nonempty, we can choose some element , and\nconsider , which is the \"normal\" intersection of\n . We can easily show that\n\n \n\nfor any whatever, and this immediately implies that\n\n \n\nfor any two elements\nof , so when contains an element , we can omit the\nsubscript and just write\n\n \n\nfor the usual intersection of members of . \n\nEven the usually troublesome case of an\nempty family is no problem. In this case we have no to\nuse for , but we can still take some other set\n and talk about , which is just\n . \n\nNow, let's return to topology. I suggested that we should consider\nthe following definition of a topology, in terms of closed sets, but\nwithout an a priori notion of the underlying space: \n\nA co-topology is a family of sets, called \"closed\"\nsets, such that: \n The union of any two elements of is again in , and\n The intersection of any subfamily of is again in .\n \n\nItem 2 begs the question of which intersection we are talking about\nhere. But now that we have nailed down the concept of intersections,\nwe can say briefly and clearly what we want: It is the intersection\nrelative to . This set contains\nanything that is in any of the closed sets, and so ,\nwhich I will henceforth call , is effectively a universe of\ndiscourse. It is certainly big enough that intersections relative to\nit will contain everything we want them to; remember that\nintersections of subfamilies of have a maximum size, so there\nis no way to make too big. \n\nIt now immediately follows that itself is a closed set, since it\nis the intersection of\nthe empty subfamily of . \n\nIf itself is empty, then so is , and , so that is all right. From here on we will assume that\n is nonempty, and therefore that , with no\nrelativization, is well-defined. \n\nWe still cannot prove that the empty set is closed; indeed, it might\nnot be, because even might not be empty. But as David\nTurner pointed out to me in email, the elements of play a role\ndual to the extratoplogical points of a topological\nspace that has been defined in terms of open sets. There might be\npoints that are not in any open set anywhere, but we may as well\nignore them, because they are topologically featureless, and just\nconsider the space to be the union of the open sets. Analogously and\ndually, we can ignore the points of , which are topologically\nfeatureless in the same way. Rather than considering , we\nshould consider , whose members are the members of ,\nbut with subtracted from each one: \n\n\n \n\nSo we may as well assume that this has been done behind the scenes and\nso that is empty. If we have done this, then the\nempty set is closed. \n\nNow we move on to open sets. An open set is defined to be the\ncomplement of a closed set, but we have to be a bit careful, because ZF\ndoes not have \na global notion of the complement of a set. Instead, it has only\nrelative complements, or differences. is defined as:\n\n \n\nHere we say that the complement of is taken relative to . \n\nFor the definition of open sets, we will say that the complement is\ntaken relative to the universe of discourse , and a set is\nopen if it has the form for some closed set . \n\nAnatoly Karp pointed out on Twitter that we know that the empty set is\nopen, because it is the relative complement of , which we already\nknow is closed. And if we ensure that is empty, as in\nthe previous paragraph, then since the empty set is closed, is\nopen, and we have recovered all the original properties of a\ntopology. \n\n \n Order \n General Topology \n \n with kickback \n no kickback \n \n\nBut gosh, what a pain it was; in contrast recovering the missing\naxioms from the corresponding open-set definition of a topology was\npainless. (John Armstrong said it was bizarre, and probably several\nother people were thinking that too. But I did not invent this\nbizarre idea; I got it from the opening paragraph of John L. Kelley 's\nfamous book General Topology , which has been in print\nsince 1955. \n\n  \n\nHere Kelley deals with the empty set and the universe in\ntwo sentences, and never worries about them again.\nIn contrast, doing the same thing for closed sets was fraught with\ntechnical difficulties, mostly arising from ZF. (The exception was the\nneed to repair the nonemptiness of the minimal closed set , which\nwas not ZF's fault.) \n\n  \n Order \n On Numbers and Games \n \n with kickback \n no kickback \n \n\nI don't think I have much of a conclusion here, except that whatever\nthe advantages of ZF as a millieu for doing set theory, it is\noverrated as an underlying formalism for actually doing\nmathematics. (Another view on this is laid out by J.H. Conway in the\nAppendix to Part Zero of On Numbers and Games (Academic\nPress, 1976).) None of the problems we encountered were technically\nilluminating, and nothing was clarified by examining them in\ndetail. \n\nOn the other hand, perhaps this conclusion is knocking down a straw\nman. I think working mathematicians probably don't concern themselves\nmuch with whether their stuff works in ZF, much less with what silly\ncontortions are required to make it work in ZF. I think day-to-day\nmathematical work, to the extent that it needs to deal with set theory\nat all, handles it in a fairly na\u00efve way, depending on a sort of\nfolk theory in which there is some reasonably but not absurdly big\nuniverse of discourse in which one can take complements and\nintersections, and without worrying about this sort of technical\ndetail. \n\n[ MathJax doesn't work in Atom or RSS syndication feeds, and can't be\nmade to work, so if you are reading a syndicated version of this\narticle, such as you would in Google Reader, or on Planet Haskell or\nPhillyLinux, you are seeing inlined images provided by the Google\nCharts API. The MathJax looks much better, and if you would like to\ncompare, please visit my\nblog's home site . ]"], "link": "http://blog.plover.com/2012/08/24#topology-closed-2", "bloglinks": {}, "links": {"http://www.powells.com/": 6, "https://secure.wikimedia.org/": 1, "http://en.wikipedia.org/": 1, "http://pic.plover.com/": 1, "http://blog.plover.com/": 2}, "blogtitle": "The Universe of Discourse"}, {"content": ["I had long thought that it doesn't matter if we define a topology in\nterms of open sets or in terms of closed sets, because the two\ndefinitions are in every way dual and equivalent. This seems not to\nbe the case: the definition in terms of closed sets seems to be\nslightly weaker than the definition in terms of open sets. \n\nWe can define a topology without reference to\nthe underlying space as follows: A family of sets is a topology\nif it is closed under pairwise intersections and arbitrary unions, and\nwe call a set \"open\" if it is an element of . From this we can\nrecover the omitted axiom that says that is open: it must\nbe in because it is the empty union . We can also recover the underlying space of the topology, or at\nleast some such space, because it is the unique maximal open set\n . The space might be embedded in some\nlarger space, but we won't ever have to care, because that larger\nspace is topologically featureless. From a topological point of view,\n is our universe of discourse. We can then say that a set is\n\"closed\" whenever is open, and prove all the usual\ntheorems. \n\nIf we choose to work with closed sets instead, we run into problems.\nWe can try starting out the same way: A family of sets is a\nco-topology if it is closed under pairwise unions and arbitrary\nintersections, and we call a set \"closed\" if it is an element of\n . But we can no longer prove that . We can\nstill recover an underlying space , but we\ncannot prove that is closed, or identify any maximal closed set\nanalogous to the maximal open set of the definition of the previous\nparagraph. We can construct a minimal closed set\n , but we don't know anything useful about it,\nand in particular we don't know whether it is empty, whereas with the\nopen-sets definition of a topology we can be sure that the empty set\nis the unique minimal open set. \n\nWe can repair part of this asymmetry by changing the \"pairwise unions\"\naxiom to \"finite unions\"; then the empty set is closed because it is a\nfinite union of closed sets. But we still can't recover any maximal\nclosed set. Given a topology, it is easy to identify the unique\nmaximal closed set, but given a co-topology, one can't, and indeed\nthere may not be one. The same thing goes wrong if one tries to define\na topology in terms of a Kuratowski closure operator. \n\nWe might like to go on and say that complements of closed sets are\nopen, but we can't, because we don't have a universe of discourse in\nwhich we can take complements. \n\nNone of this may make very much difference in practice, since we\nusually do have an a priori idea of the universe of discourse, and so\nwe do not care much whether we can define a topology without reference\nto any underlying space. But it is at least conceivable that we might\nwant to abstract away the underlying space, and if we do, it appears\nthat open and closed sets are not as exactly symmetric as I thought\nthey were. \n\nHaving thought about this some more, it seems to me that the ultimate\nsource of the asymmetry here is in our model of set theory. The role\nof union and intersection in ZF is not as symmetric as one might like.\nThere is an axiom of union, which asserts that the union of the\nmembers of some family of sets is again a set, but there is no\ncorresponding axiom of intersection. To get the intersection of a\nfamily of sets , you use a specification axiom. Because of\nthe way specification works, you cannot take an empty intersection,\nand there is no universal set. If topology were formulated in a set\ntheory with a universal set, such as NF, I imagine the asymmetry would\ngo away. \n\n[ This is my first blog post using MathJax , which I hope will\ncompletely replace the ad-hoc patchwork of systems I had been using to\ninsert mathematics. Please email me if you encounter any\nbugs. ] \n\n[ Addendum 20120823: MathJax depends on executing Javascript, and so\nit won't render in an RSS or Atom feed or on any page where the blog\ncontent is syndicated. So my syndication feed is using the Google\nCharts service to render formulas for you. If the formulas look funny,\ntry looking at http://blog.plover.com/ \ndirectly. ] \n\n[ Addendum 20120824: There is a followup to this\narticle . ]"], "link": "http://blog.plover.com/2012/08/21#topology-closed", "bloglinks": {}, "links": {"http://www.mathjax.org": 1, "http://blog.plover.com/": 2}, "blogtitle": "The Universe of Discourse"}, {"content": ["Many life insurance policies, including my own, include a clause that\nsays that they will not pay out in case of suicide. This not only\nreduces the risk to the insurance company, it also removes an\nimportant conflict of interest from the client. I own a life\ninsurance policy, and I am glad that I do not have this conflict of\ninterest, which, as I suffer from chronic depression, would only add\nto my difficulties. \n\nWithout this clause, the insurance company might find itself in the\nbusiness of enabling suicide, or even of encouraging people to commit\nsuicide. Completely aside from any legal or financial problems this\nwould cause for them, it is a totally immoral position to be in, and\nit is entirely creditable that they should try to avoid it. \n\nBut enforcement of suicide clauses raises some problems. The\ninsurance company must investigate possible suicides, and enforce the\nsuicide clauses, or else they have no value. So the company pays\ninvestigators to look into claims that might be suicides, and if their\ninvestigators determine that a death was due to suicide, the company\nmust refuse to pay out. I will repeat that: the insurance company has\na moral obligation to refuse to pay out if, in their best judgment,\nthe death was due to suicide. Otherwise they are neglecting their\nduty and enabling suicide. \n\nBut the company's investigators will not always be correct. Even if\ntheir judgments are made entirely in good faith, they will still\nsometimes judge a death to be suicide when it wasn't. Then the\ndecedent's grieving family will be denied the life insurance benefits\nto which they are actually entitled. \n\nSo here we have a situation in which even if everyone does exactly\nwhat they should be doing, and behaves in the most above-board and\nethical manner possible, someone will inevitably end up getting\nhorribly screwed. \n\n[ Addendum 20120816: It has been brought to my attention that this\npost constains significant omissions and major factual errors. I will\ninvestigate further and try to post a correction. ]"], "link": "http://blog.plover.com/2012/08/15#insurance", "bloglinks": {}, "links": {}, "blogtitle": "The Universe of Discourse"}, {"content": ["Miles Gould asked his Twitter followers whether they used git-add\n-p or git-commit -a and how often. My reply was too\nlong for Twitter, so here it is. \n\nFirst the short version: I use git-add -p frequently, and\n git-commit -a almost never. The exception is when I'm working\non the repo that holds my blog, where I rarely commit changes to more\nthan one or two files at a time. Then I'll usually just\n git-commit -a -m ... . \n\nBut I use git-add -p all the time. Typically what will happen\nis that I will be developing some fairly complicated feature. It will\nnecessitate a bunch of changes and reshuffling elsewhere in the\nsystem. I'll make commits on the topic branch as I go along without\nworrying too much about whether the commits are neatly packaged. \n\nOften I'll be in the middle of something, with a dirty work tree, when\nit's time to leave for the day. Then I'll just commit everything with\nthe subject WIP (\"work-in-progress\"). First thing the next\nmorning I'll git-reset HEAD^ and continue where I left\noff. \n\nSo the model is that the current head is usually a terrible mess,\naccumulating changes as it moves forward in time. When I'm done, I\nwill merge the topic into master and run the tests. \n\nIf they pass, I am not finished. The merge I just created is only a\ndraft merge. The topic branch is often full of all sorts of garbage,\ncommits where I tried one approach, found it didn't work later on, and\nthen tried a different approach, places where I committed debugging\ncode, and so on. So it is now time to clean up the topic branch. Only\nthe cleaned-up topic branch gets published. \n\n Cleaning up messy topic branches \n\nThe core of the cleanup procedure is to reset the head back to the\nlast place that look good, possibly all the way back to the merge-base\nif that is not too long ago. This brings all the topic changes into\nthe working directory. Then: \n\n \n  Compose the commits: Repeat until the working tree is clean: \n  \n  Eyeball the output of git-diff \n  Think of an idea for an intelligible commit\n  Use git-add -p to stage the planned commit\n  Use git diff --cached to make sure it makes sense\n  Commit it\n  \n  Order the commits: Use git-rebase --interactive \n \n\nNotice that this separates the work of composing the commits from the\nwork of ordering them. This is more important than it might appear.\nIt would be extremely difficult to try to do these at the same time.\nI can't know the sensible order for the commits until I know what the\ncommits are! But it's very hard to know what the commits are without\nactually making them. \n\nBy separating these tasks, I can proceed something like this: I\neyeball the diff, and the first thing I see is something about the\npenguin feature. I can immediately say \"Great, I'll make up a commit \nof all the stuff related to the penguin feature\", and proceed to the\n git-add -p step without worrying that there might be other\nstuff that should precede the penguin feature in the commit sequence.\nI can focus on just getting the penguin commit right without needing\nto think about any of the other changes. \n\nWhen the time comes to put the commits in order, I can do it well\nbecause by then I have abstracted away all the details, and reduced\neach group of changes to a single atomic unit with a one-line\ndescription. \n\nFor the most complicated cases, I will print out the diffs, read them\nover, and mark them up in six colors of highlighter: code to throw\naway gets marked in orange; code that I suspect is erroneous is pink.\nI make many notes in pen to remind me how I want to divide up the\nchanges into commits. When a commit occurs to me I'll jot a numbered\ncommit message, and then mark all the related parts of the diff with\nthat number. Once I have the commits planned, I'll reset the topic\nref and then run through the procedure above, using git-add\n-p repeatedly to construct the commits I planned on paper. Since\nI know ahead of time what they are I might do them in the right order,\nbut more likely I'll just do them in the order I thought of them and\nthen reorder them at the end, as usual. \n\nFor simple cases I'll just do a series of git-rebase\n--interactive passes, pausing at any leftover WIP \ncommits to run the loop above, reordering the commits to squash\nrelated commits together, and so on. \n\nThe very simplest cases of all require no cleanup, of course. \n\nFor example, here's my current topic branch, called c-domain ,\nwith the oldest commits at the top: \n\n \n  055a2f7 correction to bulk consumer template\n  d9630bd DomainActivator half of Pobox Domain consumer\n  ebebb4a Add HasDomain role to provide ->domain reader for domain consumers\n  ade6ac6 stubbed domain test\n  e170e77 start templates for Pobox domain consumers\n  067ca81 stubbed Domain::ThumbTwiddler\n  685a3ee cost calculations for DomainActivator\n  ec8b1cc test fixes; trivial domain test passes now\n  845b1f2 rename InvoiceCharge::CreateDomain to ..::RegisterDomain\n(e)  6083a97 add durations to Domain consumers and charges\n  c64fda0 tests for Domain::Activator consumer\n  41e4292 repeat activator tests for 1-year and 3-year durations\n  7d68065 tests for activator's replacement\n(d)  87f3b09 move days_in_year to Moonpig::Util\n  3cd9f3b WIP\n  e5063d4 add test for sent invoice in domain.t\n  c8dbf41 WIP\n  9e6ffa4 add missing MakesReplacement stuff\n  fc13059 bring in Net::OpenSRS module\n(c)  52c18fb OpenSRS interface\n  893f16f notes about why domain queries might fail\n(b)  f64361f rename \"croak\" method to \"fail\" to avoid conflicts\n  4e500ec Domain::Activator initial_invoice_charge_pairs\n(a)  3c5cdd4 WIP\n \n\n3c5cdd4 (a) was the end-of-day state for yesterday; I made it and\npushed it just before I dashed out the door to go home. Such commits\nrarely survive beyond the following morning, but if I didn't make them,\nI wouldn't be able to continue work from home if the mood took me to\ndo that. \n\nf64361f (b) is a prime candidate for later squashing. 5c218fb (c)\nintroduced a module with a \"croak\" method. This turned out to be a\nstupid idea, because this conflicted with the croak function\nfrom Perl's Carp module, which we use everywhere. I needed\nto rename it. By then, the intervening commit already existed. I\nprobably should have squashed these right away, but I didn't think of\nit at the time. No problem! Git means never having to say \"If only\nI'd realized sooner.\" \n\nSimilarly, 6083a97 (e) added a days_in_year function that I later\ndecided at 87f3b09 (d) should be in a utility module in a\ndifferent repository. 87f3b09 will eventually be squashed into\n6083a97 so that days_in_year never appears in this code at all. \n\nI don't know what is in the WIP commits c8dbf41 or 3cd9f3b, for which\nI didn't invent commit messages. I don't know why those are left in\nthe tree, but I can figure it out later. \n\n An example cleanup \n\nNow I'm going to clean up this branch. First I git-checkout -b\ncleanup c-domain so that if something goes awry I can start over\ncompletely fresh by doing git-reset --hard c-domain . That's\nprobably superfluous in this case because origin/c-domain is\nalso pointing to the same place, and origin is my private\nrepo, but hey, branches are cheap. \n\nThe first order of business is to get rid of those WIP \ncommits. I'll git-reset HEAD^ to bring 3c5cdd4 into the\nworking directory, then use git-status to see how many \nchanges there are: \n\n \n   M lib/Pobox/Moonpig/Consumer/Domain/Activator.pm\n   M lib/Pobox/Moonpig/Role/HasDomain.pm\n   M lib/Pobox/Moonpig/TemplateSet.pm\n  ?? bin/register_domains\n   M t/consumer/domain.t\n  ?? t/lib/MockOpenSRS.pm\n \n\n(This is the output from git-status --short , for which I have\nan alias, git s . I use this probably 99 times as often as\nplain git-status .) \n\nNot too bad, probably no need for a printout. The new\n bin/register-domains program can go in right away by itself: \n\n \n  % git add bin \n  % git commit -m 'new register_domains utility program' \n \n\nNext I'll deal with that new mock object class in\n t/lib/MockOpenSRS.pm . I'll add that, then use git-add\n-p to add the related changes from the other files: \n\n \n  % git add t/lib \n  % git add -p \n  ...\n  % git s \n  MM lib/Pobox/Moonpig/Consumer/Domain/Activator.pm\n   M lib/Pobox/Moonpig/Role/HasDomain.pm\n   M lib/Pobox/Moonpig/TemplateSet.pm\n  A t/lib/MockOpenSRS.pm\n  MM t/consumer/domain.t\n  % git ix \n  ...\n \n\nThe git ix command at the end there is an alias for git diff\n--cached : it displays what's staged in the index. The output\nlooks good, so I'll commit it: \n\n \n  % git commit -m 'mock OpenSRS object; add tests' \n \n\nNow I want to see if those tests actually pass. Maybe I forgot\nsomething!\n\n \n  % git stash \n  % make test \n  ...\n  OK\n  % git stash pop \n \n\nThe git-stash command hides the unrelated changes from the\ntest suite so that I can see if the tests I just put into\n t/consumer/domain.t work properly. They do, so I bring back\nthe stashed changes and continue. If they didn't, I'd probably amend\nthe last commit with git commit --amend and try again. \n\nContinuing: \n\n \n  % git diff \n  ...\n  % git add -p lib/Pobox/Moonpig/Role/HasDomain.pm \n  ...\n  % git commit -m 'Domains do not have explicit start dates' \n  % git diff \n  ...\n  % git add -p \n  ...\n  % git commit --fixup :/mock \n \n\nThat last bit should have been part of the \"mock OpenSRS object\"\ncommit, but I forgot it. So I make a fixup commit, which I'll merge\ninto the main commit later on. A fixup commit is one whose subject\nbegins with fixup! . Did you know that you can name a commit\nby writing :/ text , and it names the most recent commit\nwhose message contains that text? \n\nIt goes on like that for a while: \n\n \n  % git diff \n  ...\n  % git add -p ... \n  ...\n  % git commit -m 'Activator consumer can generate special charges' \n  % git diff \n  ...\n  % git checkout lib/Pobox/Moonpig/Role/HasDomain.pm \n \n\nThe only uncommitted change left in HasDomain.pm was a\nsuperfluous line, so I just threw it away. \n\n \n  % git diff \n  ...\n  % git add -u \n  % git commit -m 'separate templates for domain-registering and domain-renewing consumers' \n \n\nBy this time all the remaining changes belong in the same commit, so I\nuse git-add -u to add them all at once. The working tree is\nnow clean. The history is as I showed above, except that in place of\nthe final WIP commit, I have: \n\n \n  a3c0b92 new register_domains utility program\n  53d704d mock OpenSRS object; add tests\n  a24acd8 Domains do not have explicit start dates\n  17a915d fixup! mock OpenSRS object; add tests\n  86e472b Activator consumer can generate special charges\n  5b2ad2b separate templates for domain-registering and domain-renewing consumers\n \n\n(Again the oldest commit is first.) Now I'll get rid of that\n fixup! : \n\n \n  % git rebase -i --autosquash HEAD~6 \n \n\nBecause of --autosquash , the git-rebase menu is\nreordered so that the fixup commit is put just after\nthe commit it fixes up, and its default action is 'fixup' instead of\n'pick'. So I don't need to edit the rebase instructions at all. But\nI might as well take the opportunity to put the commits in the right\norder. The result is: \n\n \n  a3c0b92 new register_domains utility program\n  ea8dacd Domains do not have explicit start dates\n  297366a separate templates for domain-registering and domain-renewing consumers\n  4ef0e28 mock OpenSRS object; add tests\n  c3ab1eb Activator consumer can generate special charges\n \n\nI have two tools for dealing with cleaned-up\nbranches like this one. One is git-vee , which compares two branches. It's\njust a wrapper around the command git log --decorate --cherry-mark\n--oneline --graph --boundary A \"...\" B . \n\nHere's a\ncomparison the original c-domain branch and my new\n cleanup version: \n\n \n  % git vee c-domain \n  * c3ab1eb (HEAD, cleanup) Activator consumer can generate special charges\n  * 4ef0e28 mock OpenSRS object; add tests\n  * 297366a separate templates for domain-registering and domain-renewing consumer\n  * ea8dacd Domains do not have explicit start dates\n  * a3c0b92 new register_domains utility program\n  | * 3c5cdd4 (origin/c-domain, c-domain) WIP\n  |/ \n  o 4e500ec Domain::Activator initial_invoice_charge_pairs\n \n\nThis clearly shows where the original and cleaned up branches diverge,\nand what the differences are. I also use git-vee to compare\npre- and post-rebase versions of branches (with git-vee\nORIG_HEAD ) and local branches with their remote tracking branches\nafter fetching (with git-vee remote or just plain\n git-vee ). \n\nA cleaned-up branch should usually have the same final tree as the\ntree at the end of the original branch. I have another tool, git-treehash ,\nwhich compares trees. By default it compares HEAD with\n ORIG_HEAD , so after I use git-rebase to squash or to split\ncommits, I sometimes run \"git treehash\" to make sure that the tree\nhasn't changed. In this example, I do: \n\n \n  % git treehash c-domain HEAD \n  d360408d1afa90e0176aaa73bf8d3cae641a0850 HEAD\n  f0fd6ea0de7dbe60520e2a69fbec210260370d78 c-domain\n \n\nwhich tells me that they are not the same. Most often this\nhappens because I threw away all the debugging code that I put in\nearlier, but this time it was because of that line of superfluous code\nI eliminated from HasDomain.pm . When the treehashes differ, I'll use\n git-diff to make sure that the difference is innocuous: \n\n \n  % git diff c-domain \n  diff --git a/lib/Pobox/Moonpig/Role/HasDomain.pm b/lib/Pobox/Moonpig/Role/HasDomain.pm\n  index 3d8bb8c..21cb752 100644\n  --- a/lib/Pobox/Moonpig/Role/HasDomain.pm\n  +++ b/lib/Pobox/Moonpig/Role/HasDomain.pm\n  @@ -5,7 +5,6 @@ use Carp qw(croak confess);\n   use ICG::Handy qw(is_domain);\n   use Moonpig::Types qw(Factory Time);\n   use Moose::Util::TypeConstraints qw(duck_type enum subtype);\n  -use MooseX::SetOnce;\n\n   with (\n   'Moonpig::Role::StubBuild',\n\n \n\nOkay then. \n\nThe next task is probably to deal with the older WIP commits. This\ntime I'll omit all the details. But the enclosing procedure looks\nlike this: \n\n \n  % git checkout -b wip-cleanup c8dbf41 \n  % git reset HEAD^ \n  % ... (a lot of git-add -p as above) ...\n  ...\n\n  % git vee c8dbf41 \n  * 4c6ff45 (wip-cleanup) get rid of unused twiddler test\n  * b328de5 test full payment cycle\n  * 201a4f2 abstract out pay_invoice operation\n  * 55ae45e add upper limit (default 30d) to wait_until utility\n  | * c8dbf41 WIP\n  |/ \n  o e5063d4 add test for sent invoice in domain.t\n\n  % git treehash c8dbf41 HEAD \n  7f52ba68923e2ede8fda407ffa9c06c5c48338ae\n  % git checkout cleanup \n  % git rebase wip-cleanup \n \n\nThe output of git-treehash says that the tree at the end of\nthe wip-cleanup branch is identical to the one in the WIP\ncommit it is supposed to replace, so it's perfectly safe to rebase the\nrest of the cleanup branch onto it, replacing the one WIP\ncommit with the four new commits in wip-cleanup . Now the\ncleaned up branch looks like this: \n\n \n  % git vee c-domain \n  * a425aa1 (HEAD, cleanup) Activator consumer can generate special charges\n  * 2bb0932 mock OpenSRS object; add tests\n  * a77bfcb separate templates for domain-registering and domain-renewing consumer\n  * 4c44db2 Domains do not have explicit start dates\n  * fab500f new register_domains utility program\n  = 38018b6 Domain::Activator initial_invoice_charge_pairs\n  = aebbae6 rename \"croak\" method to \"fail\" to avoid conflicts\n  = 45a224d notes about why domain queries might fail\n  = 80e4a90 OpenSRS interface\n  = 27f4562 bring in Net::OpenSRS module\n  = f5cb624 add missing MakesReplacement stuff\n  * 4c6ff45 (wip-cleanup) get rid of unused twiddler test\n  * b328de5 test full payment cycle\n  * 201a4f2 abstract out pay_invoice operation\n  * 55ae45e add upper limit (default 30d) to wait_until utility\n  | * 3c5cdd4 (origin/c-domain, c-domain) WIP\n  | = 4e500ec Domain::Activator initial_invoice_charge_pairs\n  | = f64361f rename \"croak\" method to \"fail\" to avoid conflicts\n  | = 893f16f notes about why domain queries might fail\n  | = 52c18fb OpenSRS interface\n  | = fc13059 bring in Net::OpenSRS module\n  | = 9e6ffa4 add missing MakesReplacement stuff\n  | * c8dbf41 WIP\n  |/ \n  o e5063d4 add test for sent invoice in domain.t\n \n\n git-vee marks a commit with an equal sign instead of a star\nif it's equivalent to a commit in the other branch. The commits in\nthe middle marked with equals signs are the ones that weren't changed.\nThe upper WIP was replaced with five commits, and the lower one with\nfour. \n\nI've been planning for a long time to write a tool to help me with\nbreaking up WIP commits like this, and with branch cleanup in general:\nIt will write each changed hunk into a file, and then let me separate\nthe hunk files into several subdirectories, each of which represents\none commit, and then it will create the commits automatically from the\ndirectory contents. This is still only partly finished, but I think\nwhen it's done it will eliminate the six-color diff printouts. \n\n[ Addendum 20120404: Further observation has revealed that I almost\nnever use git-commit -a , even when it would be quicker to do\nso. Instead, I almost always use git-add -u and then\n git-commit the resulting index. This is just an observation,\nand not a claim that my practice is either better or worse than using\n git-commit -a . ] \n\n[ Addendum 20120825: There is now a followup article about how\nto manage rewriting of published history . ]"], "link": "http://blog.plover.com/2012/03/15#git-habits", "bloglinks": {}, "links": {"https://github.com/": 2, "http://blog.plover.com/": 1}, "blogtitle": "The Universe of Discourse"}, {"content": ["I like to be prepared ahead of time for questions, and one such\nquestion is why Git can't resolve all merge conflicts automatically.\nPeople do show up on IRC asking this from time to time. If you're a\nsophisticated user the answer is obvious, but I've made a pretty good\nliving teaching classes to people who don't find such things\nobvious. \n\nWhat we need is a nice example. In the past my example was sort of\nsilly. You have a file that contains the instruction: \n\n \n   Pay potato tax every April 15\n \n\nOne branch adds an exception:\n\n \n   Pay potato tax every April 15\n    (Except in years of potato blight.)\n \n\nWhile another branch broadens the original instruction: \n\n \n   Pay all tax due every April 15\n \n\nWhat's the correct resolution here? It's easy to understand that\nmashing together the two changes is a recipe for potential\ncatastrophe: \n\n \n   Pay all tax due every April 15\n    (Except in years of potato blight.)\n \n\nYou get fined for tax evasion after the next potato blight. And it's\nsimilarly easy to construct scenarios in which the correct resolution\nis to leave the whole thing in place including the modifier, change\nthe thing to something else completely, delete the whole thing, or to\nrefer the matter to Legal and shut down the whole system until you\nhear back. Clearly it's outside Git's scope to recognize when to call\nin the lawyers, much less to predict what their answer will be. \n\nBut a few months ago I ran into a somewhat less silly example. At\nwork we had two seprate projects, \"Moonpig\" and \"Stick\", each in its\nown repository. Moonpig contained a subsystem, \"Collections\", which\nwe decided would make more sense as part of Stick. I did this work,\nremoving the Collections code from the Moonpig project and integrating\nit into the Stick project. From the point of view of the Moonpig\nrepository, the Collections system was deleted entirely. \n\nMeanwhile, on a parallel branch of Moonpig, R.J.B. Signes made\nsome changes that included bug fixes to the Collections. After I\nremoved the collections, he tried to merge his changes into the master\nbranch, and got a merge conflict, because some of the files to which he was\nmaking bug fixes were no longer there. \n\nThe correct resolution was to perform the rest of the merge without\nthe bug fixes, which Git could conceivably have done. But then the\nunapplied bug fixes needed to be applied to the Collections module\nthat was now in the completely separate Stick project, and there is no\nway Git could have done this, or even to have known it should be\ndone. Human intervention was the only answer."], "link": "http://blog.plover.com/2012/03/04#git-merge", "bloglinks": {}, "links": {}, "blogtitle": "The Universe of Discourse"}, {"content": ["Since 2002, I've given a talk almost every December for the Philadelphia Linux Users'\nGroup . It seems like most of their talks are about the newest and\nbest developments in Linux applications, which is a topic I don't know\nmuch about. So I've usually gone the other way, talking about the\noldest and worst stuff. I gave a couple of pretty good talks about\nhow files work, for example, and what's in the inode structure. \n\nI recently posted about my work on Zach Holman's\n spark program , which culminated in a ridiculous workaround\nfor the shell's lack of fractional arithmetic.\nThat work inspired me to do a talk about all the awful\ncrap we had to deal with before we had Perl. (And the other 'P'\nlanguages that occupy a similar solution space.) Complete materials are\nhere . I hope you check them out, because i think they are fun.\nThis post is a bunch of miscellaneous notes about the talk. \n\nOne example of awful crap we had to deal with before Perl etc. were\ninvented was that some\npeople used to write 'sed scripts', although I am really not sure how\nthey did it. I tried once, without much success, and then for this\ntalk I tried again, and again did not have much success. \n\n \n \n \n \n \n\n\n\n\"The hold space\" is a sed -ism. The basic model of sed is that it\nreads the next line of data into the 'pattern space', then applies a\nbunch of transformations to it, and then prints it out. If you need\nto save this line for later examination, or for emitting later on\ninstead, you can hold it in the 'hold space'. Use of the hold space\nis what distinguishes sed experts from mere sed nobodies like me. So\nI planned to talk about the hold space, and then I got the happy idea\nto analogize the Hold Space to the Twilight Zone, or maybe the Phantom\nZone, a place where you stick naughty data when you don't want it to\nescape. I never feel like audiences appreciate the work I put into\nthis sort of thing; when I'm giving the talk it always sounds too much\nlike a private joke. Explaining it just feels like everyone is sitting\nthrough my explanation of a private joke. \n\n \nThe little guy to the right is known\nas hallucigenia . It is a creature so peculiar that when\nthe paleontologists first saw the fossils, they could not even agree\non which side was uppermost. It has nothing to do with Unix, but I\nput it on the slide to illustrate \"alien horrors from the dawn of\ntime\". \n\nBetween slides 9 and 10 (about the ed line editor) I did a\nquick demo of editing with ed . You will just have to imagine\nthis. I first learned to program with a line editor like ed ,\non a teletypewriter just like the one on slide 8 .\nModern editors are much better. But it used to\nbe that Unix sysadmins were expected to know at least a little ed ,\nbecause if your system got into some horrible state where it couldn't\nmount the /usr partition, you wouldn't be able to run\n /usr/bin/vi or /usr/local/bin/emacs , but you would\nstill be able to use /bin/ed to fix /etc/fstab or\nwhatever else was broken. Knowing ed saved my bacon several\ntimes. \n\n (Speaking of\nteletypewriters, ours had an attachment for punching paper tape, which\nyou can see on the left side of the picture. The punched chads fell\ninto a plastic chad box (which is missing in the picture), and when I\nwas about three I spilled the chad box. Chad was everywhere, and it\nwas nearly impossible to pick up. There were still chads stuck\nin the cracks in the floorboards when we moved out three years\nlater. That's why, when the contested election of 2000 came around, I\nwas one of the few people in North America who was not bemused to\nlearn that there was a name for the little punched-out bits.) \n\nAnyway, back to ed . ed has one and only one diagnostic: if you do\nsomething it didn't like, it prints ? . This explains the\nancient joke on slide\n10 , which first appeared circa 1982 in the 4.2BSD fortune \nprogram. \n\nI really wanted to present a tour de force of sed mastery,\nbut as slides\n24\u201326 say, I was not clever enough. I tried really hard and just\ncould not do it. If anyone wants to fix my not-quite-good-enough\n sed script, I will be quite grateful. \n\nOn slide\n28 I called awk a monster. This was a slip-up;\n awk is not a monster and that is why it does not otherwise\nappear in this talk. There is nothing really wrong with awk ,\nother than being a little old, a little tired, and a little\nunderpowered. \n\nIf you are interested in the details of the classify program,\ndescribed on slide 29 ,\nthe sources are still available from the\n comp.sources.unix archive . People often say \"Why don't\nyou just use diff for that?\" so I may as well answer that\nhere: You use diff if you have two files and you want to see\nhow they differ. You use classify if you have 59 files, of\nwhich 36 are identical, 17 more are also identical to each other but\ndifferent from the first 36, and the remaining 6 are all weirdos, and\nyou want to know which is which. These days you would probably just\nuse md5sum FILES | accumulate , and in\nhindsight that's probably how I should have implemented\n classify . We didn't have md5sum but we had\nsomething like it, or I could have made a checksum program. The\n accumulate utility is trivial. \n\nSeveral people have asked me to clarify my claim to have invented\n netcat . It seems that a similar program with the same name is\nattributed to someone called \"Hobbit\". Here is the clarification: In\n1991 I wrote a program with the functionality I described and called\nit \"netcat\". You would run\n netcat hostname port and it would\nopen a network socket to the indicated address, and transfer data from\nstandard input into the socket, and data from the socket to standard\noutput. I still have the source code; the copyright notice at the top\nsays \"21 October 1991\". Wikipedia says that the\nsame-named program by the other guy was released on 20 March 1996. I\ndo not claim that the other guy stole it from me, got the idea from\nme, or ever heard of my version. I do not claim to be the first or\nonly person to have invented this program. I only claim to have\ninvented mine independently. \n\nMy own current version of the spark program is on GitHub , but I think\n Zach Holman's current\nversion is probably simpler and better now."], "link": "http://blog.plover.com/2012/02/17#holdspace", "bloglinks": {}, "links": {"https://github.com/": 2, "http://ftp.sunet.se/": 1, "http://perl.plover.com/": 9, "http://en.wikipedia.org/": 2, "http://blog.plover.com/": 2, "http://www.phillylinux.org/": 1, "http://pic.plover.com/": 1}, "blogtitle": "The Universe of Discourse"}, {"content": ["A few weeks ago I wrote an\narticle about various methods of arithmetic calculation in shell\nscripts and in bash in particular, but it was all leading\nup to today's article, which I think is more interesting\ntechnically. \n\nA while back, Zach Holman (who I hadn't heard of before, but who is\napparently a bigwig at GitHub) implemented a kind of cute little\nhack, called \" spark \".\nIt's a little shell utility, spark , which gets a list of\nnumbers as its input and uses Unicode block characters to print a\nlittle bar graph of the numbers on the output. For example, the\ninvocation: \n\n \n spark 2,4,6,8\n \n\nwill print out something like: \n\n \n \u2583\u2584\u2586\u2587\n \n\nTo do this in one of the 'P' languages (Perl, Python, PHP, Puby, or\nmaybe Pickle) takes something like four lines of code. But\nM. Holman decided to implement it in bash for maximum\nportability, so it took 72 lines, not counting comments, whitespace,\netc. \n\nLet's begin by discussing the (very simple) mathematics that underlies\ndrawing bar graphs. Suppose you want to generate a set of bars for\nthe numbers $1, $9, $20. And suppose you can actually generate bars\nof integer heights only, say integers from 0\u20137:\n\n \n 0 1 \u2581 2 \u2582 3 \u2583 4 \u2584 5 \u2585 6 \u2586 7 \u2587\n \n\n(M. Holman 's original program did this, even though a height-8 bar \u2588 is\navailable. But the mathematics is the same either way.) \n\n Absolute scaling \n\nThe first step is to scale the input numbers onto the range of the\nbars. To do this, we find a scale factor f that maps dollars\nonto bar heights, say that f bar units = $1. \n\nA reasonable thing to try is to say that since your largest number is\n$20, we will set 7 bar units = $20. Then 0.35 bar units = $1, and\n3.45 bar units = $9. We'll call these the \"natural heights\" for the\nbars. \n\nUnfortunately we can't render the bars at their natural heights; we\ncan only render them at integer heights, so we have to round off.\n0.35 bar units rounds off to 0, so we will represent $1 as no bar at\nall. 3.45 bar units rounds off, badly, to 3, but that's the way it\ngoes; if you try to squeeze the numbers from 1 to 20 into the range 0\nto 7, something has to give. Anyway, this gives \n\n\n\n \n  (1,9,20) \u2192 ( \u2583\u2587)\n \n\nThe formula is: Let max be the largest input number (here,\n20) and let n be the size of the largest possible bar (here, 7).\nThen an input number x becomes a bar of size\n n \u00b7 x / max :\n\n \n \n \n\nNote that this maps max itself to n , and 0 to 0. I'll\ncall this method \"absolute scaling\", because big numbers turn into big\nbars. (It fails for negative numbers, but we'll assume that the\nnumbers are non-negative.) \n\n  (0\u202620) \u2192 ( \u2581\u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2585\u2586\u2586\u2586\u2587\u2587)\n \n\nThere are a couple of variations we might want to apply. First,\nmaybe we don't like that $1 mapped to no bar at all; it's too hard to\nsee, depending on the context. Perhaps we would like to guarantee\nthat only 0 maps to 0. One way to ensure that is to round everything\nup, instead of rounding to the nearest integer: \n\n  (0\u202620) \u2192 ( \u2581\u2581\u2582\u2582\u2582\u2583\u2583\u2583\u2584\u2584\u2584\u2585\u2585\u2585\u2586\u2586\u2586\u2587\u2587\u2587)\n  (1,9,20)  \u2192 (\u2581\u2584\u2587)\n \n\n\n\nAnother benefit of always rounding up is that it uses the bars\nequally. Suppose we're mapping numbers in the range 1\u2013100 to bars of\nheights 1\u20137. If we round off to the nearest integer, each bar\nrepresents 14 or 15 different numbers, except that the tallest bar\nonly represents the 8 numbers 93\u2013100. This is a typical situation.\nIf we always round up, each bar corresponds to a nearly equal range of\nnumbers. (Another way to adjust this is to replace n with\n n +\u00bd in the formula.) \n\n Relative scaling \n\nNow consider the numbers $18, $19, $20. Under the absolute scaling\nmethod, we get: \n\n  (18,19,20) \u2192 (\u2586\u2587\u2587)\n \n\nor, if you're rounding up, \n\n  (18,19,20) \u2192 (\u2587\u2587\u2587)\n \n\nwhich obscures the difference between the numbers. There's only an\n11% difference between the tallest and shortest bar, and that doesn't\nshow up at this resolution. Depending on your application, this might\nbe what you want, but we might also want to avail ourselves of the old\ntrick of adjusting the baseline. Instead of the bottom of the bar\nbeing 0, we can say it represents 17. This effectively reduces every\nbar by 17 before scaling it, so that the the number x is now\nrepresented by a bar with natural height\n n \u00b7( x \u221217) / ( max \u221217). \n\nThen we get these bars: \n\n  (18,19,20) \u2192 (\u2583\u2585\u2587)\n \n\nWhether this \"relative scaling\" is a better representation than \u2587\u2587\u2587\ndepends on the application. It emphasizes different properties of the\ndata. \n\nIn general, if we put the baseline at b , the natural height for\na bar representing number x is:\n\n \n \n \n\nThat is the same formula as before, except that everything has been\nshifted down by b . \n\nA reasonable choice of b would be the minimum input value,\nor perhaps a bit less than the minimum input value. \n\n The shell sucks \n\n\n\nBut anyway, what I really wanted to talk about was how to fix this\nprogram, because I think my solution was fun and interesting. There\nis a tricky problem, which is that you need to calculate values like\n( n - b )/( x - b ), which so you might like to do\nsome division, but as I wrote earlier, bash has no\nfacilities for doing fractional arithmetic . The original program\nused $((\u2026)) everywhere, which throws away fractions. You\ncan work around that, because you don't actually the fractional part\nof ( n - b )/( x - b ); you only need the greatest\ninteger part. But the inputs to the program might themselves be\nfractional numbers, like say 3.5, and $((\u2026)) barfs if you\ntry to operate on such a number: \n\n \n\t$ x=3.5; echo $((x + 1))\n\tbash: 3.5: syntax error: invalid arithmetic operator (error token is \".5\")\n \n\nand you seemingly cannot work around that. \n\nMy first response to this was to replace all the uses of $((\u2026)) \nwith bc , which, as I explained in the previous article, does\nnot share this problem. M. Holman rejected this, saying that calling out\nto bc all the time made the program too slow. And there is\nsomething to be said for this.\nM. Holman also said that bc \nis non-portable, which I find astounding, since it has been in Unix\nsince 1974, but sadly plausible. \n\nSo supposing that you take this complaint seriously, what can you do?\nAre you just doomed? No, I found a solution to the problem that\nsolves all the problems. It is portable, efficient, and correct. It\nis also slightly insane. \n\n Portable fractions in bash \n\nWe cannot use decimal numbers: \n\n \n\t$ x=3.5; echo $((x + 1))\n\tbash: 3.5: syntax error: invalid arithmetic operator (error token is \".5\")\n \n\nBut we can use fractions: \n\n \n\t$ x_n=7; x_d=2; echo $((x_n + x_d))/$((x_d))\n  9/2\n \n\nAnd we can convert decimal inputs to fractions without arithmetic: \n\n \n  # given an input number which might be a decimal, convert it to\n  # a rational number; set n and d to its numerator and\n  # denominator. For example, 3.3 becomes n=33 and d=10;\n  # 17 becomes n=17 and d=1.\n  to_rational() {\n   # Crapulent bash can't handle decimal numbers, so we will convert\n   # the input number to a rational\n   if [[ $1 =~ (.*)\\.(.*) ]] ; then\n    i_part=${BASH_REMATCH[1]}\n    f_part=${BASH_REMATCH[2]}\n    n=\"$i_part$f_part\";\n    d=$(( 10 ** ${#f_part} ))\n   else\n    n=$1\n    d=1\n   fi\n  }\n \n\nThis processes a number like 35.17 in a purely lexical way, extracting\nthe 37 and the 17, and turning them into the numerator 3517 and the\ndenominator 100. If the input number contains no decimal point, our\ntask is trivial: 23 has a numerator of 23 and a denominator of 1. \n\nNow we can rewrite all the shell arithmetic in terms of rational\nnumbers. If a_n and a_d are the numerator and\ndenominator of a , and b_n and b_d are the\nnumerator and denominator of b , then addition, subtraction,\nmultiplication, and even division of a and b are fast,\neasy, and even portable: \n\n \n  # a + b\n  sum_n = $((a_n * b_d + a_d * b_n))\n  sum_d = $((a_d * b_d))\n\n  # a - b\n  diff_n = $((a_n * b_d - a_d * b_n))\n  diff_d = $((a_d * b_d))\n\n  # a * b\n  prod_n = $((a_n * b_n))\n  prod_d = $((a_d * b_d))\n\n  # a / b\n  quot_n = $((a_n * b_d))\n  quot_d = $((a_d * b_n))\n \n\nWe can easily truncate a number to produce an integer, because the\nbuilt-in division does this for us: \n\n \n  greatest_int = $((a_n / a_d))\n \n\nAnd we can round to the nearest integer by adding 1/2 before\ntruncating: \n\n \n  nearest_int = $(( (a_n * 2 + a_d) / (a_d * 2) ))\n \n\n(Since n / d + 1/2 = (2 n + d )/2 d .) \n\nFor complicated calculations, you can work the thing out as several\nsteps, or you can solve it on paper and then just embed a big rational\nexpression. For example, suppose you want to calculate\n(( x - min )\u00b7 number_of_tiers )/ range ,\nwhere number_of_tiers is known to be an integer. You could do\neach operation in a separate step, or you could use instead: \n\n \n tick_index_n=$(( ( x_n * min_d - min_n * x_d ) * number_of_tiers * range_d ))\n tick_index_d=$(( range_n * x_d * min_d ))\n \n\nShould you need to convert to decimals for output, the following is a\nproof-of-concept converter: \n\n \n\tfunction to_dec {\n\t n=$1\n\t d=$2\n   maxit=$(( 1 + ${3:-10} ))\n\t while [ $n != 0 -a $maxit -gt -1 ]; do\n\t next=$((n/d))\n\t if [ \"$r\" = \"\" ]; then r=\"$next.\"; else r=\"$r$next\"; fi\n\t n=$(( (n - d * next) * 10 ))\n\t maxit=$(( maxit - 1 ))\n\t done\n\t r=${r:-'0.'}\n\t}\n \n\nFor example, to_dec 13 8 sets r to\n 1.625 , and to_dec 13 7 sets r to\n 1.857142857 . The optional third argument controls the\nmaximum number of digits after the decimal point, and defaults to\n10. The principal defect is that it doesn't properly round off;\n frac2dec 19 10 0 yields 1. instead of 2. ,\nbut this could be fixed without much trouble. Extending it to\nconvert to arbitrary base output is quite easy as well. \n\nComing next month, libraries in bash for computing with\ncontinued fractions using Gosper's algorithms. Ha ha, just kidding. The obvious next step is to implement base-10 floating-point numbers in bash like this: \n\n \n prod_mantissa=$((a_mantissa * b_mantissa))\n prod_exponent=$((a_exponent + b_exponent))\n \n\n[ Addendum 20120306: David\nJones corrects a number of portability problems in my\nimplementation . ]"], "link": "http://blog.plover.com/2012/02/15#spark", "bloglinks": {}, "links": {"http://drj11.wordpress.com/": 1, "https://github.com/": 1, "http://blog.plover.com/": 2}, "blogtitle": "The Universe of Discourse"}, {"content": ["I have just deactivated my Facebook account, I hope for good. \n\n This\ninterview with Eben Moglen provides many of the reasons, and was\nprobably as responsible as anything for my decision. \n\nBut the straw that broke the camel's back was a tiny one. What finally\npushed me over the edge was this: \"People\nwho can see your info can bring it with them when they use apps.\" .\nThis time, that meant that when women posted reviews of men they had\ndated on a dating review site, the review site was able to copy the\nmen's pictures from Facebook to insert into the reviews. Which\nprobably was not what the men had in mind when they first posted those\npictures to Facebook. \n\nThis was, for me, just a little thing. But it was the last straw\nbecause when I read Facebook's explanation of why this was, or wasn't,\ncounter to their policy, I realized that with Facebook, you cannot\ntell the difference. \n\nFor any particular appalling breach of personal privacy you can never\nguess whether it was something that they will defend (and then do\nagain), or something that they will apologize for (and then do again\nanyway). The repeated fuckups for which they are constantly\napologizing are indistinguishable from their business model. \n\nSo I went to abandon my account, and there was a form they wanted me\nto fill out to explain why: \"Reason for leaving (Required)\": One\nchoice was \"I have a privacy concern.\": \n\n \n\nThere was no button for \"I have 53 privacy concerns\", so I clicked\nthat one. A little yellow popup box appeared, which you can see in the\nscreenshot. It said: \n\n \nPlease remember that you can always control the information that you\nshare and who can see it. Before you deactivate, please take a moment\nto learn more about how privacy works on Facebook. If there is a\nspecific question or concern you have, we hope you'll let us know so\nwe can address it in the future.\n \n\nIt was really nice of Facebook to provide this helpful reminder that\ntheir corporation is a sociopath: \"Please remember that you can always\ncontrol the information that you share and who can see it. You, and\nmy wife, Morgan Fairchild.\" \n\nFacebook makes this insane claim in full innocence, expecting you to\nbelieve it, because they believe it themselves. They make this claim\neven after the times they have silently changed their privacy\npolicies, the times they have silently violated their own privacy\npolicies, the times they have silently opted their users into sharing\nof private information, the times they have buried the opt-out\ncontrols three pages deep under a mountain of confusing verbiage and a\nsign that said \"Beware of The Leopard\". \n\nThere's no point arguing with a person who makes a claim like that.\nNever mind that I was in the process of deactivating my\naccount. I was deactivating my account, and not destroying it,\nbecause they refuse to destroy it. They refuse to relinquish the\npersonal information they have collected about me, because after all\nit is their information, not mine, and they will never, ever\ngive it up, never. That is why they allow you only to\n deactivate your account, while they keep and continue to use\neverything, forever. \n\nBut please remember that you can always control the information that\nyou share and who can see it. Thanks, Facebook! Please destroy it\nall and never let anyone see it again. \"Er, no, we didn't mean that\nyou could have that much control.\" \n\nThis was an abusive relationship, and I'm glad I decided to walk\naway. \n\n[ Addendum 20120210: Ricardo Signes points out that these is indeed an\noption that they claim will permanently delete your account ,\nalthough it is hard to find. ]"], "link": "http://blog.plover.com/2012/02/10#facebook", "bloglinks": {}, "links": {"http://www.betabeat.com/": 1, "http://www.co.uk/": 1, "https://www.facebook.com/": 1}, "blogtitle": "The Universe of Discourse"}, {"content": ["The Test::Fatal \nmodule makes it very easy to test code that is supposed to throw\nan exception. It provides an exception function that takes a\ncode block. If the code completes normally, exception {\n code } returns undefined; if the code throws an exception,\n exception { code } returns the exception value that\nwas thrown. So for example, if you want to make sure that some\nerroneous call is detected and throws an exception, you can use\nthis: \n\n \n  isnt( exception { do_something( how_many_times => \"W\" ) },\n    undef,\n    \"how_many_times argument requires a number\" );\n \n\n\n\nwhich will succeed if do_something(\u2026) throws an exception,\nand fail if it does not. You can also write a stricter test, to look\nfor the particular exception you expect: \n\n \n  like( exception { do_something( how_many_times => \"W\" ) },\n    qr/how_many_times is not numeric/,\n    \"how_many_times argument requires a number\" );\n \n\nwhich will succeed if do_something(\u2026) throws an exception\nthat contains how_many_times is not numeric , and fail\notherwise. \n\nToday I almost made the terrible mistake of using the first form\ninstead of the second. The\nmanual suggests that you use the first form , but it's a bad\nsuggestion. The problem is that if you completely screw up the test\nand write a broken code block that dies, the first test will\ncheerfully succeed anyway. For example, suppose you make a typo in\nthe test code: \n\n \n  isnt( exception { do_something( how_many_tims => \"W\" ) },\n    undef,\n    \"how_many_times argument requires a number\" );\n \n\nHere the do_something(\u2026) call throws some totally different\nexception that we are not interested in, something like unknown\nargument 'how_many_tims' or mandatory 'how_many_times'\nargument missing , but the exception is swallowed and the test\nreports success, even though we know nothing at all about the feature\nwe were trying to test. But the test looks like it passed. \n\nIn my example today, the code looked like this: \n\n \n  isnt( exception {\n  my $invoice = gen_invoice();\n  $invoice->abandon;\n  }, undef,\n   \"Can't abandon invoice with no abandoned charges\");\n });\n \n\nThe abandon call was supposed to fail, for reasons you don't\ncare about. But in fact, the execution never got that far, because\nthere was a totally dumb bug in gen_invoice() (a missing\nrequired constructor argument) that caused it to die with a completely\ndifferent exception. \n\nI would never have noticed this error if I hadn't spontaneously\ndecided to make the test stricter: \n\n \n  like( exception {\n  my $invoice = gen_invoice();\n  $invoice->abandon;\n  }, qr/Can't.*with no abandoned charges/,\n   \"Can't abandon invoice with no abandoned charges\");\n });\n \n\nThis test failed, and the failure made clear that\n gen_invoice() , a piece of otherwise unimportant test\napparatus, was completely broken, and that several other tests I had\nwritten in the same style appeared to be passing but weren't actually\nrunning the code I thought they were. \n\nSo the rule of thumb is: even though the Test::Fatal manual\nsuggests that you use isnt( exception { \u2026 }, undef, \u2026) ,\ndo not. \n\nI mentioned this to Ricardo Signes, the author of the module, and he\nreleased a new\nversion with revised\ndocumentation before I managed to get this blog post published."], "link": "http://blog.plover.com/2012/02/09#exception-tsts", "bloglinks": {}, "links": {"https://metacpan.org/": 3, "https://github.com/": 1}, "blogtitle": "The Universe of Discourse"}, {"content": ["Last week John Speno complained about Unix commands which, when\nused incorrectly, print usage messages to standard error instead of to\nstandard output. The problem here is that if the usage message is\nlong, it might scroll off the screen, and it's a pain when you try to\npipe it through a pager with command | pager and discover\nthat the usage output has gone to stderr, missed the pager, and\nscrolled off the screen anyway. \n\nCountervailing against this, though, is the usual argument for stderr:\nif you had run the command in a pipeline, and it wrote its error\noutput to stdout instead of to stderr, then the error message would\nhave gotten lost, and would possibly have caused havoc further down\nthe pipeline. I considered this argument to be the controlling one,\nbut I ran a quick and informal survey to see if I was in the\nminority. \n\nAfter 15 people had answered the survey, Ron Echeverri pointed out\nthat although it makes sense for the usage message to go to stderr\nwhen the command is used erroneously, it also makes sense for it to go\nto stdout if the message is specifically requested, say by the\naddition of a --help flag, since in that case the message is\nnot erroneous. So I added a second question to the survey to ask\nabout where the message should go in such a case. \n\n83 people answered the first question, \" When a command is misused,\nshould it deliver its usage message to standard output or to standard\nerror? \". 62 (75%) agreed that the message should go to stderr; 11 (13%)\nsaid it should go to stdout. 10 indicated that they preferred a more\ncomplicated policy, of which 4 were essentially (or exactly) what\nM. Echeverri suggested; this brings the total in favor of stderr\nto 66 (80%). The others were: \n\n \n stdout, if it is a tty; stderr otherwise\n stdout, if it is a pipe; stderr otherwise\n A very long response that suggested syslog . \n stderr, unless an empty stdout would cause problems\n It depends, but the survey omitted the option of printing directly\non the console\n It depends\n \n\nI think #2 must have been trying to articulate #1, but (a) got it\nbackwards and (b) missed. #3 seemed to be answering a different\nquestion than the one that was asked; syslog may make sense\nfor general diagnostics, but to use it for usage messages seems\npeculiar. #5 also seems strange to me, since my idea of \"console\" is\nthe line printer hardwired to the back of the mainframe down in the\nmachine room; I think the writer might have meant \"terminal\". \n\n68 people answered the second question, \" Where should the command\nsend the output when the user specifically requests usage\ninformation? \". (15 people took the survey before I added this\nquestion.) 50 (74%) said the output should go to stdout, 12 (18%) to\nthe user's default pager and then to stdout, and 5 (7%) to stderr.\nOne person (The same as #5 above) said \"it depends\". \n\nThanks to everyone who participated."], "link": "http://blog.plover.com/2012/01/11#usage", "bloglinks": {}, "links": {}, "blogtitle": "The Universe of Discourse"}]