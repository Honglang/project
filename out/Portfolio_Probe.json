[{"blogurl": "http://www.portfolioprobe.com/blog\n", "blogroll": [], "title": "Portfolio Probe"}, {"content": ["Should you use daily or monthly returns to estimate volatility? \n Does garch explain why volatility estimated with daily data tends to be bigger than if it is estimated with monthly data? \n Previously \n There are a number of previous posts \u2014 with the variance compression tag \u2014 that discuss the phenomenon of volatility estimated with daily data tending to be higher than if estimated with monthly data.\u00a0 In particular, there is \u201cThe mystery of volatility estimates from daily versus monthly returns\u201d .\u00a0 Quite apropos of the current post is \u201cThe volatility mystery continues\u201d . \n There is also \u201cA practical introduction to garch modeling\u201d . \n Experiment \n Here\u2019s the idea: Simulate 3 years of daily data from a garch model (of the S&P 500).\u00a0 Then estimate the volatility over that three years using both the daily returns and the data aggregated into monthly returns.\u00a0 The key thing with a garch simulation is that we also have the true value of the volatility (unlike market volatility which we never get to see directly). \n Three simulations of 1000 series each were performed.\u00a0 The difference between the simulations was the starting volatility: \n \n average volatility (and hence moderate) \n small (the 1% quantile of volatility of a long history) \n large (the 99% quantile) \n \n Results \n Precision \n There is a striking difference between the accuracy of volatility estimated with daily data versus monthly data.\u00a0 Figures 1 and 2 show the actual volatility versus the estimates for the moderate volatility simulation. \n Figure 1: Volatility estimates from daily data versus true volatility in the moderate volatility simulation.  \n Figure 2: Volatility estimates from monthly data versus true volatility in the moderate volatility simulation. The plots for the small and large volatility simulations (Figures 3 through 6) tell the same story. \n Figure 3: Volatility estimates from daily data versus true volatility in the small volatility simulation.  \n Figure 4: Volatility estimates from monthly data versus true volatility in the small volatility simulation.  \n Figure 5: Volatility estimates from daily data versus true volatility in the large volatility simulation.  \n Figure 6: Volatility estimates from monthly data versus true volatility in the large volatility simulation.  \n Bias \n Figures 7 and 8 show the bootstrap distributions of the mean bias of the daily and monthly estimates in the three simulations.\u00a0 The bias is of volatility expressed in percent. \n Figure 7: Bootstrap distributions of the mean of daily estimate minus actual volatility.  \n Figure 8: Bootstrap distributions of the mean of monthly estimate minus actual volatility.  \n The bias in the monthly estimates is less than one percentage point, and the difference between daily estimates and monthly estimates is even less than that. \n Limitations \n The garch model does not exactly replicate how markets operate.\u00a0 But for these purposes it is probably pretty close. \n Questions \n Are there particular reasons to be suspicious of these results? \n What is the explanation for the daily estimates to be biased downward for very small actual volatilities? \n Summary \n For a garch model the daily estimates of volatility are clearly more accurate than the monthly estimates.\u00a0 It seems hard to believe that the model is wrong enough to overthrow that in reality. \n The garch simulations do exhibit a small amount of variance compression (monthly estimates are smaller than daily).\u00a0 However, it doesn\u2019t look like there is as much in the garch model as in market data.\u00a0 I think there is something else happening in the market as well. \n Appendix R \n \u00a0garch estimation \n See the rugarch section of \u201cA practical introduction to garch modeling\u201d for how the garch model was estimated.\u00a0 It was a garch(1,1) with t-distributed errors. \n It is like the one shown in Appendix R of \u201cThe volatility mystery continues\u201d . \n garch simulation \n To simulate using the average volatility: \n spxgsim0 <- ugarchsim(spxgar, n.sim=756, m.sim=1000,\n\u00a0\u00a0 startMethod=\"unconditional\") \n To start with the most recent volatility, you would say: \n startMethod=\"sample\" \n (which seems like a confusing name for the choice to me). \n To start with the large volatility, do: \n spxgsim01 <- ugarchsim(spxgar, n.sim=756, m.sim=1001,\n\u00a0\u00a0 presigma=quantile(spxgarvol, .01)/100 / sqrt(252)) \n This command includes an attempt to save me from myself. Slightly different numbers (1000, 1001, 1002) are used in the three simulations.\u00a0 This can sometimes signal that something is wrong if objects are inappropriately mixed. \n extract from simulation objects \n The (non-transferable) trick of getting relevant data out of rugarch simulation objects is to use as.data.frame : \n spxgsim0s <- as.data.frame(spxgsim0, which=\"series\")\nspxgsim0v <- as.data.frame(spxgsim0, which=\"sigma\") \n volatility estimation \n The two objects created just above are the inputs to the function that gives us our comparison of volatilities: \n pp.volcompare <- function(series, sigma, month=21)\n{\n\u00a0 # compare volatility estimates\n\u00a0 # placed in public domain 2012 by Burns Statistics\n\u00a0 #\n\u00a0 # testing status:\n\u00a0 # slightly tested, in that monthly returns check \n\u00a0 # results seem to make sense\n\n\u00a0 series <- as.matrix(series)\n\u00a0 sigma <- as.matrix(sigma)\n\u00a0 stopifnot(all(dim(series) == dim(sigma)),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 nrow(series) %% month == 0)\n\n\u00a0 ans <- array(NA, c(ncol(series), 3), list(NULL, \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 c(\"Daily\", \"Monthly\", \"Actual\"))) \u00a0\n\u00a0 ans[, \"Daily\"] <- apply(series, 2, sd) * sqrt(252)\n\u00a0 ans[, \"Actual\"] <- sqrt(colSums(sigma^2) * \n\u00a0\u00a0\u00a0 252/nrow(sigma))\n\u00a0 nmon <- nrow(series) / month\n\u00a0 mseries <- crossprod(kronecker(diag(nmon), \n\u00a0\u00a0\u00a0 rep(1, month)), series)\n\u00a0 ans[, \"Monthly\"] <- apply(mseries, 2, sd) * \n\u00a0\u00a0\u00a0 sqrt(252/month)\n\u00a0 ans\n} \n The bit with kronecker is some matrix magic that gets us the monthly returns that we want.\u00a0 One hint about how this works is that \n diag(36) \n is the R way of getting the 36 by 36 identity matrix. \n Another way of collapsing to monthly returns that is more flexible is: \n crossprod(diag(36)[rep(1:36, each=21),], series) \n We can generalize this formulation to months that have different numbers of days. \n use the compare function \n The function is actually used like: \n spxgsim0comp <- pp.volcompare(spxgsim0s, spxgsim0v) \n The results look like: \n > head(spxgsim0comp)\n   Daily Monthly Actual\n[1,] 0.09791878 0.1441274 0.1057857\n[2,] 0.21882895 0.2677690 0.2176373\n[3,] 0.14704830 0.1454526 0.1499731\n[4,] 0.15218896 0.1361271 0.1541258\n[5,] 0.14157192 0.1166982 0.1454488\n[6,] 0.15529995 0.2052485 0.1573691 \n bootstrap mean bias \n The bootstrapping does a little preparatory work: \n diff0da <- (spxgsim0comp[, \"Daily\"] - \n\u00a0 spxgsim0comp[, \"Actual\"]) * 100\ndiff0ma <- (spxgsim0comp[, \"Monthly\"] - \n\u00a0 spxgsim0comp[, \"Actual\"]) * 100 \n The bootstrapping proper is: \n boot0da <- numeric(1e4)\nfor(i in 1:1e4) {\n\u00a0 boot0da[i] <- mean(sample(diff0da, 1000, replace=TRUE))\n}\nboot0ma <- numeric(1e4)\nfor(i in 1:1e4) {\n\u00a0 boot0ma[i] <- mean(sample(diff0ma, 1000, replace=TRUE))\n} \n The boxplots are created like: \n boxplot(list(small=boot01da, moderate=boot0da,\n\u00a0\u00a0 large=boot99da), col=\"gold\", \n ylab=\"Mean bias in volatility\")"], "link": "http://www.portfolioprobe.com/2012/10/29/volatility-from-daily-or-monthly-garch-evidence/", "bloglinks": {}, "links": {"http://www.portfolioprobe.com/": 15, "http://www.burns-stat.com/": 1}, "blogtitle": "Portfolio Probe"}, {"content": ["US large cap market returns. \n      \n Fine print \n \n The data are from Yahoo \n Almost all of the S&P 500 stocks are used \n The initial post was \u201cReplacing market indices\u201d \n The R code is in marketportrait_funs.R"], "link": "http://www.portfolioprobe.com/2012/10/27/us-market-portrait-2012-week-44/", "bloglinks": {}, "links": {"http://finance.yahoo.com/": 1, "http://www.portfolioprobe.com/": 9}, "blogtitle": "Portfolio Probe"}, {"content": ["Value at Risk and Expected Shortfall are common risk measures.\u00a0 Here is a quick explanation. \n Ingredients \n The first two ingredients are each a number: \n \n The time horizon \u2014 how many days do we look ahead? \n The probability level \u2014 how far in the tail are we looking? \n \n Ingredient number 3 is a prediction distribution of profit and loss given the time horizon, an example is shown in Figure 1. \n Figure 1: A predicted profit and loss distribution.  \n The fourth ingredient is the quantile of the prediction given the probability level. \n Figure 2: Predicted profit and loss distribution with the quantile marked.  \n The final ingredient (not used by Value at Risk) is the tail beyond the quantile. \n Figure 3: Predicted profit and loss distribution with the quantile and tail marked.  \n The measures \n The result is a single number, which is some amount of money. \n Value at Risk (VaR) is the negative of the predicted distribution quantile at the selected probability level.\u00a0 So the VaR in Figures 2 and 3 is about 1.1 million dollars. \n Expected Shortfall (ES) is the negative of the expected value of the tail beyond the VaR (gold area in Figure 3).\u00a0 Hence it is always a larger number than the corresponding VaR. \n Aliases \n As far as I know, Value at Risk is always Value at Risk. \n Expected Shortfall \n Expected Shortfall has a number of aliases: \n \n Conditional Value at Risk (CVaR) \n Mean Shortfall \n Mean Excess Loss \n \n I find \u201cConditional Value at Risk\u201d to be confusing.\u00a0 I can see people thinking it is a Value at Risk given some condition rather than the expected loss beyond the Value at Risk. \n Mean Excess Loss seems the most descriptive name. \n Above we see one concept with several names.\u00a0 Below we see one name with multiple concepts. \n Expected Shortfall has other meanings.\u00a0 Rather than expectation beyond VaR, it can be expectation beyond some requirement (which fits the name better than the concept we are describing). \n Probability level \n Some people say 95% when I say 5%.\u00a0 No worries.\u00a0 We are always dealing with the tail \u2014 meaning something definitely less (in my terminology) than 50%.\u00a0 If a level is at the wrong end for your taste, just think one minus it. \n Abbreviations \n The abbreviation for Value at Risk has the potential for confusion with two other concepts: \n \n variance \n vector autoregression \n \n All of these can be kept from colliding with a convention on capitalization: \n \n VaR: Value at Risk \n var: variance \n VAR: vector autoregression \n \n Road to estimation \n initial ingredients \n There are two initial ingredients: \n \n the asset positions in the portfolio \n the price history of the assets involved \n \n derived ingredients \n The portfolio positions plus the current prices yields the portfolio weights. \n The price history matrix is used to get the return history matrix. \n estimation and collapse \n We are starting with information at the asset level and we need to end up with a one-dimensional view at the portfolio level. \n Perhaps there are hybrid solutions, but essentially either we need to estimate and then collapse to one dimension, or we need to collapse to one dimension and then estimate. \n estimate then collapse \n In this setting we need to estimate a variance matrix .\u00a0 This will often be a factor model , but could be some other estimate. \n The portfolio variance is computed using the variance matrix and the portfolio weights. \n The primary reason to go this route is to break the VaR or ES into pieces based on the factors. \n An alternative to estimating a static model is to fit a multivariate garch model .\u00a0 The VaR or ES could then be found using either a prediction of the garch model or a simulation of it.\u00a0 This is unlikely to be a viable choice. \n collapse then estimate \n Here we use the current weights to create a history of portfolio returns. \n There are a number of ways to get the prediction distribution given the portfolio return history: \n \n fit an assumed distribution \n simulation (use the empirical distribution over some time period) \n garch prediction \n garch simulation \n \n If you assume a normal distribution, then you can estimate the standard deviation to get your prediction distribution.\u00a0 If you assume a t-distribution, then you need additionally to estimate the degrees of freedom or assume the degrees of freedom. \n What is often called the simulation method is really just using the empirical distribution of some specific number of portfolio returns. \n Using a univariate garch model can be quite a good way to estimate VaR and ES. \n Alternatively for VaR, there is a clever approach which is to directly estimate the quantile without worrying about the distribution.\u00a0 This is CaViaR by Engle and Manganelli . \n Summary \n This introduction to Value at Risk and Expected Shortfall is just the beginning of the topic. \n They are very simple concepts \u2014 that is why they are popular.\u00a0 They are probably too simple. \n Appendix R \n The R language is quite a suitable environment for VaR and ES. \n filled areas \n You may be wondering how to fill an area in a plot as is done in Figure 3.\u00a0 The trick is to use the polygon function.\u00a0 Here is the function that created Figure 3: \n function (filename = \"preddisttail.png\") \n{\n\u00a0\u00a0 \u00a0if(length(filename)) {\n\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0png(file=filename, width=512)\n\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0par(mar=c(5,2, 0, 2) + .1)\n\u00a0\u00a0 \u00a0}\n\u00a0\u00a0 \u00a0xseq <- seq(-3, 4, length=200)\n\u00a0\u00a0 \u00a0pd <- dnorm(xseq, mean=.5, sd=1)\n\u00a0\u00a0 \u00a0plot(xseq, pd, type=\"l\", col=\"steelblue\", lwd=3, \n  yaxt=\"n\", ylab=\"\", \n  xlab=\"Predicted Profit/Loss (millions of dollars)\")\n\u00a0\u00a0 \u00a0abline(v=qnorm(.05, mean=.5, sd=1), lty=2, lwd=3)\n\u00a0\u00a0 \u00a0xseqt <- xseq[xseq < qnorm(.05, mean=.5, sd=1)]\n\u00a0\u00a0 \u00a0polygon(c(xseqt, max(xseqt)), c(dnorm(xseqt, \n  mean=.5, sd=1), 0), col=\"gold\", border=NA)\n\u00a0\u00a0 \u00a0lines(xseq, pd, type=\"l\", col=\"steelblue\", lwd=3)\n\u00a0\u00a0 \u00a0abline(h=0, col=\"gray80\", lwd=2)\n\n\u00a0\u00a0 \u00a0if(length(filename)) {\n\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0dev.off()\n\u00a0\u00a0 \u00a0}\n} \n portfolio variance calculation \n The R command to get the portfolio variance given the variance matrix and the weight vector is: \n weight %*% varianceMatrix %*% weight \n This assumes that the weight vector is perfectly aligned to the variance matrix.\u00a0 A safer command would be: \n weight %*% varianceMatrix[names(weight), \n names(weight)] %*% weight"], "link": "http://www.portfolioprobe.com/2012/10/23/the-basics-of-value-at-risk-and-expected-shortfall/", "bloglinks": {}, "links": {"http://www.portfolioprobe.com/": 7, "http://ideas.repec.org/": 1, "http://www.burns-stat.com/": 1}, "blogtitle": "Portfolio Probe"}, {"content": ["The author is Patrick Burns. \n Genesis \n While I was in the early planning stages of a programming class (the one highlighted here ), I became dissatisfied. \n Many elements of programming \u2014 especially good programming \u2014 are not usually discussed in programming classes.\u00a0 The typical class focuses on how to slot the screwdriver into the screw and twist.\u00a0 It does not wonder much about why the screw is down here and not up there.\u00a0 It does not say much about the shape of the structure, nor how to see if it is going to fall down. \n The solution to my dissatisfaction turned out to be a book.\u00a0 A book called Tao Te Programming .\u00a0 It is language-independent \u2014 it\u2019s about principles, not language specifics. \n Target audience \n The book is suitable for those who aspire to label themselves \u201cprogrammer\u201d. \n However, the main target I had in mind was what I call the \u201coccasional programmer\u201d.\u00a0 Someone with little or no formal training in programming but whose job has a programming component. \n Pay off \n My programming has changed from writing the book, so there is hope that it will change the programming of readers. \n I had great fun writing the book, so there is hope that readers will find some amusement. \n More information \n The Tao Te Programming page on the Burns Statistics website has: \n \n Some general information \n The table of contents \n The first five chapters \n Information about buying the book"], "link": "http://www.portfolioprobe.com/2012/10/22/introduction-to-tao-te-programming/", "bloglinks": {}, "links": {"http://www.portfolioprobe.com/": 2, "http://www.burns-stat.com/": 1}, "blogtitle": "Portfolio Probe"}, {"content": ["US large cap market returns. \n      \n Fine print \n \n The data are from Yahoo \n Almost all of the S&P 500 stocks are used \n The initial post was \u201cReplacing market indices\u201d \n The R code is in marketportrait_funs.R"], "link": "http://www.portfolioprobe.com/2012/10/20/us-market-portrait-2012-week-43/", "bloglinks": {}, "links": {"http://finance.yahoo.com/": 1, "http://www.portfolioprobe.com/": 9}, "blogtitle": "Portfolio Probe"}, {"content": ["The authors are Andrie de Vries and Joris Meys. \n Executive summary \n Pretty much all I\u2019d hoped for \u2014 and I had high hopes. \n Significance \n The \u201cDummies\u201d series is popular for introducing specific topics in an inviting way. R For Dummies is a worthy addition to the pack. \n There is a competitor by the name of Statistical Analysis with Excel For Dummies .\u00a0 Now this may also be well-executed \u2014 it probably is \u2014 but some of us are of the opinion that the last two words of the title are redundant. \n There are millions of people doing data analysis in spreadsheets, even though spreadsheets are dangerous .\u00a0 R For Dummies gives these people a reasonable entrance into the world of R where their analyses will be safer, faster and deeper \u2014 after an admittedly uncomfortable period of adjustment. \n The world will be more productive on account of people switching from spreadsheets to R for data analysis.\u00a0 The problem is that hardly any spreadsheet users are aware that there is a more comfortable way of living than walking along a cliff edge.\u00a0 The cliff is invisible to them.\u00a0 Hopefully R For Dummies will help change that by making R a more obvious alternative. \n Strengths \n Chapter 19 is \u201cTen Things You Can Do in R That You Would\u2019ve Done in Microsoft Excel\u201d.\u00a0 They resisted the subtitle \u201cAnd by the way, generally much easier too\u201d. \n Writing introductions to computer programs is astoundingly hard.\u00a0 A key problem is the curse of knowledge.\u00a0 Once we know something, we find it hard to imagine not knowing.\u00a0 So a writer who knows the program is not good, but a writer who doesn\u2019t know the program is no use as a navigator. \u00a0 R For Dummies sails these stormy seas well, partly from the discipline of the \u201cDummies\u201d template, partly from the skill of the authors.\u00a0 They are competent in R yet both relatively new converts. \n The really important decisions when writing introductory documentation is what not to say.\u00a0 I think Andrie and Joris made very good selections for silence. \n \u2026 you discover the power hidden behind the 18th letter of the alphabet. \n Weaknesses \n There are (at least) two major styles of learning.\u00a0 There are those (like me) who are perfectly happy to gather up detail after detail, and only eventually put the details into a big picture.\u00a0 There are others who need to see the big picture before they can pick up any details (and possibly revise the picture). \n R For Dummies is probably closer to what the big-picture people need than any other R documentation I know, but I doubt it really fulfills the need.\u00a0 However, that is probably too much to ask.\u00a0 I think a separate (short) document is called for. \n If you\u2019re a big-picture learner, I\u2019d be interested to hear what you think should go into such a document. \n Cool things I didn\u2019t know \n clipboard \n For Windows there is the writeClipboard function.\u00a0 Give it a character vector as an argument and those are put onto the clipboard: \n > writeClipboard(head(letters)) \n# hit <return> then <control>v\n> a\nError: object 'a' not found\n> b\nError: object 'b' not found\n> c\nfunction (..., recursive = FALSE)\u00a0 .Primitive(\"c\")\n> d\nError: object 'd' not found\n> e\nError: object 'e' not found\n> f\nError: object 'f' not found \n Of course the function is better used by pasting into other programs rather than pasting back into R. \n multiple sorting \n If you are using order to sort based on multiple items and you want some in increasing order and some decreasing, then there is the xtfrm function to help you. \n Specifics \n There is a companion post \u201cAnnotations for R For Dummies \u201c that has a number of specific comments on the book."], "link": "http://www.portfolioprobe.com/2012/10/15/review-of-r-for-dummies/", "bloglinks": {}, "links": {"http://www.portfolioprobe.com/": 2, "http://www.burns-stat.com/": 1, "http://www.dummies.com/": 2}, "blogtitle": "Portfolio Probe"}, {"content": ["Here are detailed comments on the book.\u00a0 Elsewhere there is a review of the book . \n How to read R For Dummies \n In order to learn R you need to do something with it.\u00a0 After you have read a little of the book, find something to do.\u00a0 Mix reading and doing your project. \n You cannot win if you do not play. \n Two complementary documents \n They are also complimentary. \n Some hints for the R beginner \n \u201cSome hints for the R beginner\u201d is\u00a0 a set of pages that give you the basics of the R language.\u00a0 It is a completely different approach to the one R For Dummies takes \u2014 you may want to investigate it. \n The R Inferno \n If you are just at the beginning of learning R, you should ignore The R Inferno (except perhaps Circle 1). \n When you start using R for real and run into problems, that is the time to pick it up and see if it helps. \n Missing piece \n There is one thing that I think is missing in R For Dummies .\u00a0 Actually it isn\u2019t missing, it comes at the very end while I think it should be at the start. \n That piece is the search function.\u00a0 More specifically the way that R operates that is highlighted by the results of the search function. \n The start of \u201cSome hints for the R beginner\u201d talks about search and how R finds objects. \n How to use these annotations \n first learning \n If you are new to R and first reading the book, then you should probably mostly ignore my comments.\u00a0 However, when you are confused by something in the book, you can look to see if there is a comment on that page that pertains to what you are confused about. \n revising \n On further reading, these comments are more likely to be of use.\u00a0 Some are clarifications, some are extensions. \n Page by page comments \n These comments are based on the first printing. \n Page 10 \n There is more history in the Inferno-ish R presentation . \n Page 11 \n distribution \n I\u2019m not a lawyer, but I think the phrasing about redistribution is not right.\u00a0 I think it should say \u201cchange and redistribute\u201d rather than \u201cchange or redistribute\u201d. \n If what you do never leaves your entity, then you can do absolutely whatever you want.\u00a0 That is the free as in speech part.\u00a0 Legalities only come into play if what you do is made available to others.\u00a0 It is a common misunderstanding that you are restricted in what you do within your own world. \n runs anywhere \n The book highlights that R runs on many operating systems.\u00a0 It fails to make clear that the objects that it creates on the operating systems are all the same.\u00a0 You can start a project on a Linux machine at work, continue it while you commute with your Mac laptop, and then finish it on your Windows machine at home.\u00a0 No problem. \n Page 12 \n The book should tell you not to be afraid of new words.\u00a0 New words like \u201cvector\u201d.\u00a0 You don\u2019t need to make friends with them right away, but don\u2019t be scared off. \n ( technical ) Unhappily the word \u201cvector\u201d in R has several meanings \u2014 so it is unfortunate that it is the first new word.\u00a0 The meaning used throughout the book is the most common meaning.\u00a0 See The R Inferno (Circle 5.1) for the gory details. \n Page 13 \n statistics \n Pretty much everywhere in the book where it says \u201cstatistics\u201d I would prefer \u201cdata analysis\u201d instead.\u00a0 Statistics in many people\u2019s mind is formal and academic, not like what they do.\u00a0 More people can feel comfortable doing data analysis than statistics. \n In addition to the fear factor, there really is a (slight) difference between data analysis and statistics.\u00a0 I think data analysis is more important even though I\u2019m trained as a statistician. \n fields of study \n There are additional fields of study where R is used that are not considered to be data hotbeds, such as music and literature.\u00a0 The flexibility of R becomes very important for data in non-traditional forms. \n Page 23 \n vectors \n If you are new to R, you shouldn\u2019t expect yourself to understand this discussion.\u00a0 Just let it sink in over time. \n Page 24 \n assignment operator \n Always put spaces around the assignment operator. That makes the code much more readable. \n The book tells you on page 63 that you can use = as well.\u00a0 You will see both used.\u00a0 They are mostly the same (differences are explained in The R Inferno , Circle 8.2.26).\u00a0 I agree with the book\u2019s approach to use <- but really you can use either. \n Page 28 \n RStudio \n A nice feature of the RStudio workspace view is that it categorizes the objects. \n Page 29 \n Windows pathnames (technical) \n The book implies that you can not write Windows pathnames with backslashes.\u00a0 Actually you can, you just need to put a double backslash where you want a backslash.\u00a0 Hence it is easier and (often) less confusing to use slashes rather than backslashes. \n Page 30 \n loading objects (technical) \n It is possible to use attach instead of load .\u00a0 If you load an object, then it is put into your global environment.\u00a0 If you attach an object, it is put separately on the search list.\u00a0 If you modify an object that has been attached, then the modified version goes into your global environment. \n Page 32 \n vectorization \n There are different forms of vectorization, and the book doesn\u2019t make that explicit.\u00a0 Vectorization can be put into three categories: \n \n vectorization along vectors \n summary \n vectorization across arguments \n \n Functions like sum and mean are vectorized in the sense that they take a vector and summarize it.\u00a0 This is done in pretty much all languages, it is not special. \n Vectorization as it is commonly spoken of in R is vectorization along vectors.\u00a0 For example the addition operator as seen on page 24.\u00a0 This is the form of vectorization that is so useful and powerful in R. \n You should not expect the third form of vectorization in R.\u00a0 However, it does exist in a few functions.\u00a0 The sum and mean functions do summary-type vectorization: \n > sum(1:3)\n[1] 6\n> mean(1:3)\n[1] 2 \n The sum function also does vectorization along arguments: \n > sum(1, 2, 3)\n[1] 6 \n That is basically anomalous.\u00a0 The mean function is more typical by not doing this form of vectorization: \n > mean(1, 2, 3) # WRONG\n[1] 1 \n Unfortunately you don\u2019t get an error or a warning in this case.\u00a0 Do not expect this form of vectorization. \n Page 33 \n error message \n Getting error messages can be frightening for a while.\u00a0 But it\u2019s not the end of the world.\u00a0 Relax. \n Page 36 \n names (technical) \n In fact it is possible to get any name that you want, but you probably don\u2019t want to. \n return (technical) \n Actually return is not a reserved word, but you should treat it as if it were. \n > break <- 1\nError in break <- 1 : invalid (NULL) left side of assignment\n> while <- 1\nError: unexpected assignment in \"while <-\" \n > return <- 1 #do NOT do this\n> \n Page 37 \n F and T \n I wish to emphasize the advice in the book: \n \n never abbreviate TRUE and FALSE to T and F \n avoid using T and F as object names \n \n Page 42 \n library \n The book suggests (with a slight revision on page 361) to load packages with the library function.\u00a0 Some of us prefer require instead of library for this use.\u00a0 The best use of library is without arguments \u2014 this gives you a list of available packages. \n > library(fortunes) # load package\n> require(fortunes) # same thing\n> library() # get list of packages\n> require() # don't do this\nLoading required package: \nFailed with error:\u00a0 \u2018invalid package name\u2019 \n contributed packages \n I think the authors might be being a little too polite in their description of the quality of contributed packages. \n I find base R to be phenomenally clean code \u2014 it is hard to find commercial code that is less buggy.\u00a0 The quality of contributed packages varies widely.\u00a0 A few are up to the standards of base R, some are quite good, I\u2019m sure there are a few dreadful ones. \n With contributed packages you need to be more cautious than when only using base R functionality.\u00a0 Or perhaps I should say that you always need to be vigilent, but if you are using contributed packages, there is a larger chance that a problem is due to a package rather than your own fault. \n Without inspecting the code, I know of two clues to suggest a package is of good quality: \n \n widely used \n good documentation \n \n A widely used package \u2014 such as those highlighted in the book \u2014 is an indication that a lot of problems with the code have been fixed or didn\u2019t exist in the first place. \n Many people use the test of the cleanliness of restaurant restrooms to infer the cleanliness of the kitchen.\u00a0 Likewise, carefully written documentation is likely to be a sign of clean code. \n Page 46 \n exponentiation (technical) \n It is not a good idea to use ** to mean exponentiation \u2014 it is not out of the question for that to go away.\u00a0 Stick to using the ^ operator. \n Page 49 \n log and exp \n The sentence a little below mid-page about creating the vector inside exp should say inside the log function. \n Page 52 \n infinity \n The last sentence on the page should say 10^309 and 10^310 rather than 10^308 and 10^309 . \n Page 54 \n table 4-3 \n You are unlikely to use any of these except for is.na , which you may use quite a lot. \n Page 55 \n types of vectors \n All of the types of vectors listed may have missing values ( NA ). \n Page 56 \n integer versus double \n One of the nice things about R is that you hardly ever need to worry about whether something is stored as an integer or a double. \n largest integer (technical) \n We can see how big the biggest integer is in a couple different ways: \n > format(2^31 - 1, big.mark=\",\")\n[1] \"2,147,483,647\"\n> .Machine$integer.max\n[1] 2147483647 \n Page 59 \n indexing \n What is called \u201cindexing\u201d in the book is more commonly called \u201csubscripting\u201d. \n Page 64 \n missing value testing \n It is a common mistake to try testing missing values with a command like: \n > x == NA \n That doesn\u2019t work \u2014 you need to use is.na . \n Page 65 \n any and all \n The last sentence on the page is a false statement.\u00a0 The any and all functions are smart enough to know when they can know the answer and when they can\u2019t: \n > all(c(NA, FALSE))\n[1] FALSE\n> all(c(NA, TRUE))\n[1] NA\n> any(c(NA, FALSE))\n[1] NA\n> any(c(NA, TRUE))\n[1] TRUE \n Page 72 \n assigning to character (technical) \n It is more correct to think of the mode being character than the class being character. \n Page 82 \n grep \n Alternatively, you can use the value argument of grep : \n > grep(\"New\", state.name, value=TRUE)\n[1] \"New Hampshire\" \"New Jersey\"\u00a0\u00a0\u00a0 \"New Mexico\"\u00a0 \u00a0\n[4] \"New York\" \n Page 83 \n sub versus gsub \n Here is an example that should make clear the difference between sub and gsub : \n > gsub(\"e\", \"a\", c(\"sheep\", \"cheap\", \"cheep\"))\n[1] \"shaap\" \"chaap\" \"chaap\"\n> sub(\"e\", \"a\", c(\"sheep\", \"cheap\", \"cheep\"))\n[1] \"shaep\" \"chaap\" \"chaep\" \n Page 86 \n factor attributes (technical) \n The book says: \n [factors are] neither character vectors nor numeric vectors, although they have some attributes of both. \n This sentence is using \u201cattribute\u201d in the non-technical sense.\u00a0 But attributes in the technical sense do come into play: factors have \u201cclass\u201d and \u201clevels\u201d attributes. \n Page 87 \n factor versus character \n Notice how the factor is printed differently than the character vector. \n Page 91 \n American regions (off topic) \n There is a brilliant analysis of North American regions called The Nine Nations of North America . \n Page 94 \n date sequences \n You might wonder what happens if you start on the thirty-first of the month rather than the first.\u00a0 If you wonder something, try it out to see what happens: \n > myStart <- as.Date(\"2012-12-31\")\n> seq(myStart, by=\"1 month\", length=6)\n[1] \"2012-12-31\" \"2013-01-31\" \"2013-03-03\" \"2013-03-31\"\n[5] \"2013-05-01\" \"2013-05-31\" \n The result is a bit Aspergery, and not to everyone\u2019s taste.\u00a0 But perhaps we can do better: \n > seq(myStart + 1, by=\"1 month\", length=6) - 1\n[1] \"2012-12-31\" \"2013-01-31\" \"2013-02-28\" \"2013-03-31\"\n[5] \"2013-04-30\" \"2013-05-31\" \n Wondering is great, experimenting is even greater. \n Page 104 \n one-dimensional arrays (technical) \n Regular vectors are not dimensional at all in the technical sense, but we think of them as being one-dimensional.\u00a0 But there really are one-dimensional arrays.\u00a0 They are almost like plain vectors but not quite. \n Page 106 \n playing with attributes \n For large objects you often won\u2019t like the response you get when you do: \n > attributes(x) \n Often better is to just look at what attributes the object has: \n > names(attributes(x)) \n Page 109 \n extracting values from matrices \n The flexibility of subscripting matrices (and data frames) as vectors is a curse as well as a blessing. \n If you want to do: \n > x[-2,] \n and you do: \n > x[-2] \n then you will get an entirely different result.\u00a0 This can be a hard mistake to find \u2014 a few pixels difference on your screen can have a big impact. \n Page 113 \n first.matrix \n The example on this page assumes that first.matrix is as it was first created, not as it has been modified in the intervening exercises. \n Page 114 \n matrix operations \n So adding numbers by row is easy.\u00a0 How to add them by column?\u00a0 One way is: \n > fmat <- matrix(1:12, ncol=4)\n> fmat + rep((1:4)*10, each=nrow(fmat))\n\u00a0\u00a0\u00a0\u00a0 [,1] [,2] [,3] [,4]\n[1,]\u00a0\u00a0 11\u00a0\u00a0 24\u00a0\u00a0 37\u00a0\u00a0 50\n[2,]\u00a0\u00a0 12\u00a0\u00a0 25\u00a0\u00a0 38\u00a0\u00a0 51\n[3,]\u00a0\u00a0 13\u00a0\u00a0 26\u00a0\u00a0 39\u00a0\u00a0 52 \n This uses the rep function to create a vector with as many elements as the matrix has (assuming the vector being replicated has length equal to the number of columns), and the replicated values are in the desired positions. \n Page 116 \n inverting a matrix \n The reason that the command to invert a matrix is not intuitive is because it is seldom the case that (explicitly) inverting a matrix is a good idea. \n Page 117 \n vectors as arrays (technical) \n Actually vectors, in general, are not arrays at all.\u00a0 The difference is of little consequence, however. \n third array dimension (technical) \n I call the items in the third dimension of an array \u201cslices\u201d rather than \u201ctables\u201d.\u00a0 I\u2019m not aware of any standardized nomenclature.\u00a0 I don\u2019t think \u201ctables\u201d is such a good choice because there are other meanings of \u201ctable\u201d in R. \n array filling (technical) \n I\u2019m not able to follow the sentence in the book describing how arrays are filled.\u00a0 How I think of it is that the first subscripts vary fastest (no matter how many dimensions are in the array). \n Page 119 \n rows and columns (technical) \n Maybe my brain went on strike, but I think that \u201crows\u201d and \u201ccolumns\u201d are reversed in the first paragraph on the page. \n Page 120 \n data frame structure \n Note that all the vectors that make up the columns need to be the same length. \n data frame structure (technical) \n It is possible for a \u201ccolumn\u201d of a data frame to be a matrix, in which case the number of rows needs to match. \n data frame length \n Note that the length of a data frame is different from the length of the equivalent matrix.\u00a0 The length of the data frame is the number of columns, while the length of the matrix is the number of columns times the number of rows. \n Page 122 \n character versus factor \n The book suggests always making sure that data frames hold character vectors instead of factors in order to reduce problems.\u00a0 The other main route to avoid frustration is to always assume that there are factors. \n The thing you don\u2019t want to do is assume that what is really a factor is a character vector. \n naming variables \n If in the middle of the page where it says \u201cIn the previous section\u201d you don\u2019t know what they are talking about, not to worry \u2014 you\u2019re not alone. \n as with matrices \n I\u2019m not clear on the reference to matrices at the very bottom of the page. \n Page 124 \n data frame subscripting \n You can get a column of a data frame using either the $ or [ form of subscripting.\u00a0 But there is a difference: \n > baskets.df$Granny\n[1] 12\u00a0 4\u00a0 5\u00a0 6\u00a0 9\u00a0 3\n> baskets.df[,Granny]\nError in `[.data.frame`(baskets.df, , Granny) : \n object 'Granny' not found\n> baskets.df[,\"Granny\"]\n[1] 12\u00a0 4\u00a0 5\u00a0 6\u00a0 9\u00a0 3 \n Note the quotes or lack thereof. \n Page 130 \n pieces of a list \n I prefer calling the pieces of a list \"components\" rather than \"elements\".\u00a0 One reason is that a component of a list can be another list, and hence not very elementary. \n Page 139 \n The functions that you write are essentially the same as the inbuilt functions.\u00a0 They are first-class citizens. \n Page 152 \n functional programming \n You can very effectively use R without having a clue what \"functional programming\" means.\u00a0 The important idea behind functional programming is safety -- the data that you want to use is almost surely the data that really is being used. \n Page 153 \n calculation example \n The object names were obviously changed midstream. fifty should be half and hundred should be full . \n Page 157 \n generic functions (technical) \n A detail that only occasionally really matters is that the argument names in methods should match the argument name in the generic.\u00a0 You don't want to have the argument called x in the generic but object in a method. \n Page 171 \n looping without loops \n Using apply functions is really hiding loops rather than eliminating them. \n Page 172 \n number of apply functions \n Not that it matters, but I count 8 apply functions in the base package in version 2.15.0.\u00a0 There is also a reasonably large number of apply functions in contributed packages. \n Page 188 \n error checking (technical) \n Another way to write the check for out of bounds values is: \n stopifnot(all(x >= 0 & x <= 1)) \n This will create an appropriate error message if there is a violation. \n This will take multiple conditions separated by commas.\u00a0 So you can have checks like: \n stopifnot(is.matrix(x), is.data.frame(y)) \n to make sure that x is a matrix and y is a data frame. \n Page 190 \n technical tip (technical) \n The first sentence starts: \n In fact, functions are generic ... \n It should read: \n In fact, some functions are generic ... \n Page 192 \n factor to numeric \n The book gives the efficient method of converting a factor to numeric: \n as.numeric(levels(x))[x] \n The slightly less efficient but easier to remember method is: \n as.numeric(as.character(x)) \n Don't forget the as.character -- it matters. \n problems with factors (technical) \n Circle 8.2 of The R Inferno starts with a number of items about factors. \n Page 193 \n documentation quality \n Unfortunately, I think the authors are painting too rosy of\u00a0 a picture of the quality of R documentation.\u00a0 There probably is some great documentation for any task or issue that you have, but you may have a significant search on your hands to find that great document. \n Page 194 \n help files \n It takes practice to learn how to use help files well.\u00a0 It doesn't help that sections of the help files are in the wrong order (in my opinion).\u00a0 The \"See also\" and \"Examples\" should be near the top, \"Details\" should be at the bottom. \n The examples often are the most important part.\u00a0 The book implies that all examples are reproducible.\u00a0 Not all are, but many are. \n You don't need to understand the whole of a help file the first time around.\u00a0 The goal should be to improve your understanding of the function. \n Page 199 \n Stack Overflow \n It is possible to subscribe via RSS to R tags. \n Page 200 \n cards \n With the cards I'm used to, the command to create cards should include 2:10 rather than 1:9 . \n Page 202 \n session info \n The book says that it is sometimes helpful to include the results of sessionInfo() in questions.\u00a0 I would change that from \"sometimes\" to \"often\". \n Page 210 \n reading in data \n The start of Circle 8.3 in The R Inferno has a number of items about problems reading data in. \n Page 216 \n changing directories \n If you are using the RGui, there is a \"change dir\" item in the File menu. \n Page 221 \n three subset operators \n The [[ operator always gets one component.\u00a0 The result is often not a list. \n In contrast the [ operator can get any number of items and (except for dropping)\u00a0 gives you back the same type of object. \n Page 226 \n removing duplicates \n The book shows the removal of duplicates using both logical subscripts and negative numeric subscripts.\u00a0 Be careful with the latter of these: \n > vec <- 1:5\n> dups <- duplicated(vec)\n> vec[!dups]\n[1] 1 2 3 4 5\n> vec[-which(dups)]\ninteger(0) \n If you create a vector of negative subscripts, you need to make sure it has at least one element.\u00a0 Otherwise you get nothing when you want everything. \n Page 240 \n apply output \n The book is in error when it says that the result of apply is always a vector.\u00a0 Other possible results include a matrix and a list. \n Page 243 \n sapply example (technical) \n The example at the very top of the page that uses ifelse would be more in the spirit of R if it instead used: \n if(is.numeric(x)) mean(x) else NA \n Page 245 \n aggregate (technical) \n Alternatives to aggregate include the by function (if you have a data frame) and the data.table package. \n Page 253 \n third paragraph \n Something seems to have gone wrong.\u00a0 That the phrase \"doesn't make sense at all\" appears in the paragraph seems apropos. \n Page 254 \n checking data \n Often checking data with graphics is best. Do plots look as expected? \n Page 260 \n mode \n There is a mode function in R, but it is not the same meaning as in the discussion of location. \n Page 270 \n missing values (technical) \n You might think that \"pairwise\" should be the default choice since it uses the most data.\u00a0 The problem with it is that the resulting correlation matrix is not guaranteed to be positive definite. \n Page 274 \n prop.table (technical) \n I wondered if prop.table recognized a table that had added margins.\u00a0 The answer is no, it thinks the margins are part of the data. \n Page 312 \n multiple plots (technical) \n If you want to put the graphics device back into a single plot state without using the old.par trick, then say: \n par(mfcol=c(1,1)) \n or \n par(mfrow=c(1,1)) \n It doesn't matter which you say. \n Page 314 \n hardcopy graphics \n If you are putting your graphics into a word processor, then often pdf is a good choice. \n If you are putting your graphics onto a webpage or into a presentation, then png can be a good choice. \n Page 326 \n boxplots (technical) \n To be clear: whiskers are at most 1.5 times the width of the box. \n Page 332 \n changing directory (technical) \n To change the working directory and then change it back to the original, you would do something like: \n > origwd <- getwd()\n> setwd(\"blah/blah\")\n> # do stuff\n> setwd(origwd) \n Page 359 \n CRAN mirrors (technical) \n While all mirrors are conceptually the same as the primary CRAN site, it takes time for changes to propagate.\u00a0 This is unlikely to be an issue unless you are trying to get a brand new release. \n Page 360 \n CRAN packages \n As of 2012 October 14, CRAN has 4087 contributed packages. \n Page 362 \n unloading packages \n I've used R pretty much every day for over a decade and never unloaded a package.\u00a0 I doubt this will be a big issue for you. \n Page 363 \n R-Forge \n R-Forge also provides mailing lists.\u00a0 The immediate significance of this for you is that some of your favorite contributed packages might have a dedicated mailing list. \n Page 364 \n own repository (technical) \n You can even set up your own repository and fill it with packages that you write. \n Page 1 \n Do you appreciate the meaning of: \n knowledge <- apply(theory, 1, sum) \n as promised? \n Epilogue \n I saw a little teddy bear. \nWell, I said to myself, \n\"I know what I want. I gotta get a bear some way.\" \n from \"You cannot win if you do not play\" by Steve Forbert"], "link": "http://www.portfolioprobe.com/2012/10/15/annotations-for-r-for-dummies/", "bloglinks": {}, "links": {"http://www.portfolioprobe.com/": 4, "http://en.wikipedia.org/": 1, "http://www.burns-stat.com/": 2}, "blogtitle": "Portfolio Probe"}, {"content": ["US large cap market returns. \n      \n Fine print \n \n The data are from Yahoo \n Almost all of the S&P 500 stocks are used \n The initial post was \u201cReplacing market indices\u201d \n The R code is in marketportrait_funs.R"], "link": "http://www.portfolioprobe.com/2012/10/13/us-market-portrait-2012-week-42/", "bloglinks": {}, "links": {"http://finance.yahoo.com/": 1, "http://www.portfolioprobe.com/": 9}, "blogtitle": "Portfolio Probe"}, {"content": ["Which sectors are coherent, and which aren\u2019t? \n Previously \n The post \u201cS&P 500 correlations up to date\u201d looked at rolling mean correlations among stocks.\u00a0 In particular it looked at rolling mean correlations of stocks within sectors. \n Of importance to this post is that the sectors used are taken from Wikipedia . \n Relative correlations \n The thought is that if the stocks within sectors really move together, then their mean correlation will be higher than the mean of the correlations across all stocks.\u00a0 The difference in correlations is a measure of the strength of the coherence of the sector. \n The data that went into Figure 1 were created by: \n \n get the mean 50-day correlations of the sector with dates in 2012 (so returns in the last quarter of 2011 have some effect) \n subtract the corresponding mean 50-day correlations for the whole universe \n this makes 193 differences per sector \n \n Figure 1 shows boxplots of the correlation differences sorted by their means. \n Figure 1: Difference of sector and market mean 50-day correlations in 2012. Note that the width of the boxes is variability across time \u2014 there is no indication of the variability of correlations within the sector across stocks. \n Financials, Utilities and Energy are the strongest sectors. \n More interesting, I think, is that Consumer Staples, Telecommunications Services and Consumer Discretionary are being anti-sectors.\u00a0 They have lower correlation among themselves than there is across the market as a whole. \n Figure 2 shows the differences from the whole time period starting in 2006. \n Figure 2: Difference of sector and market mean 50-day correlations back to 2006.\u00a0 \n Energy and Utilities have consistently been strong sectors, and Consumer Staples has consistently been an anti-sector. \n Questions \n Are the \u201canti-sectors\u201d merely because the categorization isn\u2019t very good? \n This seems like a complicated way of approaching the coherence of sectors.\u00a0 What are more direct ways of getting there? \n Summary \n Some sectors are more equal than others. \n Appendix R \n The computing and plotting were done in R . \n compute differences \n The function to create the differences conveniently is: \n pp.corcompare <- function(sectorcors, marketcor) \n{\n\u00a0 dnam <- names(marketcor)\n\u00a0 ans <- array(NA, c(length(marketcor), \n  length(sectorcors)), list(dnam, names(sectorcors)))\n\u00a0 for(i in names(sectorcors)) {\n\u00a0\u00a0\u00a0 ans[, i] <- sectorcors[[i]]$cor[dnam] - marketcor\n\u00a0 }\n\u00a0 attr(ans, \"call\") <- match.call()\n\u00a0 ans\n} \n This is used (with objects created in \u201cS&P 500 correlations up to date\u201d ) like: \n marketcor <- corboot.sp50$cor\nmarketcor12 <- marketcor[substr(names(marketcor), 1, 4)\n \u00a0== \"2012\"]\ncorcomp12 <- pp.corcompare(corboot.sectors, marketcor12) \n plot \n A function to create a plot is: \n P.corcompare12 <-\n\u00a0 function (filename = \"corcompare12.png\") \n\u00a0 {\n\u00a0\u00a0\u00a0 if(length(filename)) {\n\u00a0\u00a0\u00a0\u00a0\u00a0 png(file=filename, width=512)\n\u00a0\u00a0\u00a0\u00a0\u00a0 par(mar=c(4,11, 0, 0) + .1, las=1)\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 cco <- corcomp12[, order(colMeans(corcomp12))]\n\u00a0\u00a0\u00a0 colnames(cco)[colnames(cco) == \n  \"Telecommunications Services\"] <- \n  \"Telecommunications\"\n\u00a0\u00a0\u00a0 boxplot(cco, horizontal=TRUE, col=\"gold\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 xlab=\"Sector minus market correlations\")\n\u00a0\u00a0\u00a0 abline(v=0, col=\"steelblue\")\n\n\u00a0\u00a0\u00a0 if(length(filename)) {\n\u00a0\u00a0\u00a0\u00a0\u00a0 dev.off()\n\u00a0\u00a0\u00a0 }\n\u00a0 } \n This is the function that created Figure 1."], "link": "http://www.portfolioprobe.com/2012/10/10/sp-500-sector-strengths/", "bloglinks": {}, "links": {"http://www.portfolioprobe.com/": 5, "http://en.wikipedia.org/": 1}, "blogtitle": "Portfolio Probe"}, {"content": ["Featured \n I\u2019ll be leading two courses in the near future: \n Value-at-Risk versus Expected Shortfall \n 2012 October 30-31, London. \n 30th: \u201cAddressing the critical challenges and issues raised by the Basel proposal to replace VaR with Expected Shortfall\u201d \n 31st: \u201cVariability in Value-at-Risk and Expected Shortfall\u201d led by Patrick Burns \n Details at CFP Events . \n Finance with R Workshop \n 2012 November 6-7, London. \n Presenters will be Patrick Burns and Ronald Hochreiter. \n The brochure and the regististration form .\u00a0 When you register, please indicate that you were sent by Burns Statistics. \n New Events \n QWAFAFEW (Princeton) \n 2012 October 9 (real soon now). \nAakarsh Ramchandani on \u201cHow Industry Classification Standards Can Erode Alpha\u201d \n Details at the qwafafew website . \n Thalesians (London) \n 2012 October 10.\u00a0 Geoffrey Kendrick on \u201cIntroduction to FX and beta in FX\u201d. \n Details at the Thalesians website . \n London School of Economics \n 2012 October 15.\u00a0 Tim Harford on \u201cAdapt: Problem solving in a complex world\u201d. \n Details at the LSE website . \n QWAFAFEW (Stamford) \n 2012 October 17.\u00a0 A bunch of people speaking at the Quantitative Hedge Fund Risk Management Forum:\u00a0 \u201cWhere Theory Meets Practice\u201d. \n Details on the qwafafew website . \n \u00a0PRMIA \n 2012 October 30-31 in New York.\u00a0 David Rowe leading \u201cRisk Management beyond VaR\u201d. \n Details at the PRMIA website . \n London Quant Group \n 2012 November 6.\u00a0 Speakers include Oleg Ruban on \u201cManager Crowding: Do Risk Models Cause Managers to Herd?\u201d \n Free but you need to register.\u00a0 Details at the LQG website . \n Quantide \n 2012 November 15-16 in Milano.\u00a0 \u201cAdvanced R\u201d (taught in Italian). \n Details at Milano R net . \n Previously Announced \n 14-10 Club \n 2012 October 11. \n Rosemary Bailey \n Details at the 14-10 website . \n CambR \n 2012 October 29, Cambridge, UK.\u00a0 Martin Morgan on \u201cWriting R your friends can use\u201d. \n Details at the CambR website . \n Value-at-Risk versus Expected Shortfall \n 2012 October 30-31, London. \n 30th: \u201cAddressing the critical challenges and issues raised by the Basel proposal to replace VaR with Expected Shortfall\u201d \n 31st: \u201cVariability in Value-at-Risk and Expected Shortfall\u201d led by Patrick Burns \n Details at CFP Events . \n 14-10 Club \n 2012 November 1. \n Andy Haldane, Gordon Woo \n Details at the 14-10 website . \n Quant Invest \n 2012 November 5-7.\u00a0 Paris.\u00a0 Details . \n Finance with R Workshop \n 2012 November 6-7, London. \n Presenters will be Patrick Burns and Ronald Hochreiter. \n The brochure and the regististration form .\u00a0 When you register, please indicate that you were sent by Burns Statistics. \n City Book Fair \n 2012 November 12-15. London. \n Something that seems to be completely different. \n The website is http://www.citybookfair.co.uk/ \n Performance, Risk and Regulation \n 2012 November 14-15, London. \n Details at http://www.icbi-events.com/event/Parm-conference \n R course with Heather Turner \n 2012 November 19-20, Cambridge, UK. \n Day 1 is \u201cGetting started with R\u201d, day 2 is \u201cR programming\u201d. \n Details at Prism Training . \n Computational and Financial Econometrics \n 2012 December 1-3, Oveido, Spain \n The conference website is http://www.cfe-csda.org/cfe12/ \n LondonR \n 2012 December 4, The Counting House. \n Details and (free) registration\u00a0 at http://www.londonr.org/ \n 14-10 Club \n 2012 December 6 \n Jon Danielsson, James Sefton \n Details at the 14-10 website . \n PMAR North America \n Performance Measurement, Attribution and Risk. \n 2013 May 16-17, Philadelphia. \n Details at the Spaulding Group . \n PMAR Europe \n Performance Measurement, Attribution and Risk. \n 2013 June, London. \n Details at the Spaulding Group . \n useR! 2013 \n 2013 July 10-12, La Mancha. \n The conference website is http://www3.uclm.es/congresos/useR-2013/ \n Even more events \n MoneyScience has an events calendar."], "link": "http://www.portfolioprobe.com/2012/10/09/upcoming-events-4/", "bloglinks": {}, "links": {"http://www.milanor.net/blog": 1, "http://prmia.org/": 1, "http://www.cfe-csda.org/": 1, "http://www.banking-risk-regulation.com/": 2, "http://www.londonr.org/": 1, "http://www2.ac.uk/": 1, "http://qwafafew.org/": 2, "http://events.thalesians.com/": 1, "http://www.terrapinn.com/": 1, "http://www.org.uk/": 2, "https://vault2.secured-url.com/": 2, "http://www.rigb.org/": 3, "http://prismtrainingconsultancyltd.createsend3.com/": 1, "http://www.moneyscience.com/": 1, "http://www.icbi-events.com/": 1, "http://unicom.co.uk/": 2, "http://www.co.uk/": 1, "http://www3.uclm.es/": 1, "http://www.spauldinggrp.com/": 2}, "blogtitle": "Portfolio Probe"}]