[{"blogurl": "http://thelogcabin.wordpress.com\n", "blogroll": [], "title": "The Log Cabin"}, {"content": ["I got a ticket for parking in my driveway last weekend. \u00a0Weird, eh? \u00a0Here is my email correspondence to the Denver parking violations office: \n date: Mon, Aug 29, 2011 at 10:00 AM \nsubject: Citation #: 137105264 \n \nDear Parking Violation Office, \n \nOn Saturday, August 27 2011, I received a parking citation at my house on ??? St. As with any Broncos home game, I know to move my car to my driveway in order to not receive a ticket [1]. Therefore, I did exactly that on Saturday morning in order to prevent said violation. Upon going to my car at approximately 8.30 pm that night, I was shocked to see that the parking violations officer placed a ticket on my windshield with the Officer\u2019s Comments being \u201cIn Area HD SB Not Unsable Drive\u201d [2]. I have no idea what this means given that (1) this is a clear driveway for my house, (2) my car doesn\u2019t interfere with the sidewalk nor the street when in the driveway, and (3) I parked there for every Broncos game last year without incident. A particularly troubling aspect of this incident is that the officer had to walk into my yard/driveway in order to give me a ticket. \n \nI am happy to send you pictures of exactly where my car was in order to clear this up. I have included two pictures from Google maps using the \u2018street view\u2019 in order to show shots of the driveway. Note that my car is a 2010 Jetta and the car in the picture is a Subaru Forrester (much bigger than the Jetta). I think that the Forester could be blocking the sidewalk by about 1 foot (in these pictures), however, there is plenty of space behind the car and our car is much smaller than this Forrester. Therefore, there is no need to block either the sidewalk or the street when using our parking space. \n \nThanks for your time, \nRyan \n \n[1] \u2013 I received a parking citation for parking in the street during a Broncos game during the first pre-season game of 2010 (shortly after moving to 2749 Decatur St). Since that initial transgression, I have used my parking lot without incident. \n \n[2] \u2013 After reading this email and looking at the pictures, can you please send me an explicit explanation of what the quoted statement means in terms of the Denver City Law so that I can correctly assess whether or not my driveway is in violation of this law/ordinance? \n I forgot to tell them what I wanted from the initial email, so I had to follow-up: \n \ndate: Mon, Aug 29, 2011 at 2:00 PM \nsubject: Re: Citation #: 137105264 \n \nTo be clear, I am asking for this violation to be revoked due to a mistake in the Parking Officer\u2019s judgement; I would appreciate a response in a timely manner. \n \n-Ryan\n \n Their lame-ass response: \n \ndate: Wed, Aug 31, 2011 at 9:22 AM \nsubject: RE: Citation #: 137105264 \n \nWe have submitted your claim to the Parking Magistrates for review, we will notify you by mail of their determination. \n Here are the pictures that I referenced in the initial email:"], "link": "http://thelogcabin.wordpress.com/2011/09/02/i-fought-the-law-and-the-law/", "bloglinks": {}, "links": {"http://thelogcabin.wordpress.com/": 2, "http://feeds.wordpress.com/": 7}, "blogtitle": "The Log Cabin"}, {"content": ["I like politics; I don\u2019t like all of the lying involved. \u00a0If you ask me, I think that there should be \u201cEthics Committee\u201d investigations into all of the lying. \u00a0Sure, tweeting a picture of your junk is probably not the best idea, but neither is lying. \u00a0And nearly all politicians are guilty [1]. \u00a0Fortunately, the St. Petersburg Times started a website call Politifact \u00a0[2] with the hopes of keeping some of these people honest. \u00a0I\u2019m not sure it\u2019s helping. \n In any case, I wrote an R script to scrape the data from Politifact so that I could do some analysis. \u00a0I only got about as far as the following figure related to some of the Republican candidates and their propensity to lie. \u00a0The figure displays the number of statements made by each candidate that can be categorized as \u201cTrue\u201d, \u201cMostly True\u201d, \u2026, \u201cFalse\u201d, or \u201cPants on Fire\u201d according to Politifact. \n  \n What can we take from this? \u00a0Well, Michelle Bachmann is a big-time liar \u2014 ain\u2019t no denying that. \u00a0She\u2019s also a freaking nutjob. \u00a0Ron Paul probably lies the least, but nobody seems to care about him in the media . Tim Pawlenty doesn\u2019t lie too much. \u00a0Then again, he\u2019s a wuss and dropped out anyway. \u00a0Mitt Romney seems pretty good when it comes to speaking the truth; it\u2019s gotta be the Mormon background. \u00a0I suspect that he\u2019ll lie a bit more in the upcoming months. \u00a0And Rick Perry\u2026well, he\u2019s just bat-shit crazy, so I\u2019ll ignore him. \n If I had the time, I would try to randomly select some Republicans and Democrats from both the House and Senate and analyse of statement category (truth through pants on fire) is independent of political party and/or branch of Congress. \u00a0If you\u2019re interested in doing this, I would be happy to help you get started. \u00a0Have a look at my github repo for this project and give it a go! \n Footnotes: \n [1] \u2013 Note that I said \u2018nearly all\u2019 because Dennis Kucinich doesn\u2019t have a single statement classified as \u201cPants on Fire\u201d on Politifact . \n [2] \u2013 Winner of a Pulitzer Prize in 2009."], "link": "http://thelogcabin.wordpress.com/2011/08/17/lies-damned-lies-and-politicians/", "bloglinks": {}, "links": {"http://www.politifact.com": 2, "http://feeds.wordpress.com/": 7, "http://www.tampabay.com/": 1, "http://kucinich.house.gov/": 1, "http://thelogcabin.wordpress.com/": 1, "http://github.com/": 1, "http://www.thedailyshow.com/": 1}, "blogtitle": "The Log Cabin"}, {"content": ["Last Saturday I had the good fortune to present a talk on finding, gathering, and analyzing some sports-related data on the web at the local SABR group meeting. \u00a0In case you\u2019re not familiar with the \u201cSABR\u201d acronym, it stands for \u201cSociety for American Baseball Research\u201d; here\u2019s a link to the national organization. \u00a0The talk was light on tech and heavy on graphs (predominately made in R and in particular ggplot2 ). \u00a0Good times were had by all. \u00a0The slides from the talk are given below. \u00a0Most of the slides and code are recycled from previous talks, so I apologize in advance if you\u2019re already familiar with the content. \u00a0It was, however, new to the SABR people."], "link": "http://thelogcabin.wordpress.com/2011/08/08/slides-from-rocky-mtn-sabr-meeting/", "bloglinks": {}, "links": {"http://had.co.nz/": 1, "http://www.r-project.org": 1, "http://www.rmsabr.org/": 1, "http://feeds.wordpress.com/": 7, "http://sabr.org/": 1}, "blogtitle": "The Log Cabin"}, {"content": ["A week or so ago I saw a tweet related how the NFL lockout was affecting the search traffic for \u201cfantasy football\u201d on Google (using Google Trends ). \u00a0Basically, the search traffic (properly normalized on Google Trends) was down prior to the end of the lockout. \u00a0I decided to explore a bit with this myself and chose to look into the RGoogleTrends package for R . \u00a0The RGoogleTrends package is basically a convenient way to interact with the Google Trends API using R. \u00a0Very cool stuff. \u00a0All of my code can be found at my GoogleTrends repo on github . \n My first query was to pull the google trends data for the past seven or so years using the search term \u201cfantasy football\u201d. \u00a0The following figure shows the results over time. \u00a0It\u2019s immediately obvious that the normalized search scores for \u201cfantasy football\u201d were on the decline (2011 over previous years) prior to the end of the lockout; however, it appears that interest has since recovered. \n  \n I then decided to look at the trends for \u201cNFL\u201d. \u00a0There isn\u2019t a dramatic decrease in the (normalized) searches for \u201cNFL\u201d prior to the lockout\u2019s end, but you do see a huge spike in searches after the announcement. \n  \n A few notes: \n \n It would be interesting to align the curves in the last plot by the day of week. \u00a0That is, it would be nice to compare the trends scores, as an example, for the 7th Wednesday prior to the start of the NFL preseason or something. \n In order to use the RGoogleTrends package, you can use the cookies in Firefox to pass username and password if you just log into gmail (or another google service)."], "link": "http://thelogcabin.wordpress.com/2011/08/01/google-trends-r-and-the-nfl/", "bloglinks": {}, "links": {"https://github.com/": 1, "http://www.omegahat.org/": 1, "http://www.r-project.org/": 1, "http://thelogcabin.wordpress.com/": 4, "http://feeds.wordpress.com/": 7, "http://github.com": 1, "http://www.google.com/": 1}, "blogtitle": "The Log Cabin"}, {"content": ["Last night I presented a talk at the DRUG introducing the R wrapper for the Data Science Toolkit . \u00a0Lots of good questions, good forking, and good beer afterwards at Freshcraft . \u00a0The slides are given below."], "link": "http://thelogcabin.wordpress.com/2011/05/18/the-rdstk-presentation-at-denver-r-users-group/", "bloglinks": {}, "links": {"http://thelogcabin.wordpress.com/": 1, "http://freshcraft.com/": 1, "http://feeds.wordpress.com/": 7, "http://www.datasciencetoolkit.org/": 1, "http://www.meetup.com/": 1}, "blogtitle": "The Log Cabin"}, {"content": ["I recently decided to present a talk to the D enver R Users Group \u00a0(DRUG) on how to make an R package ( May 17 ). There were only two problems: (1) I\u2019ve never made a package and (2) I had nothing in mind to package up. \u00a0At about this same time, Pete Warden \u00a0and others were blogging about the iPhone tracking issue [1]. How are these two events related? Well, I remembered that a few of my favorite Twitter \u2018friends\u2019 posted some things related to Pete Warden\u2019s \u201c The Data Science Toolkit (DSTK)\u201d [2] a while back. And? And at the time I thought that it would be cool to have an R package/wrapper for accessing the DSTK\u2019s API, similar to Drew Conway\u2019s \u00a0 R wrapper \u00a0 for the infochimps API. \n So I\u2019m happy to announce that after spending a little time on this project in the past week, Version 0.1 of the RDSTK package \u00a0is available on github. I haven\u2019t submitted this package to CRAN and, hence, you need to install it from source (RDSTK_0.1.tar.gz). In order to do this, use the install.packages() function within R or R CMD INSTALL from the shell prompt. Note that the package depends on the RCurl , plyr , and rjson packages. \n The following functions are included in the package: \n \n street2coordinates \n ip2coordinates \n coordinates2politics \n text2sentences \n text2people \n html2text \n text2times \n \n They should be easy to use if you are familiar to the DSTK API. If not, RTFM! \n Let me know if you have any comments and/or suggestions. Happy hacking. \n Acknowledgements: \n I wanted to mention that I received a bit of help with the RCurl package from \u201cNoah\u201d on stackoverflow , Andy Gayton on stackoverflow, and Duncan Temple Lang on the R-Help list. \u00a0Thanks! \n Footnotes : \n \n To borrow a joke from Asi Behar , \u201cRight after word leaks that the iPhone has been tracking your location at all times, we find Osama. Coincidence? Thanks Apple!\u201d \n You may recall that a while back, I tweeted about disliking the phrase \u201cdata science\u201d. \u00a0My feelings have not changed."], "link": "http://thelogcabin.wordpress.com/2011/05/02/r-and-the-data-science-toolkit/", "bloglinks": {}, "links": {"http://www.ucdavis.edu/": 1, "http://www.infochimps.com/": 1, "https://stat.ethz.ch/": 1, "https://github.com/": 1, "http://www.drewconway.com/": 1, "http://stackoverflow.com/": 1, "http://www.datasciencetoolkit.org/": 1, "http://petewarden.typepad.com/": 2, "http://feeds.wordpress.com/": 7, "http://twitter.com/": 1, "http://streaming.iastate.edu/": 4, "http://www.staticloud.com/": 1, "http://www.meetup.com/": 2}, "blogtitle": "The Log Cabin"}, {"content": ["[Note: I wrote this on a flight and didn't proofread it at all. You've been warned of possibly incoherencies!] \n I\u2019m currently sitting at about 32K feet above sea level on my way from Tampa International to DIA and my options are (1) watch a shitty romantic comedy starring Reese Witherspoon, Owen Wilson, et al. or (2) finish my blog post about the NBA data .\u00a0 With a chance to also catch up on some previously downloaded podcasts, I decided on option (2). \n So where was I related to the NBA analysis?\u00a0 I downloaded some data and I was in the process of developing a predictive model.\u00a0 I\u2019m not going to get into the specifics of this model because it was an incredibly stupid model.\u00a0 The original plan was to build a logistic regression model relating several team-based metrics (e.g., shots, assists, and blocks per game, field-goal and free throw percentage, etc.) to a team\u2019s overall winning percentage.\u00a0 I was hoping to use this model as the basis of a model for an individual player\u2019s worth.\u00a0 How?\u00a0 Not sure.\u00a0 In any case, I got about half-way through this exercise and realized that this was an incredibly stupid endeavor.\u00a0 Why?\u00a0 I\u2019m glad you asked. \n Suppose that you gave a survey to roughly 1K males and asked them several questions.\u00a0 One of the questions happened to be \u201cHow tall are you (in inches)?\u201d\u00a0 The respondents were incredibly sensitive and only about half responded to this particular question.\u00a0 There were other questions with various levels of missingness as well.\u00a0 A histogram of the 500 answers the the aforementioned question is given in Figure 1. \n  \n Figure 1: A hypothetical sampling of 500 male heights. \n One of the goals of the hypothetical survey/study is to classify these males using all of the available data (and then some).\u00a0 What do I mean by the parenthetical back there?\u00a0 Well, a buddy of mine suggests that we just substitute the average height for the missing heights in order to make our data set more complete.\u00a0 Obviously, this isn\u2019t going to change the average height in the data.\u00a0 Are there any repercussions for doing this?\u00a0 Consider the variance of the heights.\u00a0 If we need to estimate the population variance of male heights, we will severely underestimate this parameter.\u00a0 See Figure 2 for the density estimates of the original 500 heights and the original plus the imputed data. \n  \n Figure 2: Density estimates of the original 500 heights + 500 imputed (mean substitution) heights. \n (Alter Ego:\u00a0 Yo Ryan \u2014 WTF are you talking about here?\u00a0 You\u2019re supposed to be talking about the NBA and building a advanced player-evaluation metric! \n Me:\u00a0 I\u2019m getting to that!) \n OK, so how does this relate to what I was doing prior to the mean-substitution tangent?\u00a0 Well, my model based on team metrics related to overall winning percentage was an exercise in mean substitution!\u00a0 The summarized data (e.g. blocks per game or free throw percentage) are averaged overall all games and I\u2019m trying to relate those numbers to n1 wins and n2 losses out of N = n1 + n2 games.\u00a0 Essentially I would have N replicates of the averaged data (my predictor variables) and and n1 and n2 successes and failures (reps.) in the logistic regression model.\u00a0 I was ignoring any variation in the individual game statistics that contributed to the individual wins and losses. \n Why didn\u2019t I just do a better job and ignore this mistake?\u00a0 Basically, I felt compelled to offer up this little caveat related to data analysis.\u00a0 Just because you can write a script to gather data and perhaps even build a model in something like R does not guarantee that your results are meaningful or that you know something about statistics!\u00a0 If you are doing a data analysis, think HARD about any questions that you want to address, study what your particular methods are doing and any subsequent implications of using said methods, and for *$@%\u2019s sake interpret your conclusions in the context of any assumptions.\u00a0 This isn\u2019t an exhaustive list of good data-analytic practice, but it\u2019s not going to hurt anything.\u00a0 Happy analyzing folks! \n As usual, all of the code and work related to this project is available at the github repo ."], "link": "http://thelogcabin.wordpress.com/2011/04/19/nba-logistic-regression-and-mean-substitution/", "bloglinks": {}, "links": {"http://thelogcabin.wordpress.com/": 3, "http://github.com/": 1, "http://feeds.wordpress.com/": 7}, "blogtitle": "The Log Cabin"}, {"content": ["I decided to spend a few hours this weekend writing the R code to scrape the individual statistics of NBA players (2010-11 only). \u00a0I originally planned to write up a few NBA-related analyses, but a friend was visiting from out of town and, of course, that means less time sitting in front of my computer\u2026which is a good thing! \u00a0So in between an in-house concert at my place (video posted soon), the Rapids first game (a win, 3-1 over Portland), brunch, and trivia at Jonesy\u2019s (3rd place), I did write some code. \u00a0The git repo can be found here on github . \n Note that this code is having a little trouble at the moment. \u00a0I have no idea why, but it\u2019s throwing an error when it tries to scrape the Bulls\u2019 and the Raptors\u2019 pages. \u00a0I\u2019m pretty sure it\u2019s NOT because the Bulls are awesome and the Raptors suck\u2026though I haven\u2019t confirmed that assertion. \n In any case, let me know if you have any ideas about what I should do with this data. \u00a0Some of the concepts that I\u2019m toying with at the moment include: \n \n Comparing the before and after performances of players who were traded at or near the trading deadline, and/or \n Examining some of the more holistic player-evaluation metrics w/r/t win-loss records for various teams. \n \n Question: \u00a0Why didn\u2019t you use BeautifulSoup for your scraping? \u00a0You seem to be a big proponent of python \u2014 what\u2019s up? \n Answer: \u00a0I wrote about scraping with R vs python in a previous post. \u00a0That little test was pretty conclusive in terms of speed and R won. \u00a0I am not totally convinced that I like the R syntax for xml/html parsing, but it is fast. \u00a0And me not liking the syntax is probably a result of me not being an XML expert rather a shortcoming of the XML package itself."], "link": "http://thelogcabin.wordpress.com/2011/03/21/nba-analysis-coming-soon/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 7, "http://www.nba.com/": 3, "http://thelogcabin.wordpress.com/": 1, "http://www.jeatbar.com/": 1, "http://www.github.com": 1, "http://www.coloradorapids.com": 1, "http://www.github.com/": 1, "http://www.crummy.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "The Log Cabin"}, {"content": ["I ride the bus to work and ride my bike home. \u00a0I really enjoy the 8 mile ride on the way home \u2014 expect when it\u2019s freezing like yesterday! \u00a0I haven\u2019t decided whether or not it\u2019s because (1) I\u2019m cheap and don\u2019t want to buy another car, (2) I work at the National Renewable Energy Lab , or (3) I like the evening workout. \u00a0To be honest, it\u2019s probably a combination of all three. \n Anyway, there are a few things that piss me off about the 28 route in Denver. \u00a0However, nothing, and I mean nothing, pisses me off more than the little side journey that the bus takes when we get to Yates and 26th . \u00a0As you can see in the link, we go south to Byron Pl, over to Sheridan, and then back north to 26th. \u00a0Why does this little sojourn piss me off you ask? \u00a0Because nobody ever uses the Byron Pl stop! \u00a0OK, there are a few people, but they should walk the 1.5 blocks to either 26th and Sheridan or 26th and Yates! \n Here\u2019s my back of the envelope calculation for how much this side trip costs RTD on its weekday routes. \n Assumptions/facts: \n \n A bus gets 5 mpg. \u00a0Is this a good assumption? \u00a0Who knows. \u00a0I really don\u2019t care. \u00a0I\u2019m just bored and want to blog about this. \n Google maps puts this side trip at 0.4 miles. \n There are 36 eastbound and 40 westbound trips per day that utilize this ridiculous Byron Pl stop. \u00a0(Note: There could be more, but I\u2019m not dealing with the routes that start at Byron Pl.) \n To keep things simple, let\u2019s say that there are 250 \u2018weekdays\u2019 for the 28 route. \n \n What does this all mean? \u00a0Using these figures, the trip uses about 0.08 gallons of fuel for each trip down to Byron Pl. \u00a0Maybe that\u2019s not entirely fair, because the bus would still go 0.1 miles if it doesn\u2019t take the stupid trip. \u00a0So adjusting point 2 above, let\u2019s say that the trip costs 0.3 miles and, hence, uses 0.06 gallons of fuel. \u00a0That\u2019s 86.4 gallons per day or about 21,600 gallons per year! \u00a0Assuming $2.50 per gallon of fuel, RTD spends about $54,000 on this unnecessary trip! \u00a0Holy shit, that doesn\u2019t even include the weekends! \n Any thoughts?"], "link": "http://thelogcabin.wordpress.com/2010/11/30/napkin-calculations/", "bloglinks": {}, "links": {"http://www3.rtd-denver.com/": 1, "http://feeds.wordpress.com/": 7, "http://www.nrel.gov": 1, "http://maps.google.com/": 1}, "blogtitle": "The Log Cabin"}, {"content": ["I am going to start working on some benchmarks for GPUs vs CPUs. \u00a0Hopefully I can write something about that soon; however, I don\u2019t have anything at the moment. \u00a0Nevertheless, I can give you a pretty sweet video illustrating the GPU vs CPU concept courtesy of Mythbusters. \u00a0Here ya go."], "link": "http://thelogcabin.wordpress.com/2010/10/21/gpusvscpus/", "bloglinks": {}, "links": {"http://thelogcabin.wordpress.com/": 1, "http://feeds.wordpress.com/": 7}, "blogtitle": "The Log Cabin"}]