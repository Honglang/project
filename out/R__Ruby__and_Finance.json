[{"blogurl": "http://viksalgorithms.blogspot.com\n", "blogroll": [], "title": "R, Ruby, and Finance"}, {"content": ["How Many Data Scientists Are There? I've seen a lot of articles lately about \u201cBig Data\u201d and the looming \u201ctalent gap.\u201d This article from the Wall Street Journal is a good example. It cites a McKinsey estimate that states that we will need 1.5 million more managers and analysts who are conversant with \u201cbig data.\u201d Of course, some of this is the media latching on the the next \u201cbig thing\u201d (data), but some of it is true. Even anecdotal evidence, such as the number of job postings you find when you search for \u201cdata science,\u201d indicates that there is a significant unmet demand for data analysis skills. This led me to wonder how we could quantify this gap, and once we figure out how to quantify it, if we can figure out if there has been a commensurate increase in the number of people with the skills to work with big data. This is interesting from the perspective of someone who works with data simply because I want to know the state of the field. I am a pretty recent entrant into the area, and I would like to see more people get into it. Potential Ways to Quantify Data Analysis Supply and Demand There are a few different ways we can go in terms of this. We can use Google Search trends to find the trend for the term \u201cbig data.\u201d -Pros: Easy. Look, I already did it. -Cons: Trivial, doesn't really give us a way to disambiguate supply (number of data scientists) and demand (companies looking for analytical skills). We can crawl job websites looking for jobs that mention data in some way. -Pros: Probably a pretty comprehensive way to look at the \u201cdemand\u201d side of the equation. We can use number of days a job posting is up as a proxy for supply. -Cons: Our proxy for supply will be pretty noisy because some job listings stay up forever, and when they are taken down doesn't necessarily correlate with when they are filled. Also, this will give us very unstructured data, and will be very dependent on which job sites we crawl. We can look at Kaggle.com, which is a website that hosts data science competitions, to see how many people enter each competition, and how that count changes over time. -Pros: Relatively structured data (competitor count). Gives us a window into both sides of the market (supply is number of competitors, demand is number of companies hosting competitions). -Cons: Not all companies have heard of Kaggle, and not all people who work with data have heard of Kaggle. The numbers will be biased because competitor count is dependent on two things: people hearing about Kaggle, and people being interested in/having the skills to work with data. We are only really interested in the second part, but it will be affected by the first part. Ultimately, I chose to go with option 3, for a few reasons: -It is simpler than creating a crawler, and much less trivial than relying on Google Trends. -I am familiar with the platform. -While the competitor count is dependent to some extent on who has heard of Kaggle, I think that it is fair to say that as of a few months ago, most people who work with data had heard of Kaggle in some form (this is strictly anecdotal, though). Defining and Solving the Problem So now, we still want to look at the supply of data scientists, and the demand for data scientists, but we want to do it within the context of Kaggle.com. If you are unfamiliar with Kaggle, it is a crowdsourcing platform that allows companies to host competitions on various aspects of their data. For example, one company wanted individuals to predict bond prices in the future. Competitors register for these competitions, and are ultimately awarded prize money based on their final position in the standings. So, this gives us an easy way to define supply and demand. Supply can be defined as the number of competitors that are actively engaged in the Kaggle platform at any one time. As each competition has an end date, a competitor will only be counted as \u201cactive\u201d if he or she has an entry in a competition that has not ended yet. Demand can simply be counted as the number of competitions that are currently active. This is a bit tricky, because it seems that demand for the Kaggle platform from the company side might be increasing faster than the overall demand for Big Data (given that Kaggle is relatively new and it takes time for word to circulate). Now, to define our procedure: Each Kaggle competition has a leaderboard associated with it. The leaderboard lists all active participants, along with their rank in the competition. Additionally, Kaggle allows for old leaderboards to be seen. Because we can see the leaderboard at various points in time, we can easily figure out how many active participants each competition had at any given time point. Adding the unique users for each active competition will give us a count of active users at that time point. If we do this for multiple time points across all competitions, we can figure out how the number of active users changed over time. Active Participants by Competition This leads us to a (fairly messy) chart that shows how each competition gained active participants over time. As data was only scraped on a weekly basis, the figures might not be 100% accurate in terms of competition start times. Once the number of competitors line becomes flat, it indicates that the competition is closed. Generally, competitions that are higher up in the legend are more recent.  This is interesting within the context of Kaggle, but it doesn't really tell us much about the overall supply and demand for data scientists. We can see that some of the older competitions gained participants faster than some of the newer ones, which may indicate that demand is outstripping supply, but we will need to look at the aggregate numbers across competitions to make judgements there. Active Participants Overall Now, we can aggregate the numbers, and look at unique active users across all competitions. So if \u201cBob\u201d is active in competition A and competition B at the same time, he is only counted once.  As participants are no longer counted as \u201cactive\u201d when a competition ends, we see some rather dramatic oscillations. We can better understand these oscillations if we graph number of active competitions alongside number of unique participants. I am scaling the number of competitions by a factor of 100 to make them appear legibly on the same chart as user count.  We can see how closely related the number of active competitions is to the number of unique competitors. In fact, the linear correlation between the two is .71. We can fit a linear model to the data and figure out what the expected number of users based solely on the number of active competitions should be.  We can see that they are pretty well correlated. Most of the variation in the number of users seems to be explained by the number of competitions, although we do see that when competitions are close to ending, there appears to be large rush of participants. Also, very recently (the month of July), we see that the actual number of unique active users is far below the expected number (ignore the very end of the chart, where all numbers drop to zero). How Quickly Competitions Attract Participants Another way to look at the supply of data scientists versus demand is to see if more recent competitions (that have come about in a time when there are more active competitions overall) gain participants more slowly or more quickly than previous competitions.  This plot shows us how many users per day each competition attracts, and how that has changed over time. Although it may look like there is a trend in this plot (particularly towards the very end, when slopes are small), there is no significant correlation between date of competition launch and number of users gained per day. Total Participants Over Time In previous charts, we looked at unique active users or unique participants over time. We can also look at aggregate number of unique users over time\u2013the total number of unique individuals who have submitted an entry to any Kaggle competition. This shows us how the platform is growing.   New Users Over Time We can figure out new users at each weekly time period (users who have submitted entries in the week who did not submit any previous entries). Graphing this allows us to see how the community is growing and expanding.  We can see that the number of new users each week is somewhat correlated with the competition count, and has remained somewhat steady over the past few months. Some Random Observations Okay, so we have some data and some (maybe) pretty pictures, but what does it tell us? We can gain some insight from this. There are clearly some competitions that are favored strongly over others. As someone who has participated in a lot of Kaggle competitions, I can safely say that these are competitions that have an interesting premise, the potential for an interesting opportunity (KDD Cup/Facebook Recruiting), or have data that is easy to work with in terms of format and presentation (Bio response). It seems like the average number of competitors per competition has been pretty constant, even as Kaggle has ramped up their number of concurrent competitions. Some recent competitions have not seen much uptake, but that could be the combination of several relatively insignificant factors (summer, uninteresting competitions, etc), rather than signalling that we have reached capacity. As we can see by the number of people who have submitted an entry overall vs the number of people who are active at any given time, there is a large data science community that only uses Kaggle when they see something interesting. As fresh people begin to compete, it seems that older users stop using the platform, whether it be from boredom, lack of time, or another issue. This keeps the number of active unique users much more constant (slow growth) than the aggregate number of users. This points to there being constant new entrants into the data science world, at least within the context of Kaggle, but it is hard to figure out if these entrants are new to data science entirely, or simply new to the Kaggle platform. The Kaggle forums suggest a mixture of the two. Along with a rising overall supply of data scientists, we have also seen rising demand, as the number of competitions has been steadily increasing. This could simply reflect rising interest in the Kaggle platform, but it might also point to a rising interest in data science at the corporate level. Inconclusive Conclusions It's hard to generalize from this data, as it call came within the context of a platform. You can think of Kaggle as a fisherman that has gradually invested in better technology and better bait. Over time, more data scientists and more companies have been \u201ccaught,\u201d but whether that reflects the better bait, or the fact that the number of fish in the ocean is increasing, it is hard to say. So what can we conclude? A few key items jump off the page: People will enter the field of data science, but only if they can find something interesting/rewarding to work on. We see a lot of active unique entrants in a few competitions that have low barriers to entry or offer commensurately high rewards. We also see a rising amount of new users surrounding particularly interesting competitions. Problems that are less exciting, or perhaps less accessible, may need to be reformulated to appeal to the mainstream data community, and crossovers from other fields. If a company wants to attract high quality talent, they need to interest and engage them. We see a lot of competitions get very little traction. The amount of new users on Kaggle seems fairly steady. This may indicate that demand may soon outstrip supply, as more competitions are run without a commensurate increase in the number of participants, but it does seem like the number of participants and competition count is pretty correlated. The fact that there is a constant stream of new users is also encouraging, because, anecdotally, most people in the data community heard about Kaggle months ago. This indicates that both existing data scientists are always looking for interesting problems to tackle, and that new people are moving into data science as they see interesting problems. Corporate interest in data science overall seems to be increasing more quickly than the supply of new data scientists. None of these are definitive, and the method used for analysis constrains the interpretability of the results. Nonetheless, I think that there are some interesting threads here, and would love to hear anyone's thoughts on this."], "link": "http://viksalgorithms.blogspot.com/feeds/7711723266060788067/comments/default", "bloglinks": {}, "links": {"http://www.indeed.com/": 1, "http://3.blogspot.com/": 2, "http://4.blogspot.com/": 2, "http://2.blogspot.com/": 3, "http://online.wsj.com/": 1, "http://www.kaggle.com/": 1, "http://www.google.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Introduction I recently posted about using the Wikileaks cable corpus to find word use patterns, both over time, and in secret cables vs unclassified cables. I received a lot of good suggestions for further topics to pursue with the corpus, and probably the most interesting was the idea to do sentiment analysis over time on a variety of named entities. Sentiment analysis is the process of discovering whether a writer feels negatively or positively about a topic. Named entities in this case would be country names such as China and India, and the names of important world figures, such as Saddam Hussein or Tony Blair. So, in essence, we are seeing how US diplomats, and by extension the US, felt about a variety of topics, and how those feelings changed over time, from the first available cables (1980's) to present. The goal is to get a chart like this one:  How will we do this? Useful sentiment analysis can be extremely complex at times, requiring a corpus of sentences to be mapped to sentiment scores. In order to make this exercise simpler, I traded off some accuracy and used a word list instead (the AFINN list). This word list assigns a \u201csentiment score\u201d of -5 to 5 to 2477 English words. For example, the word adore has a score of 3, denoting a positive sentiment, whereas the word abhorred has a sentiment of -3, indicating negative sentiment. Our next task is named entity recognition . We will use the AFINN word list in conjunction with a list of named entities. Named entities in this case would be important topics from the news, so we will use the JRC-Names word list, which pulls out important keywords from news articles. We will use these keywords to define our topics. For example, \u201cChina\u201d is a keyword, as is \u201cIndia\u201d. These are the topics that we will analyze sentiment for. Now, in order to find the sentiment for a given topic, we will need to find out whether it appears in conjunction with negative or positive words. For example, the phrase \u201cChina abandoned an environmental project\u201d would indicate negative sentiment, whereas \u201cChina is building partnerships\u201d would indicate positive sentiment. In order to do this, we will need to find out when our topic words (named entities) and our words that indicate sentiment appear together in a sentence. To accomplish this, we can use a technique called random indexing which allows us to build up a matrix that shows how topic words and sentiment words occur together. I opted to use random indexing because it builds a relatively small matrix in terms of dimensionality, and it allows us to capture information on a fairly granular level. The optimal method would be to create a full Term-Document matrix and decompose it to find relations, but it is impractical in this case due to the high sentence count. Our plan Now that we have all the prelimiaries, here is a high-level look at what we will do: Get cables for multiple time periods from the database Because there are more cables from 2000 onwards than from pre-2000, we will define 5 year time periods from 1985 to 2000, and 1 year time periods after. Split the cables into sentences. Build up matrices using random indexing that contain the topic words from JRC-Names and the sentiment words from AFINN. Use cosine similarity measures to see how often topic words occur with negative/positive words. Assign a final \u201csentiment score\u201d to each topic for each time range. This plan will give us reasonable results. Because of the way that we are doing sentiment analysis, it won't be perfect (far from it), but it will show some interesting patterns, at least. Formatting JRC-Names and AFINN JRC-Names and AFINN are not in the best format for this (you will see when you download them), so we need to reformat them to get a character vector of topics. The reformatting also needs to be done because cables frequently refer to people by only their last name and JRC-names contains a full name. We need to make everything into 1-grams. jrc_names <- read.delim(file = \"entities.txt\", stringsAsFactors = FALSE)[,  4] bad_names <- grep(\"[^\\\\w+]\", jrc_names, perl = TRUE) jrc_names <- jrc_names[-bad_names] jrc_names <- sapply(jrc_names, function(x) strsplit(x, \"+\", fixed = TRUE)) jrc_tab <- sort(table(tolower(unlist(jrc_names))), decreasing = TRUE) jrc_names <- names(jrc_tab)[jrc_tab > 2] jrc_names <- jrc_names[nchar(jrc_names) < 15 & nchar(jrc_names) >  2] afinn_list <- read.delim(file = \"AFINN-111.txt\", header = FALSE,  stringsAsFactors = FALSE) names(afinn_list) <- c(\"word\", \"score\") afinn_list$word <- tolower(afinn_list$word) full_term_list <- c(jrc_names, afinn_list$word)  This code will remove non-English words from jrc-names, split it by the + sign that appears in each term, and reconstruct a vector in which only the terms that appear at least twice are included. Defining Date Ranges We now need to define what date ranges we want our cables to come from. Because there aren't many cables available pre-2000, we will select 5 years at a time from 1985-2000. date_min_list <- c(\"1985\", \"1990\", \"1995\", \"2000\", \"2001\", \"2002\",  \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\") date_max_list <- c(\"1990\", \"1995\", \"2000\", \"2001\", \"2002\", \"2003\",  \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\")  Generating Sentiment Scores Now, we need to follow our plan from above and have the code that generates our final sentiment scores. The load or install function is documented here . This code is very inefficient, so please feel free to improve it. To get it to run on low-memory systems, you can lower the ri_cols or max_cables_to_sample attributes. A higher ri_cols or max_cables_to_sample setting will be less memory efficient, but more accurate. You can find the code for this here , as sentiment_score_generation.R. This is a very long piece of code, but it is basically doing what our plan stated. It is getting cables for each time period, splitting them into sentences, and finding out which sentiment words and topic words occur together. It is then finding out which topic is associated with negative sentiment, and which is associated with positive sentiment, and then assigning a final score to each topic on that basis. Plotting the results Now, we are ready to make plots indicating sentiment over time. You can find the plotting code here , as sentiment_plot.R. This generates the following plot:  The black line indicates the mean sentiment by year. You can see that the average US sentiment dips around 2003 (the year on the x-axis is the ending year for the gathered cables, so 2010 would be cables from January 1st, 2009 to January 1st, 2010, for example). This is likely due to countries not supporting the US war effort in Iraq. If you have a better interpretation, I would love to hear it. More country plots Here are US sentiments towards the english speaking world. \u201cNew Zealand\u201d becomes \u201cZealand\u201d because we are only dealing with 1-grams:  You can see that we seem to have much better sentiment towards the English speaking world, overall. Here are US Sentiments towards some of the countries with recent protests/overthrows. Tripoli is a proxy for Libya, and Tunis is a proxy for Tunisia, because those terms did not seem to make it into the JRC-names list that we constructed:    Country Interpretation The US seems to have slightly negative sentiment towards every country, particularly after 2003. This could be due to many factors: Countries not supporting the Iraq war. A change from Madeline Albright (1997-2001) to Colin Powell (2001-2005) to Condoleeza Rice (2005-2009). Perhaps their attitudes shaped the attitudes of the cable writers. Changes in administration from Bill Clinton (1993-2001) to George Bush (2001-2009) to Barack Obama (2009-). The attitude of the President can definitely impact cable writing, as I can attest, and you can see some upticks in sentiment from 2009-2010, when Obama took office. Personally, I think that the war may have been the biggest factor in the changing cable language, but this is just speculation, so I would love to hear any ideas on this. World Figure Plots Now, we can also plot major world figures:  The above are some of the ex-dictators that have been in the news lately. You can see some very interesting patterns (Hussein becomes associated with very negative sentiment right when the second Iraq war starts, for example). Here are US Sentiments towards some world leaders:  World figure interpretation The US seems to have some strange sentiments towards world figures/leaders. The dictators do not seem to have been universally reviled prior to their ousters. Sentiment seems to be improving from 2009-2010 (perhaps due to Obama taking office). Any more interpretation/thoughts would be appreciated! Conclusion This has been a very interesting post for me, and I hope that it can be built upon. Please let me know your thoughts, and/or if you would like to see any different analyses done."], "link": "http://viksalgorithms.blogspot.com/feeds/5153720746573977786/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 2, "http://langtech.jrc.it/": 1, "https://gist.github.com/": 2, "http://en.wikipedia.org/": 2, "http://www.ntnu.no/": 1, "http://www2.dtu.dk/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Now we are on to the finals! The algorithm enters the finals with a 6-4 record so far. Here is what we have for tonight:   So, let's see if OKC wins this one."], "link": "http://viksalgorithms.blogspot.com/feeds/2963365471469555314/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["6/18: A follow-up to this post is now available here . Recent Discoveries When I was a diplomat, I was always interested in the Wikileaks cables and what could be done with them. Unfortunately, I never got a chance to look at the site in depth, due to security policies. Now that the ex- is firmly prepended to diplomat in my resume, I think that I am finally ready to take that step. I recently realized that the wikileaks cables are available in a handy .sql file online. This of course allowed me to download all 250,000 and import them into a database table (I used psql and the /i command). If you are interested in obtaining the cables for yourself, you will need to download the torrent from here . Let me just clarify here that I will not be printing the text of any of these cables (which has been done in several newspapers), and that I will not be using any data that is not readily publicly available online. That's great, but what can we do with them? After I had the cables, I brainstormed to see what I could actually do with them that would be interesting. I came up with a few ideas: Find how topics have changed over time. It's reasonable to assume that the focus of the cables would have shifted from \u201cSoviet Union\u201d this and \u201cUSSR\u201d that to the Middle East. Find out what words typify State Department writers. Anyone who has read cables knows that while they are (mostly) in English, its a strange kind of English. Find out what words/topics typify secret/classified vs unclassified cables. What topics are more likely to be classified? Does word choice change in classified vs unclassified cables? I will get into these topics and more as we continue on through this post. Starting to work with the data The first thing we need to do is read the data from a database. I interfaced with my PostgreSQL database via ODBC. channel <- odbcConnect(db_name, uid = \"\", pwd = \"\")  Now, let's get all the cables from 2010 onwards: cable_frame <- sqlQuery(channel, \"SELECT * from cable WHERE date > '2010-01-01'\",  stringsAsFactors = FALSE, errors = TRUE)  We can make a plot of which senders sent the most cables from 2010 onwards: last_10 <- tail(sort(table(cable_frame$origin)), 10) qplot(names(last_10), last_10, geom = \"bar\") + opts(axis.title.x = theme_blank()) +  opts(axis.title.y = theme_blank()) + opts(axis.text.x = theme_text(size = 8))    We can see that the Secretary of State and Embassy Baghdad are the two biggest offenders. Now, we can get all of the cables in the database and see how cable traffic changed over time (or perhaps Wikileaks had a biased sample): all_cables <- sqlQuery(channel, \"SELECT * from cable\", stringsAsFactors = FALSE,  errors = TRUE)  date_tab <- table(as.POSIXlt(all_cables$date)$year + 1900) qplot(names(date_tab), as.numeric(date_tab), geom = \"bar\") + opts(axis.title.x = theme_blank()) +  opts(axis.title.y = theme_blank()) + opts(axis.text.x = theme_text(size = 8))    The amount of cables rises almost exponentially from 2000 until 2009. I'm assuming that only some of the cables for 2010 were leaked, explaining the low count there. We can get rid of the all_cables file, as we won't need it going forward: rm(all_cables) gc()  Comparing word usage in the 80's and 90's to word usage today Now, we can get to something interesting: we can compare how word usage/topics shifted from 1980-1995 to today. Because there are relatively few cables from early on, we have to specify a 15 year range, which nets us only around 675 cables. cable_present <- sqlQuery(channel, \"SELECT * from cable WHERE date > '2010-02-15'\",  stringsAsFactors = FALSE, errors = TRUE)  cable_past <- sqlQuery(channel, \"SELECT * from cable WHERE date > '1980-01-01' AND date < '1995-01-01'\",  stringsAsFactors = FALSE, errors = TRUE)  Now, we have two challenges. The cables all have line breaks and returns (\\r and \\n), and a lot of the older cables are in all caps. We will get rid of these issues by removing the breaks/returns and converting everything to all lower case. ppatterns <- c(\"\\\\n\", \"\\\\r\") combined <- tolower(gsub(paste(\"(\", paste(ppatterns, collapse = \"|\"),  \")\", sep = \"\"), \"\", c(cable_past$content, cable_present$content)))  Now, we can construct a term document matrix which counts the number of times each term occurs in each document: corpus <- Corpus(VectorSource(combined)) corpus <- tm_map(corpus, stripWhitespace) cable_mat <- as.matrix(TermDocumentMatrix(corpus, control = list(weighting = weightTf,  removePunctuation = TRUE, removeNumbers = TRUE, wordLengths = c(4, 15)))) cable_mat <- cable_mat[rowSums(cable_mat) > 3, ]  We remove any words that are under 4 characters or over 15 characters, and additionally remove any terms that appear less than 3 times in the whole group of cables. For convenience, we can split the matrix into one containing past cables and one containing current cables: present_mat <- cable_mat[, (nrow(cable_past) + 1):ncol(cable_mat)] past_mat <- cable_mat[, 1:nrow(cable_past)] rm(cable_mat) gc()  Now we can get to the good stuff and find differential word usage between the two sets of cables: chisq_vals <- chisq(rowSums(past_mat), ncol(past_mat) * 100, rowSums(present_mat),  ncol(present_mat) * 100) chisq_direction <- rep(-1, length(chisq_vals)) mean_frame <- data.frame(past_mean = rowSums(past_mat)/ncol(past_mat),  present_mean = rowSums(present_mat)/ncol(present_mat)) chisq_direction[mean_frame[, 2] > mean_frame[, 1]] <- 1 chisq_vals <- chisq_vals * chisq_direction cloud_frame <- data.frame(word = rownames(present_mat), chisq = chisq_vals,  past_sum = rowSums(past_mat), present_sum = rowSums(present_mat)) pal <- brewer.pal(9, \"Set1\")  The above code will calculate the statistical difference (chisq) between the terms in the first set of cables (1980-1995), and the second set (cables from february 2010). Now we can make some word clouds. This first cloud contains words that appear in the 2010 cables in a more significant way than in the 1980-1995 cables. A larger size indicates that it more significantly appears in the 2010 cables: wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3),  min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal,  vfont = c(\"sans serif\", \"plain\"))   This second cloud indicates the words that appear in a significant way in the 1980-1995 cables, but not in the 2010 cables: wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3),  min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal,  vfont = c(\"sans serif\", \"plain\"))    As we can see, february is very significant in the first plot, which is to be expected, because all of the cables are from february. But, we can also see interesting patterns, like trafficking becoming very important in 2010 vs 1980-1995, and words like development and training gaining prominence. In the second plot, we see more interest in topics like zagreb, soviet, saudi, and croatia. Find out what words typify secret/classified cables vs unclassified in 2010 Let's take a look at what words/topics are more prevalent in secret or classified cables. Let's first look at how many cables of each type are in our cable_present data frame: table(cable_present$classification)  ## ##      CONFIDENTIAL    CONFIDENTIAL//NOFORN ##         719         67 ##        SECRET      SECRET//NOFORN ##         188         51 ##      UNCLASSIFIED UNCLASSIFIED//FOR OFFICIAL USE ONLY ##         643         756  Now, we will do something similar to what we did above, where the data was split into 2 chunks and the words in each chunk were compared to generate clouds. I have made the code generic by changing the names to set one and set two. cable_set_one <- cable_present[cable_present$classification %in%  c(\"SECRET\", \"SECRET//NOFORN\"), ] cable_set_two <- cable_present[cable_present$classification %in%  c(\"UNCLASSIFIED\", \"UNCLASSIFIED//FOR OFFICIAL USE ONLY\"), ] ppatterns <- c(\"\\\\n\", \"\\\\r\") combined <- tolower(gsub(paste(\"(\", paste(ppatterns, collapse = \"|\"),  \")\", sep = \"\"), \"\", c(cable_set_one$content, cable_set_two$content))) corpus <- Corpus(VectorSource(combined)) corpus <- tm_map(corpus, stripWhitespace) cable_mat <- as.matrix(TermDocumentMatrix(corpus, control = list(weighting = weightTf,  removePunctuation = TRUE, removeNumbers = TRUE, wordLengths = c(4, 15)))) cable_mat <- cable_mat[rowSums(cable_mat) > 3, ] one_mat <- cable_mat[, 1:nrow(cable_set_one)] two_mat <- cable_mat[, (nrow(cable_set_one) + 1):ncol(cable_mat)] rm(cable_mat) gc() chisq_vals <- chisq(rowSums(one_mat), ncol(one_mat) * 100, rowSums(two_mat),  ncol(two_mat) * 100) chisq_direction <- rep(-1, length(chisq_vals)) mean_frame <- data.frame(one_mean = rowSums(one_mat)/ncol(one_mat),  two_mean = rowSums(two_mat)/ncol(two_mat)) chisq_direction[mean_frame[, 2] > mean_frame[, 1]] <- 1 chisq_vals <- chisq_vals * chisq_direction cloud_frame <- data.frame(word = rownames(one_mat), chisq = chisq_vals,  one_sum = rowSums(one_mat), two_sum = rowSums(two_mat)) pal <- brewer.pal(9, \"Set1\")  We are now ready to plot these new word clouds. Here are words that are typical of set 1 (secret cables) that separate it from set 2 (unclassified cables): wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3),  min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal,  vfont = c(\"sans serif\", \"plain\"))   And here are words that are typical of set 2 (unclassified cables) that separate it from set 1 (secret cables) : wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3),  min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal,  vfont = c(\"sans serif\", \"plain\"))   This makes sense, as the first cloud has words like icbms and bombers, whereas the second has words like labor and victims, which would be typical of the trafficking in persons/human rights reports. Find out what words typify secret/classified cables vs unclassified from 1960-2000 Now, we can look at what words differentiated secret cables from unclassified cables from 1960 to 2000. Here is the cloud that shows what words appear significantly in the secret cables, but not in the unclassified cables: wordcloud(cloud_frame$word, -cloud_frame$chisq, scale = c(8, 0.3),  min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal,  vfont = c(\"sans serif\", \"plain\"))   And here is the cloud that shows what words appear significantly in the unclassified cables, but not in the secret cables: wordcloud(cloud_frame$word, cloud_frame$chisq, scale = c(8, 0.3),  min.freq = 2, max.words = 100, random.order = T, rot.per = 0.15, colors = pal,  vfont = c(\"sans serif\", \"plain\"))   Conclusion It's very interesting to see how these patterns change over time. Particularly, seeing what the classified topics were from 1960-2000 versus unclassified is interesting. I really wanted to see how State Department writers differ from normal english writers, but I don't have the time to do it right now. It will have to wait for the next post."], "link": "http://viksalgorithms.blogspot.com/feeds/9007696286801530443/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://file.wikileaks.org/": 1, "http://viksalgorithms.blogspot.com/": 1, "http://2.blogspot.com/": 6, "http://1.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["This is the sixth post in my series on predicting the NBA playoffs with an algorithm. After the Boston loss in their last game, the algorithm is now 5-4 in the playoffs. Hopefully it is correct tonight! Open Sourcing the Code I have had a couple of requests to open source the code, which I had planned to do at the end of this series of posts. However, there is one stumbling block in that the data I am scraping cannot be redistributed (I think). If anyone has access to box score data for the 2010-2011 and 2011-2012 seasons that has a public license, please let me know. You can contact me via the email in my profile, or in the comments section. Being able to get the data would simplify things a lot, but I would still have to clean up and comment the code a bit. Expect to see it out in a week or so. Predictions The algorithm likes Miami tonight:"], "link": "http://viksalgorithms.blogspot.com/feeds/7889460415866570979/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://viksalgorithms.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["This is update 4 to my original post about predicting the NBA playoffs with R . With the Thunder beating the Spurs and the Heat losing to the Celtics, the algorithm went 1-1 on predictions, making it 5-3 so far. Making some improvements I have been posting for some time about incorporating more data into the models, and I finally got around to it. It is a common truism in data science that more (high-quality) data almost always leads to a better model, and it is no exception here. The fact that the 2011-2012 season was strike-shortened also meant that only relying on data from this season really limited the potential of the algorithm. I decided to start slowly, and incorporate data from both the 2010-2011 season, and the 2011-2012 season. Due to the aforementioned strike, this actually increases the data available by 128% overall. I made no other tweaks to the algorithm in this time, so this is a good test of how much value additional data on its own can add.  The new accuracy value across both seasons is 65.6% , which means that it is predicting 1.91 times as many winners as losers. Here is the confusion matrix:   Differences between seasons I thought it would be interesting to look at p-values between different season statistics to see if there were any significant differences between the 2010-2011 season and the 2011-2012 season. A big deal is always made about how the lockout affected different statistics, but I haven't seen any analysis on it yet. We can easily do a t-test on each column of the data frame with all of the per team statistics: p_vals { t.test(frame_2011[,i],frame_2012[,i])$p.value } We end up with a table of each calculated statistic the p-values associated with each one. A p-value indicates if there is a statistically significant difference between two distributions. In this case, we might be looking to see whether there is a statistically significant difference between rebounding in the 2010-2011 season, and the 2011-2012 season. The test gave back some interesting results. Here are some of them: 1. There was a very significant difference in the number of players who fouled out between the two seasons. In 2012, far fewer players fouled out than in 2011. Also, less personal fouls were assessed overall, which also dropped the number of free throws attempted. 2. Starters played significantly more unique positions in 2012. For example, if the starting lineup consists of a C, a PF, an SF, an SG/SF, and a PG, there are 5 unique positions. On the other hand, if it consists of a PF, a PF, an SF/SG, an SF/SG, and a PG, that is only 3 unique positions. I am not sure why this increased between seasons, but maybe it indicates the rise of more true centers? Maybe a different way of keeping track of positions? 3. 3 point percentage went down significantly from 2010-2011 to 2011-2012. Less practice time? Not clear why this happened. 4. Rebounding went up overall, as did defensive rebounding. 5. Starters played less minutes in 2011-2012 than in 2010-2011.  Predictions for Tonight And finally, the algorithm is predicting Boston to win tonight:"], "link": "http://viksalgorithms.blogspot.com/feeds/9115540751698028905/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["This is my third update to my original post on predicting the NBA playoffs with an algorithm. Here are updates 1 and 2 . The algorithm correctly predicted a Boston win, but missed on the Spurs/Thunder game, so it is currently 4-2 . Haven't had any time to update yet, so I will only be able to give you predictions for the next games, unfortunately:  Predicting a Miami win and an Oklahoma City win."], "link": "http://viksalgorithms.blogspot.com/feeds/6109649427445817068/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 3, "http://1.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["This is my second follow-up to my previous two posts which were about predicting NBA games with an algorithm , and my first update to the algorithm . The algorithm's record is now 3-1 , as it correctly predicted Boston and Oklahoma City as winners of their past games. Upcoming things to do Sadly, I have been a bit busy, and I have not been able to do any work on the algorithm the past couple of days. My next steps are going to be to add in more historical data, and to implement a good backtesting framework to get a more reliable error estimate. Predictions for the Next Games  So, here the algorithm is predicting that San Antonio and Boston will win their next games (Boston's game is tonight, and San Antonio's is tomorrow). Let's see how it plays out, first game is in a few hours!"], "link": "http://viksalgorithms.blogspot.com/feeds/4409129673173821400/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 2, "http://3.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Game Results I recently made a post about developing an algorithm to predict the NBA playoffs, and I concluded with 2 predictions. Although Miami beat the Celtics to make my algorithm 1-0 in terms of predictions, it fell to 1-1 when the Thunder beat the Spurs. So, we are now at .500 . Considering that the algorithm was about 61.5% accurate over the whole season, this is to be expected. I made some improvements to the algorithm which improved accuracy, and then used this to make new predictions for the next game in each of the current series (Spurs vs. Thunder and Celtics vs. Heat). You can scroll down all the way to see the predictions, or read through to see what I did. Improvements in Variables I created rough code initially, and I didn't fully utilize all of the information that I had. The first step in making improvements was to add some variables relating to different player positions and bench vs. starter performance. For example, this plot shows that the average number of seconds bench players played over the last 10 games has a decent correlation with winning percentage:  A reasonable conclusion to this is that winning teams generally have a stronger bench that they can rely on more. Improvements in Machine Learning After adding some variables, I moved on to adjusting the models that I used. I had initially spent very little time on the machine learning framework, and most of the time on the data, and that did not change here, but I was able to tweak what I was predicting. Initially, I was predicting a binary value- whether a team won or not. I adjusted this to predict the ratio between a team's score and another team's score. This gave the machine learning algorithms a lot more information than a 1/0 target, and also had the benefit of being a normal distribution, as this quantile-quantile plot shows:  The straight red line is a normal distribution, and this is very close, which makes it an ideal target variable. After the machine learning algorithms predicted this, I was able to combine the results via a simple mean method and convert them to binary win/loss values. This made the algorithm much more accurate than before. Updated Season Accuracy Results With the improvements, the accuracy now comes to 63.6% for the season, which is a reasonable improvement over the previous results. This results in this confusion matrix:  The main hindrance to accuracy is only being able to get team names and home/away information for future games. If it was possible to get more information, such as officials, lineups, etc, it would be possible to make a much more accurate model. Having more data from past seasons would also help a lot, and I might look into getting that. Predictions for Upcoming Games As before, I will leave you with predictions for the two upcoming games.  Here, the algorithm is predicting a Boston win/Miami loss, and an Oklahoma City win/San Antonio loss. Let's see how it plays out! I can do a decent amount of analysis on the data, so please let me know if you want to see something specific next time. I'm going to make posts predicting all of the games in the series and the finals."], "link": "http://viksalgorithms.blogspot.com/feeds/5415591293840639618/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://viksalgorithms.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://2.blogspot.com/": 2}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["This is the initial post about the algorithm. See updates 1 , 2 , and 3 for more. The algorithm is currently 4-2 in the playoffs! Overview I was struck by Martin O'Leary's recent post on predicting the Eurovision finals , which led me to decide that I would try to predict NBA games using mathematical models. As the finals are ongoing, this is a quite timely decision! You can read through everything or scroll to the end for finals predictions and accuracy results. This model is necessarily very different (read: nothing in common at all) from Martin's, because the underlying concept that we are predicting, and how it is derived, are very different. The outcome of NBA games are determined by a variety of factors, such as the scoring ability of each team, whether the teams are playing at home or away, and whether or not any players are injured. It stands to reason that several of these factors can be modeled, and used to predict whether a team will win or lose a given game. But, we are getting ahead of ourselves. The first step, as always, is to acquire the data. Getting the Data Game data is actually quite difficult to get in machine readable form. This is likely because while there is a major demand for data that can be casually read and looked over, there isn't a major public demand for sports data feeds. As I lacked enough of a budget to become an ESPN \"partner\", I had to resort to more utilitarian methods, which gave me data in essentially a \"box score\" format. This format gave the statistics for each team for each game.  Reformatting the Box Scores The majority of the problem at this point is actually reformatting the data to make it into a useful form for analysis. A box score is good, but if we want to predict who will win a game in the future, it isn't very useful. The problem is in getting from something like the table below to something that an algorithm can read in:  As always, this is where the not so glamorous side of data analysis comes in; it takes a lot of time to convert data into a useful form.  Computing Summary Statistics I first computed summary statistics for each team for each game, such as rebounds per game, blocks per game, etc:  This is truncated, and a lot of variables are left out, but it gives you an idea of how it came about. Basically, I summarized the box scores and converted them into \"observations\", where each teams performance becomes one row. I computed indicators such as rebounds per player, turnovers per player, etc. The next problem with this is that there is very little predictive ability in just the previous game's statistics. In order to predict future games, we need to know a teams performance for the whole season. At this point, I split the data up by team, and treated the season for each team as a time series. This allowed me to compute summary statistics for the whole season, and for the last 10 games for each team. We could see the rebounds per game, for example, for the whole season. Here is how the data looked at this point:  Note that this is truncated in terms of both rows and columns. Essentially, for each game, the running statistics for the team up to that point are available. For example, here are Denver's season running averages for assists, points, and rebounds:  And here are Denver's 10 day back averages:  As you can see, there are some interesting patterns in the 10 day back statistics that do not appear in the season running averages. This is why it is useful to have both, and to take the ratio between the two. Putting all the data together The next step was to put together the statistics for each team for each game with the similar statistics for their opponent. Once this was accomplished, the result could be fed into predictive algorithms. This left me with a table where each row contained data for a team, and for its opponent, in terms of their season performance. Finally, Some Machine Learning I was finally able to put all of the data together, and predict game winners based on this. I used all of the games for the season (minus 10 games for each team at the beginning needed to initialize the 10 period means), and used cross validation to predict results. This causes some issues because it has future data to predict \"past\" data, but should still be relatively accurate. For the actual machine learning framework, I used a simple combination of three different classification algorithms (they were combined via their median). The predictions were made as 1 (the team won), or 0 (the team lost). This resulted in the following table:  The x-axis is predicted result, and the y axis is the actual result. As you can see, the predicted result matches the actual result in the majority of the cases. To be specific, the algorithm correctly predicted 1323 winners/losers, and incorrectly predicted 823 winners/losers over the whole season. This gives it a 61.5% prediction accuracy for the season, which is pretty good for a days work! Now, if we feed in data for the first 2/3rds of the season, and we predict the final 1/3 of the season, we get the following matrix:  This is pretty similar to the cross validated error, and the prediction accuracy comes to about 60.5%. The accuracy is likely lower than it should be simply because the predictions are being made for 1 month or more at a time. If predictions were restricted to one day/game ahead, accuracy would be much improved. Potential Improvements Of course, this is a very rough system, and both the data and the methods can use a lot of refinement. 61% is a good lower bound for accuracy, but with some work, it could go up significantly. The main improvements that could be made are in the data acquisition, in the variable calculation, and in the final models that are used to calculate the win/loss. The system can also be modified to predict points or point spreads, which might make things a bit better as well. And of Course, Predictions for the Finals! And, last but not least, I will leave you with some NBA finals predictions. I am currently predicting one day/game ahead for the finals, but I might work on altering this to predict further in advance if needed.  So, here it is predicting that both Miami and San Antonio will win their next game. This seems strange in practice, but we will see how it plays out!"], "link": "http://viksalgorithms.blogspot.com/feeds/202561860220765707/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 3, "http://3.blogspot.com/": 4, "http://mewo2.github.com/": 1, "http://1.blogspot.com/": 1, "http://4.blogspot.com/": 2, "http://2.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["I have posted previously about the open data available on Socrata (https://opendata.socrata.com/), and I was looking at the site again today when I stumbled upon a listing of levels of various radioactive isotopes by US city and state. The data is available at https://opendata.socrata.com/Government/Sorted-RadNet-Laboratory-Analysis/w9fb-tgv6 . You will need to click export, and then download it as a csv.  I was struck by how the data was in a very nice format for analysis. I initially wanted to look at summary statistics, but I have wanted to try some mapping for a while now, and this seemed like the right time.  Reading in the Data  To begin exploring and mapping the data, we first need to download it. After downloading, it can be read in using the read.csv command: rad_levels stringsAsFactors=FALSE,strip.white=TRUE,header=TRUE,quote=\"\") This gives us a nice data frame where each row is an observation of a different type (Drinking Water, Precipitation, etc), and the columns contain location data and data on levels of various isotopes.  Associating Coordinates with the Data  Unfortunately, we are not provided with any coordinates, and in order to map this data, we will need to figure out the coordinates associated with each observation. Thankfully, I recently wrote a quick function to map place names to coordinates.  does_zip_exist { zip_exists if(exists(\"zips\")) { zip_exists if(!nrow(zips)==43623) zip_exists } zip_exists } get.coordinates.name { city state if(!does_zip_exist()) load(\"zips.RData\",.GlobalEnv) if(nchar(state)==2) { zip_row }else { zip_row } if(nrow(zip_row)==0) zip_row zip_row } These two functions together will enable you to get the coordinates of a given US city and state. They both depend on a file called zips.RData, which can be downloaded at http://dl.dropbox.com/u/20597506/zips.RData . The file must be present in your R working directory for the functions to work. I wrote these functions really quickly, and don't feel like rewriting them now, so please feel free to improve their performance if you want. I will make a longer post about them later if needed.  Now, we have our radiation data, and the ability to associate the place names in the data to coordinates. What we need to do now is to perform that association. rad_level_coords rad_level_coords rad_level_frame This code will take each row of the radiation levels data and find the coordinates associated with the place name. It will then combine the coordinate data into a data frame, and merge it with the radiation data.  Cleaning up the Data One problem at this stage is that some of the place names in the radiation data will not properly map to coordinates. The get.coordinates.name function outputs a zip code of 00000 in this case. Thus, we can remove rows that do not have proper coordinates by filtering out the rows with a zip code of 00000. rad_level_frame We can also check the classes of the columns in rad_level_frame to ensure that the coordinates are numeric. sapply(rad_level_frame,class) Unfortunately, they are not numeric columns, and they will need to be in order to use them to generate a map, so we will need to convert them: rad_level_frame$lat rad_level_frame$long Setting up the Map We now need to do some preliminary setup before we get started on making the map. We need to define which type(s) of measurements that we want to plot, and what isotope levels that we want to plot. For this first plot, we will look at I-131 in drinking water. types current_type target The types variable simply lists the types of measurements that are in the radiation data for convenience, and the current_type and target variables will allow us to simplify the code a bit.  Since we want to plot the different levels of radiation in Drinking Water, it will help if we can bin the variable. Binning allows us to split up an interval into discrete units. In this case, binning will help us by changing the target variable into a set of colors. These colors will range from \"green\" (least radioactive) to red (most radioactive), and will allow us to plot the data. binned_target rad_level_frame$Sample.Type %in% current_type]), breaks=5, labels=c(\"green\",\"blue\",\"yellow\",\"orange\",\"red\")) Cut is a function that will create a factor from a continuous numeric variable. Note that we have filtered out the instances of the target where nothing could be detected by removing the target variable when its value is \"Non-detect\". This could also be handled by changing the \"Non-detect\" to a zero, but as it is unclear whether \"Non-detect\" means a zero value, or whether it indicates equipment failure or something else, it is best to remove it entirely. We have also removed observations where the measurement type is not \"Drinking Water\". This will ensure that we only plot observations of drinking water radiation levels. Breaks specifies that we want to separate the data into 5 categories, which have been assigned labels according to the color that they will be plotted in.  Mapping the Data  Now, we are ready to create our base plot of the United States: plot(as.numeric(zips$long[zips$long as.numeric(zips$lat[zips$long type=\"p\",col=\"gray40\",pch=20,cex=0.2,xlab=\"\",ylab=\"\") This will plot all the longitudes and latitudes from the zips.RData file in a gray color, which gives us a good US map. Specifying that the longitude be under -60 removes some of the island possessions of the US, which unnecessarily stretch out the map. The gray provides a good neutral color over which to plot our radiation levels. The pch option gives us a closed circle plotting symbol, and the cex option makes the individual points fairly small.  Now, we are ready to plot our radiation levels over the base map: points(as.numeric(rad_level_frame$long[target!=\"Non-detect\"& rad_level_frame$Sample.Type %in% current_type]), as.numeric(rad_level_frame$lat[target!=\"Non-detect\"& rad_level_frame$Sample.Type %in% current_type]), type=\"p\",col=as.character(binned_target),pch=20,cex=1) The points function adds points to a plot generated by the plot function. We have only plotted the latitudes and longitudes for the radiation level observations which are not \"Non-detect\", and which match our type, which is \"Drinking Water\". We also set the color (col) using the binned_target variable that we created earlier. The cex value is set higher so that the points appear large relative to the map.  We should end up with this:  Drinking Water Radiation Map   We can change our current_type variable and run the plotting again to generate the rest of the plots: Air Filter Radiation Map   Precipitation Radiation Map   Milk Radiation Map   You can do more mapping with the other targets if you wish at this point. Here is the full code: setwd(\"~\") rad_levels stringsAsFactors=FALSE,strip.white=TRUE,header=TRUE,quote=\"\") rad_level_coords rad_level_coords rad_level_frame rad_level_frame rad_level_frame$lat rad_level_frame$long types current_type target binned_target rad_level_frame$Sample.Type %in% current_type]), breaks=5, labels=c(\"green\",\"blue\",\"yellow\",\"orange\",\"red\")) plot(as.numeric(zips$long[zips$long as.numeric(zips$lat[zips$long pch=20,cex=0.2,xlab=\"\",ylab=\"\") points(as.numeric(rad_level_frame$long[target!=\"Non-detect\"& rad_level_frame$Sample.Type %in% current_type]), as.numeric(rad_level_frame$lat[target!=\"Non-detect\"& rad_level_frame$Sample.Type %in% current_type]),type=\"p\", col=as.character(binned_target),pch=20,cex=2.5)"], "link": "http://viksalgorithms.blogspot.com/feeds/1621926639359048504/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2, "http://1.blogspot.com/": 2}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["In R, the traditional way to load packages can sometimes lead to situations where several lines of code need to be written just to load packages. These lines can cause errors if the packages are not installed, and can also be hard to maintain, particularly during deployment. Fortunately, there is a way to create a function in R that will automatically load our packages for us. In this post, I will walk you through conceiving and creating such a function.  In order to write a function that checks if a package is installed, loads it if it is, and installs it if it isn't, we first need a way to check if a package is installed. Thankfully, this function does the job: is_installed The above function comes from a post on the R mailing list, although I do not know if it is the original source. This function will test if a given function name is in the list of installed packages. We can access the list directly by using installed.packages()[,1] , and we can use the function by trying is_installed(\"foreach\") .   Now that we know how to test if a package is installed or not, we can move on to writing the function. At this moment, we have two hurdles. The first is how to load a package from a character vector of names. The second is how to install a package programmatically. Typically, loading a package will look like this: library(MASS) Fortunately for us, there is a character.only option in the library function that allows us to specify the package name as a string. library(\"MASS\",character.only=TRUE) The above gives us the functionality that we need to pass the name of a package to a function as a string and have it loaded. Now, we need to find how to install packages, which can be done with the install.packages function: install.packages(\"MASS\",repos=\"http://lib.stat.cmu.edu/R/CRAN\") Explicitly setting the repo will avoid having R ask us for it when the function is executed for the first time. I chose statlib for convenience, but feel free to use any repo you like.  Now, we have a way to test if a package is installed, a way to install the package, and a way to load the package. All we need to do is wrap it up with an if statement. if(!is_installed(package_name)) { install.packages(package_name,repos=\"http://lib.stat.cmu.edu/R/CRAN\") } library(package_name,character.only=TRUE,quietly=TRUE,verbose=FALSE) The above if statement will test to see if a package is installed, and then install it if it isn't. It will then load the package.  This gets us most of the way to what we want, but if we want to pass a character vector to the function and have it load multiple packages at once, we need to wrap everything in a for loop. for(package_name in package_names) { if(!is_installed(package_name)) {  install.packages(package_name,repos=\"http://lib.stat.cmu.edu/R/CRAN\") } library(package_name,character.only=TRUE,quietly=TRUE,verbose=FALSE) } This for loop will perform the operations that we need on the character vector package_names. Now, we can just wrap everything into a neat function that is passed a character vector called package_names. load_or_install { for(package_name in package_names) {  if(!is_installed(package_name))  {  install.packages(package_name,repos=\"http://lib.stat.cmu.edu/R/CRAN\")  }  library(package_name,character.only=TRUE,quietly=TRUE,verbose=FALSE) } } We can call the function with the following syntax (substitute the function names with your own): load_or_install(c(\"foreach\",\"MASS\",\"doParallel\")) And with that, we are done, and now have a function that can load or install packages as needed from a character vector. This function will terminate with an error if the install.packages function or the library function cannot find the specified package name, but you can fix that by using a try statement, which is an exercise that I will leave to you for the moment."], "link": "http://viksalgorithms.blogspot.com/feeds/3169893668697261422/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["The foreach package for R is excellent, and allows for code to easily be run in parallel. One problem with foreach is that it creates new RScript instances for each iteration of the loop, which prevents status messages from being logged to the console output. This is particularly frustrating during long-running tasks, when we are often unsure how much longer we need to wait, or even if the code is doing what it is intended to. The solution to this can be found in the sink() function. This function redirects output to a file. I will show you a simple example of this using the iris data set. The code below will execute without printing any status messages, even though do.trace is enabled, which periodically displays the status of the randomForest. The random forest code is slightly adapted from one of the foreach package examples. library(foreach) library(doSNOW) library(randomForest) data(iris) cores num_trees c1 registerDoSNOW(c1) rf_fit .packages=c(\"randomForest\")) %dopar% { randomForest(iris[,-5],y=iris[,5],ntree=ntree,do.trace=100) } stopCluster(c1)  We can easily correct this with the sink function: library(foreach) library(doSNOW) library(randomForest) data(iris) cores num_trees c1 registerDoSNOW(c1) writeLines(c(\"\"), \"log.txt\") rf_fit .packages=c(\"randomForest\")) %dopar% { sink(\"log.txt\", append=TRUE) randomForest(iris[,-5],y=iris[,5],ntree=ntree,do.trace=100) } stopCluster(c1) The writeLines function will clear out the file \"log.txt\" before the loop is run, ensuring that only output that is relevant to the current run is displayed when the file is opened. The log file can be opened at any time during the run, and the progress can be checked. We can even print the number of the iteration as we go through: rf_fit .combine=combine,.packages=c(\"randomForest\")) %dopar% { sink(\"log.txt\", append=TRUE) cat(paste(\"Starting iteration\",iteration,\"\\n\")) randomForest(iris[,-5],y=iris[,5],ntree=ntree,do.trace=100) } This is useful when you have significantly more iterations than processor cores, and you want to know how far along you are."], "link": "http://viksalgorithms.blogspot.com/feeds/7097400540850195138/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["LaTeX is a typesetting system that can easily be used to create reports and scientific articles, and has excellent formatting options for displaying code and mathematical formulas. Sweave is a package in base R that can execute R code embedded in LaTeX files and display the output. This can be used to generate reports and quickly fix errors when needed. There are some barriers to entry with LaTeX that seem much steeper than they actually are. In this article, I will show you how to setup LaTeX and an IDE for it in Windows, and how to start developing for it. Note that you will at least R version 2.14 to follow this entire article, because the command line sweave pdf export option was added then. Install MiKTeX and TeXnicCenter LaTeX documents are edited in a text editor, then compiled by a compiler, and finally are displayed in a PDF or postscript viewer. We will begin by installing a LaTeX compiler for Windows, MiKTeX . Once you grab the installer, you can go ahead and install it. Please see the bottom of the post for the full edit, but installing LaTeX in paths with spaces in them can cause issues. As such, I would recommend installing to a directory without them. Our next step will be to install TeXnicCenter , an IDE for LaTeX. After installing TeXnicCenter and starting it for the first time, you will be asked to find your LaTeX executable file in the configuration wizard. This can generally be found in the folder C:\\Program Files\\MiKTeX 2.9\\miktex\\bin by default, or a custom path if you did not use spaces in your path. You can leave the fields for PostScript viewer blank, unless you need the functionality. You should now be finished with the installation. Setup TeXnicCenter to Work with Sweave Now, we can setup TeXnicCenter to use Sweave directly. This will require at least R 2.14. To do this, click on the Build menu and go to Define Output Profiles. Hit the \"Add\" button in the bottom left to create a new output profile. You can name the profile anything you like. I named mine \"Sweave\".  Now we need to configure the new output profile. The first tab should look similar to mine if you use 64-bit R, but your R path will be different if you use 32-bit or used a non-default installation directory. The directory will need to match yours. The command line arguments box should read: CMD Sweave --pdf %nm .  Now, we only need to worry about the viewer tab. I use Foxit Reader, so my settings are below. I suggest using the same settings as you find in your LaTeX=>PDF output profile.  Once we have all that set, you can go ahead and write your first document. You will need to name the document something.Rnw. To build the file, you will have to select \"Sweave\"(or whatever you named your profile\" in the drop down box below the build item on the menu bar, and then select Build->Current File->Build and View to build your first file.  The above shows a trivial R script. <<>>= signifies the start of an R script, and @ signifies the end. Further Reading To learn more about Sweave, I suggest reading the user manual . Here is a good discussion of workflow with big documents and Sweave. To learn more about LaTeX, I suggest the LaTeX wikibook , and The Not So Short Introduction to LaTeX . This page shows how to make a basic document in LaTeX, and can be a good template to work off of. This link gave me some of the concepts used above, and also has instructions for how to use Stangle with TeXnicCenter.  Edit A commenter has pointed out that using a directory path with spaces in it can create bugs in LaTeX. You can find more information on this here , and here . I have updated my post."], "link": "http://viksalgorithms.blogspot.com/feeds/5897587368307715451/comments/default", "bloglinks": {}, "links": {"http://miktex.org/": 1, "http://www.lmu.de/": 1, "http://stackoverflow.com/": 1, "http://www.texniccenter.org/": 1, "http://amath.colorado.edu/": 1, "http://tex.stackexchange.com/": 1, "http://www.uni-muenchen.de/": 1, "http://sachaem47.versio.nl/": 1, "http://4.blogspot.com/": 1, "http://groups.google.com/": 1, "http://2.blogspot.com/": 2, "http://en.wikibooks.org/": 1, "http://tobi.oetiker.ch/": 1, "http://www.latex-project.org/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Modifying R code to run in parallel can lead to huge performance gains. Although a significant amount of code can easily be run in parallel, there are some learning techniques, such as the Support Vector Machine, that cannot be easily parallelized. However, there is an often overlooked way to speed up these and other models. It involves executing the code that generates predictions and other analytics in parallel, instead of executing the model building phase in parallel, which is sometimes impossible. I will show you how this can be done in this post. First, we will set up our variables. The setup is fairly similar to the one I have used in other posts, but note that the length of the vectors has been increased by a magnitude of 100 to more easily show how much time can be saved by parallelizing the prediction building phase. set.seed(10) y x1 x2 x3 all_data positions training testing We now have a testing set of 25,000 rows, and a training set of 75,000 rows, which is somewhat linear. We will train an SVM on this data. Note that it may take more than 10 minutes to train an SVM on this data, particularly if you have an older computer. If you do have an older computer, feel free to reduce the number of rows in the data frame as needed. library(e1071) svm_fit svm_predictions error error After we have built the model, we generate predictions for it, which yields an error of 13045.9 for me(although this may be different for your data). Our next step is timing the prediction phase to see how long it takes. system.time(predict(svm_fit,newdata=testing)) On my system, this took 35.1 seconds. We are now ready to set up our parallel infrastructure. I run Windows, and will use foreach and doSNOW here, although you can certainly use other parallel packages here if you prefer. You can read this post if you want an introduction to foreach, doSNOW, and doMC. If you do not elect to use doSNOW, you will not need to use the stopCluster() function that appears in some of the code below. library(foreach) library(doSNOW) cl registerDoSNOW(cl)  Now, we have the groundwork for our parallel foreach loop, but we need to find a way to split the data up in order to perform predictions on small sets of data in parallel.  num_splits split_testing This will create a numeric vector that can be used to split the testing data frame into 4 parts. I suggest setting num_splits to some multiple of your number of CPU cores in order to execute the below foreach loop as quickly as possible. Now that we have a way to split the data up, we can go ahead and create a loop that will generate predictions in parallel. svm_predictions .combine=c,.packages=c(\"e1071\")) %dopar% { as.numeric(predict(svm_fit,newdata=testing[split_testing==i,])) } stopCluster(c1) It is very important that the .packages argument be used to load the package that corresponds to the prediction function you are going to use in the loop, or R will get confused about which prediction function to use and generate an error. The .combine argument tells the foreach loop to combine the outputs of the foreach loop into a vector. A hidden argument that defaults to true ensures that all the outputs remain in order. Now, we test to make sure that everything is okay by checking what the error value is: error error I got 13045.9, which matches the value I got before, and confirms that both the parallel and non-parallel prediction routines return the exact same results. Now, we can create a function and time it to see how fast the parallel technique is: parallel_predictions { cl registerDoSNOW(cl) num_splits split_testing predictions .combine=c,.packages=c(\"e1071\")) %dopar% { as.numeric(predict(fit,newdata=testing[split_testing==i,])) } stopCluster(cl) predictions } system.time(parallel_predictions(svm_fit,testing)) This takes 12.76 seconds on my system, which is significantly faster than the non-parallel implementation. This technique can be extended to other analytics functions that can be run after the model is built, and it can generate predictions for any model, not just for the svm that the example uses. While creating the model can take up much more time than generating predictions, it is not always feasible to parallelize model creation. Running the prediction phase in parallel, particularly on high dimensional data, can save significant time."], "link": "http://viksalgorithms.blogspot.com/feeds/3736701907267312270/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["As I was exploring open data sources, I came across USA spending . This site contains information on US government contract awards and other disbursements, such as grants and loans. In this post, we will look at data on contracts awarded in the state of Maryland in the fiscal year 2011, which is available by selecting \"Maryland\" as the state where the contract was received and awarded here . I will use Maryland as a proxy for the nation, as the data set for the whole nation will be a bit more unwieldy to analyze, and the USA spending site appears to need a significant amount of time to generate the data file for it. We may take a look at the data for the whole nation later on. First, we download the data file(leave all the options on the usa spending site the same, except select Maryland as the state where the contracts were received and performed when you download the file if you want to follow along), and read it into R: spend<-read.csv(file=\"marylandspendingbasic.csv\", head=TRUE,sep=\",\",stringsAsFactors=FALSE,strip.white=TRUE) By using the names() function, we can see that the data file has a lot of interesting columns. One column is called extentcompeted, and has values indicating how much competition was involved in the bidding process for the contracts. Understandably, having an open bidding process is in the public interest as it can reduce taxpayer costs. We will start by looking at competition in the bidding process for contracts: library(ggplot2) competition<-tapply(spend$ExtentCompeted,spend$ExtentCompeted, function(x) length(x)/length(spend$ExtentCompeted)) qplot(names(competition),weight=competition*100,xlab=\"Competition Type\", ylab=\"Percent of Contracts\",fill=names(competition)) + opts(axis.text.x = theme_blank()) + scale_fill_discrete(\"Competition Type\") This lets us see what percentage of the data falls into each competition category, and then graphs it using the excellent ggplot2 package.  This shows us that about 58% of contracts went through a full competitive bidding process, whereas only approximately 15% of the contracts did not undergo any kind of competition process. Now, lets see how many dollars of spending fall into each competition category: comp_dollars<-tapply(spend$DollarsObligated,spend$ExtentCompeted, function(x) sum(x)/sum(spend$DollarsObligated,na.rm=TRUE)) qplot(names(comp_dollars),weight=comp_dollars*100,xlab=\"Competition Type\", ylab=\"Percentage of Dollars Obligated\",fill=names(comp_dollars)) + opts(axis.text.x = theme_blank()) + scale_fill_discrete(\"Competition Type\")  This plot tells a very different story, showing that about 31% of all dollars that were spent by the government went to contracts that did not involve competition. Further, only 44.5% of all dollars that were obligated went to contracts that were bid on under a fair and open competitive bidding process. This indicates that large contracts tend to receive less bidding than small contracts. This is strange, as large contracts are the ones that are more likely to have reduced costs as a result of competitive bidding, simply because saving 5% of a larger sum is preferable to saving 5% of a smaller sum. Let's take a look at where these large no-bid contracts are going. companies<-sort(tapply(spend$DollarsObligated,spend$RecipientName,sum)) top_companies<-tail(companies/sum(spend$DollarsObligated),10)*100 sum(spend$DollarsObligated/1e9) qplot(names(top_companies),weight=top_companies,xlab=\"Company\", ylab=\"Percentage of Total Dollars Obligated\",fill=names(top_companies)) + opts(axis.text.x = theme_blank()) + scale_fill_discrete(\"Company Name\") + opts(legend.text = theme_text(size = 8))  First, we note that 15.7 billion dollars in federal contracts were obligated to vendors in the state of Maryland in the fiscal year 2011. Next, we note that about 11.5% of the total federal money allocated in contracts went to Lockheed Martin! In fact, the top 10 companies in terms of dollar value of contracts received were given 41% of all the contracted dollars, which amounts to about 6.5 billion dollars. The top 100 contractors received 72% of all contracted dollars. Now, it is becoming clearer where the large no-bid contracts are going. We will look more in depth at the contracts that did not involve competitive bidding to zero in on the issue: nc_spend | spend$ExtentCompeted==\"Not Competed\" | spend$ExtentCompeted==\"Not Available for Competition\" | spend$ExtentCompeted==\"Full and Open Competition after exclusion of sources\",] nc_companies nc_top_companies sum(nc_spend$DollarsObligated)/1e9 sum(tail(nc_companies,10))/sum(nc_spend$DollarsObligated) This tells us that out of the 7.6 billion dollars in government contracts that were disbursed with limited or no competition, 55% of these went to the top 10 contractors who received contracts with limited or no competition. This shows that large contractors tend to receive a much greater share of no compete contracts than other contractors, as the top 10 contractors only received 41% of all federal dollars, yet received 55% of contracts with limited competition. mean(nc_spend$DollarsObligated) mean(spend[spend$ExtentCompeted==\"Full and Open Competition\",]$DollarsObligated) This shows that the average contract size with no or limited competition was $253507.1, whereas the average contract size with full and open competition was $113936.3, meaning that, on average, contracts twice as large are given out with no competition.  I will leave this analysis here for now, but please feel free to continue on if you wish. This is a very interesting data set, and I may come back to it if I have time down the line."], "link": "http://viksalgorithms.blogspot.com/feeds/7517490718162120166/comments/default", "bloglinks": {}, "links": {"http://usaspending.gov/": 2, "http://2.blogspot.com/": 2, "http://3.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Linear regression can be a fast and powerful tool to model complex phenomena. However, it makes several assumptions about your data, and quickly breaks down when these assumptions, such as the assumption that a linear relationship exists between the predictors and the dependent variable, break down. In this post, I will introduce some diagnostics that you can perform to ensure that your regression does not violate these basic assumptions. To begin with, I highly suggest reading this article on the major assumptions that linear regression is predicated on. We will make use of the same variables as we did in the intro to ensemble learning post:  set.seed(10) y x1 x2 x3 Note that x2 and x3 are significantly nonlinear by design(due to the squared and log modifications), and will cause linear regression to make spurious estimates. Component residual plots, an extension of partial residual plots, are a good way to see if the predictors have a linear relationship to the dependent variable. A partial residual plot essentially attempts to model the residuals of one predictor against the dependent variable. A component residual plot adds a line indicating where the line of best fit lies. A significant difference between the residual line and the component line indicates that the predictor does not have a linear relationship with the dependent variable. A good way to generate these plots in R is the car package.  library(car) lm_fit crPlots(lm_fit) Your plot should look like this:   Looking at the plot, we see that 2 of the predictors(x2 and x3) are significantly non-normal, based on the differences between the component and the residual lines. In order to \"correct\" these differences, we can attempt to alter the predictors. Typical alterations are sqrt(x), 1/x, log(x), and n^x. In this situation, we already know how the data are nonlinear, so we will be able to easily apply the appropriate transformation. In a \"real-world\" situation, it may take trial and error to come up with an appropriate transformation to make the predictor appear more linear. If none of the transformations work, you may have to consider not using the predictor, or switching to a nonlinear model.  library(car) lm_fit crPlots(lm_fit) The above code modifies the linear model by using the square root of x2 as a predictor, and e^x3 as a predictor, which cancel out the squared transformation on x2 and the natural log transformation on x3, respectively. Your plot should look like this:   As you can see, this plot shows a much more linear relationship between x2 and y and x3 and y. However, the component and residual lines for the predictors x1, x2, and x3 do not show a perfect overlap. We will look more into this in a later post."], "link": "http://viksalgorithms.blogspot.com/feeds/5452396106797238718/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://www.duke.edu/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["I was searching for open data recently, and stumbled on Socrata . Socrata has a lot of interesting data sets, and while I was browsing around, I found a data set on federal bailout recipients. Here is the data set. However, data sets on Socrata are not always the most recent versions, so I followed a link to the data source at Propublica , where I was able to find a data set that was last updated on January 17, 2012. I downloaded the data in csv format. In the rest of this post, I will perform basic analysis on this data, and show that R can be used to do the same analysis as Excel in a much simpler and more powerful way. The data contains several columns, the most salient of which are the name of the company, its location, its industry, the amount of bailout funds received, and the amount repaid. First, we read in the data: bailout<-read.csv(file=\"bailout.csv\",head=TRUE,sep=\",\", stringsAsFactors=FALSE,strip.white=TRUE) Next, we generate a variable called owed that calculates how much the company owes the government, or how much the government has made in profit from the company, and create a new data frame that is sorted based on how much is owed the government: bailout<-transform(bailout,owed=bailout$total_disbursed -bailout$total_revenue-bailout$total_payback) ordered_bailout<-bailout[with(bailout,order(-owed)),] Now, we can use ggplot2 to create a nice looking bar chart, and use dev.copy() to export it(you do not need to run the last two lines, as they are solely for exporting the chart): library(ggplot2) qplot(name,weight=owed/1e9, data = ordered_bailout[1:4,], geom = \"bar\", xlab = \"Institution Name\",ylab=\"Amount Owed in Billions\") dev.copy(png,\"most_owed.png\") dev.off() This results in this chart:  As you can see, Fannie Mae, Freddie Mac, GM, and AIG all still owe huge amounts of money to the government. Now, we look at the top companies that the government made a profit from: qplot(name,weight=abs(owed/1e9), data = tail(ordered_bailout,4), geom = \"bar\", xlab = \"Institution Name\", ylab=\"Government Profit in Billions\")  As we can see, banks have given the government a significant profit. When we look at the sum of the money owed to the government, we see that 242.81 billion dollars have yet to be repaid: sum(ordered_bailout$owed)/1e9 Using the tapply function reveals the following(each heading is a sector name, and the number below is how much they owe the government in billions): tapply(bailout$owed/1e9,bailout$category,sum)       Bank    FHA Refinance Fund      -11.36362217      0.05000000   SBA Security Purchases    State Housing Orgs      0.06002858      0.65537461    Mortgage Servicer  Financial Services Company      1.93507502      10.47640782     Investment Fund     Auto Company      13.32980009      28.53374026    Insurance Company Government-Sponsored Enterprise      48.36100458     150.77000000 As you can see, the government sponsored enterprises(Fannie Mae and Freddie Mac), and the insurance companies(primarily AIG), owe the most while the banks have made a profit for the government. I will not inject any analysis into this, as we are only looking at the numbers here. Now, we will look at how much is owed by the top 10 states: states qplot(tail(names(states),10),weight=tail(states,10))   Not surprisingly, considering that Fannie Mae and Freddie Mac are based in DC, DC owes the most, followed by Virginia. Puerto Rico is a very interesting addition to the top 10 states/territories that owe the government, and further inspection reveals this: bailout[bailout$state==\"PR\",]$name [1] \"Popular, Inc.\"    \"First BanCorp\"    [3] \"RG Mortgage Corporation\"  \"Scotiabank de Puerto Rico\" [5] \"Banco Popular de Puerto Rico\" Apparently the banking sector in Puerto Rico did not do well in the financial crisis!  I noticed an interesting column in the data called \"is_stress_tested\", which took on a false value, a true value, or a NULL value. I am not an expert on banking, and if someone can please shed more light on the stress test, I would appreciate it, but I believe that stress testing is a method that discovers how prone to failure the institution is. Now, when we see how much money the institutions that have had stress testing performed versus those that have not owe, we get the following: tapply(bailout$owed/1e9,bailout$is_stress_tested,sum)    false  true 13.31646 242.71883 -13.22748 This tells us that institutions that have not been stress tested owe 242 billion to the government, whereas those that have been stress tested have made a profit of 13.2 billion for the government. I am not sure if stress testing can be performed on an institution like Fannie Mae, but it seems odd that the testing has not been done on those institutions that collectively owe 242 billion dollars. Finally, looking at the percentage of disbursed funds that are still outstanding by sector reveals the following(each heading is a sector name, and the number below is the percentage of the bailout funds that they still owe): sort(tapply(bailout$owed,bailout$category,sum)/ tapply(bailout$total_disbursed,bailout$category,sum))       Bank   SBA Security Purchases      -0.04811175      0.16305670      Auto Company  Financial Services Company      0.46090662      0.46762480    Insurance Company Government-Sponsored Enterprise      0.66995920      0.82478118     Investment Fund    FHA Refinance Fund      0.84152859      1.00000000    Mortgage Servicer    State Housing Orgs      1.00000000      1.00000000 In general, it appears that Investment Funds, Government-Sponsored Enterprises, and Insurance Companies have been poor at repaying the money they were lent. The high values for Government Sponsored Enterprises and Insurance Companies are explained by the high outstanding amounts from Fannie Mae, Freddie Mac, and AIG, but the trend in Investment Funds in more interesting. Despite relatively small bailouts(and I emphasize the relatively, because 15.8 billion dollars were disbursed to Investment Funds), it appears that money has been extremely slow to come back to the Government, with 84% of the funds still outstanding. With that, I conclude this post. I hope that this has shown you how simple data analysis can be done in R in an quick and efficient manner, and I hope that I have also been able to do interesting analyses and draw interesting facts from this data."], "link": "http://viksalgorithms.blogspot.com/feeds/8388541914380065480/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://2.blogspot.com/": 1, "http://opendata.socrata.com/": 2, "http://projects.propublica.org/": 1, "http://1.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Introduction This post incorporates parts of yesterday's post about bagging. If you are unfamiliar with bagging, I suggest that you read it before continuing with this article. I would like to give a basic overview of ensemble learning. Ensemble learning involves combining multiple predictions derived by different techniques in order to create a stronger overall prediction. For example, the predictions of a random forest, a support vector machine, and a simple linear model may be combined to create a stronger final prediction set. The key to creating a powerful ensemble is model diversity. An ensemble with two techniques that are very similar in nature will perform more poorly than a more diverse model set.  Some ensemble learning techniques, such as Bayesian model combination and stacking, attempt to weight the models prior to combining them. I will save the discussion of how to use these techniques for another day, and will instead focus on simple model combination. Initial Setup We will begin with similar variables to those that I used yesterday in the introduction to bagging. However, I have altered x2 and x3 to introduce distinct nonlinear tendencies, in order to evaluate the performance of nonlinear learning techniques. set.seed(10) y x1 x2 x3 lm_fit summary(lm_fit) As you can see, the R-squared value is now .6658, indicating that the predictors have less of a linear correlation to the dependent variable than the predictors from the bagging intro yesterday. set.seed(10) all_data positions training testing lm_fit predictions error error Dividing the data into training and testing sets and using a simple linear model to make predictions about the testing set yields a root mean squared error of 177.36. library(foreach) length_divisor iterations predictions training_positions train_pos lm_fit predict(lm_fit,newdata=testing) } predictions error error Creating 5000 simple linear models and averaging the results creates a prediction error of 177.2591, which is marginally superior to the initial results.  Creating the First Ensemble To create our initial ensemble, we will use the linear model, and a random forest. A random forest can be a powerful learning technique. It creates multiple decision trees from randomized sets of predictors and observations. It will be less useful here, as we only have three predictors, but will still illustrate the general point. Now, we will test the efficacy of a random forest on our data: library(randomForest) rf_fit predictions error error My error value came to 134.98 with the above code. As you can see, random forests can make much better predictions than linear models. In this case, the linear model could not deal with the nonlinear predictors, whereas the random forest could. One should beware the trap of overfitting, however, as while random forests are much less prone to it than a single decision tree, it can still occur in very noisy data. Note that a random forest already incorporates the idea of bagging into the basic algorithm, so we will gain little to nothing by running a random forest through our bagging function. What we will do instead is this: length_divisor iterations predictions training_positions train_pos lm_fit predict(lm_fit,newdata=testing) } lm_predictions library(randomForest) rf_fit rf_predictions predictions error error This combines the results of the linear model and the random forest using equal weights. Not surprisingly, considering that the linear model is weaker than the random forest, we end up with an error of 147.97. While this is not a fantastic result, it does illustrate ensemble learning. Improving the Performance of Our Ensemble From here, we have two options. We can either combine the predictions of the linear model and the random forest in different ratios until we achieve a better result(I would do this with caution in the real world, and use one hold out set to find the optimal weights, and test the weightings on another hold out set, as this can lead to overfitting if done improperly), or we can replace the linear model with a better one. We will try both, but first we will try combining the results in different ratios. Our prior knowledge of the error rates tells us to use a small ratio for the linear model.  predictions error error This results in an error rate of 136.23, which is not an improvement on the random Forest alone. Next, we replace the linear model with a support vector machine(svm) from the e1071 package, which provides an R interface to libSVM. A support vector machine is based on some complicated mathematics, but is basically a stronger machine learning technique that can pick up nonlinear tendencies in the data, depending on what kind of kernel is used. library(e1071) svm_fit svm_predictions error error The error when using an svm is 129.87, which is superior to both the random forest and the linear models. Next we will try using the svm with our bagging function. length_divisor iterations predictions training_positions train_pos svm_fit predict(svm_fit,newdata=testing) } svm2_predictions error error The error with 5000 iterations of an svm model is 141.14. In this case, it appears that the svm performs better without bagging techniques. It may be that there is too little data for it to be effective when used like this. However, the time it takes an svm increases exponentially(I believe) with more observations, so sometimes various techniques, including reduction in the number of observations or features, will need to be performed to improve svm performance to tolerable levels. Going forward, we will use the results of the single svm for the rest of this article. predictions error error When we equally combine the svm predictions from the single model with the random forest predictions, we get an error rate of 128.8, which is superior to either the svm model alone, or the random forest model alone. predictions error error If we tweak the ratios to emphasize the stronger svm model, we lower our error to 128.34.  Conclusion As we have seen, ensemble learning can outperform any one single model when used properly. A good exercise to continue on from here would be to see if other models can improve performance when added to the ensemble. Please feel free to email me or leave a comment if you see something incorrect or have any questions."], "link": "http://viksalgorithms.blogspot.com/feeds/4026568941847141175/comments/default", "bloglinks": {}, "links": {"http://viksalgorithms.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Bagging, aka bootstrap aggregation, is a relatively simple way to increase the power of a predictive statistical model by taking multiple random samples(with replacement) from your training data set, and using each of these samples to construct a separate model and separate predictions for your test set. These predictions are then averaged to create a, hopefully more accurate, final prediction value.  One can quickly intuit that this technique will be more useful when the predictors are more unstable. In other words, if the random samples that you draw from your training set are very different, they will generally lead to very different sets of predictions. This greater variability will lead to a stronger final result. When the samples are extremely similar, all of the predictions derived from the samples will likewise be extremely similar, making bagging a bit superfluous.  Taking smaller samples from your training set will induce greater instability, but taking samples that are too small will result in useless models. Generally, some fraction of your training set between 1/50th and 1/2 will be useful for bagging purposes(of course, this greatly depends on how many observations are in your training set). The smaller your bagging samples, the more samples you will need to collect and the more models you will need to generate to create more stability in the final predictors.  While there are some libraries that will take care of bagging for you in R, writing your own function allows you a greater degree of control and understanding over the process, and it is a relatively quick exercise.  Okay, enough theoretical framework. Lets jump into the code. Anyone who wants more theory can consult this paper about bagging.  I'm going to construct a relatively trivial example where a dependent variable, y, can be predicted by some combination of the independent variables x1, x2, and x3.  set.seed(10) y x1 x2 x3 As you can see, y is a sequence of the values from 1 to 1000. x1, x2, and x3 are permutations of y, but with random errors added. runif generates a specified number of random numbers from 0 to 1, unless a min and max are specified, in which case the numbers fall between those values. Each of the x sequences will roughly approximate y, but with random errors thrown in. The set.seed function is simply to ensure that the subsequent random number generation proceeds in a predictable fashion, so that your results match mine.  Fitting a linear model to the variables results in an R squared of .7042: lm_fit summary(lm_fit) Now we will see how well the x values predict y. First, we designate a random sample of y to be our \"test\" set. The rest will be the training set. set.seed(10) all_data positions training testing The above code places all of our variables into a data frame, then randomly selects 3/4 of the data to be the training set, and places the rest into the testing set. We are now able to generate predictions for the testing set by creating a linear model on the training set and applying it to the testing set. We are also able to calculate the prediction error by subtracting the actual values from the predicted values (the error calculation here is root mean squared error): lm_fit predictions error The calculated error should be 161.15. The next step is to run a function that implements bagging. In order to do this, I will be using the foreach package. Although I will not use it in parallel mode, this code is designed for parallel execution, and I highly recommend reading my post about how to do it if you do not know how. library(foreach) length_divisor iterations predictions training_positions train_pos lm_fit predict(lm_fit,newdata=testing) } predictions error The above code randomly samples 1/4 of the training set in each iteration, and generates predictions for the testing set based the sample. It will execute the number of time specified by iterations. When iterations was set to 10, I received an error value of 161.10. At 300 iterations, error went to 161.12, at 500 iterations, error went to 161.19, at 1000 iterations, error went to 161.13, and at 5000 iterations, error went to 161.07. Eventually, bagging will converge, and more iterations will not help any further. However, the potential for improvement in results exists. You should be extremely cautious and assess the stability of the results before deploying this approach, however, as too few iterations or too large a length divisor can cause extremely unstable results. This example is trivial, but this can lead to better results in a more \"real-world\" application. Finally, we can place this code into a function to wrap it up nicely: bagging { predictions training_positions train_pos lm_fit predict(lm_fit,newdata=testing) } rowMeans(predictions) } As you can see, bagging can be a useful tool when used correctly. Although this is a trivial example, you can replace the data and even replace the simple linear model with more powerful models to make this function more useful."], "link": "http://viksalgorithms.blogspot.com/feeds/7265748392976191068/comments/default", "bloglinks": {}, "links": {"http://citeseerx.psu.edu/": 1, "http://viksalgorithms.blogspot.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["I recently posted an introduction to the Kaggle Algorithmic Trading Challenge , which I competed in. I said that I would post about my experiences, and this is hopefully the first of a series. We were given tick data from the London Stock Exchange(specifically, the FTSE 100) over random time intervals during parts of 37 days. Each data row that we were given corresponded to a liquidity shock event and the surrounding bid/ask prices. Given 50 bid prices and 50 ask prices prior to the liquidity shock event, we had to predict the next 50 bid and ask prices. I, and others, noticed that there were distinct areas of high volatility in the tick data at certain times of day. Specifically, 10:15, 13:30, and 15:00 (london time) all showed abnormal volatility.  The above chart plots the residuals of a simple linear model that predicts the average of the first five bid and ask prices after the liquidity shock event at t=50 against the time the trade was made. The x-axis(time the trade was made) is in the units of hours after 8:00, so a value of 2 on the x-axis corresponds to 10:00. The residuals of the linear fit indicate how \"hard\" the values of the bid-ask time series were to predict, which is a proxy for volatility. The below R code generated the plot: lm_fit<-lm(current_formula,data=cbind(extracted_training_dependent_variable, extracted_training_data)) plot(x=extracted_training_data$trade_time_50,y=lm_fit$residuals) This can also be observed when plotting the normalized standard deviation of the bid series against trade time, although the spike at 10:15 is harder to see:   Although the spike in volatility at 13:30 appears to be caused by the opening of the NASDAQ and NYSE, it is unclear what the other two spikes might represent, although other markets may be opening at those times. Specifically, the spikes appear to be caused by other markets/traders valuing the stocks differently, which leads to an opportunity for profit taking. Here is a typical series in which a spike occurs:   In this series, a liquidity shock occurs when x=50(although there are several shocks prior to this). The whole sequence from x=1 to x=50 takes place over 13 seconds, and each x value is a distinct trade or quote event. The red points are ask prices, and the blue points are bid prices. As you can see, there appears to be a significant arbitrage opportunity over a fairly long time scale as the ask prices lower, perhaps in response to the volatile bid prices(which may be a result of traders from other areas eating up the available liquidity on the bid side), then rise again after the shock at t=50. This same pattern exists in other tick data time series that were given to competitors in this challenge, which could make for potentially interesting arbitrage opportunities if it is able to be exploited. Another interesting feature of the tick data when plotted over time is the fact that there is much higher volatility earlier in the day. This corresponds with several papers that noted the same phenomenon. When training a predictive model on tick data, a significant portion of the days error(30-40%) can occur in the first hour of trading. This points to the need for specialized models to be developed for the first hour of trading, but it also points to the fact that the first hour, is, in some ways, relatively unpredictable. A model that does not attempt to make predictions in the first hour may make significantly more \"correct\" guesses and thus significantly more profit than one that does not. The below plot is an easier way to look at this. It uses the tapply function to find the mean normalized residual for each one-minute time slice in the trading day:  As you can see, there is more than 4 times as much prediction error(remember, a proxy for volatility) in the first minute as there is in the last. I will wrap this post up at this point. I will post more of my findings soon."], "link": "http://viksalgorithms.blogspot.com/feeds/4707394308354424625/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://viksalgorithms.blogspot.com/": 1, "http://3.blogspot.com/": 1, "http://www.kaggle.com/": 1, "http://1.blogspot.com/": 2}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Parallel computation may seem difficult to implement and a pain to use, but it is actually quite simple to use. The foreach package provides the basic loop structure, which can utilize various parallel backends to execute the loop in parallel. First, let's go over the basic structure of a foreach loop. To get the foreach package, run the following command: install.packages(\"foreach\") Then, initialize the library: library(foreach) A basic, nonparallel, foreach loop looks like this: foreach(i=1:10) %do% { #loop contents here } To execute the loop in parallel, the %do% command must be replaced with %dopar%: foreach(i=1:10) %dopar% { #loop contents here } To capture the return values of the loop: list<-foreach(i=1:10) %do% { i } Note that the foreach loop returns a list of values by default. The foreach package will always return a result with the items in the same order as the counter, even when running in parallel. For example, the above loop will return a list with indices 1 through 10, each containing the same value as their index(1 to 10). In order to return the results as a matrix, you will need to alter the .combine behavior of the foreach loop. This is done in the following code: matrix<-foreach(i=1:10,.combine=cbind) %do% { i } This will return a matrix with 10 columns, with values in order from 1 to 10. Likewise, this will return a matrix with 10 rows: matrix<-foreach(i=1:10,.combine=rbind) %do% { i } This can be done with multiple return values to create n x k matrices. For example, this will return a 10 x 2 matrix: matrix<-foreach(i=1:10,.combine=rbind) %do% { c(i,i) } Parallel Backends In order to run the foreach loop in parallel(using the %dopar% command), you will need to install and register a parallel backend. Because windows does not support forking, the same backend that works a linux or an OS X environment will not work for windows. Under linux, the doMC package provides a convenient parallel backend. Here is how to use the package(of course, you need to install doMC first): library(foreach) library(doMC) registerDoMC(2) #change the 2 to your number of CPU cores foreach(i=1:10) %dopar% { #loop contents here } Under windows, the doSNOW package is very convenient, although it has some issues. I do not recommend the doSMP package, as it has significant issues. library(doSNOW) library(foreach) cl<-makeCluster(2) #change the 2 to your number of CPU cores registerDoSNOW(cl) foreach(i=1:10) %dopar% { #loop contents here } Edit: Thanks to an alert reader, I noticed that I neglected to add in the code to stop the clusters. This will need to be run after you finish executing all of your parallel code if you are using doSNOW. stopCluster(cl) Also please note that you will need to set the parameter in the makeCluster and registerDoMC functions to the number of CPU cores that your computer possesses, or less if you do not want to use all of your CPU cores. I hope that this has been a good introduction to parallel loops in R. The new version of R(2.14), also includes the parallel package, which I will discuss further in a later post. You can find more information on the packages mentioned in this article on CRAN. Foreach , doSNOW , and doMC can all be found there."], "link": "http://viksalgorithms.blogspot.com/feeds/2026240887464657682/comments/default", "bloglinks": {}, "links": {"http://cran.r-project.org/": 3}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["There are an abundance of resources on the web to help novices teach themselves statistics and machine learning, but they can be hard to track down. Below are a few resources that have helped me in the past. 1. Khan Academy has a series of excellent videos on the basics of statistics and probability. They range from 5 minutes to 20 minutes in length. You can create an account to track your progress if you desire. Although good for the basics, Khan Academy does not cover advanced topics. 2. The Elements of Statistical Learning , a free ebook, is a relatively in-depth overview of the major concepts and techniques involved in machine learning. You will need to learn inferential statistics before reading this book, as it assumes that the reader understands the basics.  3. The Statsoft Online Statistics Textbook is a good resource on statistics and machine learning. A reader will need a basic understanding of statistics and probability before reading this text. It is not particularly in depth, but is good for an overview of major concepts, or as a refresher. 4. Machine Learning Videos from mathematicalmonk can be found on youtube(the link is to the full playlist). Although I have not viewed all of these videos personally, they appear to be a good overview of machine learning techniques. 5. Andrew Ng's Online Machine Learning Class is a simplified version of the Stanford class CS229. It glosses over most of the mathematics involved, but is a very good introductory resource for machine learning. Mlclass also features quizzes and programming exercises that create additional engagement. I would recommend that the reader study basic statistics before attempting this class. 6. Concepts and Applications of Inferential Statistics is written by a professor at Vassar, and is a good introduction to basic statistics. 7. Online Stat Book is a good resource for basic statistical concepts. It has lots of interactive exercises interspersed throughout the text. 8. Introduction to Statistical Thought covers basic probability and statistics, and covers some more advanced topics such as time series and survival analysis. It also has R code that the reader can implement. 9. Linear Algebra , a free to download ebook, provides an overview of linear algebra equivalent to an initial undergraduate course. I have not read through it yet. 10. MIT Opencourseware is an excellent site that has full video lecture series and problem sets for hundreds of classes including linear algebra, calculus, and statistics.  Further Reading  This is an interesting blog post on how to learn mathematics as a programmer."], "link": "http://viksalgorithms.blogspot.com/feeds/218547080807202456/comments/default", "bloglinks": {}, "links": {"http://www.umass.edu/": 1, "http://ocw.mit.edu/": 1, "http://www.ml-class.org/": 1, "http://joshua.smcvt.edu/": 1, "http://www.khanacademy.org/": 1, "http://www.youtube.com/": 1, "http://www-stat.stanford.edu/": 1, "http://steve-yegge.blogspot.com/": 1, "http://faculty.vassar.edu/": 1, "http://www.statsoft.com/": 1, "http://onlinestatbook.com/": 1}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["Cointegration can be a valuable tool in determining the mean reverting properties of 2 time series. A full description of cointegration can be found on Wikipedia . Essentially, it seeks to find stationary linear combinations of the two vectors. The below R code, which has been modified from here , will test two series for integration and return the p-value indicating the likelihood of correlation. It runs significantly faster than the original code, however. I used this for relatively short time series(50 observations), and while it functioned relatively quickly for small numbers of series, it became cumbersome to use when attempting to serially cointegrate over 100k pairs of bid-ask price series when using it with an mapply function. So scaling up may be an issue. library(tseries) cointegration { vals beta (adf.test(vals[,2]-beta*vals[,1], alternative=\"stationary\", k=0))$p.value } This runs an augmented Dickey-Fuller test and will return a p-value indicating whether the series are mean-reverting or not. You can use the typical p-value as a test of significance if you like(ie, a p-value below .05 indicates a mean-reverting spread), or you can use an alternate value. This assumes that your two series were observed at the same time points. The original post that this code as modified from contains a further description of cointegration, along with more time series data type handling."], "link": "http://viksalgorithms.blogspot.com/feeds/4281820141342452496/comments/default", "bloglinks": {}, "links": {"http://quanttrader.info/": 2, "http://en.wikipedia.org/": 2}, "blogtitle": "R, Ruby, and Finance"}, {"content": ["I recently needed to play music in Ruby in order to create an alarm clock of sorts. The code to do this was (surprise surprise) fairly simple, but I want to post it for posterity. require 'win32ole' player = WIN32OLE.new('WMPlayer.OCX') player.OpenPlayer('C:\\alarm.mp3') This will open a new instance of Windows Media Player that plays the selected song. Note that win32ole should be installed by default, and does not need to be installed as a gem."], "link": "http://viksalgorithms.blogspot.com/feeds/8113942018418819016/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "R, Ruby, and Finance"}]