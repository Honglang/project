[{"blogurl": "http://blog.rguha.net\n", "blogroll": [], "title": "So much to do, so little time"}, {"content": ["I\u2019ve just pushed a new version of the  fingerprint package that contains an update provided by Abhik Seal that significantly speeds up calculation of pairwise similarity matrices when using the Dice similarity method. A ran a simple comparison using different numbers of random fingerprints (1024 bits, with 512 bits set to one, randomly) and measured the time to evaluate the pairwise similarity matrix. As you can see from the figure alongside, the new code is significantly faster (with speed ups of 450x to 500x). The code to generate the timings is below \u2013 it probably should wrapped in a loop to multiple times for each set size. \n  1 2 3 4 5  fpls <- lapply ( seq ( 10 , 300 , by = 10 ) , \n    function ( i ) sapply ( 1 : i, \n         function ( x ) random. fingerprint ( 1024 , 512 ) ) ) \ntimes <- sapply ( fpls, \n     function ( fpl ) system. time ( fp. sim . matrix ( fpl, method = 'dice' ) ) [ 3 ] )"], "link": "http://blog.rguha.net/?p=1068", "bloglinks": {}, "links": {"http://blog.rguha.net/": 1, "http://en.wikipedia.org/": 1, "http://mypage.iu.edu/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "So much to do, so little time"}, {"content": ["I\u2019m pleased to announce that the ACS Division of Chemical Information will be hosting a series of free webinars on topics related to chemical information. The webinars will be open to everybody and our first speaker in this series will be Dr. Alex Clark, who\u2019ll be talking about cheminformatics workflows and mobile applications. More details below or at http://www.acscinf.org/about/news/20120918.php \n Webinar : Practical cheminformatics workflows with mobile apps \n Date : October 3, 2012 \n Time : 11 am Eastern time (US) \n View the Webinar : \u00a0 http://acspubs2.adobeconnect.com/cinfwebinar-oct2012 \n Abstract \n In recent years smartphones and tablets have attained sufficient power and sophistication to replace conventional desktop and laptop computers for many tasks. Chemistry software is late to the party, but rapidly catching up. This webinar will explore some of the cheminformatics functionality that can currently be performed using mobile apps. A number of workflow scenarios will be discussed, such as: creating and maintaining chemical data (molecules, reactions, numbers & text); searching chemical databases and utilising the results; structure-aware lab notebooks; visualisation and structure-activity analysis; property calculation using remote webservices; and a multitude of ways to share data collaboratively, and integrate modular apps within distributed and heterogeneous workflows. \n Speaker \n Alex M. Clark graduated from the University of Auckland, New Zealand, with a Ph.D. in synthetic organometallic chemistry, then went on to work in computational chemistry. His chemistry background spans both the lab bench and development of software for a broad variety of 2D and 3D computer aided molecular design algorithms and user interfaces. He is the founder of Molecular Materials Informatics, Inc. , which is dedicated to producing next-generation cheminformatics software for emerging platforms such as mobile devices and cloud computing environments."], "link": "http://blog.rguha.net/?p=1062", "bloglinks": {}, "links": {"http://www.acscinf.org/": 2, "http://molmatinf.com/": 1, "http://acspubs2.adobeconnect.com/": 1}, "blogtitle": "So much to do, so little time"}, {"content": ["While contributing to a book chapter on high content screening I came across the problem of characterizing screen quality. In a traditional assay development scenario the Z factor (or Z\u2019) is used as one of the measures of assay performance (using the positive and negative control samples). The definition of Z\u2019 is based on a 1-D readout, which is the case with most non-high content screens. But what happens when we have to deal with 10 or 20 readouts, which can commonly occur in a high content screen? \n Assuming one has identified a small set of biologically relevant phenotypic parameters (from the tens or hundreds spit out by HCA software), it makes sense that one measure the assay performance in terms of the overall biology, rather than one specific aspect of the biology. In other words, a useful performance measure should be able to take into account multiple (preferably orthogonal) readouts. In fact, in many high content screening assays, the use of the traditional Z\u2019 with a single readout leads to very low values suggesting a poor quality assay, when in fact, that is not the case if one were to consider the overall biology. \n One approach that has been described in the literature is an extension of the Z\u2019, termed the multivariate Z\u2019. The approach was first described by Kummel et al , which develops an LDA model, trained on the positive and negative wells. Each well is described by N phenotypic parameters and the assumption is that one has pre-selected these parameters to be meaningful and relevant. The key to using the model for a Z\u2019 calculation is to replace the N-dimensional values for a given well by the 1-dimensional linear projection of that well: \n \n where is the 1-D projected value, is the weight for the \u2019th pheontypic parameter and is the value of the \u2019th parameter for the \u2019th well. \n The projected value is then used in the Z\u2019 calculation as usual. Kummel et al showed that this approach leads to better (i.e., higher) Z\u2019 values compared to the use of univariate Z\u2019. Subsequently, Kozak & Csucs extended this approach and used a kernel method to project the N-dimensional well values in a non-linear manner. Unsurprisingly, they show a better Z\u2019 than what would be obtained via a linear projection. \n And this is where I have my beef with these methods. In fact, a number of beefs: \n \n These methods are model based and so can suffer from over-fitting. No checks were made and if over-fitting were to occur one would obtain a falsely optimistic Z\u2019 \n These methods assert success when they perform better than a univariate Z\u2019 or when a non-linear projection does better than a linear projection. But neither comparison is a true indication that they have captured the assay performance in an absolute sense. In other words, what is the \u201cground truth\u201d that one should be aiming for, when developing multivariate Z\u2019 methods? Given that the upper bound of Z\u2019 is 1.0, one can imagine developing methods that give you increasing Z\u2019 values \u2013 but does a method that gives Z\u2019 close to 1 really mean a better assay? \u00a0It seems that published efforts are measured relative to other implementations and not necessarily to an actual assay quality (however that is characterized). \n While the fundamental idea of separation of positive and negative control reponses as a measure of assay performance is good, methods that are based on learning this separation are at risk of generating overly optimistic assesments of performance. \n \n A counter-example \n As an example, I looked at a recent high content siRNA screen we ran that had 104 parameters associated with it. The first figure shows the Z\u2019 calculated using each layer individually (excluding layers with abnormally low Z\u2019) \n As you can see, the highest Z\u2019 is about 0.2. After removing those with no variation and members of correlated pairs I ended up with a set of 15 phenotypic parameters. If we compare the per-parameter distributions of the positive and negative control responses, we see very poor separation in all layers but one, as shown in the density plots below (the scales are all independent) \n  \n I then used these 15 parameters to build an LDA model and obtain a multivariate Z\u2019 as described by Kummel et al . Now, the multivariate Z\u2019 turns out to be 0.68, suggesting a well performing assay. I also performed MDS on the 15 parameter set to get lower dimensional (3D, 4D, 5D, 6D etc) datasets and performed the same calculation, leading to similar Z\u2019 values (0.41 \u2013 0.58) \n But in fact, from the biological point of view, the assay performance was quite poor due to poor performance of the positive control (we haven\u2019t found a good one yet). In practice then, the model based multivariate Z\u2019 (at least as described by Kummel et al can be misleading. One could argue that I had not chosen an appropriate set of phenotypic parameters \u2013 but I checkout a variety of other subsets (though not exhaustively) and I got similar Z\u2019 values. \n Alternatives \n Of course, it\u2019s easy to complain and while I haven\u2019t worked out a rigorous alternative, the idea of describing the distance between multivariate distributions as a measure of assay performance (as opposed to learning the separation) allows us to attack the problem in a variety of ways. There is a nice discussion on StackExchange regarding this exact question. Some possibilities include \n \n Bhattacharya distance \n Mahalanobis distance \n Mantel test (though this is really a measure of correlation than a measure of effect size) \n The cross match test by Paul Rosenbaum (with a handy R package ) \u2013 though this is more a measure of whether two distributions are different or not, rather than a distance between distributions \n An approach described by Loudin & Miettinen based on kernel density estimates and a 1-D Kolmogorov Smirnov test \n \n \n It might be useful to perform a more comprehensive investigation of these methods as a way to measure assay performance"], "link": "http://blog.rguha.net/?p=1036", "bloglinks": {}, "links": {"http://www.landesbioscience.com/": 1, "http://blog.rguha.net/": 2, "http://www.stanford.edu/": 1, "http://stats.stackexchange.com/": 1, "http://en.wikipedia.org/": 7, "http://www-stat.upenn.edu/": 2, "http://cran.r-project.org/": 1, "http://dx.doi.org/": 4}, "blogtitle": "So much to do, so little time"}, {"content": ["While at the ACS National Meeting in Philadelphia I attended a talk by David Thompson of Boehringer Ingelheim (BI), where he spoke about a recent competition BI sponsored on Kaggle \u2013 a web site that hosts data mining competitions. In this instance, BI provided a dataset that contained only object identifiers and about 1700 numerical features and a binary dependent variable. The contest was open to anybody and who ever got the best classification model (as measured by log loss ) was selected as the winner. You can read more about the details of the competition and also on Davids\u2019 slides . \n But I\u2019m curious about the utility of such a competition. During the competition, all contestents had access to were the numerical features. So the contestants had no idea of the domain from where the data came \u2013 placing the onus on pure modeling ability and no need for domain knowledge. But in fact the dataset provided to them, as announced by David at the ACS, was the Hansen AMES mutagenicity dataset characterized using a collection of 2D descriptors (continuous topological descriptors as well as binary fingerprints). \n BI included some \u201cdefault\u201d models and the winning models certainly performed better (10% for the winning model). This is not surprising, as they did not attempt build optimized models. But then we also see that the top 5 models differed only incrementally in their log loss values. Thus any one of the top 3 or 4 models could be regarded as a winner in terms of actual predictions. \n What I\u2019d really like to know is how well such an approach leads to better chemistry or biology. First, it\u2019s clear that such an approach leads to the optimization of pure predictive performance and cannot provide insight into why the model makes an active or inactive call. In many scenario\u2019s this is sufficient, but more often than not, domain specific diagnostics are invaluable. Second, how does the relative increase in model performance lead to better decision making? Granted, the crowd-sourced, gamified approach is a nice way to eke out the last bits of predictive performance on a dataset \u2013 but does it really matter that one model performs 1% better than the next best model? The fact that the winning model was 10% better than the \u201cdefault\u201d BI model is not too informative. So a specific qustion I have is, was there a benefit, in terms of model performance, and downstream decision making by asking the crowd for a better model, compared to what BI had developed using (implicit or explicit) chemical knowledge? \n My motivation is to try and understand whether the winning model was an incremental improvement or whether it was a significant jump, not just in terms of numerical performance, but in terms of the predicted chemistry/biology. People have been making noises of how data trumps knowledge (or rather hypotheses and models) and I believe that in some cases this can be true. But I also wonder to what extent this holds for chemical data mining. \n But it\u2019s equally important to understand what such a model is to be used for. In a virtual screening scenario, one could probably ignore interpretability and go for pure predictive performance. In such cases, for increasingly large libraries, it might make sense for one to have a model that s 1% better than the state of the art. (In fact, there was a very interesting talk by Nigel Duffy of Numerate, where he spoke about a closed form, analytical expression for the hit rate in a virtual screen, which indicates that for improvements in the overall performance of a VS workflow, the best investment is to increase the accuracy of the predictive model. Indeed, his results seem to indicate that even incremental improvements in model accuracy lead to a decent boost to the hit rate). \n I want to stress that I\u2019m not claiming that BI (or any other organization involved in this type of activity) has the absolute best models and that nobody can do better. I firmly believe that however good you are at something, there\u2019s likely to be someone better at it (after all, there are 6 billion people in the world). But I\u2019d also like to know how and whether incrementally better models do when put to the test of real, prospective predictions."], "link": "http://blog.rguha.net/?p=1032", "bloglinks": {}, "links": {"http://www.linkedin.com/": 2, "http://www.slideshare.net/": 1, "http://www.wired.com/": 1, "http://blog.rguha.net/": 1, "http://us.boehringer-ingelheim.com/": 1, "http://lingpipe-blog.com/": 1, "http://dx.doi.org/": 1}, "blogtitle": "So much to do, so little time"}, {"content": ["A common task for is to run database queries on gene symbols or compound identifiers. This involves constructing an SQL query as a string and sending that off to the database. In the case of the ROracle package, the query strings are limited to a 1000 (?) or so characters. This means that directly querying for a thousand identifiers won\u2019t work. And going through the list of identifiers one at a time is inefficient. What we need in this situation is a to \u201cchunk\u201d the list (or vector) of identifiers and work on individual chunks. With the help of the itertools package, this is very easy: \n  1 2 3 4 5 6 7 8  library ( itertools ) \nn <- 1 : 11 \nchunk. size <- 3 \nit <- ihasNext ( ichunk ( n, chunk. size ) ) \n while ( itertools :: hasNext ( it ) ) { \n achunk <- unlist ( nextElem ( it ) ) \n print ( achunk ) \n }"], "link": "http://blog.rguha.net/?p=1030", "bloglinks": {}, "links": {"http://cran.r-project.org/": 2}, "blogtitle": "So much to do, so little time"}, {"content": ["A few days back, Derek Lowe posted a comment from a reader who suggested a way to approach the current employment challenges in the pharmaceutical industry would be the formation of a Federation of Independent Scientists . Such a federation would be open to consultants, small companies etc and would use its size to obtain group rates on various things \u2013 journal access, health insurance and so on. Obviously, there\u2019s a lot of details left out here and when you go in the nitty gritty a lot of issues arise that don\u2019t have simple answers. Nevertheless, an interesting (and welcome, as evidenced by the comment thread) idea. \n One aspect raised by a commenter was access to modeling and docking software by such a group. He mentioned that he\u2019d \n \u2026 like to see an open source initiative develop a free, open source drug discovery package.Why not, all the underlying force fields and QM models have been published \u2026 it would just take a team of dedicated programmers and computational chemists time and passion to create it. \n This is the very essence of the Blue Obelisk movement, under whose umbrella there is now a wide variety of computation chemistry and cheminformatics software. There\u2019s certainly no lack of passion in the Open Source chemistry software community. As most of it is based on volunteer effort, time is always an issue. This has a direct effect on the features provided by Open Source chemistry software \u2013 such software does not always match up to commercial tools. But as the commenter above pointed out, much of the algorithms underlying proprietrary software is published. It just needs somebody with the time and expertise to implement them. And the combination of these two (in the absence of funding) is not always easy to find. \n Of course, having access to the software is just one step. A scientists requires (possibly significant) hardware resources to run the software. Another comment raised this issue and asked about the possibility of a cloud based install of comp chem software. \n \n With regards the sophisticated modelling tools \u2013 do they have to be locally installed? \n How do the big pharma companies deploy the software now? I would be very suprised if it wasn\u2019t easily packaged, although I guess the number of people using it is limited. \n I\u2019m thinking of some kind of virtual server, or remote desktop style operation. Your individual contractor can connect from whereever, and have full access to a range of tools, then transfer their data back to their own location for safekeeping. \n \n Unlike CloudBioLinux , which provides a collection of bioinformatics and structural biology software as a prepackaged AMI for Amazons EC2 platform, I\u2019m not aware of a similarly prepackaged set of Open Source tools for chemistry. And certainly not based on the cloud. (There are some companies that host comp chem software on the cloud and provide access to these installations for a fee). While some Linux distribibutions do package a number of scientific packages ( UbuntuScience for example), I don\u2019t think that these would support a computational drug discovery operation. (The above comment does\u2019nt necessarily focus just on Open Source software. One could consider commercial software hosted on remote servers, though I wonder what type of licensing would be involved). \n The last component would be the issue of data, primarily for cloud based solutions. While compute cycles on such platforms are usually cheap, bandwidth can be expensive. Granted, chemical data is not as big as biological data (cf. 1000Genomes on AWS ), but sending a large collection of conformers over the network may not be very cost-effective. One way to bypass this would be to generate \u201cstandard\u201d conformer collections and other such libraries and host them on the cloud. But what is \u201cstandard\u201d and who would pay for hosting costs is an open question. \n But I do think there is a sufficiently rich ecosystem of Open Source software that could serve much of the computational needs of a \u201cFederation of Independent Scientists\u201d. It\u2019d be interesting to put together a list of Open Source based on requirements from the the commenters in that thread ."], "link": "http://blog.rguha.net/?p=1026", "bloglinks": {}, "links": {"http://sourceforge.net/": 1, "http://aws.amazon.com/": 2, "https://aws.amazon.com/": 1, "http://pipeline.corante.com/": 5, "https://help.ubuntu.com/": 1, "http://cloudbiolinux.org/": 1, "http://www.hbarsol.com/": 1}, "blogtitle": "So much to do, so little time"}, {"content": ["Gamification is a hot topic and companies such as Tunedit and Kaggle are succesfully hosting a variety of data mining competitions. These competitions employ data from a variety of domains such as bond trading , essay scoring and so on. Recently, both platforms have hosted a QSAR challenge (though not officially denoted as such). The most recent one is the challenge hosted at Kaggle by Boehringer Ingelheim. \n While it\u2019s good to see these competitions raise the profile of \u201cdata science\u201d (and make some money for the winners), I must admit that these are not particularly interesting to me as it really boils down to looking at numbers with no context (aka domain knowledge). For example, in the Kaggle & BI example, there are 1,776 descriptors that have been normalized but no indication of the chemistry or biology. One could ask whether a certain mechanism of action is known to play a role in the biology being tested which could suggest a certain class of descriptors over another. Alternatively, one could ask whether there are a few distinct chemotypes present thus suggesting multiple local models versus a single global model. (I suppose that the supplied descriptors may lend themselves to a clustering, but a scaffold based approach would be much more direct and chemically intuitive). \n This is not to say that such competitions are useless. On the contrary, lack of domain knowledge doesn\u2019t preclude one from apply sophisticated statistical and machine learning methods to unannotated data and obtaining impressive results. The issue of data versus domain knowledge has been discussed in several places . \n In contrast to the currently hosted challenge at Kaggle, an interesting twist would be to try and reverse engineer the structures from their descriptor values. There have been some previous discussions on reverse engineering structures from descriptor data. Obviously, we\u2019re not going to be able to verify our results, but it would be an interesting challenge."], "link": "http://blog.rguha.net/?p=1024", "bloglinks": {}, "links": {"http://tunedit.org/": 1, "http://www.kdnuggets.com/": 1, "http://www.cloudave.com/": 1, "http://pubs.acs.org/": 1, "http://en.wikipedia.org/": 1, "http://www.kaggle.com/": 4, "http://www.springerlink.com/": 1, "http://www.jcheminf.com/": 1}, "blogtitle": "So much to do, so little time"}, {"content": ["Another ACS National meeting is over, this time in San Diego. It was\u00a0good to catch up with old friends and meet many new, interesting people. As I was there for a relatively short period, I bounced around most sessions. \n MEDI and COMP had a joint session on desktop modeling and its utility in medicinal chemistry. Anthony Nicholls gave an excellent talk, where he differentiated between \u201cstrong signals\u201d and \u201cweak signals\u201d, the former being extremely obvious trends, features or facts that do not require a high degree of specialized exerptise to detect and the latter being those that do require significantly more expertise to identify. An example of a strong signal would be an empty region of a binding pocket that is not occupied by a ligand feature \u2013 it\u2019s pretty easy to spot this and when hihglighted the possible actions are also obvious. A weak signal could be a pi-stacking interaction which could be difficult to identify in a crowded 3D diagram. He then highlighted how simple modifications to traditional 2D depictions can be used to make the obvious more obvious and make features that might be subtle, say in 3D, more obvious in a 2D depiction. Overall, an elegant talk, that focused on how simple visual cues in 2D & pseudo-3D depictions can key the mind to focus on important elements. \n There were two\u00a0other symposia that\u00a0were of particular interest. On\u00a0Sunday Shuxing\u00a0Zhang and Sean Eakins organized a symposium on polypharmacology with an excellent line up of\u00a0speakers\u00a0including Chris\u00a0Lipinski . Curt\u00a0Breneman gave a nice talk that highlighted best practices in\u00a0QSAR modeling and Marti Head gave a great talk on the role and value\u00a0of docking in computational modeling projects. \n On Tuesday, Jan Kuras\u00a0and Tudor Oprea organized a session\u00a0on System\u00a0Chemical Biology . Though the session appeared to be more on the\u00a0lines of drug repurposing, there were several interesting\u00a0talks. Ebelebola\u00a0May from Sandia Labs gave a very interesting talk on a system\u00a0level model of small molecule inhibition of M. Tuberculosis and F. Tularensis -\u00a0combining metabolic pathway models and cheminformatics. \n John Overington gave a very interesting talk on identifying drug combinations to improve safety. Contrary to much of my reading in this area, he points out the value of \u201cme-too\u201d drugs and taking combinations of such drugs. Given that such drugs hit the same target, he pointed out that this results in the fact that off-targets will see reduced concentrations of the individual drugs (hopefully reducing side effects) while the on-target will see the pooled concentration (thus maintaining efficacy (?)). It\u2019s definitely a contrasting view to the one where we identify combinations of drugs hitting different targets (which I\u2019d guess is a tougher proposition, since identifying a truly synergistic combination requires a detailed knowledge of the underlying pathways and interactions). He also pointed out that his analyses indicated that combination dosing is not actually reduced, in contrast to the current dogma. \n As before we had a CINFlash session which I think went quite well \u2013 8\u00a0diverse speakers with a pretty good audience. The slides of the talks\u00a0have been made available and we plan to have another session in Philadelphia this Fall, so consider submitting something. We also had a great Scholarships for Scientific Excellence poster session \u2013 15 posters covering topics ranging from reaction prediction to an analysis of retractions. Excellent work, and very encouraging to see newcomers to CINF interested in getting more invovled. \n The only downsides to the meeting was the chilly and unsunny weather and the fact that people still think that displaying tables of numbers in a slide actually transmits any information!"], "link": "http://blog.rguha.net/?p=1020", "bloglinks": {}, "links": {"http://chembl.blogspot.com/": 1, "http://www.linkedin.com/": 1, "http://faculty.mdanderson.org/": 1, "http://www.collabchem.com/": 1, "http://www.nature.com/": 1, "http://www.eyesopen.com/": 1, "http://bio.sandia.gov/": 1, "http://acscinf.org/": 2, "http://www.rpi.edu/": 1, "http://bulletin.acscinf.org/": 1}, "blogtitle": "So much to do, so little time"}, {"content": ["For some time I have been thinking of the analogy between linguistics (and text mining of language data) and chemistry, specifically from the point of view of fragments (though, the relationship between the two fields is actually quite long and deep, since many techniques from IR have been employed in cheminformatics). For example, atoms and bonds can be considered an \u201calphabet\u201d for chemical structures. Going one level up, one can consider fragments as words, which can be joined together to form larger structures (with the linguistic analog being sentences). In a\u00a0 talk I gave at the ACS sometime back I compared fragments with n-grams (though LINGO \u2019s are probably a more direct analog). \n On these lines I have been playing with text mining and modeling tools in R, mainly via the excellent tm package. One of the techniques I have been playing around with is Latent Dirichlet Allocation . This is a generative modeling approach, allowing one to associate a document (composed of a set of words) with a \u201ctopic\u201d. Here, a topic is a group of words that have a higher probability of being generated from that topic than another topic. The technique assumes that a document is comprised of a mixture of topics \u2013 as a result, one can assign a document to different topics with different probabilities. There have been a number of applications of LDA in bioinformatics with some applications focusing on topic models as way\u00a0to cluster objects such as genes [ 1 , 2 ], whereas others have used it in the more traditional document grouping context [ 3 ]. \n In text mining scenario, developing an LDA model for a set of documents is relatively straightforward (in R) \u2013 perform a series of pre-processing steps (mainly to standardize the text) such as\u00a0converting everything to lower case, removing stopwords and so on. At the end of this one has a series of documents, each one being represented as a bag of words. The collection of words across all documents can be converted to a document-term matrix (documents in the rows, words in the columns) which is then used as input to the LDA routine. \n Those familiar with building predictive models with keyed fingerprints will find this quite familiar \u2013 the individual bit positions represent structural fragments, thus are the chemical analogs of words. Based on this observation I wondered what I would get (and what it would mean) by applying a technique like LDA to a collection structures and their fragments. \n My initial thought is that the use of LDA to determine a set of topics for a collection of chemical structures is essentially a clustering of the molecules, with the terms associated with the topics being representative substructures for that \u201ccluster\u201d. With these topics in hand, it wil be interesting to see what (or whether) properties (physical, chemical , biological) may be correlated with the clusters/topics identified. The rest of this post describes a quick first look at this, using ChEMBL as the source of structures and R for performing pre-processing and modeling. \n Structures & fragments \n We had previously fragmented ChEMBL (v8) in house, so obtaining the data was just a matter of running an SQL query to identify all fragments that occured in 50 or molecules and retrieving their structures and the molecules they were associated with. This gives us 190,252 molecules covered by 6,110 fragments. While a traditional text document-based modeling project would involved a series of pre-processing steps, the only one I need to perform in this scenario is the removal of small (and thus likely very common) fragments such as benzene \u2013 the cheminformatics equivalent of removing stopwords. (Ideally I would also remove fragments that already occur in other fragments \u2013 the cheminformatics equivalent of stemming ) \n The data file I have is of the form \n  1  fragment_id, molregno, smiles, natom  \n where natom is the number of atoms in the fragment. The R code to generate (relatively) clean data, read to feed to the LDA function looks like: \n  1 2 3 4 5 6 7  frags <- read. table ( 'chembl.data' , header = TRUE, as. is = TRUE, comment = '' , sep = ',' ) \n names ( frags ) <- c ( 'fid' , 'molid' , 'smiles' , 'natom' ) \nfrags <- subset ( frags, natom & gt ;= 8 ) \n ## now we create the \"documents\" \ntmp <- by ( frags, frags$molid, function ( x ) return ( c ( x$molid [ 1 ] , join ( x$smiles, ' ' ) ) ) ) \ntmp <- data. frame ( do. call ( 'rbind' , tmp ) , stringsAsFactors = FALSE ) \n names ( tmp ) <- c ( 'title' , 'text' )  \n In the code above, we rearrange the data to create \u201cdocuments\u201d \u2013 identified by a title (the molecule identifier) with the body of the document being the space concatenated SMILES for the fragments associated with that molecule. In other words, a molecule (document) is constructed from a set of fragments (words). With the data arranged in this form we can go ahead and reuse code from the tm and\u00a0 topicmodels packages. \n  1 2 3 4  ## Get a document-term matrix \n library ( tm ) \ncorpus <- Corpus ( VectorSource ( tmp$text ) ) \ndtm <- DocumentTermMatrix ( corpus, control = list ( tolower = FALSE ) )  \n Finally, we\u2019re ready to develop some models, starting of with 6 topics. \n  1 2 3  library ( topicmodels ) \nSEED <- 1234 \nlda. model <- LDA ( dtm, k = 6 , control = list ( seed = SEED ) )  \n So, what are the topics that have been identified? As I noted above, each topic is really a set of \u201cwords\u201d that have a higher probability of being generated by that topic. In the case of this model we obtain the following top 4 fragments associated with each topic (most likely fragments are at the top of the table): \n  \n Visual inspection clearly suggests distinct differences in the topics \u2013 topic 1 appears to be characterized primarily by the lack of aromaticity, whereas topic 2 appears to be characterized by quinoline and indole type structures. This is just a rough inspection of the most likely \u201cterms\u201d for each topic. It\u2019s also interesting to look at how the molecules (a.k.a., documents) are assigned to the topics. The barchart indicates the distribution of molecules amongst the 6 topics.  \n As with other unsupervised clustering methods, the choice of k (i.e., the number of topics) is tricky. A priori there is no reason to choose one over the other. Blei in his original paper used \u201cperplexity\u201d as a measure of the models generalizability (smaller values are better). In this case, we can vary k and evaluate the perplexity: with 6 topics the perplexity is 1122, with 12 topics it drops to 786 and with 100 topics it drops to 308 \u2013 you can see that it seems to continuously decrease with increase in number of topics (which has been observed elsewhere , though in my case, the hyperparameters are kept constant). Wallach et al have discussed various approaches to evaluating topic models. \n Numerical evaluation of these models is useful, but we\u2019re more interested in how these assignments correlate with chemical or biological features. First, one could look at the structural homogenity of the molecules assigned to topics. For k = 6, this is probably not useful, as the individual groups are very large. With k = 100 one obtains a much more sensible estimate of homogeneity (but this is to be expected). Another way to evaluate the topics from chemical point of view is to look at some property or activity. Given that ChEMBL provides assay and target information for the molecules, we have many ways to perform this evaluation. As a brief example, we can consider activity distrbutions derived from the molecules associated with each topic. Most ChEMBL molecules have multiple activities associated with them as many are tested in multiple assays. To allow comparison we converted activities in a given assay to Z-scores, allow comparison of activitives across assays. Then for each molecule, we identified the minimum activity, only considering those activities that were annotated as IC 50 and as exact (i.e., not < or >). After removal of a few extreme outliers we obtain: \n  \n Clearly, within each group, the Z-scores cluster tightly around 0. It appears that the groups differentiate from each other in terms of the extreme values. Indeed plotting summary statistics for each group confirms this \u2013 in fact the median Z-score has a range of 0.05 and the mean Z-score a range of 0.11 across the six groups. In other words, the bulk of the groups are quite similar. \n Other possibilities \n The example shown here is rather simplistic and is the equivalent of unsupervised clustering. One obvious next step is to search the parameter space of the LDA model, evaluate different approaches to estimating the posterior distribution ( EM or Gibbs sampling ) and so on. A number of extensions to the basic LDA technique have been proposed, one of them being a supervised form of LDA. \n It\u2019d also be useful to look at this method on a slightly smaller, labeled dataset \u2013 I\u2019ve run some preliminary experiments on the Bursi AMES but those results need a little more work. More generally, smaller datasets can be problematic as the number of unique fragments can be low. In addition fewer observations means that the estimates of the posterior distribution becomes fuzzier. One way around this is to develop a model on something like the ChEMBL dataset I used here and then apply that to smaller datasets. Obviously, this goes towards ideas of applicability \u2013 but given the size of ChEMBL, it may indeed \u201ccover\u201d many smaller datasets. \n Is this useful? \n At first sight, it\u2019s an interesting method that identifies groupings in an unsupervised manner. Of course, one could easily run k-means or any of the hierarchical clustering methods to achieve the same result. However, the generative aspect of LDA models is what is of interest to me, but also seems the part that is difficult to map to a chemical setting \u2013 unlike topics in a document, which one can (usually) understand based on the likely terms for that topic, it\u2019s not clear what a topic is for a collection of molecules in an unsupervised setting. And then, how does one infer the meaning of a topic from fragments? While it\u2019s certainly true that certain fragments are associated with specific properties/activities, this is certainly not a given (unlike words, where each one does have an individual meaning). Furthermore, in an unsupervised setting like the one I\u2019ve described here, fishing for a correlation between (some set of) properties and groupings of molecules is probably not the way to go."], "link": "http://blog.rguha.net/?p=997", "bloglinks": {}, "links": {"http://www.umass.edu/": 1, "http://www.plosone.org/": 1, "http://blog.rguha.net/": 3, "https://www.ac.uk/": 1, "http://www.slideshare.net/": 1, "http://www.princeton.edu/": 1, "https://lists.princeton.edu/": 1, "http://www.cheminformatics.org/": 1, "http://en.wikipedia.org/": 9, "http://dx.doi.org/": 1, "http://cran.r-project.org/": 3, "http://www.springerlink.com/": 1, "http://bioinformatics.oxfordjournals.org/": 1}, "blogtitle": "So much to do, so little time"}, {"content": ["The other day I was exchanging emails with John Van Drie regarding open challenges in cheminformatics (which I\u2019ll say more about later). One of his comments concerned the slow speed of chemical searches \n Google searches are screamingly fast, so fast that the type-ahead\u00a0feature is doing the search as you key characters in. \u00a0Why are all\u00a0chemical searches so sloooow? \u2026 Ideally, as you\u00a0sketch your mol in, the searches should be happening at the same pace,\u00a0like the typeahead feature. \n Now, he doesn\u2019t specifically mention what type of chemical search \u2013 it could be exact matches, similarity searches, substructure or pharmacophore searches. The first two can be done very quickly and lend themselves easily to type ahead type search interfaces. In light of the work my colleague has been doing , the substructure searches are now also amenable to a type ahead interface. \n So I quickly put together a simple web page that lets you type in a SMILES (or SMARTS) and as you type it retrieves the results of a substructure search via the NCTT Search Server REST API. (In some cases the depiction is broken \u2013 that\u2019s a bug on my side). Of course, typing in SMILES is not the most intuitive of interfaces. Since Trung employs the ChemDoodle sketcher , an ideal interface would respond to drawing events (say drawing a bond or adding atoms etc) and pull up matches on the fly. Another obvious extension is to rank (or filter) the results \u2013 all the while, maintaining the near real time speed of the application. \n As I said before , seriously fast substructure searches. It also helps that I can build these examples via a public REST API. I\u2019m sure there are reasons for SOAP , XML and so on. But it\u2019s 2011. So lets help make extensions and mashups easier. \n UPDATE : Yes, it\u2019s easy to create patterns (especially with SMARTS) that DoS the server. We have some filters for excessively generic patterns; so some queries may not behave in the expected manner"], "link": "http://blog.rguha.net/?p=993", "bloglinks": {}, "links": {"http://pubchem.nih.gov/": 1, "http://rguha.net/": 1, "http://tripod.nih.gov": 2, "http://www.vandrieresearch.com/": 1, "http://blog.rguha.net/": 1, "http://www.chemspider.com/blog": 1, "http://web.chemdoodle.com/": 2}, "blogtitle": "So much to do, so little time"}]