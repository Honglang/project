[{"blogurl": "http://yaroslavvb.blogspot.com\n", "blogroll": [], "title": "Machine Learning, etc"}, {"content": ["We are looking for a summer intern to apply Deep Learning techniques to the problem of reading text in the wild. More details here"], "link": "http://yaroslavvb.blogspot.com/feeds/1430768426325198816/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["I came across this post post where the author created a font by averaging together all fonts on his machine. I thought it would be cool to do the same for all fonts on the internet -- here's the average of about 375k distinct fonts\n\n\n\n\nIt's interesting that shapes are clearly seen even though fonts on the web are quite noisy, here's a random sample of things that make up the A above"], "link": "http://yaroslavvb.blogspot.com/feeds/3802708886693327086/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["There's a number of accepted papers whose camera-ready versions have been posted already. Here are the ones I found interesting. I'll give further update on these after the conference.\n\n\nEfficient Inference in Fully Connected CRFs with Gaussian Edge Potentials, P. Kr\u00e4henb\u00fchl, V. Koltun\n\nFast and Accurate k-means For Large Datasets, M. Shindler, A. Wong, A. Meyerson\n\nHashing Algorithms for"], "link": "http://yaroslavvb.blogspot.com/feeds/6304110592998816522/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Here's a cool tool I stumbled across reading John Cook's blog -- Shape Catcher looks up Unicode value from a drawing of a character.\n\n\n\nApparently it uses Shape Context features.\n\n\n\nThis motivated me to put together another dataset, unlike notMNIST this focuses on the tail end of Unicode, this is 370k bitmaps representing 29k Unicode values, grouped by Unicode \nUnicode 370k"], "link": "http://yaroslavvb.blogspot.com/feeds/8870314489439247400/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["This is a dataset of scans of 1000 public domain books that was released to the public at ICDAR 2007.\nAt the time there was no public serving infrastructure, so few people actually got the 120GB dataset.\nIt has since been hosted on Google Cloud Storage and made available for public download\n\n\nhttp://commondatastorage.googleapis.com/books/icdar2007/README.txt\nhttp://"], "link": "http://yaroslavvb.blogspot.com/feeds/1603248061403845397/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Below is an illustration of b-matching from (Huang,Jebara AISTATS 2007) paper. You start with a weighted graph and the goal is to connect each v to k u's to minimize total edge cost. If v's represent labelled datapoints, u's unlabeled and weights correspond to distances, this works as a robust version of kNN classifier (k=2 in the picture) because it prevents any datapoint from exhibiting too"], "link": "http://yaroslavvb.blogspot.com/feeds/7865189674426685465/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["My group has intern openings for winter and summer. Winter may be too late (but if you really want winter, ping me and I'll find out feasibility). We use\u00a0OCR for Google Books, frames from YouTube videos, spam images, unreadable PDFs encountered by the crawler, images from Google's StreetView cameras, Android and few other areas.\u00a0Recognizing individual character candidates is a key step in OCR"], "link": "http://yaroslavvb.blogspot.com/feeds/1913671509611268521/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["A discussion came up on Guido von Rossum's Google Plus post. It comes down to the fact that 2.1 is not exactly represented as a floating point number. Internally it's 2.0999999999999996, and this causes unexpected behavior.\n\nThese kinds of issues often come up. The confusion is caused by treating floating point numbers as exact numbers, and expecting calculations with them to produce results"], "link": "http://yaroslavvb.blogspot.com/feeds/5971713134997549505/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["I've taken some publicly available fonts and extracted glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J taken from different fonts.\n\nHere are some examples of letter \"A\"\n\n\nJudging by the examples, one would expect this to be a harder task than MNIST. This seems to be the case -- logistic regression on top of stacked auto-encoder with fine-tuning gets"], "link": "http://yaroslavvb.blogspot.com/feeds/6380637522936189490/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["In the old days you could statically link your program and run it on another Unix station without worrying about dependencies. Unfortunately static linking no longer works, so you need to make sure that your target platform has the right libraries.\n\nFor instance, in order to get Matlab compiled code running on a server, you have to copy over libraries and set environment variables as specified"], "link": "http://yaroslavvb.blogspot.com/feeds/6270374438911760263/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Google+ seems to have a fair number of Machine Learning people, I was able to track down 50 people I've met at conferences by starting at Andrew McCallum's circles. If you add me on Google Circles I'll assume you came from this blog and add you to my \"Machine Learning\" circle"], "link": "http://yaroslavvb.blogspot.com/feeds/8438166030503417500/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Computers are supposed to be deterministic. This is often the case for single processor machines. However, as you scale up, guaranteeing determinism becomes increasingly expensive.Even on single processor machines you are facing non-determinism on semi-regular basis. Here are some examples Bugs + poor OS memory control that allows programs to read uninitialized memory. A recent example for me was"], "link": "http://yaroslavvb.blogspot.com/feeds/7633989648266744620/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Google is hiring and there are lots of opportunities to do Machine Learning-related work here. Kevin Murphy is applying Bayesian methods to video recommendation, Andrew Ng is working on a neural network that can run on millions of cores, and that's just the tip of the iceberg that I've discovered working here for last 3 months.There is machine learning work in both \"researcher\" and \"engineer\""], "link": "http://yaroslavvb.blogspot.com/feeds/4934083198091321883/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Five years ago I ran some queries on Google Scholar to see trends on the number of papers that mention particular phrase. The number of hits for each year was divided by the number of hits for \"machine learning\". Back then it looked like NN's started gaining in popularity with invention of back-propagation in 1980's, peaked in 1993 and went downhill from there.Since then, there's been several"], "link": "http://yaroslavvb.blogspot.com/feeds/3390642347975977336/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["I just noticed that Justin Domke has a blog -- He's one of the strongest researchers in the field of graphical models. I first came across his dissertation when looking for a way to improve loopy-Belief Propagation based training. His thesis gives one such idea -- instead of maximizing the fit of an intractable model, and using BP as intermediate step, maximize the fit of BP marginals directly."], "link": "http://yaroslavvb.blogspot.com/feeds/6060120941658754484/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["I've accepted an offer from Google and will be joining their Tesseract team next week.I first got interested in OCR when I faced a project at my previous job involving OCR of outdoor scenes and found it to be a very complex task, yet highly rewarding because it's easy to make incremental progress and see your learners working.Current state-of-the-art OCR tools are not at human level of reading,"], "link": "http://yaroslavvb.blogspot.com/feeds/1041951345399867160/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Maximum independent set, or \"maximum stable\" set is one of classical NP-complete problems described in Richard Karp's 1972 paper \"Reducibility Among Combinatorial Problems\". Other NP-complete problems often have a simple reduction to it, for instance, p.3 of Tony Jebara's \"MAP Estimation, Message Passing, and Perfect Graphs\" shows how MAP inference in an arbitrary MRF reduces to Maximum Weight"], "link": "http://yaroslavvb.blogspot.com/feeds/574649679618068310/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["A recent discussion on stackoverflow brought up the issue of results of floating point arithmetic being non-reproducibleA reader asked what one could do to guarantee that result of floating point computation is always the same, and Daniel Lichtblau, a veteran developer at the kernel group of WRI replied that \"it is impossible with current hardware and software\"One problem is that IEEE 754"], "link": "http://yaroslavvb.blogspot.com/feeds/1477497617535178174/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Today I got Google Alert today on the following pending patent -- Belief Propagation for Generalized Matching. I like to stay up on Belief Propagation literature, so I took a closer look. The PDF linked gives a fairly detailed explanation of belief propagation for solving matching problems, including pseudocode which is very detailed, looking like an excerpt of a C program. Appendix A seems to"], "link": "http://yaroslavvb.blogspot.com/feeds/4457050953210076046/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["With regular distributive law you can do things like$$\\sum_{x_1,x_2,x_3} \\exp(x_1 + x_2 + x_3)=\\sum_{x_1} \\exp x_1 \\sum_{x_2} \\exp x_2 \\sum_{x_3} \\exp x_3$$This breaks the original large sum into 3 small sums which can be computed independently.A more realistic scenario requires factorization into overlapping parts. For instance take the following$$\\sum_{x1,x2,x3,x_4,x_5} \\exp(x_1 x_2 + x_2 x_3 +"], "link": "http://yaroslavvb.blogspot.com/feeds/2471923041123774551/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Suppose we want to count the number of independent sets in a graph below.There are 9 independent sets.Because the graphs are disjoint we could simplify the task by counting graphs in each connected component separately and multiplying the resultVariational approach is one way of extending this decomposition idea to connected graphs.Consider the problem of counting independent sets in the"], "link": "http://yaroslavvb.blogspot.com/feeds/4401426861035057027/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["There's a neat connection between Cholesky factorization and graph triangulations -- graph corresponding to Cholesky factorization of a sparse matrix is precisely the triangulation of the sparse matrix (when viewed as a graph) using canonical elimination ordering.Here's an example of a matrix (corresponding to the graph with black edges only), and its Cholesky factorization. We triangulate the"], "link": "http://yaroslavvb.blogspot.com/feeds/1794183837846514202/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Here's a 367 vertex Apollonian Network and its Junction Tree (aka Tree Decomposition)A Junction Tree provides an efficient data structure to do exact probabilistic inference. In the context of traditional graph algorithms, it is known as the \"Tree Decomposition\". The amount of structure apparent from the junction tree shows that problems structured as Apollonian Networks have very low"], "link": "http://yaroslavvb.blogspot.com/feeds/1104371989658594605/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["Here's a page linking 65 attempts of resolving P vs NP problem. A couple of papers were published in peer-reviewed journals or conferences, while most are \"arxiv\" published. Some statistics:35 prove P=NP28 prove P!=NP2 prove it can go either way (undecidable)"], "link": "http://yaroslavvb.blogspot.com/feeds/2943157058843811036/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}, {"content": ["First programmers wrote in machine code and assemblers simplified this task significantly by letting them give algorithms at a higher level. I still find stacks of punch cards like below in my St.Petersburg homeWouldn't it be great if we could extend this idea further and have the computer compile the problem into machine code?Actually, we already have such tools restricted to various versions of"], "link": "http://yaroslavvb.blogspot.com/feeds/290409703089736894/comments/default", "bloglinks": {}, "links": {}, "blogtitle": "Machine Learning, etc"}]