[{"blogurl": "http://infostructuralist.wordpress.com\n", "blogroll": [], "title": "The Information Structuralist"}, {"content": ["As we have seen in Part I , the rational inattention framework of Christopher Sims aims to capture the best a rational agent can do when his capacity for processing information is limited. This rationally inattentive agent, however, has no reason to question his statistical model. In this post we will examine the robustness framework of Thomas Sargent , which deals with the issue of model uncertainty, but does not assume any capacity limitations. \n \n \n \nAgain, for simplicity I will stick to single-stage problems. The setting is exactly the same as before: we have an agent who must make a decision pertaining to some state of the world on the basis of some signal correlated with . If the quality of a decision procedure is measured in terms of its expected cost , then the agent faces the optimization problem\n \n and the optimal decision function is given by\n \n In order to compute , the agent must know (the distribution of ) and the observation model that relates the observed signal to the unobserved state . In other words, the agent must have a probabilistic model , embodied in the joint distribution of and . \n \nAll of this is standard fare in the realm of Bayesian rationality. But now let\u2019s suppose that the agent treats the model only as an approximation that was adopted for reasons of computational tractability, limited knowledge, etc. What should the agent do in this situation? Sargent\u2019s proposal, inspired by ideas from robust control and game theory, is this: instead of simply optimizing the decision procedure to the \u201cnominal\u201d model (which may, after all, prove to be inaccurate), the agent should hedge his bets and allow for the fact that the \u201ctrue\u201d model (which we will denote by ) may differ from the nominal model , but the difference is bounded. While there are many ways of quantifying this model uncertainty , Sargent has opted for the divergence bound . The parameter is chosen by the agent and reflects his degree of confidence in : the smaller this , the more the agent will tend to trust his model-building skills. With the problem framed in this way, the natural strategy is the minimax one: if we define, for any distribution of and for any strategy , the expected cost\n \n then the agent should solve the optimization problem \n \n The idea here is that the resulting strategy will be robust against some amount of model uncertainty, as determined by the magnitude of . We can also envision a malicious adversary, who tries to thwart the agent\u2019s objective by choosing the worst possible joint distribution of the state and the signal, subject to the divergence constraint . Thus, we can view the quantity defined in (1) as the upper value of a zero-sum game between the agent and the adversary. The agent\u2019s moves are the strategies (so that mixed strategies would involve additional randomization on the agent\u2019s part), while the adversary\u2019s moves are the distributions in the \u201cdivergence ball\u201d of radius around the nominal distribution . \n \nNow let\u2019s see what we can say about the optimal strategy in (1) . We start by examining the inner maximization for a fixed . If we introduce a Lagrange multiplier for the constraint , then it can be shown that strong duality holds, so\n \n where now the supremum is over all probability distributions for and . We will now show that, for a fixed , we can compute the supremum over in closed form. To that end, we will need the following result, known as the Donsker-Varadhan formula : \n Lemma 1 (Donsker-Varadhan)  For any probability measure on some space and any measurable function such that , \t \n \n  \n Remark 1 This result is so fundamental and so simple that it has been rediscovered multiple times (e.g., by the machine learning community ). \n Proof: To keep things simple, I will present the proof for finite . Let us define the tilted distribution \n \n Then the supremum in (2) is achieved by . Indeed, for any other we will have\n \n \twhere equality holds if and only if . \n \nNow we can use (2) and write\n \n Consequently, we can express the optimal value of the optimization problem (1) as\n \n This is as far as we can get \u2014 we have no way of doing the optimization over the choice of the strategy and the Lagrange multiplier in closed form. But we can gain some intuition by focusing on some fixed value of . To that end, let us define \n \n Now let\u2019s examine the quantity under the logarithm. It is actually a standard Bayesian optimum cost problem for the nominal model , except now instead of the original cost we have the exponentiated cost . So for each value of the optimal strategy minimizes the expected cost : \n \n So the robust optimization problem (1) is, actually, an ordinary Bayesian optimization problem in disguise, but with a different cost function. This may not seem like a big deal, but now we can actually see how cautious a robust strategy is compared to a non-robust one. Let\u2019s suppose that, instead of optimizing the expected cost over all , we wanted to minimize the probability that the cost exceeds some threshold . Then the problem is\n \n The solution to this problem will, of course, depend on . But now we will see that a robust strategy will work for all (even though it will not be optimal for each individual ). To see this, we will exploit the fact that the graph of the step function with the step at , i.e., of the indicator function of the semi-infinite interval , lies below the graph of the shifted exponential function for any : \n \n The two graphs touch precisely at the point , and the bound is tighter for larger values of . Therefore, using (5) and the definition of in (3) , we can write\n \n Moreover, this upper bound is actually achieved by . The main thing to note here is the presence of the exponentially decaying factor . So, if an agent decides to use a robust strategy and if his model actually turns out to be correct, then he ends up ensuring not only a small average cost, but also small probability of large excursions of the cost! For this reason, the strategy (4) is often called risk-sensitive , the problem of minimizing the expected exponential cost is called risk-sensitive optimization , and the Lagrange multiplier is called the risk aversion factor . \n For a nice treatment of robustness and risk-sensitive optimization in the context of information theory and statistical estimation, check out this paper by Neri Merhav ."], "link": "http://infostructuralist.wordpress.com/2012/07/20/information-theory-in-economics-part-ii-robustness/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1, "https://files.nyu.edu/": 1, "http://www.princeton.edu/": 1, "http://webee.ac.il/": 2, "http://infostructuralist.wordpress.com/": 10, "http://mark.reid.name/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["Economic activity involves making decisions. In order to make decisions, agents need information. Thus, the problem of acquisition, transmission, and uses of information has been occupying the economists\u2019 attention for some time now (there is even a whole subfield of \u201cinformation economics\u201d ). It is not surprising, therefore, that information theory, the brainchild of Claude Shannon, would eventually make its way into economics. In this post and the one to follow, I will briefly describe two specific strands of information-theoretic work in economics: the rational inattention framework of Christopher Sims and the robustness ideas of Thomas Sargent . (As an interesting aside: Sims and Sargent have shared the 2011 Nobel Memorial Prize in Economics , although not directly for their information-theoretic work, but rather for their work related to causality.) \n \n \n \nIn a nutshell, both Sims and Sargent aim to mitigate a significant shortcoming of the rational expectations hypothesis , namely that (to quote Cosma ) \u201cwhat game theorists [and economists] somewhat disturbingly call rationality is assumed throughout \u2014 \u2026 game players are assumed to be hedonistic yet infinitely calculating sociopaths endowed with supernatural computing abilities.\u201d To put this in more charitable terms, the basic tenet of rational expectations is that all economic agents are continuously optimizing, have access to all relevant information, can react to it instantly, and have unlimited computational capabilities. This is, to put it mildly, a huge oversimplification that does not mesh well with empirical observations. In reality, we see all sorts of \u201cinertia\u201d and delayed reaction effects (what Keynes has referred to as \u201cstickiness\u201d of prices, wages, etc.). Moreover, even disregarding stickiness, there is no reason to believe that the models used by the agents are at all accurate (indeed, if the 2008 financial crisis has taught us anything, quite the opposite is true). Thus, two adjustments are needed to the rational expectations framework: one to account for the fact that economic agents and institutions have only limited resources and capacity for acquiring and processing information, and another to formalize the pervasiveness of model uncertainty. \n \nThis post will focus on rational inattention, which seeks to address the first issue. (A follow-up post will discuss robustness, which tackles the second issue.) The main tenet of rational inattention is that any economic agent can observe all relevant information only through a capacity-limited channel, where capacity is understood in the sense of Shannon. This capacity constraint is meant to capture the idea that, contrary to the rational expectations viewpoint, no one really keeps track of all available information, with perfect accuracy, all the time. This is the inattention part. The rational part captures the notion that an agent faced with an information capacity constraint will aim to optimize the observation channel through which he receives the relevant information, subject to that constraint. So, the question is \u2014 what should guide the optimization process? \n \nHere is my take on what Sims is proposing. To keep things simple, I will limit the discussion to one-stage decision problems, although the same can be done for multistage problems too. Consider an agent who wishes to make a decision (or take an action) pertaining to some state of the world (modeled as a random variable with a known and fixed distribution ) on the basis of some related signal under the mutual information constraint . The decision variable is assumed to be some (possibly randomized) function of the signal , and the overall quality of the decision is measured in terms of expected cost, . Let\u2019s assume for the moment that the signal space is fixed. So now the agent has to optimize two objects: the observation channel and the decision procedure . Given , the best choice of is to minimize the expected posterior cost: \n \n (if there are several minimizers, select one at random). Now we want to optimize the observation channel . Clearly, the best such channel, under the given constraint , is the one that yields the largest reduction in expected cost relative to the best \u201cno-information\u201d expected cost :\n \n Once the optimal observation channel is found, the optimal decision procedure is given by (1) . \n \nThus, a rationally inattentive agent must maximize the value of available information for the purpose of decision-making. As we have seen before , the notion of the value of information is intimately related to rate-distortion theory . Indeed, if we now allow the agent to choose not only the observation channel and the decision procedure, but also the signal space , then the smallest attainable cost will be given by the distortion-rate function (DRF) \n \n More precisely, as proved by Stratonovich , the DRF gives the best attainable reduction of expected costs under a mutual information constraint:\n \n where the first supremum is over the choice of signal space , while the second is over all admissible observation channels with output in . In fact, the supremum over is attained by choosing to be the space of beliefs , and then the optimum is the posterior distribution induced by and the test channel that achieves the optimum in (2) . \n \nThis simple idea goes a long way, as anyone can see by browsing the work of Sims on the topic. For example, if all random objects are actually time series (or stochastic processes), then the rational inattention framework can indeed account for stickiness ; it can account for discreteness phenomena in retail price or wage changes; and it can even (somewhat speculatively) shed light on transparency in monetary policy : why do financial markets often react so strongly and drastically to the Fed\u2019s very succinct discrete summaries of its monetary policy changes, as compared to more smooth reactions to other central banks that provide more regular and more detailed summaries? \n \nOf course, as any information theorist knows, the Shannon DRF is only an asymptotic measure of performance, and it needs to be related to an operational criterion via an appropriate coding theorem. In a communication system, the relevant operational criteria pertain to the quality of signal reconstruction at the receiver, and the mutual information constraint is only an asymptotic abstraction of the channel\u2019s reliability by way of the law of large numbers. However, in economics it is not at all clear, which operational interpretation one should attach to mutual information constraints faced by rationally inattentive agents. If we view an economic institution (say, a firm or an investment bank) as an organism, then it interfaces with its environment via its \u201csensory organs\u201d (electronic communications, eyes, ears and brains of its analysts, etc.), and so the mutual information constraint may represent the institutions\u2019s \u201csensory capacity\u201d (taking us all the way back to early cybernetics work on the information capacity of single neurons ). But even then any talk of capacity should be tied to coding, which will inevitably introduce delays and layers of complexity. While Sims acknowledges the issues of delay and complexity in his papers, there is (as of yet) no satisfactory operationalization of rational inattention."], "link": "http://infostructuralist.wordpress.com/2012/06/01/information-theory-in-economics-part-i-rational-inattention/", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://bactra.org/": 1, "http://www.americanscientist.org/": 1, "https://files.nyu.edu/": 1, "http://www.nobelprize.org/": 1, "http://deepblue.umich.edu/": 1, "http://www.princeton.edu/": 4, "http://infostructuralist.wordpress.com/": 4, "http://en.wikipedia.org/": 4, "http://feeds.wordpress.com/": 1, "http://www.econlib.org/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["The abstracts for these two papers (one a classic in robust control theory, the other in mathematical physics) are almost Zen in their simplicity and perfection: \n J. C. Doyle, \u201cGuaranteed margins for LQG regulators,\u201d IEEE Transactions on Automatic Control , vol. 23, no. 4, pp. 756-757, 1978 \n Abstract: There are none. \n J. E. Avron, L. Sadun, J. Segert and B. Simon, \u201cChern numbers, quaternions, and Berry\u2019s phases \nin Fermi systems,\u201d Communications in Mathematical Physics , vol. 124, pp. 595-627, 1989 \n Abstract: Yes, but some parts are reasonably concrete. \n P.S. (More or less) regular blogging to resume in 3, 2, 1 \u2026 ."], "link": "http://infostructuralist.wordpress.com/2012/05/31/abstract-snark/", "bloglinks": {}, "links": {"http://feeds.wordpress.com/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["This morning, I received an email from Sergio Verd\u00fa with the extremely sad news that Tom Cover died on March 26th . He was 73. \n Tom\u2019s passing is a great loss. His work has left an indelible mark not only on information theory, but also on machine learning (for instance, his result with Peter Hart on the probability of error of the nearest-neighbor rule being at most twice the Bayes rate is one of the cornerstones of the theory of pattern recognition), statistics, probability, finance, etc. ( here is Sergio\u2019s wonderful presentation of Tom\u2019s work across all these fields). His famous textbook, Elements of Information Theory , written with Joy Thomas, was the first one to feature not only the standard topics, such as source and channel coding, but also the more advanced material, such as multiterminal information theory, Kolmogorov complexity, connections between information theory and statistics, connections between information theory and gambling, or universal source coding, in a characteristically lucid manner that made them accessible to beginners. Everything Tom did was touched by elegance, simplicity, and grace. He will be missed, but never forgotten."], "link": "http://infostructuralist.wordpress.com/2012/03/28/thomas-m-cover-1938-2012/", "bloglinks": {}, "links": {"https://twitter.com/": 1, "http://feeds.wordpress.com/": 1, "http://www.princeton.edu/": 2, "http://www.stanford.edu/": 1, "http://www.elementsofinformationtheory.com/": 1, "https://sites.google.com/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["Getting settled in at my new academic home , with all the attendant administrivia, has sucked up almost two months of missed blogging opportunities. So, here goes. \n \n This year\u2019s ITA was held at a new location, Catamaran Resort on Mission Bay. Anand has already blogged about some talks ; this is my selection, which is disjoint from his. I will follow Cosma\u2019s example and just give very telegraphic summaries of some of my favorites with links to abstracts and/or papers (when available). \n \n Chandra Nair , \u201cUpper concave envelopes and broadcast channels\u201d ( abstract ) \n\u2013 whenever we use auxiliary random variables in multiterminal information theory, convex analysis is lurking somewhere in the background; Chandra showed how to bring it to the foreground; is there a connection to comparison of experiments in the sense of Blackwell?\n \n Four talks on various information-theoretic problems in the finite-blocklength regime (via either the dispersion approach of Polyanskiy, Poor and Verd\u00fa based on the Berry-Esseen theorem , or various sharp asymptotic results like the Bahadur-Rao theorem ):\n Oliver Kosut , \u201cFinite-blocklength Slepian-Wolf coding,\u201d joint work with Vincent Tan ( paper ) \n En-Hui Yang , \u201cNon-asymptotic equipartition properties for independent and identically distributed sources\u201d, joint work with Jin Meng ( paper ) \n Amir Ingber, \u201cThe dispersion of source(-channel) coding,\u201d joint work with Yuval Kochman and Da Wang ( abstract , arXiv:1109.6310 ) \n Pierre Moulin , \u201cThe log-volume of optimal codes for memoryless channels, within a few nats\u201d ( abstract ) \n \n Tsachy Weissman , \u201cNew estimators of directed information,\u201d joint work with Jiantao Jiao, Haim Permuter , Lei Zhao , Young-Han Kim ( abstract , arXiv:1201.2334 ) \n\u2013 how to use universal probability assignment (with or without smoothing) to estimate directed information between two random processes \n Michelle Effros , \u201cTowards a computational information theory,\u201d ( abstract , related paper ) \n\u2013 exact computation of capacities (and achievable rate regions) in large networks is largely intractable; let\u2019s forget exact results and develop approximation and bounding techniques: when can we replace one or more channels in a network with some others and still stay in the achievable rate region? \n Stefano Soatto , \u201cInformation forests,\u201d joint work with Zhao Yi, Maneesh Dewan, and Yiqiang Zhan ( paper ) \n\u2013 Stefano described an information-theoretic approach to designing decision trees (or forests, to be more precise) for classification \n Urbashi Mitra , \u201cState-dependent active communication,\u201d joint work with Chiranjib Choudhuri ( abstract , related paper ) \n\u2013 how can we simultaneously transmit information over a channel with state and estimate the state sequence? the state sequence is modulated at the encoder by means of actions , which may or may not depend on previous channel states; the proof of converse used a neat data processing inequality for statistical estimation in the spirit of Blackwell\u2019s comparison of experiments \n Two talks by Dayu Huang and Sean Meyn ( Eminent Scholar ) on statistical estimation in the sparse-sample regime:\n \n \u201cError exponent for goodness of fit test with sparse samples\u201d ( abstract ) \n \u201cOptimality of coincidence-based goodness of fit test for sparse sample problems\u201d ( paper ) \n Mohammad Naghshvar, \u201cActive sequential hypothesis testing,\u201d joint work with Tara Javidi ( abstract ) \n\u2013 Mohammad presented some new results on the topic; Jensen-Shannon divergence (i.e., the gap in Jensen\u2019s inequality for entropy) made a surprising appearance \n Sirin Nitinawarat , \u201cControlled sensing for hypothesis testing,\u201d joint work with George Atia and Venu Veeravalli ( abstract ) \n\u2013 this talk was thematically related to Mohammad\u2019s, but Sirin focused on fixed sample size problems"], "link": "http://infostructuralist.wordpress.com/2012/02/19/ita-2012-some-evanescent-reflections/", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://www.catamaranresort.com/": 1, "https://netfiles.uiuc.edu/": 1, "http://circuit.ucsd.edu/": 1, "https://sites.google.com/": 1, "http://www.mit.edu/": 1, "http://ita.ucsd.edu/": 13, "http://www.ucla.edu/": 1, "http://www.ufl.edu/": 1, "http://feeds.wordpress.com/": 1, "http://allegro.mit.edu/": 1, "http://ee2.caltech.edu/": 1, "http://chandra.edu.hk/": 1, "http://ergodicity.net/": 2, "http://csi.usc.edu/": 1, "http://projecteuclid.org/": 1, "http://www.illinois.edu/": 5, "http://www.princeton.edu/": 1, "http://arxiv.org/": 4, "http://infostructuralist.wordpress.com/": 1, "http://fleece.ucsd.edu/": 1, "http://cscs.umich.edu/": 1, "http://ergodicity.net": 1, "http://www.uwaterloo.ca/": 1, "http://www.stanford.edu/": 2, "http://en.wikipedia.org/": 2, "http://homepages.wisc.edu/": 2, "http://ewh.ieee.org/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["Sasha Rakhlin and I will be presenting our paper \u201cLower bounds for passive and active learning\u201d at this year\u2019s NIPS , which will be taking place in Granada, Spain from December 12 to December 15. The proofs of our main results rely heavily on information-theoretic techniques, specifically the data processing inequality for -divergences and a certain type of constant-weight binary codes . \n \n \n \nThe paper deals with the well-known binary classification problem : We have two correlated random variables and , where traditionally is referred to as an instance or a feature and as the (binary) label . The joint distribution of and is unknown, and the goal is to learn a classifier , i.e., a mapping , whose probability of error is as small as possible. More formally, the accuracy of a classifier is measured by its excess risk w.r.t. :\n \n It is known that the infimum on the right-hand side is achieved by the Bayes classifier , which has the following form: Given , let be the so-called regression function . Then the Bayes classifier is\n \n In other words, the optimum strategy is to classify a given as a if and only if the conditional probability (under ) that given is at least half. \n \nThe classical set-up for this problem assumes that the learning agent has access to a large number of i.i.d. samples from , and can use these samples to select a candidate classifier from some fixed class . The goal of learning is to ensure that, with high probability, the candidate classifier is as close to as possible. This is the passive learning model, since the learning agent has no control over the process of collecting the data. By contrast, under the active model, at each time step the learner selects the new feature based on all the past data , and then requests the corresponding label from an \u201coracle.\u201d When a suitably large number of feature-label pairs has been collected, the learner outputs a candidate classifier , whose performance is measured, as before, by the excess risk. \n Remark 1 This is not the only active learning model out there. Another widely studied setting, which can be more accurately called \u201cselective sampling,\u201d involves i.i.d. training samples just like in the passive case, but the learner accesses them sequentially and has the freedom to decide, for each of the examples, whether or not he wants the corresponding label to be revealed. In this set-up, unlabeled features are essentially free; only the number of label requests matters. \t \n \nClearly, the active learning model is stronger than the passive model. But how much do we gain by allowing active learning? One way to quantify it is to look at sample complexity of the underlying learning problem: given some class of possible distributions of and a class of candidate classifiers, what is the minimum number of feature-label pairs that the learner needs to see in order to achieve a given level of excess risk with a given probability of success? Here the probability of success is computed w.r.t. the probability measure that governs the data-gathering process, so in the passive case it\u2019s just , while in the active case it is given by interconnecting the agent\u2019s (possibly stochastic) rules for selecting on the basis of for each with the conditional probability distribution , while respecting the causal ordering\n \n The sample complexity is an \u201cinformation-theoretic\u201d limit in the sense that no strategy, no matter how clever or computationally powerful, can make do with fewer training samples. \n \nTo date, there has been quite a great deal of work on developing and analyzing algorithms for active learning \u2014 see, e.g., the papers by Steve Hanneke (see also the preprint ) and Vladimir Koltchinskii and references therein. A theoretical analysis of any given algorithm gives us upper bounds on the sample complexity, so if we find an upper bound for active learning that is much lower than existing tight lower bounds for passive learning, then we can claim that active learning indeed does help. Both Hanneke and Koltchinskii show that the sample complexity of a number of very natural schemes for active learning can be upper-bounded in terms of what Hanneke has called the disagreement coefficient , which is defined as follows. Consider the underlying distribution of and a class of candidate classifiers. For each define the -minimal set \n \n where is the Bayes classifier corresponding to . In words, consists of all classifiers that disagree with the Bayes classifier with -probability at most . Next, define the disagreement set \n \n In words, the disagreement set consists of all features , for which there exists at least one classifier in the -minimal set that disagrees with at . Now consider the quantity \n \n which measures the \u201csize\u201d of the disagreement set (relative to the \u201cradius\u201d of ). \n Remark 2 The function defined in (1) has been used by Alexander in his work on deviation inequalities for empirical processes, and more recently by Gin\u00e9 and Koltchinskii in their work on passive learning . The latter authors have termed the function the Alexander capacity . \n \nThe disagreement coefficient for the pair is then defined as\n \n Hanneke\u2019s paper contains several examples of how the disagreement coefficient may be computed (or bounded) for specific learning problems. Moreover, if is finite, then a natural active learning scheme can be based on the observation that only the features in the disagreement set are \u201cinformative.\u201d Of course, the disagreement set depends on the unknown distribution, but, as Koltchinskii has shown, it is possible to estimate this set from data. In particular, if the regression function corresponding to the distribution has margin , i.e., if for all , then the algorithm proposed by Koltchinskii needs on the order of \n \n examples in order to attain an excess risk of at most with probability at least , where is the VC dimension of the class . \n \nWhat Sasha and I have proved is a minimax lower bound on the sample complexity of active learning under the margin assumption. Roughly speaking, we have shown that, for any admissible capacity function and any sufficiently small , and , there exists a choice of the pair that has margin , whose Alexander capacity at is equal to (it may be different for other values ), and any active learning algorithm that attains excess risk of at most with -probability at least needs at least order of \n \n examples. We have also proved the corresponding lower bound for passive learning: must be at least order of \n \n \n \nNote that, in contrast with Koltchinskii\u2019s upper bound (2) that involves the supremum of Alexander\u2019s capacity, our lower bound actually depends on the value of the capacity at the given . Thus, we can quantify the relative advantage of active learning over passive learning in terms of the rate of growth of as a function of . For instance, if , then active learning has very little advantage over passive learning, since both will require at least examples. On the other hand, if is close to , then the best passive learner will need a factor of more examples than the best active learner. \n \nAs I\u2019ve mentioned earlier, our proof of the bounds (3) and (4) makes essential use of the data processing inequality for -divergences (in the paper, we use the term \u201c -divergence,\u201d since is reserved for a generic classifier). This technique is much stronger than any method based on Fano\u2019s inequality , since the latter generally cannot give the term proportional to and, more importantly, gives very loose bounds for the active case. This is where the freedom of choosing a suitable -divergence comes to save the day, since it allows us to decouple the conditional information gain at each time step from the variables that describe the \u201cglobal\u201d behavior of the learning algorithm (e.g., the total number of times a given feature point has been queried by the learner). It should be pointed out that we were definitely not the first to use the data processing inequality for -divergences in a statistical context. It was used implicitly by Adityanand Guntuboyina and explicitly by Alexander Gushchin (in a really obscure paper that you\u2019ve probably never heard of ) to improve upon Fano\u2019s method for deriving minimax lower bounds on the risk of passive statistical estimation procedures."], "link": "http://infostructuralist.wordpress.com/2011/11/10/lower-bounds-for-passive-and-active-learning/", "bloglinks": {}, "links": {"http://arxiv.org/": 1, "http://feeds.wordpress.com/": 1, "http://infostructuralist.wordpress.com/": 5, "http://www.uconn.edu/": 1, "http://knowyourmeme.com/": 1, "http://jmlr.mit.edu/": 1, "http://www.cmu.edu/": 1, "http://www.yale.edu/": 1, "http://www-bcf.usc.edu/": 1, "http://en.wikipedia.org/": 2, "http://people.duke.edu/": 3, "http://www-stat.upenn.edu/": 1, "http://www.berkeley.edu/": 1, "http://nips.cc/": 1, "http://www.mathnet.ru/": 1, "http://projecteuclid.org/": 2}, "blogtitle": "The Information Structuralist"}, {"content": ["Just a couple of short items, while I catch my breath. \n 1. First of all, starting January 1, 2012 I will find myself amidst the lovely cornfields of Central Illinois, where I will be an assistant professor in the Department of Electrical and Computer Engineering at UIUC . This will be a homecoming of sorts, since I have spent three years there as a Beckman Fellow . My new home will be in the Coordinated Science Laboratory , where I will continue doing (and blogging about) the same things I do (and blog about). \n 2. Speaking of Central Illinois, last week I was at the Allerton Conference , where I had tried my best to preach Uncle Judea \u2018s gospel to anyone willing to listen information theorists and their fellow travelers. The paper, entitled \u201cDirected information and Pearl\u2019s causal calculus,\u201d is now up on arxiv , and here is the abstract: \n \nProbabilistic graphical models are a fundamental tool in statistics, machine learning, signal processing, and control. When such a model is defined on a directed acyclic graph (DAG), one can assign a partial ordering to the events occurring in the corresponding stochastic system. Based on the work of Judea Pearl and others, these DAG-based \u201ccausal factorizations\u201d of joint probability measures have been used for characterization and inference of functional dependencies (causal links). This mostly expository paper focuses on several connections between Pearl\u2019s formalism (and in particular his notion of \u201cintervention\u201d) and information-theoretic notions of causality and feedback (such as causal conditioning, directed stochastic kernels, and directed information). As an application, we show how conditional directed information can be used to develop an information-theoretic version of Pearl\u2019s \u201cback-door\u201d criterion for identifiability of causal effects from passive observations. This suggests that the back-door criterion can be thought of as a causal analog of statistical sufficiency. \n If you had seen my posts on stochastic kernels , directed information , and causal interventions , you will, more or less, know what to expect. \n Incidentally, due to my forthcoming move to UIUC, this will be my last Allerton paper!"], "link": "http://infostructuralist.wordpress.com/2011/10/05/updates-get-your-updates-here/", "bloglinks": {}, "links": {"http://www.illinois.edu/": 1, "http://bayes.ucla.edu/": 2, "http://illinois.edu": 1, "http://infostructuralist.wordpress.com/": 3, "http://www.uiuc.edu/": 2, "http://arxiv.org/": 1, "http://ece.illinois.edu": 1, "http://feeds.wordpress.com/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["Alekh Agarwal and Sasha Rakhlin are organizing a workshop at this year\u2019s NIPS . I\u2019m on the program committee, so it is my duty (and distinct pleasure) to invite you all to peruse the full call for papers here , or at least to check out this key snippet: \n We would like to welcome high-quality submissions on topics including but not limited to: \n \n Fundamental statistical limits with bounded computation\n Trade-offs between statistical accuracy and computational costs\n Computation-preserving reductions between statistical problems\n Algorithms to learn under budget constraints\n Budget constraints on other resources (e.g. bounded memory)\n Computationally aware approaches such as coarse-to-fine learning\n \n Interesting submissions in other relevant topics not listed above are welcome too. Due to the time constraints, most accepted submissions will be presented as poster spotlights. \n Oh, and did I mention that the workshop will take place in mid-December in Sierra Nevada, Spain?"], "link": "http://infostructuralist.wordpress.com/2011/09/01/cost-nips-2011-workshop-on-computational-trade-offs-in-statistical-learning/", "bloglinks": {}, "links": {"http://www-stat.upenn.edu/": 1, "http://feeds.wordpress.com/": 1, "https://sites.google.com/": 1, "http://nips.cc/": 1, "http://www.berkeley.edu/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["Obligatory disclaimer: YMMV, \u201cfavorite\u201d does not mean \u201cbest,\u201d etc. etc. \n \n Emmanuel Abbe and Andrew Barron , \u201cPolar coding schemes for the AWGN channel\u201d ( pdf ) \n The problem of constructing polar codes for channels with continuous input and output alphabets can be reduced, in a certain sense, to the problem of constructing finitely supported approximations to capacity-achieving distributions. This work analyzes several such approximations for the AWGN channel. In particular, one approximation uses quantiles and approaches capacity at a rate that decays exponentially with support size. The proof of this fact uses a neat trick of upper-bounding the Kullback-Leibler divergence by the chi-square distance and then exploiting the law of large numbers. \n Tom Cover , \u201cOn the St. Petersburg paradox\u201d \n A fitting topic, since this year\u2019s ISIT took place in St. Petersburg! Tom has presented a reformulation of the problem underlying this (in)famous paradox in terms of finding the best allocation of initial capital so as to optimize various notions of relative wealth. This reformulation obviates the need for various extra assumptions, such as diminishing marginal returns (i.e., concave utilities), and thus provides a means of resolving the paradox from first principles. \n Paul Cuff , Tom Cover, Gowtham Kumar , Lei Zhao , \u201cA lattice of gambles\u201d \n There is a well-known correspondence between martingales and \u201cfair\u201d gambling systems. Paul and co-authors explore another correspondence, between fair gambles and Lorenz curves used in econometric modeling, to study certain stochastic orderings and transformations of martingales. There are nice links to the theory of majorization and, through that, to Blackwell\u2019s framework for comparing statistical experiments in terms of their expected risks. \n Ioanna Ioannou, Charalambos Charalambous , Sergey Loyka , \u201cOutage probability under channel distribution uncertainty\u201d ( pdf ; longer version: arxiv:1102.1103 ) \n The outage probability of a general channel with stochastic fading is the probability that the conditional input-output mutual information given the fading state falls below the given rate. In this paper, it is assumed that the state distribution is not known exactly, but there is an upper bound on its divergence from some fixed \u201cnominal\u201d distribution (this model of statistical uncertainty has been used previously in the context of robust control). The variational representation of the divergence (as a Legendre-Fenchel transform of the moment-generating function) then allows for a clean asymptotic analysis of the outage probability. \n Mohammad Naghshvar, Tara Javidi , \u201cPerformance bounds for active sequential hypothesis testing\u201d \n Mohammad and Tara show how dynamic programming techniques can be used to develop tight converse bounds for sequential hypothesis testing problems with feedback, in which it is possible to adaptively control the quality of the observation channel. This viewpoint is a lot cleaner and more conceptually straightforward than \u201cclassical\u201d proofs based on martingales (\u00e0 la Burnashev ). This new technique is used to analyze asymptotically optimal strategies for sequential -ary hypothesis testing, variable-length coding with feedback, and noisy dynamic search. \n Chris Quinn, Negar Kiyavash , Todd Coleman , \u201cEquivalence between minimal generative model graphs and directed information graphs\u201d ( pdf ) \n For networks of interacting discrete-time stochastic processes possessing a certain conditional independence structure (motivating example: discrete-time approximations of smooth dynamical systems), Chris, Negar and Todd show the equivalence between two types of graphical models for these networks: (1) generative models that are minimal in a certain \u201ccombinatorial\u201d sense and (2) information-theoretic graphs, in which the edges are drawn based on directed information . \n Ofer Shayevitz , \u201cOn R\u00e9nyi measures and hypothesis testing\u201d (long version: arxiv:1012.4401 ) \n Ofer obtained a new variational characterization of R\u00e9nyi entropy and divergence that considerably simplifies their analysis, in many cases completely replacing delicate arguments based on Taylor expansions with purely information-theoretic proofs. He also develops a new operational characterization of these information measures in terms of distributed composite hypothesis testing."], "link": "http://infostructuralist.wordpress.com/2011/09/01/isit-2011-favorite-talks/", "bloglinks": {}, "links": {"http://coleman.ucsd.edu/": 1, "http://infostructuralist.wordpress.com/": 1, "http://www.stanford.edu/": 3, "http://colemant.illinois.edu/": 1, "http://ita.ucsd.edu/": 1, "http://plato.stanford.edu/": 1, "http://www.princeton.edu/": 1, "http://arxiv.org/": 2, "http://circuit.ucsd.edu/": 1, "http://www.yale.edu/": 1, "http://ipg.epfl.ch/": 2, "http://en.wikipedia.org/": 2, "http://feeds.wordpress.com/": 1, "http://ise.illinois.edu/": 1, "http://www.uottawa.ca/": 2, "http://www.ac.cy/": 1, "http://mi.mathnet.ru/": 1, "http://projecteuclid.org/": 1}, "blogtitle": "The Information Structuralist"}, {"content": ["Better late than never, right? Besides, Anand all but made sure that I would blog about it eventually. \n I have attended this year\u2019s Shannon lecture and all the plenaries except for Wojtek Szpankowski \u2018s talk on the information theory of algorithms and combinatorics (I bravely fought the jetlag and lost), but you can watch it here . Here are, shall we say, some impressionistic sketches based on the notes I was taking. \n \n The plenaries \n Erdal Arikan , Polar Coding: Status and Prospects \nArikan\u2019s award-winning paper on channel polarization introduced a new way of thinking about and constructing capacity-achieving codes for a class of binary-input channels satisfying a certain symmetry condition. The main idea underlying these polar codes is that, using an encoder with a particular recursive structure and a successive-cancellation decoder, it is possible to transform parallel uses of such a channel with capacity \u2014 which lies between 0 and 1 and, owing to symmetry, is achieved by using equiprobable inputs \u2014 into (roughly) uses of a nearly noiseless binary-input channel, (roughly) uses of a nearly useless (zero-capacity) binary-input channel, and the remainder consisting of channels that are neither particularly good, nor particularly bad. Arikan gave one motivation for his construction using the chain rule for mutual information, discussed the difference between choosing a channel \u201cpolarizer\u201d at random and using his deterministic recursive construction, went over a number of interesting results that followed his initial work, presented a detailed quantitative comparison of polar codes and turbo codes, and finished with a discussion of open questions and promising further directions. One interesting direction that was not mentioned in the talk would be to explore other manifestations of structure vs. randomness in information theory. \n Zhanna Reznikova , Ants and Bits \nZhanna Reznikova is an ethologist and the author of Animal Intelligence: From Individual to Social Cognition . She was talking about her work on understanding the communication capabilities of red wood ants, performed in collaboration with Boris Ryabko . She started by listing three possible approaches to studying animal communication: (1) direct decoding of signals; (2) the use of \u201cintermediate\u201d languages; and (3) the use of information theory. The first approach met with some degree of success in only a few cases (dances of honey bees; acoustic signals of monkeys), one of the main difficulties having to do with encountering natural animal communication in repeatable situations. The second approach only works by substituting natural animal communication with adapted human languages and artificial stimuli, so its usefulness is somewhat limited. Finally, the information-theoretic approach, used by Reznikova and Ryabko, bypasses the problem of decoding in favor of studying the patterns of communication (e.g., duration and complexity of messages) in reproducible laboratory settings in order to detect whether the animals perform any \u201ccompression\u201d of observed statistical regularities in their environment. According to Reznikova, their experiments have shown that ants are capable of detecting and efficiently representing certain regular patterns in their environments; transmit to one another information about the number of objects of interest; and even add and subtract small numbers. I cannot comment on the experimental protocol used by Reznikova and Ryabko, and I am somewhat skeptical about the general validity of inference from observed duration and complexity of messages to the structure, meaning, and purpose of actual communication processes, but the idea of using information-theoretic \u201cfeatures\u201d to understand the variety of animal languages seems intriguing. Perhaps, looking beyond single numbers like entropy and focusing on more refined features, like the entire information spectrum , may lead to interesting advances in the future. \n Vladimir Tikhomirov , Entropy in Information Theory and Functional Analysis \nThe seminal paper of Kolmogorov and Tikhomirov on the metric entropy of function spaces has influenced a staggering number of disciplines, e.g., functional analysis, analysis of operators, ergodic theory, theory of dynamical systems, probability theory, statistical learning theory, geometric functional analysis, and many more. Tikhomirov\u2019s talk was devoted to the roots of this work in Shannon\u2019s information theory, its relation to foundations of mathematics (in particular, Hilbert\u2019s 13th problem ), and its impact on Kolmogorov\u2019s thinking about the fundamental notions of complexity and randomness in both continuous and discrete worlds. It is only natural that, since then, we have come full circle: see, for instance, David Donoho\u2019s ingenious use of Shannon\u2019s rate-distortion theory to obtain sharp bounds on the Kolmogorov entropy of a ball in Sobolev space . \n The 2011 Shannon Lecture \n Shlomo Shamai , From Constrained Signaling to Network Interference Alignment via an Information-Estimation Perspective \nThis was a comprehensive and compelling story of the now-famous I-MMSE relation of Guo, Shamai, and Verd\u00fa. Its simplest form is as follows: if is a real-valued random variable with a finite second moment and is a standard Gaussian random variable independent of , then \n , \n where is the signal-to-noise ratio (SNR), the quantity on the left is the derivative of the mutual information, and the quantity on the right is one half of the minimum mean squared error (MMSE) of estimating based on the noisy observation . The remarkable thing is that this simple formula holds for regardless of the distribution of (under the second-moment condition and provided the SNR is not a parameter of this distribution). There are extensions of this basic relation to vector channels, continuous time, channels with feedback, and so on. \n Shamai\u2019s lecture was a whirlwind tour of applications of the I-MMSE relations to a wide array of problems in information theory (channels with input constraints; multiterminal settings, such as relay, broadcast or wiretap channels and cellular networks; interference alignment), estimation theory (relations between optimal causal and noncausal filtering), and coding (the occasional merits of using \u201cbad\u201d channel codes). The I-MMSE formula and its relatives show that there are intimate links between information theory (which was conceived by Shannon as a theory of digital communication and storage) and estimation theory (which tends to focus on the \u201canalog\u201d world). Just like with Arikan\u2019s channel polarization idea, I\u2019m sure we\u2019ve seen only the tiniest fraction of what one can do with the conceptual framework behind this deceptively simple formula."], "link": "http://infostructuralist.wordpress.com/2011/08/30/isit-2011-plenaries-the-shannon-lecture/", "bloglinks": {}, "links": {"http://www.itsoc.org/": 3, "http://en.wikipedia.org/": 1, "http://webee.ac.il/": 1, "http://www.purdue.edu/": 1, "http://www.cambridge.org/": 1, "http://iopscience.iop.org/": 1, "http://arxiv.org/": 3, "http://www.isit2011.org/": 4, "http://feeds.wordpress.com/": 1, "http://www-stat.stanford.edu/": 1, "http://ergodicity.net/": 1, "http://www.reznikova.net/": 1, "http://soihub.org/": 1, "http://boris.ryabko.net/": 1, "http://www.mathnet.ru/": 2, "http://projecteuclid.org/": 1}, "blogtitle": "The Information Structuralist"}]