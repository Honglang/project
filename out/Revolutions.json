[{"blogurl": "http://blog.revolutionanalytics.com\n", "blogroll": [], "title": "Revolutions"}, {"content": ["As the clean-up continues on the eastern seaboard, I wanted to follow up on Monday's post on tracking Hurricane Sandy with Open Data with a couple of other R-based data applications spawned by the storm. \n Josef Fruehwald \u00a0created an\u00a0 R script to tap into local weather sensors to keep track of air pressure, wind speed and rainfall near his home and for other affected locations. Check out the plunge in barometric pressure as the centre of the storm passed near Atlantic City on Monday night, and then over Philadelphia a couple of hours later: \n \n The greater scale of destruction in Atlantic City compared to Philadelphia is reflected in this comparison of wind speeds over the same period: \n \n Joseph used R to download the data from Weather Underground station records ( like this one ) in XML format, and then used the ggplot2 package to create the charts. You can find the complete R script at\u00a0RPubs:\u00a0 Analysis of a Philadelphia Weather Station Data during Hurricane Sandy . \n Twitter's Ed Chen took a different look at the effects of Sandy: how the storm knocked out power around the NY region, and the race by the utulities to restore power. Where did he find such data? Tweets from Twitter users, of course! Geolocated tweets (from smartphones, presumably)\u00a0along the lines of \"I've just lost power!\" , \"Still no power here!\" and \"The power is back on!\" gave Ed information he needed to use R to create an animation over time of the power going out and coming back on across NYC. \n \n The above is just a snapshot; visit Ed Chen's blog for an animated version , with a slider to see the changes during the night of the storm."], "link": "http://blog.revolutionanalytics.com/2012/10/more-data-apps-spawned-by-sandy.html", "bloglinks": {}, "links": {"http://blog.echen.me/": 1, "http://blog.revolutionanalytics.com/": 1, "http://api.wunderground.com/": 1, "http://rpubs.com/": 3, "http://revolution-computing.typepad.com/": 3, "http://www.revolutionanalytics.com/": 1}, "blogtitle": "Revolutions"}, {"content": ["Tim Gasper (Product Manager at Big Data platform Infochimps ) has an informative article at TechCrunch that provides an overview of five open-source technologies\u00a0 trending now for Big Data applications . They are: \n \n Storm and Kafka (for processing stream data) \n Drill and Dremel (for ad-hoc queries of big data) \n R (for data science with big data) \n Gremlin and Giraph (for graph analysis, e.g. of social networks) \n SAP HANA (for in-memory analytics). HANA isn't an open-source tool though, so perhaps the fifth slot should really go to ... \n Honourable mention D3 , for web-based visualization. \n \n Regarding R, Gasper says that it is \"incredibly powerful\", \"the new standard for statistics\", and that \"the R community is one of the most thrilling places to be in Big Data right now\".\u00a0He also mentions the RHadoop project (\"R work very well with Hadoop\") and the up-and-coming Julia project . \n You can read Gasper's complete overview of R and the other trending big-data technologies at link below. \n TechCrunch:\u00a0 Big Data Right Now: Five Trendy Open Source Technologies"], "link": "http://blog.revolutionanalytics.com/2012/10/r-among-techcrunchs-5-trendy-open-source-techs-for-big-data.html", "bloglinks": {}, "links": {"http://www.infochimps.com/": 1, "http://www.revolutionanalytics.com/": 1, "http://blog.revolutionanalytics.com/": 2, "http://techcrunch.com/": 2}, "blogtitle": "Revolutions"}, {"content": ["Hurricane Sandy is shaping up to be a major, and very dangerous, meteorological event for the US's East coast. Naturally, everyone is looking for the latest information and forecasts. Fortunately, the wealth of public meteorological data available on the open web, combined with real-time on-the-ground updates via social media, means that an ecosystem of on-line apps is now available providing all the up-to-date information you need. O'Reilly's Strata site has a good overview of the open-data Sandy-tracking apps available. \n For example, R user Bob Rudis has written an R script to grab up-to-date hurricane tracking data from Unisys Weather , and combine it with the forecast cone from Google's crisis map , to create the following combined picture of where Sandy's been, and where it's going: \n \n \n The forecast track shows Sandy heading north into Canada. It differers from the version on Bob's blog post because I ran the R script just now (at 16:38 PST) \u2014 that's the power of using live data! \n If you're anywhere near Sandy, stay informed and above all, stay safe. You can find information on how to keep safe at Ready.gov . \n Update Oct 30 : See also this\u00a0 Analysis of a Philadelphia Weather Station Data during Hurricane Sandy by\u00a0Josef Fruehwald, featuring charts of barametric pressure, wind speed and rose charts of wind direction created from live data using R. \n rud.is:\u00a0 Watch Sandy in \u201cR\u201d (Including Forecast Cone)"], "link": "http://blog.revolutionanalytics.com/2012/10/tracking-hurricane-sandy-with-open-data-and-r.html", "bloglinks": {}, "links": {"http://weather.unisys.com/": 1, "http://rud.is/": 2, "http://strata.oreilly.com/": 1, "http://revolution-computing.typepad.com/": 1, "http://rpubs.com/": 1, "http://www.ready.gov/": 1, "http://crisislanding.appspot.com/": 1}, "blogtitle": "Revolutions"}, {"content": ["It's almost All Hallow's Eve , and in the it's tradition in many places here in the States to decorate houses with \"spooky\" decorations. Some homeowners take the process to extremes \u2014 to both delight and chagrin of neighbours \u2014 such as with this light show in Leesburg, Virginia\u00a0set to the Korean pop hit Gangnam Style : \n \u00a0 \n The display includes more than\u00a08,500 lights, and required 250 separate lighting channels to support the computer-generated animation. Enjoy the Hallowe'en festivities in your own region this weekend!"], "link": "http://blog.revolutionanalytics.com/2012/10/because-its-friday-gangnam-halloween.html", "bloglinks": {}, "links": {"http://blog.revolutionanalytics.com/": 1, "http://en.wikipedia.org/": 1}, "blogtitle": "Revolutions"}, {"content": ["As promised , the source distribution for R 2.15.2 is now available for download from the master CRAN repository. (Binary distributions for Windows, MacOS and Linux will be available from the CRAN mirror network in the coming days.) This latest point-update \u2014 codenamed \"Trick or Treat\" \u2014 improves the performance of the R engine and adds a few minor but useful features. Detailed changes can be found in the NEWS \u00a0file, but highlights of the improvements include:\u00a0 \n \n New statistical analysis method: Multistratum MANOVA \n Hyman's method of constructing monotonic interpolation splines is now available. \n Improved support for Polish language users \n Functions in the parallel package (such as parLapply ) will make use of a default cluster if one is specified. \n Improved performance and reduced memory usage for some commonly-used functions including\u00a0array,\u00a0rep,\u00a0tabulate\u00a0and\u00a0hist \n Increased memory available for data on 64-bit systems (increased to 32Gb from 16Gb) \n Several minor bugfixes \n \n There is likely to be at least one further update to the 2.15.x series: a round-up of any further changes will probable be released as 2.15.3 shortly before R 2.16.0 is released, most likely around March 2013. \n r-announce mailing list:\u00a0 R 2.15.2 is released"], "link": "http://blog.revolutionanalytics.com/2012/10/r-2152-now-available.html", "bloglinks": {}, "links": {"https://stat.ethz.ch/": 2, "http://blog.revolutionanalytics.com/": 1, "http://www.inside-r.org/": 3, "http://cran.r-project.org/": 2}, "blogtitle": "Revolutions"}, {"content": ["At the\u00a0 Strata conference in New York today, Steve Yun (Principal Predictive Modeler at Allstate's Research and Planning Center) described the various ways he tackled the problem of fitting a generalized linear model to 150M records of insurance data. He evaluated several approaches: \n \n Proc GENMOD in SAS \n Installing a Hadoop cluster \n Using open-source R (both on the full data set, and on using sampling) \n Running the data through Revolution R Enterprise\u00a0 \n \n Steve described each of the approaches as follows. \n Approach 1 is current practice. (Allstate is a big user of SAS, but there's a growing contingent of R users.) Proc GENMOD takes around 5 hours to return results for a Poisson model with 150 million observations and 70 degrees of freedom. \"It's difficult to be productive on a tight schedule if it takes over 5 hours to fit one candidate models!\", Steve said. \n Approach 2: It was hoped that installing a Hadoop cluster and running the model there would improve performace. According to Steve, \"a lot of plumbing was required: this involved coding the matrix equations for iteratively-reweighted least squares as a map-reduce task\u00a0and manually coding the factor variables as indicator columns in the design matrix. Unfortunately, each iteration took abour 1.5 hours, with 5-10 iterations required to convergence. (Even then, there were problems with singularites in the design matrix.) \n Approach 3: Perhaps installing R on a server with lots of RAM would help. (Because open-source R runs in-memory, you need RAM in the order of several times the size of the data to make it work.) Alas, not even a 250Gb server was sufficient: even after waiting three days, the data couldn't even be loaded. Sampling the data down into 10 partitions was more successful, and allowed for the use of the glmnet package and L1 regularization to automate the variable selection process. But each glmnet fit on a partition still took over 30 minutes, and Steve said it would be difficult for managers to accept a process that involved sampling. \n Approach 4: Steve turned to Revolution Analytics' Joe Rickert to evaluate how long the same model would take using the big-data RevoScaleR package in Revolution R Enterprise. Joe loaded the data onto a 5-node cluster (20 cores total), and used the distributed rxGlm function, which was able to process the data in 5.7 minutes. Joe demonstrated this process live during the session. \n So in summary, here's how the four approaches fared: \n \n \n \n \n \n \n \n \n Approach \n \n \n Platform \n \n \n Time to fit \n \n \n \n \n 1: SAS \n \n \n 16-core\u00a0 Sun Server \n \n \n 5 hours \n \n \n \n \n 2: rmr / map-reduce \n \n \n 10-node (8 cores / node) Hadoop\n cluster \n \n \n > 10 hours \n \n \n \n \n 3: Open source R \n \n \n 250 GB Server \n \n \n Impossible (> 3 days) \n \n \n \n \n 4: RevoScaleR \n \n \n 5-node (4 cores / node) LSF\n cluster \n \n \n 5.7 minutes \n \n \n \n \n That's quite a difference! So what have we learned: \n \n SAS works, but is slow. \n It's possible to program the model in Hadoop, but it's even slower. \n The data is too big for open-source R, even on a very large server. \n Revolution R Enterprise gets the same results as SAS, but about 50x faster. \n \n Steve and Joe's slides and video of the presentation, Start Small Before Going Big ,\u00a0will be available on the Strata website in due course."], "link": "http://blog.revolutionanalytics.com/2012/10/allstate-big-data-glm.html", "bloglinks": {}, "links": {"http://cdn.oreillystatic.com/": 1, "http://strataconf.com/": 2, "http://www.revolutionanalytics.com/": 1, "http://www.r-project.org": 1, "http://featherfiles.aviary.com/": 1}, "blogtitle": "Revolutions"}, {"content": ["The O'Reilly Strata conferences are always great fun to attend, and this latest installment in New York City is no exception. This one is super-busy though; the conference has been sold out for weeks -- and not just marketing-sold-out, it's fire-department-sold out. It's non-stop conversations and presentations, and it's tough to move through the hallways in between. \n Nonetheless, I thought I'd pause for a couple of minutes and share some of the highlights for me so far. \n \n Ed Kohlwey and Stephanie Beben gave a three-hour tutorial on the RHadoop project , showing the packed room how to crunch big data. They shared how consulting firm Booz Allen Hamilton uses R and Hadoop for data exploration; to run many tasks in parallel; and to sort, sample and join data. They've also create a very handy VirtualBox VM including R, Hadoop, RHadoop and RStudio (along with demonstration script files) which I hope to be able to post a download link for soon. \n Stan Humphries from Zillow gave a presentation on how data and statistical analysis drives Zillow's home valuation service . One fascinating tidbit: while Zillow has long used R to fit their valuation model, until recently they recoded the model scoring algorithm in C++ for use on the production site. The process of re-implementing a new version of the model, validating it, and deploying it used to take 9 months. But now that they run R in production via the Amazon cloud, without the need to recode the model in another language, the deployment time for new valuation models is just four weeks. \n Mike Driscoll from Metamarkets shared the technology behind their data stack:\u00a0node.js and D3 for\u00a0visualization;\u00a0R and Scala for analytics; Druid as the data store; and Hadoop and Kafka for ETL. Druid is MetaMarket's home-grown high-performance, which they announced today is now available as open source software. \n In a similar vein, Cloudera announced the release of Impala , an open-source project two years in the making to bring high-performance real-time analytics to Hadoop. \n And there were even more announcements: Kaggle launched a partnership with EMC to give Greenplum users direct access to the roster of Kaggle data scientists competitors. \n \n It's been a great conference so far, and this is only day one! Looking forward to more great talks and conversations tomorrow."], "link": "http://blog.revolutionanalytics.com/2012/10/quick-notes-from-strata-nyc-2012.html", "bloglinks": {}, "links": {"http://strataconf.com/": 3, "http://www.wired.com/": 1, "http://www.cloudera.com/": 1, "http://metamarkets.com/": 1}, "blogtitle": "Revolutions"}, {"content": ["On Thursday next week (November 1), I'll be giving a new webinar on the topic of Big Data, Data Science and R. Titled \" The Rise of Data Science in the Age of Big Data Analytics: Why Data Distillation and Machine Learning Aren\u2019t Enough \", this is a provocative look at why data scientists cannot be replaced by technology, and why R is the ideal environment for building data science applications. Here's the abstract: \n \n The reason why Big Data is important is because we want to use it to make sense of our world. It\u2019s tempting to think there\u2019s some \u201cmagic bullet\u201d for analyzing big data, but simple \u201cdata distillation\u201d often isn\u2019t enough, and unsupervised machine-learning systems can be dangerous. (Like, bringing-down-the-entire-financial-system dangerous.) Data Science is the key to unlocking insight from Big Data: by combining computer science skills with statistical analysis and a deep understanding of the data and problem we can not only make better predictions, but also fill in gaps in our knowledge, and even find answers to questions we hadn\u2019t even thought of yet. In this talk, David will: \n \n Introduce the concept of Data Science, and give examples of where Data Science succeeds with Big Data \u2026 and where automated systems have failed. \n Describe the Data Scientists\u2019 Toolkit: the systems and technology components Data Scientists need to explore, analyze and create data apps from Big Data. \n Share some thoughts about the future of Big Data Analytics, and the diverging use cases for computing grids, data appliances, and Hadoop clusters \n Discuss the skills needed to succeed \n Talk about the technology stack that a data scientist needs to be effective with Big Data, and describe emerging trends in the use of various data platforms for analytics: specifically, Hadoop for data storage and data \u201crefinement\u201d; data appliances for performance and production, and computing grids for data exploration and model development. \n \n \n You can register for this free webinar at the Revolution Analytics website. \n Also, if you're attending the Strata / Hadoop World conference in New York this week, be sure to check out Thursday's talk by Steve Yun from Allstate Insurance and Joe Rickert from Revolution Analytics, which will include some real-world benchmarks of doing big-data predictive modeling with Hadoop and Revolution R Enterprise. \n \n Start Small Before Going Big \n The availability of Hadoop and other big data technologies has made it possible to build models with more data than statisticians of even a decade ago would have thought possible. However, the best practices for effectively using massive amounts of data in the construction and evaluation of statistical models are still being invented. As is the case with most difficult complex problems: \u201cIf you\u2019re not failing, you\u2019re not trying hard enough\u201d. The majority of ideas tried do not work. Best practices should include keeping failures small and inexpensive, quickly eliminating approaches that are not likely to work out, and keeping track these failures so they won\u2019t be repeated. Every development environment should encourage trying multiple approaches to problem solving. \n This talk presents a case study of statistical modeling in the insurance industry and examines the trade-offs between working with all of the data in a Hadoop cluster, dealing with complex programming, significant set-up times and a batch-like programming mentality, versus rapidly iterating through models on smaller data sets in a dynamic R environment at the possible expense of model accuracy. We will examine the benefits and shortcomings of both approaches and include model accuracy, job execution time and overall project time among the performance measures. Technologies examined will include programming a Hadoop cluster from R using the RHadoop interface and the RevoScaleR package from Revolution Analytics. \n \n You can find more details about this talk and the Hadoop World conference (of which Revolution Analytics is a proud sponsor) here ."], "link": "http://blog.revolutionanalytics.com/2012/10/two-talks-on-data-science-big-data-and-r.html", "bloglinks": {}, "links": {"http://strataconf.com/": 1, "http://www.revolutionanalytics.com/": 5}, "blogtitle": "Revolutions"}, {"content": ["There are new local R user groups \u00a0in eight (!) countries to announce this month: \n \n Sweden is host to the first R user group in Scandinavia. StockholmR has been holding meetings since September, and their next meeting on November 28 will be on\u00a0 Teaching R and Data visualization using R . \n In Taiwan , the Taipei-based\u00a0 Taiwan useR Group holds regular Monday meetings on machine learning and data mining with R. Their next meeting is on October 29 . \n The first R user group in Thailand, Chang-Mai R \u00a0has brings together R users in Chiang Mai province. You can also connect with other Thai R users via their Facebook page . \n The\u00a0 Marburg R Local User Group boosts a strong R presence in Germany , bringing the total number of R user groups in the country to eight. \n Luxembourg \u00a0also boasts a new local R user group. Join LuxR on MeetUp to help organize their first meeting.\u00a0 \n The\u00a0 Pozna\u0144 R Users \u00a0is a new group for R users in Poland . Visit the Thinking in R blog for news of upcoming meetings and slides and reviews from previous meetups. \n The brand-new\u00a0 New Delhi UseR Group \u00a0joins Bangalore as the second R user group in India . No meetings are scheduled yet, but you can join the group to help organize the first. \n Last but not least,\u00a0 Comunidad R Hispano \u00a0is an on-line forum for R users across Spain . Local chapters in Madrid and Barcelona \u00a0host meetings for local R users in those cities. \n \n Several of these groups have taken advantage of sponsorship from Revolution Analytics to get their R user group up and running. If you're thinking about starting an R user group in your city, or help organize an existing group, applications for R user group sponsorship for 2013 are now open. If you'd like to join an R user group in your area, you can find a directory at the link below. \n Revolutions:\u00a0 Local R User Group Directory"], "link": "http://blog.revolutionanalytics.com/2012/10/new-r-user-groups-worldwide.html", "bloglinks": {}, "links": {"http://blog.revolutionanalytics.com/": 2, "http://r-es.org/": 3, "http://www.revolutionanalytics.com/": 1, "http://thinking-in-r.blogspot.com/": 2, "http://chiangmair.wordpress.com/": 1, "http://erget.github.com/": 1, "http://www.meetup.com/": 6, "http://www.facebook.com/": 1}, "blogtitle": "Revolutions"}, {"content": ["The population of the world has been over 7 billion for about a year now. But those seven billion aren't distributed equally around the globe. 1.2 billion people \u2014 about \u00a0in India alone (despite it havingjust 2% of the world's land area). At the other end of the spectrum, the entire continent of Australia houses about 0.3% of Australia. \n So what would the world look like if we divided it into continents not by political or geographic boundaries, but by population? For the sake of argument, what if the Earth were divided into seven continents of a billion inhabitents each? Armed with MS paint and Wikipedia's table of country populations, Redittor delugetheory drew this map : \n \n \n Each color on the map represents 1 billion people. The continent of Africa, neatly, already has a total population of around a billion, as do North America and South America combined. (For some reason Greenland and Australia/NZ, with comparatively negligible populations, got thrown in Continent Green as well). For the rest of the world we need some judicious reassembling of the continents.\u00a0 \n While this map was assembled by hand, it would be interesting to see what map an algorithmic approach would create. It would be kind of like a 7-way\u00a0 knapsack problem , I guess. Unfortunately, you couldn't just use country populations as the basis, since China and India already have more than 1 billion inhabitants and need to be split apart. I guess you could arbitrarily break them up into 2 countries each, or perhaps work from the city population list (and distribute the remaining population in countries uniformly over their area). Any R programmers up to the task of automating this problem?"], "link": "http://blog.revolutionanalytics.com/2012/10/because-its-friday-7-billion-person-continents.html", "bloglinks": {}, "links": {"http://www.reddit.com/": 1, "http://www.scientificamerican.com/": 1, "http://en.wikipedia.org/": 1, "http://revolution-computing.typepad.com/": 1}, "blogtitle": "Revolutions"}]