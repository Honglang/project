[{"blogurl": "http://digitheadslabnotebook.blogspot.com\n", "blogroll": [], "title": "Digithead's Lab Notebook"}, {"content": ["& & && \n\n\n\n\n ...being a test of the Knitr document generation tool for R. \n\n library(glmnet)\n \n\n Let's give the glmnet package a little workout. We'll generate data for a bunch of features, some of which yield a response. Many of the features are unrelated to the response. Of course, we'll inject some noise into the data, too. \n\n generate.data <- function(n=1000, m=5, sig.features=1:5, noise.level=0.10) {\n\n # create bogus feature matrix\n features <- matrix(runif(n*m), nrow=n, ncol=m)\n rownames(features) <- sprintf(\"ex%04d\",seq(n))\n colnames(features) <- sprintf(\"feat%04d\",seq(m))\n\n # generate random model parameters\n intercept <- rnorm(1)\n coefs <- rnorm(length(sig.features))\n names(coefs) <- colnames(features)[sig.features]\n\n # create response data\n response <- features[,sig.features] %*% coefs + intercept \n     + rnorm(n, sd=noise.level)\n\n # return generated data\n list(n=n, m=m,\n  sig.features=sig.features,\n  noise.level=noise.level,\n  features=features,\n  params=list(intercept=intercept, coefs=coefs),\n  response=response)\n}\n \n\n A function to check correspondence between model parameters and fit \n\n compare.coefs <- function(data, fit) {\n coefs <- data$params$coefs\n intercept <- data$params$intercept\n merge(\n data.frame(feature.name=c('(Intercept)', names(coefs)),\n    param=c(`(Intercept)`=intercept, coefs)),\n subset(\n  data.frame(feature.name=rownames(coef(fit)),\n     coef=coef(fit)[,1]),\n  coef!=0),\n all=TRUE)\n}\n \n\n Make predicted vs actual plots \n\n plot.predicted.vs.actual <-\n function(data, predicted, actual, noise.level, label=NULL) {\n\n correlation.predicted.actual <- cor(predicted, actual)\n order.by.predicted <- order(predicted)\n\n ## create a plot of predicted vs actual\n plot(actual[order.by.predicted],\n  pch=21, col=\"#aaaaaaaa\", bg=\"#cc000030\",\n  ylab=\"response\", xlab=\"sample\")\n\n title(main=\"predicted vs. actual\", col.main=\"#666666\")\n\n lines(predicted[order.by.predicted], col='blue', lwd=2)\n\n legend(\"topleft\", pch=c(NA, 21), lwd=c(2,NA), \n   col=c(\"blue\", \"#aaaaaa\"),\n   pt.bg=c(NA,\"#cc000030\"),\n   legend=c('predicted','actual'))\n\n if (!is.null(label)) mtext(label, padj=-0.5)\n\n legend(\"bottomright\",\n   legend=c(sprintf('corr=%0.3f', correlation.predicted.actual),\n     if (abs(noise.level) >= 2.0)\n     sprintf('noise=%0.1fx', noise.level)\n     else\n     sprintf('noise=%0.0f%%', noise.level*100)\n   ))\n}\n \n\n Generate data \n\n n <- 1000\nnoise.level <- 0.50\ndata <- generate.data(n, m=20, sig.features=1:5, noise.level)\n \n\n Select training and testing sets \n\n train <- sample.int(n, n*0.85)\ntest <- setdiff(seq(n), train)\n \n\n Fit model using elastic net \n\n fit <- cv.glmnet(data$features[train,], \n     data$response[train, drop=FALSE],\n     alpha=0.7)\ncompare.coefs(data, fit)\n \n\n feature.name param  coef\n1 (Intercept) -2.2119 -2.32176\n2  feat0001 -1.4536 -1.23601\n3  feat0002 1.2987 1.12962\n4  feat0003 -0.6622 -0.56728\n5  feat0004 0.8445 0.70143\n6  feat0005 -2.4279 -2.25502\n7  feat0013  NA -0.04111\n \n\n Check our ability to model training data \n\n p <- predict(fit, data$features[train,])\nplot.predicted.vs.actual(data$features[train,], p, data$response[train,], noise.level, \"training\")\n \n\n \n\n Correlation with training data \n\n cor(p, data$response[train,])\n \n\n  [,1]\n1 0.8831\n \n\n Check our ability to predict test data \n\n p <- predict(fit, data$features[test,])\nplot.predicted.vs.actual(data$features[test,], p, data$response[test,], noise.level, \"testing\")\n \n\n \n\n Correlation with testing data \n\n cor(p, data$response[test,])\n \n\n  [,1]\n1 0.8799\n \n\n Created with the help of the excellent package Knitr . This document is also published at http://rpubs.com/cbare/2341 ."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/6933882890149185798/comments/default", "bloglinks": {}, "links": {"http://rpubs.com/": 1, "http://yihui.name/": 2, "http://cran.r-project.org/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["One hundred years from now, the role of science and technology will be about becoming part of nature rather than trying to control it. - Joichi Ito , MIT Media Lab director \n\n In complex systems, trade-offs are everywhere. Engineers designing technological artifacts carefully balance competing objectives. An economy allocates resources like land, labor and capital to one use or another. Even evolution faces trade-offs. \n\n \n Optimal bundles for three different incomes--2 normal goods. Price Theory , David D. Friedman \n\n Economists study trade-offs using tools like utility and indifference curves . The figure shows the optimal bundle consisting of two goods (apples and oranges, of course) for three different income levels. The curves represent trade-offs of equal utility; red lines represent income. They intersect at the points X, Y and Z, the points of highest utility achievable at each level of income. \n\n Pareto optimality \n \n At a macro level, millions of competing buyers and sellers make thousands of such trade-offs daily. Such multiobjective optimization problems often shake out in a way described by Pareto optimality . If an allocation of resources in an economy is such that no-one could be made better off without making someone else worse off, that allocation is said to be Pareto optimal. Competitive markets can be shown to deliver a Pareto optimal allocation of resources. Rather than a single optimal point, the Pareto optimal frontier defines a surface in high-dimensional space of best possible trade-offs. The notion of Pareto optimality turns out to be portable to other fields of study. \n\n \n  Shoval et al., 2012 \n\n Evolutionary Trade-Offs, Pareto Optimality, and the Geometry of Phenotype Space (Shoval et al., Science 2012), a short-but-awesome paper from Uri Alon's lab , applies the concept of Pareto optimality to evolutionary biology. The morphology of finch beaks, ant heads, and bat wings are all re-examined in the light of Pareto optimality. The idea applies at the molecular level as well. Microbial gene expression moves along an axis between competing priorities of growth and stress response. \n\n In each case, there is a trade-off among competing objectives - performance at various specialized tasks. One of the simplifying ideas in the paper is to work in terms of measurable task performance rather than the resulting contribution to fitness. Fitness, like the economic concept of utility, is hard to quantify and maybe not precisely knowable. \n\n For example, a beak might be optimized for a particular diet: cracking hard seeds, chewing soft seeds or plucking insects from their hiding places in the bark of a tree. The Pareto frontier defines solutions in which it is not possible to improve performance of one task without sacrificing performance of another. Selection favors phenotypes near the Pareto front. Local environment dictates the distribution of species and individuals along the front. \n\n Complex adaptive systems \n\n Biology, economics and, to some extent, engineering are all turning into the study of complex adaptive systems. Economics is the ecology of money. Biology is just the economics of the jungle. Here are a few big ideas and questions that generalize across disciples. \n\n \n \n Biology - adaptability \n Living systems are adaptive. They deal with extremes of environment, degrading gracefully with damage and keep on functioning in situations that stop mechanical systems dead. Economic systems, too, are adaptable. New information is constantly being factored into prices carrying signals that balance the needs of consumers with productive capacity. So far, engineered systems fall short in terms of adaptability.\n  \n\n \n Engineering - modularity \n A hallmark of engineered systems is modularity. In software, we struggle against the entropy of spaghetti-code. But, somehow modularity emerges spontaneously in living systems, their processes algorithmic in nature . Evolved systems are clearly different from engineered systems and are clearly far from fully modular, but, to a surprising degree, modularity is found in biology. What forces cause modularity to arise and what are the opposing forces?\n  \n\n \n Economics - distributed information \n Information flows through complex systems. Price is an emergent property integrate over millions of individuals each making hundreds of economic decisions daily. Living things, too, aggregate information in order to compute how best to allocate their budget of energy and scarce nutrients to achieve competing objectives. How do complex systems achieve balance, find and maintain stable states ( homeostasis ) and tend towards optimality? How do feedback mechanisms promote or disrupt stability?\n  \n \n\n These some of the fundamental features of complex systems: adaptivity, information flow and modularity. (I'm calling the book \u201c Darwin, Hayek, Knuth \u201d. Nice, huh?) Adaptability and robustness emerge from information flowing across interconnected networks often hierarchical in structure. \n\n\n \n  Marshall Clemens and the New England Complex Systems Institute \n\n After bouts of physics envy, biology and economics have arrived at roughly the same place. The toolkits used to study both are converging on the same set of statistical techniques and machine learning algorithms for computationally deriving models from big data. \n\n Both fields are wrestling with the same problems: complexity and uncertainty. With an emerging understanding of the properties of complex systems, we might see more robust engineered systems and the ability to re-engineer systems like economies and metabolisms or ecosystems or at least know the limits to which these systems can be engineered. Learning to engineer complex systems is the frontier of the 21st century. \n\n\n\n More \n \n Evolutionary Trade-Offs, Pareto Optimality, and the Geometry of Phenotype Space Shoval et al., Science 2012 \n Nature, an economic history by Geerat J. Vermeij \n Robert Frank of Cornell University and author of The Darwin Economy talks about Darwin's vision of the competitive process , and appeared on PBS in 2011 Was Charles Darwin the Father of Economics as Well? \n The Architecture of Complexity (Herbert Simon, 1962) \n Barabasi also wrote a paper titled The Architecture of Complexity \n Caldwell on Hayek \n Evolutionary Algorithms for Solving Multi-Objective Problems : section 1.2.3 Multiobjective optimization problems \n Yaneer Bar-Yam's New England Complex Systems Institute predicts food riots next year . \n To Understand Is to Perceive Patterns"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/7053105652064114678/comments/default", "bloglinks": {}, "links": {"http://www.ac.il/": 1, "http://www.pbs.org/": 1, "http://barabasilab.com/": 1, "http://www.technologyreview.com/": 2, "http://www.econtalk.org/": 2, "http://digitheadslabnotebook.blogspot.com/": 2, "http://www.sciencemag.org/": 4, "http://www.daviddfriedman.com/": 2, "http://necsi.edu/": 2, "http://1.blogspot.com/": 1, "http://web.mit.edu/": 2, "http://www.nature.com/": 1, "http://en.wikipedia.org/": 1, "http://moneyterms.co.uk/": 1, "http://press.princeton.edu/": 1, "http://4.blogspot.com/": 1, "http://www.mit.edu/": 1, "http://www.infoq.com/": 1, "http://vimeo.com/": 1, "http://www.idiagram.com/": 1, "http://www.springer.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["On-line education is blossoming in a virtuous cycle of innovation, threatening disruption to expensive traditional universities and opening access to higher learning for anyone with an internet connection and a curious mind. \n\n For developers, the on-line education boom means rich opportunities to learn, to create learning environments, and to analyze the data collected in the process of running massive open online courses - hacking education itself. \n\n Coursera, an early leader, signed up 17 more universities on top of the 12 that joined in July and are now offering 198 classes from 33 schools . \n\n Founders Daphne Koller & Andrew Ng published an article in Forbes, Log On and Learn: The Promise of Access in Online Education . Koller, whose \"Probabilistic Graphical Models\" strained my few remaining brain cells, spoke at TED on What we\u2019re learning from online education \n\n \n  An Old Scholar - Salomon Koninck \n\n A Seattle Times piece on the recent wave of educational startups featured enthusiastic comments from Ed Lazowska at UW and Vitalina Komashko at Sage Bionetworks . \n\n Why some of the best universities are giving away their courses Each has answers. But basically it comes down to these: To serve the greater good. To win a public-relations race. And, most especially, to enhance reputations. \n\n On-line education is a perfect complement for hotness that is data science . Not only is it a means for transferring trendy skills, but the data collected in the process should have amazing things to teach us about learning. \n\n Not all the action is in cyber-space, either. If you have a hungry mind that needs feeding, you can: \n \n study for coursera classes at meetups in 928 cities around the world \n study improvisation with Jazz vibraphonist Gary Burton (Your neighbors will love it!) \n go to Hacker School , Startup School or hacker dojo \n attend Biohacker Boot Camp at genspace \n learn about the Design of Computer Programs from the CTO of Google \n \n The only limit is your own bandwidth. That, and the tolerance of your spouse. \n\n The flashy technology is new but, the ideal of open access to knowledge has been around for a long time. The Seattle Times quotes Dave Cillay, executive director of WSU Online, \"We've had MOOCs and open learning resources for centuries. They're called libraries.\" \n\n Echoing Carnegie and his libraries , the Gates Foundation announced in June $9 million in grants for on-line secondary education , including a million to the MIT/Harvard venture edX . \n\n I remember poking my head into a cinder-block schoolhouse in a tiny village in Laos, back in my traveling days. There were 2 books; a book that appeared to be equal parts farming manual and government propaganda and another of Buddhist scripture. The potential to mitigate that kind of information-poverty in the remote corners of the world is one of the most exciting aspects of on-line education. \n\n Building on previous innovation is key to progress, especially in science and technology. Hacking education will help information flow faster, getting people to the frontier where they can start pushing the envelope and maybe make the world a slightly better place . That's why these are exciting times for those that love learning. \n\n More \n Still don't believe me that there's a lot going on? Here's more Ed-tech news : \n \n Scaling higher education \n Economists Tyler Cowen and Alex Tabarrok introduced Marginal Revolution University . The first course offered at MRUniversity is Development Economics. \n An NPR story on Coursera Online Education Grows Up, And For Now, It's Free \n \nCollege 2.0: Inside the Coursera Contract: How an Upstart Company Might Profit From Free Courses \n The School of Data is a collaboration between the Open Knowledge Foundation and P2PU (Peer to Peer University) \n University of the People: A Free Online University Tests the Waters \n CodeNow teaches coding to kids in DC. \"Coding is the new literacy. It gives individuals the power to innovate and create.\" \n Three Tips For a New Wave of Ed-Tech Entrepreneurs from the founder of ShowMe"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/1617582314679252734/comments/default", "bloglinks": {}, "links": {"https://www.edx.org/": 1, "http://seattletimes.com/": 2, "http://3.blogspot.com/": 1, "http://www.reddit.com/": 1, "http://www.forbes.com/": 1, "http://genspace.org/": 2, "http://marginalrevolution.com/": 1, "http://startupschool.org/": 1, "https://plus.google.com/": 1, "http://www.npr.org/": 1, "http://codenow.org": 1, "http://sagebase.org/": 1, "http://chronicle.com/": 1, "http://okfn.org/": 1, "http://www.showme.com/": 1, "http://www.nytimes.com/": 1, "http://www.udacity.com/": 1, "http://www.hackerdojo.com/": 1, "http://pragmaticpoliticaleconomy.blogspot.com/": 1, "http://www.gatesfoundation.org/": 1, "http://www.hackerschool.com/": 1, "http://blog.coursera.org/": 3, "http://www.washington.edu/": 1, "http://mruniversity.com/": 1, "http://www.arthermitage.org/": 1, "http://pandodaily.com/": 1, "http://digitheadslabnotebook.blogspot.com/": 2, "https://p2pu.org/": 1, "http://en.wikipedia.org/": 1, "http://web.mit.edu/": 1, "https://www.coursera.org/": 1, "http://schoolofdata.org/": 1, "http://www.meetup.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Classic developer tools have a timeless quality to them. Greybeards and college kids alike happily hack away in Emacs, vi, Bash and a host of other tools older than many of their users. It's surprisingly hard to improve upon these old tools. \n\n But, interest in designing new developer tools seems finally to be emerging. Rather than replacing powerful and expressive textual interfaces with pretty but limiting graphical interfaces, these new tools augment the command-line experience with immediate visual feedback. \n\n Light Table \n\n Chris Granger 's re-imagining of the IDE, Light Table , seeks to enhance the developer's \"ability to traverse abstraction\". Slick demo videos here and here show Light Table's good looks, but also its presentation of functions as the primary unit of abstraction, the ability to show values propagating through code and its handy access to documentation. \n\n During Granger's talk at StrangeLoop, one questioner raised the issue of whether dynamic languages lead to a different style of interaction between tool and developer. Static languages lend themselves to autocompletion and refactoring tools as in Eclipse, whereas dynamic languages emphasize the REPL and perhaps metaprogramming and DSLs. \n\n Though originally funded through KickStarter project , Light Table is closed-source, at least for now. It's scheduled for release next May with support for Clojure, Javascript, and Python. \n\n Neo4j \n\n Modern browsers provide capabilities like accelerated graphics, advanced page layout and process isolation, enabling environments like Neo4j's console demo that combine command shells with interactive graphics. \n\n \n \n\n As far as I can tell, that's just a demo. The real admin console for Neo4j doesn't suck either, but requires tabbing between command shell and graph visualization. \n\n \n \n\n Dev tools in browsers \n\n Browsers keep getting better. In-browser REPLs exists for numerous languages: clojure , haskell , javascript and others . These are typically targeted at language learners, as is Chas Emerick's Clojure Atlas , a visual and conceptual interface for navigating Clojure's documentation. But, I expect more advanced tools will find their way into the browser over time. Fogus's Himera project shows one way forward, delegating some of the heavy lifting to the server. \n\n Amazingly, R Studio Server puts full IDE into the browser, with the help of QTWebkit , GWT and some grand-master level wizardry. With Knitr integration, R Studio approaches live document capabilities. \n\n Sublime Text \n\n On the desktop, the Sublime Text editor picks up where TextMate left off. Sublime can use syntax files from TextMate, which means it already supports your favorite language, plus it's programmable in Python. \n\n \n \n\n Xiki \n\n I heard about Xiki (for executable wiki) from Tom Henderson , who owes me a book , by the way. An impressive demo video shows off Xiki's merger of advanced UI and command shell. \n\n Xiki integration with Sublime is progressing . \n\n Design principles for programming tools \n\n Bret Victor , known for his design work at Apple and lots of other cool things , has thought deeply about using technology to help people \u201clearn, understand and create\u201d. Victor followed up the inspiring talk Inventing on principle with another titled Visible Programming, a presentation of 5 design principles for programming tools. \n\n Tools should enable the programmer to: \n \n Read the vocabulary \n Follow the flow \n See the state \n Create by reacting \n Create by abstracting \n \n\n What this means is roughly this: Quick access to docs, often triggered by mouse-over, simplifies reading. Visibility into flow and state increases comprehension. \u201cDumping the parts bucket onto the floor&rduo encourages mixing and matching and provides visual prompting emphasizing recognition over recall. Abstractions are created by starting concrete and generalizing. \n\n Code is written for a dual audience: machine and human reader, requiring a difficult combination of precision and clarity. As the tools get smarter, the conversation between machine and programmer will get richer. The common thread here is supporting the programmer without imposing limitations, providing an experience more like a blank page and a box of sharp pencils than a menu of canned options, helping to create what Bret Victor calls, \u201cenvironments that function as an external imagination\u201d."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/2747520947212672447/comments/default", "bloglinks": {}, "links": {"http://repl.it/": 1, "http://3.blogspot.com/": 2, "http://www.sublimetext.com/": 2, "http://yihui.name/": 1, "http://www.chris-granger.com/": 2, "http://4.blogspot.com/": 1, "http://vimeo.com/": 3, "http://tryclj.com/": 1, "http://xiki.org/": 1, "http://doc.digia.com/": 1, "http://console.neo4j.org/": 1, "http://www.fastcodesign.com/": 1, "http://www.punkmathematics.com/": 1, "http://jsfiddle.net/": 1, "https://github.com/": 1, "http://www.kickstarter.com/": 2, "http://www.clojureatlas.com/": 1, "https://twitter.com/": 2, "http://tryhaskell.org/": 1, "http://digitheadslabnotebook.blogspot.com/": 3, "http://rstudio.org/": 1, "http://www.youtube.com/": 1, "http://2.blogspot.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Do you ever see strange lights in the sky? Do you wonder what really goes on in Area 51? Would you like to use your R hacking skills to get to the bottom of the whole UFO conspiracy? Of course, you would! \n\n UFO data from infochimps is the focus of a data munging exercise in Chapter 1 of Machine Learning for Hackers by Drew Conway and John Myles White, two social scientists with a penchant for statistical computing. \n\n The exercise starts with slightly messy data, proceeds through cleaning up some dates. I think I slightly improved on the code given in the book . Have a look ( gist:3775873 ) and see if you agree. \n\n \n \n\n Dividing the data up by state (for sightings in the US), I noticed something funny. My home state of Washington has a lot of UFO sightings. Normalizing by population, this becomes even more pronounced. \n\n I learned a neat trick from the chapter. The transform function helps to compute derived fields in a data.frame . I use transform to compute UFO sightings per capita, after merging in population data by state from the 2000 census. \n\n sightings.by.state <- transform (\n sightings.by.state,\n state=state, state.name=name,\n sightings=sightings,\n sightings.per.cap=sightings/pop)\n \n\n Creating the plot above, with a pile of ggplot code, we see that Washington state really is off the deep end when it comes to UFO sightings. Our northwest neighbors in Oregon come in second. I asked a couple fellow Washington residents what they thought. The first reasonably conjectured a relationship to the number of air bases. The second Washingtonian gave the explanation I favor: \"High kook density\". \n\n \n \n\n If you'd like to the data, it's from Chapter 1 of Machine Learning for Hackers . Data and code can be found in John Myles White's github repo ."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/2500129007474505036/comments/default", "bloglinks": {}, "links": {"http://www.infochimps.com/": 1, "https://github.com/": 2, "http://shop.oreilly.com/": 2, "http://2.blogspot.com/": 2, "http://docs.ggplot2.org/": 1, "https://gist.github.com/": 1, "http://stat.ethz.ch/": 3}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["\"Is there a package for obfuscating code in #rstats?\", someone asked. \" The S4 object system?! \" came the snarky reply. If you're smiling right now, you know that it wouldn't be funny if it weren't at least a little bit true. \n\n\n Options: S3, S4 or R5? \n\n There can be little doubt that object oriented programming in R is the cause of some confusion. We'll look at S4 classes more closely in a minute, but be warned that S4 classes are just one of at least three object systems available to the R programmer: \n\n \n S3 : simple and lightweight \n S4 : formal classes implemented by the methods package \n R5 : Reference classes \n \n\n It's not super clear when to use which, at least not to me. It seems to depend strongly on style and personal preference. The Bioconductor folks, for example, make heavy use of S4 classes. Google, on the other hand, advises to \" avoid S4 objects and methods when possible \". \n\n Here's the way it looks to me. S3 classes feel a bit like Javascript classes - easy, loose and informal. S4 classes are rigid, verbose and harder to understand. But, they offer a better separation between interface and implementation, along with some advanced features like multiple dispatch, validation and type coercion. Reference classes (aka R5) encapsulate mutable state and look more like familiar Java-style classes. They're new and pass-by-reference can violate expectations of R users. \n\n An S4 class example \n\n Now, let's return to S4 classes with a simple example. First, we define a class to represent people. \n\n\n\n # define an S4 class for people \n setClass (\n \"Person\" ,\n representation (name= \"character\" , age= \"numeric\" ),\n prototype(name= NA_character_ , age= NA_real_ )\n)\n \n\n A person has a name and an age, which default to NAs of their respective types - character string and numeric. For the sake of demonstrating polymorphism, let's define a couple subclasses. \n\n # define subclasses for different types of people \n setClass ( \"Musician\" ,\n representation (instrument= \"character\" ),\n contains= \"Person\" )\n\n setClass ( \"Programmer\" ,\n representation (language= \"character\" ),\n contains= \"Person\" )\n \n\n There's no reason not to write normal R functions that take S4 classes as arguments. Polymorphism is called for when a method has different implementations for different classes. In that case, we declare a generic method. \n\n # create a generic method called 'talent' that\n# dispatches on the type of object it's applied to \n setGeneric (\n \"talent\" ,\n function (object) {\n  standardGeneric ( \"talent\" )\n }\n)\n \n\n The following code implements two subtypes of person, each with a talent for something. \n\n setMethod (\n \"talent\" ,\n signature ( \"Programmer\" ),\n function (object) {\n paste( \"Codes in\" , \n  paste(object@language, collapse= \", \" ))\n }\n)\n\n setMethod (\n \"talent\" ,\n signature ( \"Musician\" ),\n function (object) {\n paste( \"Plays the\" ,\n  paste(object@instrument, collapse= \", \" ))\n }\n)\n \n\n Now, let's make some talented people. \n\n # create some talented people\ndonald <- new ( \"Programmer\" ,\n name= \"Donald Knuth\" ,\n age=74,\n language=c( \"MMIX\" ))\n\ncoltrane <- new ( \"Musician\" ,\n name= \"John Coltrane\" ,\n age=40,\n instrument=c( \"Tenor Sax\" , \"Alto Sax\" ))\n\nmiles <- new ( \"Musician\" ,\n name= \"Miles Dewey Davis\" ,\n instrument=c( \"Trumpet\" ))\n\nmonk <- new ( \"Musician\" ,\n name= \"Theloneous Sphere Monk\" ,\n instrument=c( \"Piano\" ))\n\ntalent(miles)\n[1] \"Plays the Trumpet\" \n\ntalent(donald)\n[1] \"Codes in MMIX\" \n\ntalent(coltrane)\n[1] \"Plays the Tenor Sax, Alto Sax\" \n \n\n Mutability \n\n One common stumbling block with S4 classes concerns changes in state. For instance, we might want to give our hard-working employees a raise. \n\n setClass ( \"Employee\" ,\n representation (boss= \"Person\" , salary= \"numeric\" ),\n contains = \"Person\" \n)\n\n setGeneric (\n \"raise\" ,\n function (object, percent=0) {\n  standardGeneric ( \"raise\" )\n }\n)\n\n setMethod (\n \"raise\" ,\n signature ( \"Employee\" ),\n function (object, percent=0) {\n object@salary <- object@salary * (1+percent)\n object\n }\n)\n \n\n True to it's functional heritage, R deals with immutable values. Changes in state happen by making new objects. The trick is to return the new object from the mutator methods and capture it on the way out. \n\n smithers <- new ( \"Employee\" ,\n name= \"Waylon Smithers\" ,\n boss=new( \"Person\" ,name= \"Mr. Burns\" ),\n salary=100000)\n\n # doesn't work?!?! \nraise(smithers, percent=15)\nsmithers@salary\n[1] 100000\n \n\n Setting a new salary creates a new value. Notice that we return the modified object from the raise function. Don't forget to catch it. \n\n \n # remember to reassign smithers to the new value \nsmithers <- raise(smithers, percent=15)\nsmithers@salary\n[1] 115000\n \n\n Multiple Inheritance \n\n Through the magic of multiple inheritance, the lowly Code Monkey is both a programmer and an employee. Just set the contains value to indicate its two parent classes. \n\n \n setClass ( \"Code Monkey\" ,\n contains=c( \"Programmer\" , \"Employee\" ))\n\n setMethod (\n \"talent\" ,\n signature ( \"Code Monkey\" ),\n function (object) {\n paste( \"Codes in\" ,\n  paste(object@language, collapse= \", \" ),\n   \"for\" , object@boss@name)\n }\n)\n\nchris <- new ( \"Code Monkey\" ,\n name= \"Chris\" ,\n age=29,\n boss=new( \"Person\" , name= \"The Man\" ),\n salary=2L,\n language=c( \"Java\" , \"R\" , \"Python\" , \"Clojure\" ))\n \n\n \ntalent(chris)\n[1] \"Codes in Java, R, Python, Clojure for The Man\" \n \n\n So, there you have it - encapsulation, polymorphism and inheritance in S4 classes. Complete code for this example is in gist:3670578 . \n\n OO in R resources \n\n It's lucky that there are loads of places to go to learn about S4 classes. \n\n \n\n First, look at Hadley Wickhams's devtools wiki which has a boatload of information for R package developers, in addition to info on S3 , S4 and reference classes . Also from Hadley is a slide deck on Object Oriented Programming . \n\n S4 Classes in 15 pages, more or less \n\n How S4 Methods Work by John Chambers \n\n The R-docs for the Methods package are comprehensive. See:\n \n as \n setClass \n setGeneric \n setMethod \n showMethods \n getMethod \n Class Definitions \n Tools for Managing Generic Functions \n validObject \n \n...wait.. just do help(package=\"methods\") \n\n Dirk Eddelbuettel and Romain Francois gave a Google TechTalk titled Integrating R with C++: Rcpp, RInside, and RProtobuf , covering integration between R and C++, but also has some good information on OO programming in R, particularly starting around the one hour mark (1:00). Romain Fancois' slide deck Object Oriented Design(s) in R is really good. \n\n\n Inside-R's references on the class systems:\n \n S3 classes \n S4 classes \n ReferenceClasses \n \n \n\n A section of Introduction to R covers Classes, generic functions and object orientation with S3 classes , \nThe classic bank-account example in the section on Scope \n\n R5 Reference classes \n\n Slides from Martin Morgan on Reference Classes \n\n R for Programmers , by Norman Matloff of UCSD, or buy Norm's book: The Art of R Programming \n\n A (Not So) Short Introduction to S4"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/238783046361967441/comments/default", "bloglinks": {}, "links": {"https://twitter.com/": 2, "https://github.com/": 7, "http://cran.r-project.org/": 3, "http://developer.r-project.org/": 1, "http://www.slideshare.net/": 1, "http://www.r-project.org/": 1, "http://courses.co.nz/": 1, "http://wiki.fhcrc.org/": 1, "http://romainfrancois.free.fr/": 1, "http://www.bioconductor.org/": 2, "http://www.inside-r.org/": 3, "http://blog.revolutionanalytics.com/": 2, "http://heather.ucdavis.edu/": 1, "http://userpage.fu-berlin.de/": 1, "http://nostarch.com/": 1, "http://google-styleguide.googlecode.com/": 1, "http://dirk.eddelbuettel.com/": 1, "https://gist.github.com/": 1, "http://stat.ethz.ch/": 39}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Clojure is a modern Lisp dialect that runs on the JVM. Since its release in 2007, it's become quite popular with elite hackers of a certain persuasion. Michael Fogus and Chris Houser 's Joy of Clojure is a thoroughly enjoyable bit of summer reading explaining the philosophy behind the language, showing how Clojure promotes simplicity and flexibility, deals with time and state, and generally brings fun back into programming. \n\n Concurrency is one of the motivating factors behind Clojure. Hickey's approach to sane management of state is anchored on the concept of immutability. The language encourages \"immutability by default\" with persistent collections - efficient implementations of list, vector, map and set that preserve historical versions of their state. When concurrency becomes necessary, Clojure provides some very modern constructs including STM (software transactional memory), agents similar to the actor model of Erlang, and atomic values. \n\n Aside from the parentheses, one big difference between Clojure and Java (as well as fellow JVM resident, Scala) is dynamic typing. Entities in Clojure are typically modeled in terms of the basic data structures already mentioned, especially maps and vectors. In practice, this ends up looking like JSON or Javascript objects, which are just bags of properties. The book includes a basic implementation of prototype based inheritance as an example. I've always thought that was more appropriate to dynamic languages than building class hierarchies (as in Python and Ruby). You might go so far as to say that JSON is to Clojure what lists are to classic Lisp. \n\n Clojure's interop with it's host platform gracefully bridges the conceptual gap between Java and Lisp , enabling Clojure to call into Java code making available the wealth of Java libraries. It's equally possible to expose Clojure APIs to Java code. Dynamically generating Java classes and proxies as well as creating and implementing interfaces on the fly leads to the belief in the Clojure community that \" Clojure does Java better than Java .\" In spite of the differences in semantics, Clojure feels like a natural layer on top of Java, raising the levels of abstraction and dynamism while easing many pain-points that every Java programmer stumbles over. \n\n The Clojure language is also branching out beyond the JVM. Clojurescript is a Clojure to javascript cross-compiler. Another offshoot targets Microsoft dot.net's CLR. \n\n Aside from persistent data structures and Java interop, Clojure comes with a bunch of functional goodness built in. Clojure's multimethods put control of method dispatch into the programmer's hands. Macros let you construct your own flow-of-control structures, as demonstrated by do-until and unless examples in the book. Illustrating how Lisp and it's macros can be used to construct little languages or DSLs , the book implements a mini-SQL interpreter. \n\n \n \n\n One especially nice aspect of the book is the putting it all together sections, which cover examples like: lazy quicksort, A*, and a builder for chess moves. These longer examples are still bite sized, making them more easily digested than the extended case studies found in some books. \n\n The Joy of Clojure is not an introductory book nor is it a language reference. It will appeal to the reader who already has some programming experience. It's a good idea to spend some time with the online tutorials first. Where the book is strongest is answering the why questions, getting you started learning how to think about programming in Clojure and showing you how Clojure changes the way you think. \n\n Companies using Clojure: \n \n Prismatic : a highly addictive social content recommendation engine who's backend pipeline and APIs are written mostly in Clojure. Prismatic's architecture is a great example of how to use machine learning and social graphs in a real application. \n Climate corporation : data driven weather insurance \n Relevance : a consultancy and home of Clojure/core . \n Lonocloud \n \n\n Popular Clojure projects and libraries: \n \n Leiningen a build tool and dependency management system \n Incanter a Clojure-based, R-like platform for statistical computing and graphics \n Chris Granger 's Light Table IDE \n Noir web framework \n Datomic : a distributed rethinking of the database \n core.logic : Prolog-like logic programming for Clojure \n \n\n Videos \n Luckily for those wanting to learn more without leaving their hammock, there are lots of videos about Clojure. A lot of clear thinking about software, whether you do it in Clojure or not, can be found in Rich Hickey's talks. \n \n Are we there yet? \n Hammock driven development \n Simplicity made easy \n Rich Hickey's talks on InfoQ \n Clojure on Blip.TV \n More Clojure Videos \n \n\n More Clojure Stuff \n \n Planet Clojure"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/7708037718983189746/comments/default", "bloglinks": {}, "links": {"http://www.chris-granger.com/": 2, "http://4.blogspot.com/": 1, "http://blog.fogus.me/": 1, "http://www.infoq.com/": 3, "http://webnoir.org/": 1, "http://www.datomic.com/": 1, "http://highscalability.com/blog": 1, "http://thinkrelevance.com/blog": 1, "http://www.climate.com/": 1, "http://lonocloud.com": 1, "http://www.paulgraham.com/": 1, "http://leiningen.org/": 1, "http://clojure.org/": 2, "http://clojure.com/": 1, "https://github.com/": 2, "http://joyofclojure.com/": 1, "http://www.washington.edu/": 1, "http://martinfowler.com/": 1, "http://getprismatic.com/": 1, "http://learn-clojure.com/": 2, "https://twitter.com/": 1, "http://blip.tv/": 3, "http://incanter.org/": 1, "http://digitheadslabnotebook.blogspot.com/": 3, "http://bc.tech.coop/blog": 1, "http://www.manning.com/": 1, "http://www.gotw.ca/": 1, "http://planet.clojure.in/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Astronomer Joshua Bloom gave a talk titled Python as Super Glue for the Modern Scientific Workflow at SciPy2012. Bloom teaches Python for Scientific Computing at Berkeley ( available as a podcast ). Bloom showcased Pythonic contributions to work on Supernova and machine-learning in astronomy . \n\n Python has solid support for data analysis and scientific computing, starting with Numpy for matrix manipulation and SciPy , which adds diff-eqs, optimization, statistics, etc. and matplotlib for graphics. I keep meaning to check out Pandas , scikit-learn , and Sage . \n \n IPython keeps getting more impressive and appears to be evolving in the direction of Mathematica's Live Notebooks. I have a long-standing thing for mixing prose and code . Fernando Perez\u2019s talk on IPython at SciPy2012 and a the longer version IPython in-depth are in my viewing queue. \n\n \n \n\n Part of the beauty of Python is it's breadth as a general purpose programming language. Libraries for everyday programming tasks like web application and database interaction are well developed in the Python world. There are good arguments in favor of DSLs for math and statistics, the approach embodied by R, Matlab, Mathematica and Julia. On the other hand, some may agree with John Cook, who puts it like this : \"I\u2019d rather do math in a general-purpose language than try to do general-purpose programming in a math language.\" \n\n More \n \n Data Mining and Machine-Learning in Time-Domain Discovery & Classification , Bloom and Richards \n BigMACC : Bloom's machine classified catalog of variable stars \n pydata.org \n NetworkX , a Python network library"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/3498022921708463958/comments/default", "bloglinks": {}, "links": {"http://www.bigmacc.info/": 1, "http://pydata.org/": 1, "http://www.johndcook.com/blog": 1, "http://networkx.lanl.gov/": 1, "http://matplotlib.sourceforge.net/": 1, "http://pandas.pydata.org/": 1, "http://astro.berkeley.edu/": 1, "http://digitheadslabnotebook.blogspot.com/": 2, "http://numpy.scipy.org/": 1, "http://itunes.apple.com/": 1, "http://ipython.org/": 1, "http://www.youtube.com/": 3, "http://arxiv.org/": 3, "http://www.scipy.org/": 1, "http://scikit-learn.github.com/": 1, "http://python.org": 1, "http://docs.scipy.org/": 1, "http://cse.berkeley.edu/": 1, "http://2.blogspot.com/": 1, "http://www.sagemath.org/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Rich Hickey , inventor of Clojure , gave a talk at last year's Strange Loop called Simple Made Easy . In it, he makes the case that Lispy languages and Clojure in particular provide tools for achieving modularity and simplicity. Simplicity Matters , Hickey's keynote at RailsConf in April of this year is a variant on the same themes. Here are my notes, either quoted verbatim or paraphrased. \n\n \u201cSimplicity is a prerequisite for reliability\u201d \n- Edsger Dijkstra \n\n \u201cSimplicity is the ultimate sophistication\u201d \n- Leonardo da Vinci \n\n\n Simplicity vs complexity \n\n \n \n\n\n\n Complexity comes from braiding together, interleaving or conflating multiple concepts. Complect (braid together) vs. compose (place together) \n\n \n \n\n Simplicity is defined as having one role - one task, one concept, one dimension. Composing simple components is the way we write robust software. A good designer splits things apart. When concepts are split apart, it becomes possible to vary them independently, use them in different contexts or to farm out their implementation. \n\n \n \n\n There are reasons why you have to get off of this list. But there is NO reason why you shouldn't start with it. \n\n\n Abstraction for simplicity \n\n Analyze programs by asking who, what, when, where, why and how. The more your software components can say, \"I don't know, I don't want to know\", the better. \n\n \n what : operations - specify interface and semantics, don't complect with how;\n  how can be somebody else's problem \n who : data or entities \n how : implementation, connected by polymorphism constructs \n when and where : use queues to make when and where less of a problem \n why : policy and rules of the application \n \n\n\n Data representation \n\n Data is really simple. There are not a lot of variation in the essential nature of data. There are maps, there are sets, there are linear sequential things... We create hundreds of thousands of variations that have nothing to do with the essence of the stuff and that make it hard to manipulate the essence of the stuff. We should just manipulate the essence of the stuff. It's not hard, it's simpler. Same goes for communications... (44:30) \n\n Information is simple, represent data as data, use maps and sets directly (56:30) \n\n (From the RailsConf talk:) Subsystems must have well defined boundaries and interfaces and take and return data as lists as maps (aka JSON). ReSTful interfaces are popular in the context of web services. Why not do the same within our programs? \n\n\n Choosing simplicity \n\n Simplicity is a choice, but it takes work. Simplicity requires vigilance, sensibility and care. Choose simple constructs. Simplicity enables change and is the source of true agility. Simplicity = opportunity. Go make simple things."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/5217198152914567418/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2, "https://thestrangeloop.com/": 1, "http://www.youtube.com/": 1, "http://2.blogspot.com/": 1, "http://www.infoq.com/": 1, "http://clojure.org/": 1, "https://github.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Comments are supposed to make digital media more engaging and interactive. Somewhere in the crowd of readers and viewers are ideas, insight and thoughtful criticism. However, the comments found on popular internet sites like YouTube or news sites generally inspire a loss of faith in humanity. From the comfort and pseudo-anonymity of thousands of living rooms comes a stream of abuse, wingnuttery and outright stupidity that overwhelms etiquette and common sense. \n\n Clay Shirky thinks Gawker is on to something with its attempts to surface quality comments . Gawker redesigned their comments section to serve the people reading the comments, rather than the people writing them, moving most comments off the main page of an article and enabling enhanced curation. \n\n Manual curation is labor intensive. I wonder whether a machine learning approach might be able to do a reasonable job of identifying good comments, or at least weeding out most of the inane ones. It's not really much different than spam filtering. That might make a fun little project."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/531575989777298154/comments/default", "bloglinks": {}, "links": {"http://www.niemanlab.org/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["In Andrew Ng's Machine Learning class , the first section demonstrates gradient descent by using it on a familiar problem, that of fitting a linear function to data. \n\n \n \n\n Let's start off, by generating some bogus data with known characteristics. Let's make y just a noisy version of x. Let's also add 3 to give the intercept term something to do. \n\n \n # generate random data in which y is a noisy function of x \nx <- runif ( 1000 , -5 , 5 )\ny <- x + rnorm ( 1000 ) + 3 \n\n # fit a linear model \nres <- lm ( y ~ x )\n print (res)\n\n Call:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)   x \n  2.9930  0.9981\n \n\n Fitting a linear model, we should get a slope of 1 and an intercept of 3. Sure enough, we get pretty close. Let's plot it and see how it looks. \n\n \n # plot the data and the model \n plot (x,y, col=rgb( 0.2 , 0.4 , 0.6 , 0.4 ), main= &#x27;Linear regression by gradient descent&#x27; )\n abline (res, col= &#x27;blue&#x27; )\n \n\n \n \n\n As a learning exercise, we'll do the same thing using gradient descent. As discussed previously , the main idea is to take the partial derivative of the cost function with respect to theta. That gradient, multiplied by a learning rate, becomes the update rule for the estimated values of the parameters. Iterate and things should converge nicely. \n\n \n # squared error cost function \ncost <- function (X, y, theta) {\n sum ( (X %*% theta - y)^ 2 ) / ( 2 * length (y))\n}\n\n # learning rate and iteration limit \nalpha <- 0.01 \nnum_iters <- 1000 \n\n # keep history \ncost_history <- double (num_iters)\ntheta_history <- list (num_iters)\n\n # initialize coefficients \ntheta <- matrix ( c ( 0 , 0 ), nrow= 2 )\n\n # add a column of 1&#x27;s for the intercept coefficient \nX <- cbind ( 1 , matrix (x))\n\n # gradient descent \nfor (i in 1 :num_iters) {\n error <- (X %*% theta - y)\n delta <- t (X) %*% error / length (y)\n theta <- theta - alpha * delta\n cost_history[i] <- cost(X, y, theta)\n theta_history[[i]] <- theta\n}\n\n print (theta)\n \n   [,1]\n[1,] 2.9928978\n[2,] 0.9981226\n \n\n As expected, theta winds up with the same values as lm returned. Let's do some more plotting: \n\n \n # plot data and converging fit \n plot (x,y, col=rgb( 0.2 , 0.4 , 0.6 , 0.4 ), main= &#x27;Linear regression by gradient descent&#x27; )\nfor (i in c( 1 , 3 , 6 , 10 , 14 , seq ( 20 ,num_iters,by= 10 ))) {\n abline (coef=theta_history[[i]], col=rgb( 0.8 , 0 , 0 , 0.3 ))\n}\n abline (coef=theta, col= &#x27;blue&#x27; )\n \n\n \n \n\n Taking a look at how quickly the cost decreases, I might have done with fewer iterations. \n\n \n plot (cost_history, type= &#x27;line&#x27; , col= &#x27;blue&#x27; , lwd= 2 , main= &#x27;Cost function&#x27; , ylab= &#x27;cost&#x27; , xlab= &#x27;Iterations&#x27; )\n \n\n \n \n\n That was easy enough. The next step is to look into some of the more advanced optimization methods available within R. I'll try to translate more of the Machine Learning class into R. I know others are doing that as well. \n\n \n The code for this post is available on GitHub as a gist"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/2151781197262525389/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 2, "http://digitheadslabnotebook.blogspot.com/": 5, "http://1.blogspot.com/": 1, "http://econometricsense.blogspot.com/": 1, "http://4.blogspot.com/": 1, "http://stat.ethz.ch/": 23, "https://gist.github.com/": 1, "http://cran.r-project.org/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Two Cytoscape engineers pointed me towards Plug and Play Macroscopes by Katy B\u00f6rner . The paper envisions highly flexible and configurable software tools for science through the mechanism of plugin architecture, and is highly worth reading if you're involved in building scientific software. \n\n Decision making in science, industry, and politics, as well as in daily life, requires that we make sense of data sets representing the structure and dynamics of complex systems. Analysis, navigation, and management of these continuously evolving data sets require a new kind of data-analysis and visualization tool we call a macroscope... \n\n \n \n\n A macroscope is a modular software framework enabling end users - biologists, physicists, social scientists - to assemble customized tools reusing and recombining data sources, algorithms and visualizations. These tools consist of reconfigurable bundles of software plug-ins, using the same OSGi framework as the Eclipse IDE. Once a component has been packaged as a plug-in, it can be shared and combined with other components in new and creative ways to make sense of complexity, synthesizing related elements, finding patterns and detecting outliers. B\u00f6rner sees these software tools as instruments on par with the microscope and the telescope. \n\n CIShell \n These concepts are implemented in a framework called Cyberinfrastructure Shell ( CIShell ), who's source is in the\n CIShell repo on GitHub . The core abstraction inside CIShell is that of an algorithm - something that can be executed, might throw exceptions, and returns some data. Data is some object that has a format and key/value metadata. \n\n \npublic interface Algorithm {\n public Data[] execute() throws AlgorithmExecutionException; \n}\n\npublic interface Data {\n public Dictionary getMetadata();\n public Object getData();\n public String getFormat();\n}\n \n\n Parenthetically, it's too bad there's no really universal abstraction for a function in Java... Callable , Runnable , Method , Function . In general, trying to wedge dynamic code into the highly static world of Java is not the most natural fit. \n\n \n \n\n I'm guessing that integrating a tool involves wrapping it's functionality in a set of algorithm implementations. \n\n The framework also features what looks to be support for dynamic GUI construction, a database abstraction with a slot for a named schema and support for scripting in Python. \n\n An upcoming version of Cytoscape is built on OSGi. Someone should write a genome browser along these same lines. \n\n \"To serve the needs of scientists the core architecture must empower non-programmers to plug, play, and share their algorithms and to design custom macroscopes and other tools. \" In my experience, scientists who are capable of designing workflows in visual tools are not afraid of learning enough R or Python to accomplish much the same thing. I'm not saying it's obvious that they should do that. Just that the trade-offs are worth considering. The real benefit comes from raising the level of abstraction, rather than replacing command-line code with point-and-click GUIs. \n\n Means of composition \n\n Plugin architecture isn't the only way to compose independently developed software tools. My lab's Gaggle framework links software through messaging. Service oriented architecture boils down to composition of web services, for example Taverna and MyGrid. GenePattern and Galaxy both fit into this space, although I'm not sure I can do a good job of characterizing them. If I understand correctly, both seem to use common file formats and conversions between them as the intermediary between programs. The classic means of composition are Unix pipes and filters - small programs loosely joined - and scripting. \n\n Visualization \n\n In a keynote on Visual Design Principles at VIZBI in March of this year, B\u00f6rner channeled inspiration from Yann Arthus-Bertrand's Home and Earth from Above, Drew Berry and Edward Tufte, and advised students of visualization to study human visual perception and cognitive processing first in order to \"design for the human system\". \n\n Her workflow combines data-centric steps similar to processes outlined by Jeffrey Heer and Hadley Wickham with a detailed breakdown of the elements of visualization . \n\n \n \n\n Katy B\u00f6rner directs the Cyberinfrastructure for Network Science Center at Indiana University. Not content to have written the Atlas of Science (MIT press 2010), she is currently hanging out in Amsterdam writing an Atlas of Knowledge. \n\n\n More \n \n After the VizBi talk, someone asked about shared data models and mentioned UIMA (Unstructured Information Management Architecture) as an example of a common type system for interfaces between components \n B\u00f6rner co-authored a chapter on Network Science \n Whole Brain Catalog \n Edward Tufte is doing some visualization workshops in Seattle and Portland this month \n VisWeek 2012 is in Seattle on October 14-19 \n Howard Hughes Medical Institute BioInteractive"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/1135641186358336251/comments/default", "bloglinks": {}, "links": {"http://uima.apache.org/": 1, "http://wiki.cytoscape.org/": 1, "http://3.blogspot.com/": 2, "http://visweek.org/": 1, "http://www.edwardtufte.com/": 1, "http://cishell.iu.edu/": 1, "http://digitheadslabnotebook.blogspot.com/": 4, "http://vizbi.org/": 1, "http://cns.iu.edu/": 1, "http://docs.oracle.com/": 3, "https://github.com/": 3, "http://google-collections.googlecode.com/": 1, "http://ella.indiana.edu/": 1, "http://ivl.iu.edu/": 2, "http://cacm.acm.org/": 1, "http://www.hhmi.org/": 1, "http://wholebraincatalog.org/": 1, "http://cishell.org/": 1, "http://gaggle.systemsbiology.net/": 2}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["The Shmulevich Lab scored a nice feature in the day 2 keynote at Google I/O. (See video starting at about 40:25 and again at 46:05.) Their work on the The Cancer Genome Atlas (TCGA) was part of the introduction of Google Compute Engine . \n\n Ilya and Hector are quoted in the case study, Cancer Investigators Use Google Compute \nEngine to Accelerate Life-Saving Research . \n\n \n \n\n The machine learning component is a random forest application called RF-ACE and the visualization is circvis . More Shmulevich lab code can be found at Code for Systems Biology ."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/7942933561191461535/comments/default", "bloglinks": {}, "links": {"http://static.googleusercontent.com/": 1, "http://shmulevich.systemsbiology.net/": 1, "http://www.youtube.com/": 1, "http://code.google.com/": 2, "http://cloud.google.com/": 1, "http://2.blogspot.com/": 1, "http://vis.systemsbiology.net/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["About a year ago, I ran a kooky idea past some colleagues. They gave it a big WTF, so I sat on it for a while. Not to be deterred, I still like the idea, so here it is. \n\n Workflows are key to bioinformatics. For example, an analysis of gene expression might go something like the following. Measure gene expression using arrays or RNA-seq. After a bit of normalization, cluster genes by correlated expression. Then compute functional enrichment on the clusters. This helps get at questions about how the cell reacts to some stimulus. Whether that's nutrients, toxins, pH, sunlight, whatever. Or, what's the difference between a sick cell and a healthy one. The answer is in terms of processes or pathways up or down regulated. \n\n \n\n You might implement our analysis of gene expression in R or Python. You could do it with point and click software like MeV and web tools like DAVID or workflow tools like Galaxy . You might use k-means, hierarchical clustering or something fancier. You might use GO terms to annotate gene function or KEGG pathways. There are lots of options, but the central idea is conserved. \n\n The notion of a data analysis workflow is something like the concept of a software design pattern , an idea that software engineers borrowed from architects. A design pattern is a general reusable template for how to solve a commonly occurring problem. Naming and documenting a design pattern makes sharing knowledge easier and lets you talk and think at a higher level of abstraction. \n\n \n\n Thinking of the workflow as an asset, it follows that they should be collected, documented and published. \"In bioinformatics, we are familiar with the idea of curated data as a prerequisite for data integration. We neglect, often to our cost, the curation and cataloguing of the processes that we use to integrate and analyse our data.\" ( Carol Goble et al. 2008) Both Taverna and Galaxy have mechanisms for doing this, albeit within the context of those tools. Maybe a better place to document workflows would be in journals. Philip Bourne , Editor in chief of PLOS Computational Biology says, \"I want the publisher of the future to be the guardian of these workflows create a better scholarly record.\" He's speaking here of scientific workflows in a more general context than just data analysis. \n\n Design patterns are documented in a specific format, detailing the scenarios in which that pattern applies, the intent behind it, its structure, implementation and consequences. Examples are usually given of real usages of the pattern along with discussion of alternatives or related patterns and its risks and pitfalls. For data analysis workflows, we'd probably want to discuss possible sources of error, required conditions and statistical properties. \n\n This is not to say we need formalism for the sake of formalism. It's tempting to get caught up with impractical methodological hoo-ha, focusing on process to the exclusion of real and practical goals. A workflow should be a tool in a researcher's toolbox, a pragmatic way to package a bite-sized piece of knowledge. To be avoided is the over-abstraction and questionable utility that some (i.e. me) see in BPEL, BPM, MDA and related (increasingly defunct) technology trends. \n\n Rather than design patterns, maybe a more biologist-friendly term is a protocol. Plus, there's already a long history of journals dedicated to lab protocols. Why not do the same for protocols for data analysis? \n\n\n More \n Like every idea, good or crackpot, people have thought of this one before me. \n\n \n What Do I Want from the Publisher of the Future? Bourne 2010 \n\n Combining ontologies and workflows to design formal protocols for biological laboratories , Maccagnan et al. 2010 \n\n Wikipedia's list of biological workflow management systems \n\n Data curation + process curation=data integration + science Carole Goble et al. 2008 \n\n myExperiment: a repository and social network for the sharing of bioinformatics workflows \n\n Pattern-Based Evaluation of Scientific Workflow Management Systems , Migliorini et al. 2011 \n\n Towards scientific workflow patterns , Yildiz et al. 2009 \n\n Characterization of scientific workflows Bharathi et al. 2008"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/731470015733238853/comments/default", "bloglinks": {}, "links": {"http://dl.acm.org/": 1, "http://eprints.edu.au/": 1, "http://www.tm4.org/": 1, "http://genomebiology.com/": 1, "http://www.sdsc.edu/": 1, "http://www.ac.uk/": 1, "http://bib.oxfordjournals.org/": 1, "http://www.myexperiment.org/": 1, "http://www.genome.jp/": 1, "http://david.ncifcrf.gov/": 1, "http://en.wikipedia.org/": 2, "http://ieeexplore.ieee.org/": 1, "http://4.blogspot.com/": 1, "http://galaxy.psu.edu/": 1, "http://nar.oxfordjournals.org/": 1, "http://www.aejournal.net/": 1, "https://main.psu.edu/": 1, "http://www.ploscompbiol.org/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Clojurist, technomancer and Leiningen creator, Phil Hagelberg does a nice job of dissecting \"two ways to compose a number of small programs into a coherent system\". Read the original in which three programming methods are compared . These are my notes, quoted mostly verbatim: \n\n The Unix way \n Consists of many small programs which communicate by sending text over pipes or using the occasional signal. Around this compelling simplicity and universality has grown a rich ecosystem of text-based processes with a long history of well-understood conventions. Anyone can tie into it with programs written in any language. But it's not well-suited for everything: sometimes the requirement of keeping each part of the system in its own process is too high a price to pay, and sometimes circumstances require a richer communication channel than just a stream of text. \n\n The Emacs way \n A small core written in a low-level language implements a higher-level language in which most of the rest of the program is implemented. Not only does the higher-level language ease the development of the trickier parts of the program, but it also makes it much easier to implement a good extension system since extensions are placed on even ground with the original program itself. \n\n The core Mozilla platform is implemented mostly in a gnarly mash of C++, but applications like Firefox and Conkeror are primarily written in JavaScript, as are extensions."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/5976305209804450259/comments/default", "bloglinks": {}, "links": {"http://technomancy.us/": 1, "http://2.blogspot.com/": 1, "https://github.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Hanchuan Peng of Janelia Farm spoke on bioimaging at ISB a couple weeks back. He's doing some very cool work mining microscopy images doing registration - aligning individual cells across images. They've created a 3D atlas of C. elegans which tracks every cell. The still pictures don't don't do it justice. Check out the movies . \n\n \n\n By localizing and registering neural fibers in 2,954 fly brains, Peng's group constructing this wiring diagram of the fly's 100,000 neurons. \n\n \n\n More \n \n A 3d digital atlas of C. elegans and its application to single-cell analyses , Nature Methods 2009 \n BrainAligner: 3d registration atlases of Drosophila brains , Nature Methods 2011"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/4824851940977127461/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://4.blogspot.com/": 1, "http://www.janelia.org/": 1, "http://systemsbiology.org": 1, "http://penglab.janelia.org/": 4}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["In the fall of last year, over 100,000 students signed up for Andrew Ng's Machine Learning Class and more than 12,000 of them completed the course. Sebastian Thrun and Peter Norvig taught Artificial Intelligence with similarly impressive numbers. \n\n I was one of the thousands in the Machine Learning class. I had so much fun with that, I also took Daphne Koller 's Probabilistic Graphical Models . That one was a quite a bit harder, covering some fairly advanced stuff at least for my few remaining brain cells. But, I finished! For the PGM class, 6702 took the first quiz and 1441 took the final - pretty good retention for such a ball-buster of a class. \n\n This spring, at least two new companies offering online courses were founded. Andrew Ng and Daphne Koller founded Coursera . Partnering with professors from Princeton, Penn, University of Michigan, and Berkeley, they've broadened their course catalog from a base in computer science to include classes in history, mathematics and even poetry. Sebastian Thrun, founder of Udacity , calls his approach University 2.0 and speaks of \"democratizing higher eduction\" and \"empowering students\" especially in the developing world where access to higher education is more limited. Almost three quarters of Coursera's students are outside the US, in countries like Brazil, Britain, India and Russia. \n\n The classes are surprisingly fun. The formula boils down to two key elements: \n \n short segments \n interaction \n \n\n In the mold of Khan academy, the lectures are broken into short segments of 10 to 15 minutes, which fit nicely into busy schedules. Short quizzes test the student's understanding. The courses have social aspect, as well. Online forums provide a place for questions and a sense of camaraderie while struggling through difficult concepts. Meetups and study groups have sprung up in several cities across the world. \n\n The programming exercises are where the real fun begins. Students write code that implements the crux of an operation, filling in the blanks in provided boilerplate code. Grading works a bit like unit testing. Progressing through the assignment by getting tests to pass gives gratifyingly immediate feedback. Completing an assignment results in working code for handwriting recognition, spam classification, image processing or recognizing an action from kinect position sensing data. \n\n Thomas Friedman says, Let the revolution come : \n\n Welcome to the college education revolution. Big breakthroughs happen when what is suddenly possible meets what is desperately necessary. \n\n With the cost of tuition rising, and public funding falling, the timing might be right for some disruptive innovation in higher education. And, the skills on offer are in high demand. One proposed business model is to offer classes for free and charge employers for access to the data. \n\n Refactoring the university classroom to function at internet scale meshes with the building momentum behind open access journals that some are calling an Academic Spring as well as with citizen science projects like Galaxy Zoo . \n\n Increasing openness in academics, in both teaching and research, can only reduce friction in the process of transferring technology from the lab to production and may help engage the public with science. Look for lots of interesting developments in the next few years, as technology knocks a new door into the ivory tower. \n\n More \n \n Harvard and M.I.T. Team Up to Offer Free Online Courses \n Inside Higher Ed asks Who Takes MOOCs? \n From Silicon Valley, A New Approach To Education from NPR \n Solving College With Big Data \n Can Free Online Courses Transform the Higher Education Industry? Published: June 20, 2012 in Knowledge@Wharton \n Opening Ivy League Universities To The Masses \n It'll be interesting to see if (when) they come out with a writing class, whether they can effectively create machine assisted methods for learning to write . \n Twilight of the Lecture Eric Mazur on new interactive teaching techniques. The trend toward \u201cactive learning\u201d may overthrow the style of teaching that has ruled universities for 600 years. \n \n\n Updates \n Coursera continues to generate lots of news, signing up 12 new universities , including the University of Washington (yay, UW!), attracting the attention of Bill Gate, and being described as The Single Most Important Experiment in Higher Education by the Atlantic and The Beginning of the End for Traditional Higher Education by Fortune and Reshaping Education on the Web by the NYT. \n\n \n The Trouble With Online Education \n The Revolution: Top Ten Disruptors of Education \n I'm stoked about Functional Programming in Scala taught by Martin Odersky \n The Simply Statistics folks on Why we are teaching massive open online courses (MOOCs) in R/statistics for Coursera \n The Siege of Academe : For years, Silicon Valley has failed to breach the walls of higher education with disruptive technology. But the tide of battle is changing. A report from the front lines."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/3260288939825149177/comments/default", "bloglinks": {}, "links": {"http://3.blogspot.com/": 1, "http://www.forbes.com/": 2, "https://class.coursera.org/": 1, "http://www.theatlantic.com/": 1, "http://www.co.uk/": 1, "http://norvig.com/": 1, "http://www.washingtonmonthly.com/": 1, "http://simplystatistics.org/": 2, "http://new.livestream.com/": 1, "http://www.nytimes.com/": 6, "http://mfeldstein.com/": 1, "http://harvardmagazine.com/": 1, "http://www.insidehighered.com/": 1, "http://blog.coursera.org/": 1, "http://www.npr.org/blog": 1, "http://robots.stanford.edu/": 1, "http://www.galaxyzoo.org/": 1, "https://www.ai-class.com/": 1, "http://knowledge.upenn.edu/": 1, "http://www.readwriteweb.com/": 1, "http://www.udacity.com/": 1, "http://www.huffingtonpost.com/": 1, "https://www.coursera.org/": 4}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["The benefits of working in academics are: \n\n \n Important and interesting work \n Opportunities for development and growth \n Freedom and fun \n \n\n Those paying attention will recognize Daniel Pink's elements\nof motivation - autonomy, mastery, and purpose. \n\n The downside? Having to explain to your spouse why you're not\nmaking as much money as so-and-so, who has basically the same\nskills as you. Or worse yet, why you make less than some\nother so-and-so who can barely tie his own shoes. \n\n That is, unless your spouse is in academics, too. In which case, God\nhelp you. \n\n Explaining this is not fun, but if the three elements above are\nin abundant supply, a fairly convincing case can be made. If\nthose factors start to run low... well, your spouse might be\nright. \n\n More \n \n Google Tech Lead Kevin Gibbs on how to Build a team they'll never leave: the 4 things that matter \n Valve Employee Handbook : how a game company runs without managers \n On Leaving Academia by University of New Mexico Computer Science Professor Terran Lane on moving to Google \n Tim O'Reilly says, Work on Stuff that Matters \n Attention, Intelligence, Creativity and Flow \n The Tiger Mom and A Clockwork Orange"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/569720624113217902/comments/default", "bloglinks": {}, "links": {"http://www.kevgibbs.com/": 1, "http://radar.oreilly.com/": 1, "http://digitheadslabnotebook.blogspot.com/": 2, "http://1.blogspot.com/": 1, "http://newcdn.flamehaus.com/": 1, "http://cs.unm.edu/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Clojure hacker Stuart Sierra wrote an insightful piece on the design philosophies of developer tools . His conclusions are paraphrased here: \n\n Git is an elegantly designed system of many small programs , linked by the shared repository data structure. \n\n Maven plugins, by contrast, are not designed to be composed and the APIs are underdocumented. With Maven, plugins fit into the common framework of the engine, which strikes me as maybe a more difficult proposition than many small programs working on a common data structure. \n\n Of the anarchic situation with Ruby package management (Gems, Bundler, and RVM) Sierra says, \u201clayers of indirection make debugging harder,\u201d and \u201cThe speed of development comes with its own cost.\u201d \n\n Principles \n\n \n Plan for integration \n Rigorously specify the boundaries and extension points of your system \n \n\n ...and I really like this idea: \n\n \n The filesystem is the universal integration point \n Fork/exec is the universal plugin architecture"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/3366279363084186179/comments/default", "bloglinks": {}, "links": {"http://stuartsierra.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Biological networks are often compared to random networks in terms of properties like degree distribution, clustering, robustness and over-representation of network motifs. In a talk this morning, Areejit Samal , a Postdoctoral Fellow in the Price lab at ISB , proposed a new null model based on Markov Chain Monte Carlo (MCMC) sampling to generate realistic benchmark ensembles for metabolic networks and gene regulatory networks. Based on this improved background model, the conclusion is that \u201cphenotypic constraints drive the architecture of biological networks\u201d, which is also the title of the talk. \n\n \n \n doi:info:doi/10.1371/journal.pone.0022295.g004 \n\n Applied to the metabolic model of E. coli , the sampling technique involves two steps. In the swap step, a reaction is removed from the network and a new reaction is drawn randomly from the KEGG database to replace it. The proposed new network is then tested by flux-balance analysis. If the new network is viable, it is accepted. If not, it is rejected. So, only networks that are biochemically plausible are sampled. \n\n The technique has implications for systems and synthetic biology as well as evolutionary theory. By approximating the space of all possible networks that might lead to a viable functioning organism, we can better understand what properties these networks have, and maybe better design new networks. By changing the acceptance criteria to enforce viability in a second environment, the algorithm nicely models the evolutionary emergence of modularity. \n\n Samal also showed the same general algorithm applied to the gene regulatory network controlling flowering in Arabidopsis . \n\n I especially appreciated seeing a really cool application of Markov Chain Monte Carlo sampling, a topic covered only a couple weeks back in the Probabilistic Graphical Models class . \n\n Links \n \n Randomizing Genome-Scale Metabolic Networks , Areejit Samal, Olivier C. Martin (2011) \n Environmental versatility promotes modularity in genome-scale metabolic networks (2011) \n Genotype networks in metabolic reaction spaces (2010) \n Neutralism and selectionism: a network-based reconciliation , A. Wagner (2008) \n The road to modularity , G. Wagner (2007)"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/2116651070828914053/comments/default", "bloglinks": {}, "links": {"http://www.biomedcentral.com/": 2, "http://3.blogspot.com/": 1, "https://www.systemsbiology.org/": 1, "http://digitheadslabnotebook.blogspot.com/": 1, "http://www.plosone.org/": 2, "http://www.nature.com/": 2, "https://sites.google.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Week 6 of the Daphne Koller's Probabilistic Graphical Models class looks at Decision Theory, which integrates the concept of utility functions from economics into our models. I'm getting flashbacks of Dr. Welsh's Econ 101 in Schwab Auditorium. \n\n In the Influence network below, we're making a decision about whether to found a company. Our success is determined by market conditions, which we can't observe. But, we can observe the results of a survey, which will give us some information on which to make our decision. So, how would you find the optimal decision rule? \n\n \n \n\n The factor fm represents the probability that market conditions (var 1) are good (3), fair (2) or poor (1). \n fm.var = 1\nfm.card = 3\nfm.val = [0.5 0.3 0.2]\nPrintFactor(fm)\n1 \n1 0.500000\n2 0.300000\n3 0.200000\n \n\n The factor fsm represents the results of a survey (var 2) given the underlying market conditions (var 1). \n fsm.var = [2 1]\nfsm.card = [3 3]\nfsm.val = [0.6 0.3 0.1 0.3 0.4 0.3 0.1 0.4 0.5]\nPrintFactor(fsm)\n2 1 \n1 1 0.600000\n2 1 0.300000\n3 1 0.100000\n1 2 0.300000\n2 2 0.400000\n3 2 0.300000\n1 3 0.100000\n2 3 0.400000\n3 3 0.500000\n \n\n The factor fufm represents the utility function, given the market conditions (var 1) and the decision to found (var 3) a company, yes (2) or no (1). \n fufm.var = [3 1]\nfufm.card = [2 3]\nfufm.val = [0 -7 0 5 0 20]\nPrintFactor(fufm)\n3 1 \n1 1 0.000000\n2 1 -7.000000\n1 2 0.000000\n2 2 5.000000\n1 3 0.000000\n2 3 20.000000\n \n\n Compute mu of F , S by taking the factor product of factors representing the market, survey and utility function, then summing out over all possible market conditions (var 1). \n PrintFactor(FactorMarginalization(FactorProduct(FactorProduct(fm, fsm), fufm), [1]))\n2 3 \n1 1 0.000000\n2 1 0.000000\n3 1 0.000000\n1 2 -1.250000\n2 2 1.150000\n3 2 2.100000\n \n\n We can build our decision rule by walking down all possible survey results (var 2) and selecting the decision to found or not which maximizes expected utility. See the red circles at the bottom of the diagram. \n\n We can make a decision factor, which depends on the survey (var 2). We'll fill in dummy values, for now. \n fd.var = [3 2]\nfd.card = [2 3]\nfd.val = ones(1,prod(fd.card))\n \n\n Now, we can use the code from the programming assignment to compute the optimal decision rule and the expected utility it yields. \n marketI.RandomFactors = [fm fsm]\nmarketI.UtilityFactors = [fufm]\nmarketI.DecisionFactors = [fd]\n[meu optdr] = OptimizeMEU(marketI)\nmeu = 3.2500\noptdr =\n scalar structure containing the fields:\n var =  3 2\n card =  2 3\n val =  1 0 0 1 0 1"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/2723305551573787225/comments/default", "bloglinks": {}, "links": {"http://digitheadslabnotebook.blogspot.com/": 1, "http://3.blogspot.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["When wrestling with a gnarly problem, it's interesting to compare notes with others who've faced the same dilemma. Having worked on an interoperability framework, a system called Gaggle , I had a feeling of familiarity when I came across this well-thought-out paper: \n\n The Scalable Adapter Design Pattern: Enabling Interoperability between Educational Software Tools, Harrer, Pinkwart, McLaren, Scheuer, IEEE TRANSACTIONS ON LEARNING TECHNOLOGIES, 2008 \n\n The paper describes a design pattern for getting heterogeneous software tools to interoperate with each other by exchanging data. Each application is augmented with an adapter that can interact with a shared representation. This hub-and-spokes model makes sense because it reduces the effort from writing n(n-1) adapters to connect all pairs of applications to writing one adapter per application. \n\n \n \n\n Scalability refers to the composite design pattern, implementing (what I would call) a more general concept, that of hierarchically structured data. If you've ever worked with large XML documents, calling them scalable might seem like an overstatement, but I see their point. XML nicely represents small objects, like events, as well as moderately sized data documents. The same can be said of JSON. \n\n Applications remain decoupled from the data structure, with an adapter mediating between the two. The adapter also provides events when the shared data structure is updated. A nice effect of the hierarchical data structure is that client applications can register their interest in receiving events at different levels of the tree structure. \n\n The Scalable Adapter pattern combines of well established patterns - Adapter, Composite and Publish-subscribe yielding a flexible way for arbitrary application data to be exchanged at different levels of granularity. \n\n The main difference between Scalable Adapter and Gaggle is that Gaggle focused on message passing rather than centrally stored data. The paper says, \"it is critical that both the syntax and semantics of the data represented in the composite data structure...\", but they don't really address what belongs in the \"Basic Element\" - the data to be shared. Gaggle solves this problem by explicitly defining a handful of universal data structures. Applications are free to implement their own data model, achieving interoperability by mapping (also in an adapter) their internal data structures onto the shared Gaggle data types. \n\n The Scalable Adapter paper breaks the problem down systematically in terms of design patterns, while Gaggle was motivated by the software engineering strategies of separation of concerns and parsimony, plus the linguistic concept of semantic flexibility. It's remarkable that the two systems worked out quite similarly, given the different domains they were built for."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/8580167526067895971/comments/default", "bloglinks": {}, "links": {"http://4.blogspot.com/": 1, "http://www.biomedcentral.com/": 1, "http://www.computer.org/": 1, "http://dl.acm.org/": 1, "http://gaggle.systemsbiology.net/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["Michael Kellen, Director of Technology at Sage Bionetworks, is trying to build a GitHub for science . It's called Synapse and Kellen described it in a talk at the Sage Bionetworks Commons Congress 2012 , this past weekend: 'Synapse' Pilot for Building an 'Information Commons' . \n\n To paraphrase a Kellen's intro: \n\n Science works better when people build off of each other's works. Every great advance is preceded by a multitude of smaller advances. It's no accident that the invention of the printing press and the emergence of the first scientific journals coincide with the many great scientific discoveries of the age of enlightenment. But scientific journals are stuck in a paradigm revolving around the printing press. In other domains, namely open source software, people are more radically reinventing systems for sharing information with each other. Github is a collaborative environment for the domain of software. Synapse aims to be a similar environment for medical and genomic research. \n\n The Synapse concept of a project packages together data and the code to process it. I tried to download the R script shown in the contents and couldn't, either because I'm a knucklehead or because Synapse is a work in progress. On the plus side, they give you a helpful cut-n-paste snippet of R code in the lower right corner to access the project through their R API. When this is fully implemented, it could provide a key piece of computing infrastructure for reproducible data-driven science. \n\n \n \n\n Sage intends to explore ways of connecting to traditional scientific journals. Picture figures that link to interactive visualizations or computational methods that link to code. I'm a big fan of the \" live document \" concept and it would be great to see journal articles evolve in that direction. \n\n An unintended consequence of NGS, Robert Gentleman points out, is that the data is too big for existing pipes. Any concept of a GitHub for science will have to incorporate processing biological data in the cloud . I could imagine a Synapse project containing data sets, code and a recipe for standing up an EC2 instance (or several). At a click, a scripted process would run, bootstrapping the machines, installing software and dependencies, running a processing pipeline, and visualizing the results in a browser. How would that be for reproducible science ? \n\n Michael Kellen's blog has a bunch of interesting stuff about why building a GitHub for biology is more fun than selling sheets . I bet it is."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/1108463292106675018/comments/default", "bloglinks": {}, "links": {"http://marciovm.com/": 1, "http://sciencereengineered.com/": 3, "http://digitheadslabnotebook.blogspot.com/": 1, "http://www.sciencemag.org/": 1, "http://1.blogspot.com/": 1, "http://www.nature.com/": 3, "http://fora.tv/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["\u201cTo be most effective, visual analytics tools must support the fluent and flexible use of visualizations at rates resonant with the pace of human thought.\u201d \n\n This comes from a recent paper by Jeffrey Heer of Stanford's Visualization Group and Ben Schneiderman titled Interactive Dynamics for Visual Analysis in ACM Queue, following up on another Queue article from 2010 by Jeffrey Heer, Michael Bostock and Vadim Ogievetsky, A Tour Through the Visualization Zoo . The Interactive Dynamics article categorizes aspects of visual analysis that deserve careful consideration when designing visual analysis tools. \n\n \n Taxonomy of interactive dynamics for visual analysis \n \n Data and View Specification\n \n Visualize \n Filter \n Sort \n Derive (values or models) \n \n\n View Manipulation\n \n Select \n Navigate (zoom, drill-down) \n Coordinate (linked views) \n Organize (multiple windows) \n \n\n Analysis Process & Provenance\n \n Record \n Annotate \n Share \n Guide \n \n \n \n\n Heer and collaborators created a series of software libraries for interactive visualization: prefuse , flare , Protovis and D3 . These frameworks are designed to \"represent data to facilitate reasoning\", \"flexibly construct representations\" and \"enable representational shifts\" or transformations. \n\n Protovis, which I've fooled around with a bit , is a functional domain specific language for data visualization. It's successor, D3 (Data Driven Documents), is an adaptation that increases performance and expressivity by making more direct use of the model (the DOM) inherent in the browser. \n\n Jeffrey Heer spoke recently at the University of Washington (video available) about his research, citing as an early influence a 1962 paper by John W. Tukey titled The Future of Data Analysis . \n\n In some ways, Heer nicely echoes themes from Hadley Wickam's talk on Engineering Data Analysis . \n\n Data analysis pipeline: \n Heer outlines an iterative process with these steps: Acquisition > Cleaning > Integration > Visualization > Modeling > Presentation > Dissemination. \n\n \n \n\n Also from the Interactive Dynamics paper: \n \u201cIn concert with data-management systems and statistical algorithms, analysis requires contextualized human judgments regarding the domain-specific significance of the clusters, trends, and outliers discovered in data.\u201d \n\n I've been a Jeffrey Heer fan-boy for some time, starting with his 2006 paper, Software Design Patterns for Information Visualization . Those interested in learning data science skills could do a lot worse than study Jeffrey Heer's work. \n\n More papers \n \n GraphPrism: Compact Visualization of Network Structure \n Orion , a tool for network modeling and analysis \n Wrangler , a declarative data transformation language \n Stanford Vis group papers \n Interactive High-Dimensional Data Visualization (Buja 1996)"], "link": "http://digitheadslabnotebook.blogspot.com/feeds/2980384648685364439/comments/default", "bloglinks": {}, "links": {"http://mbostock.github.com/": 2, "http://hci.stanford.edu/": 1, "http://digitheadslabnotebook.blogspot.com/": 4, "https://www.washington.edu/": 1, "http://1.blogspot.com/": 1, "http://queue.acm.org/": 2, "http://prefuse.org/": 1, "http://flare.prefuse.org/": 1, "http://vis.stanford.edu/": 6, "http://2.blogspot.com/": 1, "http://projecteuclid.org/": 1}, "blogtitle": "Digithead's Lab Notebook"}, {"content": ["I'm taking Daphne Koller's class on Probabilistic Graphical Models . Wish me luck - it looks tough. So, first off, why graphical models? \n\n The first chapter of the book lays out the rational. PGMS are a general framework that can be used to allow a computer to take the available information about an uncertain situation and reach conclusions, both about what might be true in the world and about how to act. Uncertainty arises because of limitations in our ability to observe the world, limitations in our ability to model it, and possibly even because of innate nondeterminism. We can only rarely (if ever) provide a deterministic specification of a complex system. Probabilistic models make this fact explicit, and therefore often provide a model which is more faithful to reality. \n\n More concretely, our knowledge about the system is encoded the graphical model which helps us exploit efficiencies arising from the structure of the system. \n\n \nDistributions over many variables can be expensive to represent naively. For example, a table of joint probabilities of n binary variables requires storing O(2 n ) foating-point numbers. The insight of the graphical modeling perspective is that a distribution over very many variables can often be represented as a product of local functions that each depend on a much smaller subset of variables. This factorization turns out to have a close connection to certain conditional independence relationships among the variables - both types of information being easily summarized by a graph. Indeed, this relationship between factorization, conditional independence, and graph structure comprises much of the power of the graphical modeling framework: the conditional independence viewpoint is most useful for designing models, and\nthe factorization viewpoint is most useful for designing inference algorithms\n \n An Introduction to Conditional Random Fields by Charles Sutton and Andrew McCallum \n\n Bayesian networks and Markov networks (aka Markov random fields) are the two basic models used in the class, the key difference being directed vs. undirected edges. In a Bayesian network, the edges are directed while they are undirected in a Markov network. \n\n How are these different kinds of graphic models related? Let's hope we'll find out. \n\n There's a study group meetup here at the Institute for Systems Biology (and maybe other locations) on Thursday night. Come join us, if you're in Seattle and you're doing the class. \n\n Supplemental reading \n \n Inference in Bayesian networks \n What is a hidden Markov model? , a primer by Sean Eddy \n Rabiner's Tutorial on Hidden Markov Models \n Octave Cheat Sheet . Like Andrew Ng's Machine learning class , the PGM class uses Octave, too."], "link": "http://digitheadslabnotebook.blogspot.com/feeds/245924670903290755/comments/default", "bloglinks": {}, "links": {"http://digitheadslabnotebook.blogspot.com/": 3, "http://arxiv.org/": 2, "https://class.coursera.org/": 2, "http://www.nature.com/": 2, "http://www.ucsb.edu/": 1, "http://mitpress.mit.edu/": 1, "http://www.meetup.com/": 1}, "blogtitle": "Digithead's Lab Notebook"}]